{
  "archeology-easy-10": {
    "m_q": {
      "target_metric": {
        "value": "highest average population across cities per country",
        "confidence": 0.3333333333333333,
        "votes": [
          "highest average population across cities per country",
          "country with the highest average population across its cities",
          "average population of cities within each country"
        ]
      },
      "filters": {
        "value": [
          "population not null",
          "population > 0",
          "exclude cities with null/missing population values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "population not null",
            "population > 0"
          ],
          [
            "exclude cities with null/missing population values"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "country"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "country"
          ],
          [
            "country"
          ],
          [
            "country"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the average population per city for each country?",
          "Which country has the maximum value of these averages?",
          "What is the average population of cities for each country?",
          "Which country has the maximum average city population?",
          "How many cities does each country have in the dataset?",
          "Are there cities with missing population data that should be excluded?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the average population per city for each country?",
            "Which country has the maximum value of these averages?"
          ],
          [
            "What is the average population of cities for each country?",
            "Which country has the maximum average city population?",
            "How many cities does each country have in the dataset?",
            "Are there cities with missing population data that should be excluded?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "worldcities.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "worldcities.csv"
          ],
          [
            "worldcities.csv"
          ],
          [
            "worldcities.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "population": "people",
          "lat": "degrees",
          "lng": "degrees"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "population": "people"
          },
          {
            "population": "persons",
            "lat": "degrees",
            "lng": "degrees"
          },
          {
            "population": "people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "population values are very large (millions), ensure appropriate scaling for calculations",
          "population values appear to be absolute counts, not scaled"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "population values are very large (millions), ensure appropriate scaling for calculations"
          ],
          [
            "population values appear to be absolute counts, not scaled"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "population >= 0",
          "iso2 and iso3 should be unique per country",
          "each city should have one country",
          "population values must be non-null for inclusion in average calculation",
          "population values should be non-negative",
          "each city should belong to exactly one country",
          "population must be a non-negative number"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "population >= 0",
            "iso2 and iso3 should be unique per country",
            "each city should have one country"
          ],
          [
            "population values must be non-null for inclusion in average calculation",
            "population values should be non-negative",
            "each city should belong to exactly one country"
          ],
          [
            "population must be a non-negative number"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "exclude cities with missing population data",
          "consider only cities with positive population",
          "filter out rows where population is null or NaN",
          "optionally filter out cities with population = 0 if they represent data quality issues"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "exclude cities with missing population data",
            "consider only cities with positive population"
          ],
          [
            "filter out rows where population is null or NaN",
            "optionally filter out cities with population = 0 if they represent data quality issues"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "check for outliers in population values",
          "verify distribution of city populations per country"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "check for outliers in population values",
            "verify distribution of city populations per country"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "country name",
          "average population value",
          "output should identify the country name",
          "output should include the calculated average population value",
          "optionally include the number of cities used in the calculation per country",
          "output should be a table with country and average population"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "country name",
            "average population value"
          ],
          [
            "output should identify the country name",
            "output should include the calculated average population value",
            "optionally include the number of cities used in the calculation per country"
          ],
          [
            "output should be a table with country and average population"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5944444444444446
  },
  "archeology-easy-11": {
    "m_q": {
      "target_metric": {
        "value": "Average latitude of capital cities, with only one capital per country (the one with largest population), rounded to 4 decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "Average latitude of capital cities, with only one capital per country (the one with largest population), rounded to 4 decimal places",
          "Average latitude of capital cities, counting only the highest population capital per country",
          "average latitude of capital cities, considering only the capital with the largest population per country"
        ]
      },
      "filters": {
        "value": [
          "capital column is not null/empty",
          "only include rows where capital column indicates a capital city",
          "for countries with multiple capitals, keep only the one with maximum population",
          "capital column is not null or empty",
          "capital column indicates a capital city (values like 'primary', 'admin', or other capital indicators)",
          "For countries with multiple capitals, select only the one with the largest population",
          "capital is not null",
          "population is not null"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "capital column is not null/empty",
            "only include rows where capital column indicates a capital city",
            "for countries with multiple capitals, keep only the one with maximum population"
          ],
          [
            "capital column is not null or empty",
            "capital column indicates a capital city (values like 'primary', 'admin', or other capital indicators)",
            "For countries with multiple capitals, select only the one with the largest population"
          ],
          [
            "capital is not null",
            "population is not null"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "country"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "country"
          ],
          [
            "country"
          ],
          [
            "country"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Identify which cities are capitals",
          "Group capitals by country",
          "For each country, select the capital with largest population",
          "Calculate average latitude of selected capitals",
          "Round result to 4 decimal places",
          "What values in the 'capital' column indicate a city is a capital?",
          "How to filter for capital cities only?",
          "How to identify countries with multiple capital cities?",
          "For each country, which capital has the largest population?",
          "What is the average of the latitude values for the selected capitals?",
          "How to round the result to 4 decimal places?",
          "Identify capital cities.",
          "For each country, find the capital city with the largest population.",
          "Calculate the average latitude of the selected capital cities."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify which cities are capitals",
            "Group capitals by country",
            "For each country, select the capital with largest population",
            "Calculate average latitude of selected capitals",
            "Round result to 4 decimal places"
          ],
          [
            "What values in the 'capital' column indicate a city is a capital?",
            "How to filter for capital cities only?",
            "How to identify countries with multiple capital cities?",
            "For each country, which capital has the largest population?",
            "What is the average of the latitude values for the selected capitals?",
            "How to round the result to 4 decimal places?"
          ],
          [
            "Identify capital cities.",
            "For each country, find the capital city with the largest population.",
            "Calculate the average latitude of the selected capital cities."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "worldcities.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "worldcities.csv"
          ],
          [
            "worldcities.csv"
          ],
          [
            "worldcities.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "lat": "degrees",
          "lng": "degrees",
          "population": "people"
        },
        "confidence": 1.0,
        "votes": [
          {
            "lat": "degrees",
            "lng": "degrees",
            "population": "people"
          },
          {
            "lat": "degrees (decimal degrees format)",
            "lng": "degrees (decimal degrees format)",
            "population": "number of people"
          },
          {
            "lat": "degrees",
            "lng": "degrees",
            "population": "people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Population values appear to be very large (millions)",
          "Latitude values range from -90 to 90 degrees"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Population values appear to be very large (millions)",
            "Latitude values range from -90 to 90 degrees"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each country should contribute exactly one capital city to the average",
          "Only cities marked as capitals should be considered",
          "Population values must be valid numbers for comparison",
          "Latitude values must be within valid range (-90 to 90)",
          "Only include rows where capital column is not empty (indicates the city is a capital)",
          "If a country has multiple capitals, select only the capital with the maximum population value",
          "Handle null or missing population values appropriately when determining the largest population",
          "Exclude cities where capital column is empty or does not indicate capital status",
          "If a country has multiple capital cities, only the capital with the largest population should be considered.",
          "The final result should be rounded to 4 decimal places."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Each country should contribute exactly one capital city to the average",
            "Only cities marked as capitals should be considered",
            "Population values must be valid numbers for comparison",
            "Latitude values must be within valid range (-90 to 90)"
          ],
          [
            "Only include rows where capital column is not empty (indicates the city is a capital)",
            "If a country has multiple capitals, select only the capital with the maximum population value",
            "Handle null or missing population values appropriately when determining the largest population",
            "Exclude cities where capital column is empty or does not indicate capital status"
          ],
          [
            "If a country has multiple capital cities, only the capital with the largest population should be considered.",
            "The final result should be rounded to 4 decimal places."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "capital != ''",
          "capital is not null",
          "population is not null",
          "lat is not null",
          "Filter: capital != '' AND capital IS NOT NULL",
          "Group by country and select row with max(population) per group",
          "capital_flag: derived from 'capital' column (primary, admin, minor, or null)",
          "largest_capital_population_per_country: derived by grouping by country and selecting the capital with the maximum population"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "capital != ''",
            "capital is not null",
            "population is not null",
            "lat is not null"
          ],
          [
            "Filter: capital != '' AND capital IS NOT NULL",
            "Group by country and select row with max(population) per group"
          ],
          [
            "capital_flag: derived from 'capital' column (primary, admin, minor, or null)",
            "largest_capital_population_per_country: derived by grouping by country and selecting the capital with the maximum population"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate capital designations within countries",
          "Verify latitude distribution is reasonable",
          "Check for outliers in population values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate capital designations within countries",
            "Verify latitude distribution is reasonable",
            "Check for outliers in population values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Result must be a single numeric value",
          "Value must be rounded to 4 decimal places",
          "Result should be a float representing average latitude",
          "Result must be rounded to 4 decimal places",
          "Output should be a single numeric value (scalar)",
          "The output should be a single numerical value representing the average latitude, rounded to 4 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Result must be a single numeric value",
            "Value must be rounded to 4 decimal places",
            "Result should be a float representing average latitude"
          ],
          [
            "Result must be rounded to 4 decimal places",
            "Output should be a single numeric value (scalar)"
          ],
          [
            "The output should be a single numerical value representing the average latitude, rounded to 4 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6333333333333334
  },
  "archeology-easy-3": {
    "m_q": {
      "target_metric": {
        "value": "average of Barrington Atlas Rank values for cities in Greece, handling 'or' values by taking the average of two numbers, rounded to 4 decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "average of Barrington Atlas Rank values for cities in Greece, handling 'or' values by taking the average of two numbers, rounded to 4 decimal places",
          "Average rank of cities in Greece",
          "average of the Barrington Atlas Rank for cities in Greece, where if the rank is a range (e.g., '3 or 4'), the average of the two numbers is used. The final result is rounded to 4 decimal places."
        ]
      },
      "filters": {
        "value": [
          "Country = 'Greece'",
          "Barrington Atlas Rank is not null or empty",
          "Country == 'Greece'",
          "Country is Greece"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Country = 'Greece'",
            "Barrington Atlas Rank is not null or empty"
          ],
          [
            "Country == 'Greece'"
          ],
          [
            "Country is Greece"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to interpret 'or' values in Barrington Atlas Rank column?",
          "Should missing Barrington Atlas Rank values be excluded?",
          "Are there duplicate cities that need deduplication?",
          "Which cities are located in Greece?",
          "What is the rank value for each Greek city?",
          "How to handle the 'or' instruction if rank values contain ranges?",
          "What is the average of all rank values?",
          "How to round the result to 4 decimal places?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How to interpret 'or' values in Barrington Atlas Rank column?",
            "Should missing Barrington Atlas Rank values be excluded?",
            "Are there duplicate cities that need deduplication?"
          ],
          [
            "Which cities are located in Greece?",
            "What is the rank value for each Greek city?",
            "How to handle the 'or' instruction if rank values contain ranges?",
            "What is the average of all rank values?",
            "How to round the result to 4 decimal places?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "roman_cities.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "roman_cities.csv"
          ],
          [
            "roman_cities.csv"
          ],
          [
            "roman_cities.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "barrington atlas rank": "ordinal rank (1=highest, 3=lowest based on sample)",
          "longitude (x)": "degrees",
          "latitude (y)": "degrees",
          "start date": "year BCE",
          "end date": "year BCE"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Barrington Atlas Rank": "ordinal rank (1=highest, 3=lowest based on sample)",
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "Start Date": "year BCE",
            "End Date": "year BCE"
          },
          {
            "Barrington Atlas Rank": "dimensionless_rank",
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "Start Date": "year",
            "End Date": "year"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Barrington Atlas Rank appears to be ordinal (1, 2, 3) but may contain 'or' values that need special handling",
          "Some End Date values are negative (BCE) while others are null"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Barrington Atlas Rank appears to be ordinal (1, 2, 3) but may contain 'or' values that need special handling",
            "Some End Date values are negative (BCE) while others are null"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "null"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Barrington Atlas Rank values should be between 1 and 3 based on sample data",
          "Country column must contain 'Greece' for relevant rows",
          "Primary Key should be unique",
          "Only include rows where Country == 'Greece'",
          "Exclude rows with missing rank values (NaN)",
          "If rank contains 'or' between two numbers, take average of those two numbers",
          "Round final result to exactly 4 decimal places",
          "The 'Barrington Atlas Rank' column may contain null values, which should be handled appropriately (e.g., excluded from the average calculation).",
          "The 'Barrington Atlas Rank' column may contain values like '3 or 4'. These need to be parsed and averaged."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Barrington Atlas Rank values should be between 1 and 3 based on sample data",
            "Country column must contain 'Greece' for relevant rows",
            "Primary Key should be unique"
          ],
          [
            "Only include rows where Country == 'Greece'",
            "Exclude rows with missing rank values (NaN)",
            "If rank contains 'or' between two numbers, take average of those two numbers",
            "Round final result to exactly 4 decimal places"
          ],
          [
            "The 'Barrington Atlas Rank' column may contain null values, which should be handled appropriately (e.g., excluded from the average calculation).",
            "The 'Barrington Atlas Rank' column may contain values like '3 or 4'. These need to be parsed and averaged."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Country = 'Greece'",
          "Filter rows where Barrington Atlas Rank is not null",
          "Handle 'or' values by splitting and averaging",
          "Filter: Country == 'Greece'",
          "Filter: Barrington Atlas Rank is not null",
          "Country == 'Greece'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Country = 'Greece'",
            "Filter rows where Barrington Atlas Rank is not null",
            "Handle 'or' values by splitting and averaging"
          ],
          [
            "Filter: Country == 'Greece'",
            "Filter: Barrington Atlas Rank is not null"
          ],
          [
            "Country == 'Greece'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of Barrington Atlas Rank values",
          "Verify no duplicate cities for same location",
          "Test handling of 'or' values in calculation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of Barrington Atlas Rank values",
            "Verify no duplicate cities for same location",
            "Test handling of 'or' values in calculation"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Result must be rounded to 4 decimal places",
          "Return single numeric value",
          "Output must be a single numeric value",
          "Handle any text-based rank values (e.g., '2 or 3') by averaging the numbers",
          "The final average rank should be rounded to 4 decimal places."
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Result must be rounded to 4 decimal places",
            "Return single numeric value"
          ],
          [
            "Output must be a single numeric value",
            "Result must be rounded to 4 decimal places",
            "Handle any text-based rank values (e.g., '2 or 3') by averaging the numbers"
          ],
          [
            "The final average rank should be rounded to 4 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5616666666666668
  },
  "archeology-easy-4": {
    "m_q": {
      "target_metric": {
        "value": "Calcium concentration in ppm when the ODP 967 Dust proxy is minimized, with ties broken by the minimum ODP 967 wet-dry index",
        "confidence": 0.3333333333333333,
        "votes": [
          "Calcium concentration in ppm when the ODP 967 Dust proxy is minimized, with ties broken by the minimum ODP 967 wet-dry index",
          "Amount of calcium in ppm when the dust proxy was the smallest, breaking ties by the minimum wet-dry index",
          "calcium concentration in ppm"
        ]
      },
      "filters": {
        "value": [
          "Find rows where 'ODP 967 Dust proxy' is at its minimum value",
          "If multiple rows have the same minimum dust proxy value, select the one with the minimum 'ODP 967 wet-dry index'",
          "Find row(s) with minimum 'ODP 967 Dust proxy' value",
          "If multiple rows with same minimum dust proxy, select row with minimum 'ODP 967 wet-dry index'",
          "ODP 967 Dust proxy is the minimum value",
          "Minimum wet-dry index is used to break ties when multiple rows have the minimum dust proxy value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Find rows where 'ODP 967 Dust proxy' is at its minimum value",
            "If multiple rows have the same minimum dust proxy value, select the one with the minimum 'ODP 967 wet-dry index'"
          ],
          [
            "Find row(s) with minimum 'ODP 967 Dust proxy' value",
            "If multiple rows with same minimum dust proxy, select row with minimum 'ODP 967 wet-dry index'"
          ],
          [
            "ODP 967 Dust proxy is the minimum value",
            "Minimum wet-dry index is used to break ties when multiple rows have the minimum dust proxy value"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the minimum value of 'ODP 967 Dust proxy'?",
          "Which rows have this minimum dust proxy value?",
          "Among those rows, which has the minimum 'ODP 967 wet-dry index'?",
          "What is the 'Ca' value for that row?",
          "What is the minimum value in 'ODP 967 Dust proxy' column?",
          "Which row(s) have this minimum dust proxy value?",
          "If multiple rows exist, which has the minimum 'ODP 967 wet-dry index'?",
          "What is the 'Ca' value for this row?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the minimum value of 'ODP 967 Dust proxy'?",
            "Which rows have this minimum dust proxy value?",
            "Among those rows, which has the minimum 'ODP 967 wet-dry index'?",
            "What is the 'Ca' value for that row?"
          ],
          [
            "What is the minimum value in 'ODP 967 Dust proxy' column?",
            "Which row(s) have this minimum dust proxy value?",
            "If multiple rows exist, which has the minimum 'ODP 967 wet-dry index'?",
            "What is the 'Ca' value for this row?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "climateMeasurements.xlsx"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "climateMeasurements.xlsx"
          ],
          [
            "climateMeasurements.xlsx"
          ],
          [
            "climateMeasurements.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple 'Age_ky' columns exist: 'Age_ky', 'Age_ky.1', 'Age_ky.2', 'Age_ky.3'",
          "Multiple unnamed columns: 'Unnamed: 20', 'Unnamed: 24', 'Unnamed: 27'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple 'Age_ky' columns exist: 'Age_ky', 'Age_ky.1', 'Age_ky.2', 'Age_ky.3'",
            "Multiple unnamed columns: 'Unnamed: 20', 'Unnamed: 24', 'Unnamed: 27'"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "ca": "ppm",
          "al": "ppm",
          "si": "ppm",
          "s": "ppm",
          "k": "ppm",
          "ti": "ppm",
          "mn": "ppm",
          "fe": "ppm",
          "rb": "ppm",
          "sr": "ppm",
          "zr": "ppm",
          "ba": "ppm",
          "depth below seafloor_m": "meters",
          "splice depth_m": "meters",
          "age_ky": "thousand years",
          "interval_cm": "centimeters",
          "odp 967 dust proxy": "dimensionless",
          "odp 967 wet-dry index": "dimensionless"
        },
        "confidence": 0.685185185185185,
        "votes": [
          {
            "Ca": "ppm",
            "Al": "ppm",
            "Si": "ppm",
            "S": "ppm",
            "K": "ppm",
            "Ti": "ppm",
            "Mn": "ppm",
            "Fe": "ppm",
            "Rb": "ppm",
            "Sr": "ppm",
            "Zr": "ppm",
            "Ba": "ppm",
            "Depth below seafloor_m": "meters",
            "Splice depth_m": "meters",
            "Age_ky": "thousand years",
            "Interval_cm": "centimeters"
          },
          {
            "Ca": "ppm",
            "Al": "ppm",
            "Si": "ppm",
            "S": "ppm",
            "K": "ppm",
            "Ti": "ppm",
            "Mn": "ppm",
            "Fe": "ppm",
            "Rb": "ppm",
            "Sr": "ppm",
            "Zr": "ppm",
            "Ba": "ppm",
            "Depth below seafloor_m": "meters",
            "Splice depth_m": "meters",
            "Age_ky": "kiloyears",
            "Interval_cm": "centimeters",
            "ODP 967 Dust proxy": "dimensionless",
            "ODP 967 wet-dry index": "dimensionless"
          },
          {
            "Ca": "ppm",
            "ODP 967 Dust proxy": "arbitrary units",
            "ODP 967 wet-dry index": "arbitrary units"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Element concentrations (Al, Si, etc.) appear to be in ppm but have very large values (e.g., 64819.9 for Al)",
          "Multiple age columns with potentially different scales or measurements"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Element concentrations (Al, Si, etc.) appear to be in ppm but have very large values (e.g., 64819.9 for Al)",
            "Multiple age columns with potentially different scales or measurements"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "nan"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 30.0,
        "confidence": 1.0,
        "votes": [
          30.0,
          30.0,
          30.0
        ]
      },
      "file_format": {
        "value": "xlsx",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "xlsx",
          "xlsx"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Answer must be rounded to 4 decimal places",
          "Must handle potential ties in dust proxy values",
          "Calcium value must be extracted from the 'Ca' column",
          "Must find global minimum of 'ODP 967 Dust proxy'",
          "Ties must be broken by minimum 'ODP 967 wet-dry index'",
          "Both 'ODP 967 Dust proxy' and 'ODP 967 wet-dry index' must be non-null",
          "Ca value must be non-null for selected row",
          "Find the minimum value of 'ODP 967 Dust proxy'",
          "If multiple rows have the minimum 'ODP 967 Dust proxy', find the minimum 'ODP 967 wet-dry index' among those rows",
          "Return the 'Ca' value for the row(s) that satisfy both conditions",
          "Round the final result to 4 decimal places"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Answer must be rounded to 4 decimal places",
            "Must handle potential ties in dust proxy values",
            "Calcium value must be extracted from the 'Ca' column"
          ],
          [
            "Must find global minimum of 'ODP 967 Dust proxy'",
            "Ties must be broken by minimum 'ODP 967 wet-dry index'",
            "Both 'ODP 967 Dust proxy' and 'ODP 967 wet-dry index' must be non-null",
            "Ca value must be non-null for selected row"
          ],
          [
            "Find the minimum value of 'ODP 967 Dust proxy'",
            "If multiple rows have the minimum 'ODP 967 Dust proxy', find the minimum 'ODP 967 wet-dry index' among those rows",
            "Return the 'Ca' value for the row(s) that satisfy both conditions",
            "Round the final result to 4 decimal places"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter for minimum 'ODP 967 Dust proxy'",
          "Secondary filter for minimum 'ODP 967 wet-dry index' among tied rows",
          "Filter out rows where 'ODP 967 Dust proxy' is null",
          "Filter out rows where 'ODP 967 wet-dry index' is null",
          "Filter out rows where 'Ca' is null"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter for minimum 'ODP 967 Dust proxy'",
            "Secondary filter for minimum 'ODP 967 wet-dry index' among tied rows"
          ],
          [
            "Filter out rows where 'ODP 967 Dust proxy' is null",
            "Filter out rows where 'ODP 967 wet-dry index' is null",
            "Filter out rows where 'Ca' is null"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Find minimum of 'ODP 967 Dust proxy' column",
          "Find minimum of 'ODP 967 wet-dry index' among rows with minimum dust proxy"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Find minimum of 'ODP 967 Dust proxy' column",
            "Find minimum of 'ODP 967 wet-dry index' among rows with minimum dust proxy"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numeric value with 4 decimal places",
          "Units: ppm",
          "Round final answer to 4 decimal places",
          "Return single numeric value",
          "Format as decimal number",
          "The output should be a single numerical value rounded to 4 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric value with 4 decimal places",
            "Units: ppm"
          ],
          [
            "Round final answer to 4 decimal places",
            "Return single numeric value",
            "Format as decimal number"
          ],
          [
            "The output should be a single numerical value rounded to 4 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5592592592592593
  },
  "archeology-easy-6": {
    "m_q": {
      "target_metric": {
        "value": "city with highest population",
        "confidence": 0.6666666666666666,
        "votes": [
          "city with highest population",
          "city with highest population",
          "Maximum population"
        ]
      },
      "filters": {
        "value": [
          "lat < 0 (southern hemisphere)",
          "lng < 0 (western hemisphere)",
          "Cities in the Southern Hemisphere (lat < 0)",
          "Cities in the Western Hemisphere (lng < 0)"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "lat < 0 (southern hemisphere)",
            "lng < 0 (western hemisphere)"
          ],
          [
            "lat < 0 (southern hemisphere)",
            "lng < 0 (western hemisphere)"
          ],
          [
            "Cities in the Southern Hemisphere (lat < 0)",
            "Cities in the Western Hemisphere (lng < 0)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "city",
          "country"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "city",
            "country"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which cities are in southern hemisphere?",
          "Which cities are in western hemisphere?",
          "What is the intersection of these two sets?",
          "Which city in the intersection has maximum population?",
          "Which cities are in the southern hemisphere (lat < 0)?",
          "Which cities are in the western hemisphere (lng < 0)?",
          "Which cities satisfy both hemisphere conditions?",
          "Among filtered cities, which has the maximum population?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which cities are in southern hemisphere?",
            "Which cities are in western hemisphere?",
            "What is the intersection of these two sets?",
            "Which city in the intersection has maximum population?"
          ],
          [
            "Which cities are in the southern hemisphere (lat < 0)?",
            "Which cities are in the western hemisphere (lng < 0)?",
            "Which cities satisfy both hemisphere conditions?",
            "Among filtered cities, which has the maximum population?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "worldcities.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "worldcities.csv"
          ],
          [
            "worldcities.csv"
          ],
          [
            "worldcities.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "lat": "degrees",
          "lng": "degrees",
          "population": "people"
        },
        "confidence": 1.0,
        "votes": [
          {
            "lat": "degrees",
            "lng": "degrees",
            "population": "people"
          },
          {
            "lat": "degrees (decimal)",
            "lng": "degrees (decimal)",
            "population": "count (persons)"
          },
          {
            "lat": "degrees",
            "lng": "degrees",
            "population": "people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "population values are very large (millions)",
          "population values appear to be actual counts, not scaled"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "population values are very large (millions)"
          ],
          [
            "population values appear to be actual counts, not scaled"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "population must be positive",
          "lat between -90 and 90",
          "lng between -180 and 180",
          "lat < 0 AND lng < 0",
          "population IS NOT NULL",
          "population > 0",
          "lat values should be between -90 and 90",
          "lng values should be between -180 and 180",
          "population values should be non-negative"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "population must be positive",
            "lat between -90 and 90",
            "lng between -180 and 180"
          ],
          [
            "lat < 0 AND lng < 0",
            "population IS NOT NULL",
            "population > 0"
          ],
          [
            "lat values should be between -90 and 90",
            "lng values should be between -180 and 180",
            "population values should be non-negative"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "lat < 0 AND lng < 0",
          "Filter to southern hemisphere: lat < 0",
          "Filter to western hemisphere: lng < 0",
          "Exclude rows with missing or zero population",
          "Southern Hemisphere: lat < 0",
          "Western Hemisphere: lng < 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "lat < 0 AND lng < 0"
          ],
          [
            "Filter to southern hemisphere: lat < 0",
            "Filter to western hemisphere: lng < 0",
            "Exclude rows with missing or zero population"
          ],
          [
            "Southern Hemisphere: lat < 0",
            "Western Hemisphere: lng < 0"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing population values",
          "Verify hemisphere logic is correctly applied"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing population values",
            "Verify hemisphere logic is correctly applied"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single city name with population",
          "Return single city name",
          "Include population value for verification",
          "Include lat and lng coordinates for verification",
          "City name",
          "Population"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single city name with population"
          ],
          [
            "Return single city name",
            "Include population value for verification",
            "Include lat and lng coordinates for verification"
          ],
          [
            "City name",
            "Population"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6138888888888892
  },
  "archeology-easy-8": {
    "m_q": {
      "target_metric": {
        "value": "count of unique sources in the 'Select Bibliography' column",
        "confidence": 0.3333333333333333,
        "votes": [
          "count of unique sources in the 'Select Bibliography' column",
          "count of unique sources",
          "Count of unique sources used in the 'Select Bibliography' column"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How are sources separated in the 'Select Bibliography' column?",
          "Are there any missing values in the 'Select Bibliography' column?",
          "Do source abbreviations need to be standardized (e.g., 'BNP' vs 'BNP;')?",
          "What column contains source information?",
          "How are multiple sources delimited within the source column?",
          "What constitutes a unique source (e.g., acronym vs full citation)?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How are sources separated in the 'Select Bibliography' column?",
            "Are there any missing values in the 'Select Bibliography' column?",
            "Do source abbreviations need to be standardized (e.g., 'BNP' vs 'BNP;')?"
          ],
          [
            "What column contains source information?",
            "How are multiple sources delimited within the source column?",
            "What constitutes a unique source (e.g., acronym vs full citation)?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "roman_cities.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "roman_cities.csv"
          ],
          [
            "roman_cities.csv"
          ],
          [
            "roman_cities.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "barrington atlas rank": "ordinal scale (1.0-3.0)",
          "start date": "year (negative values for BCE)",
          "end date": "year (negative values for BCE)",
          "longitude (x)": "decimal degrees",
          "latitude (y)": "decimal degrees"
        },
        "confidence": 0.5999999999999999,
        "votes": [
          {
            "Barrington Atlas Rank": "ordinal scale (1.0-3.0)",
            "Start Date": "year (negative values for BCE)",
            "End Date": "year (negative values for BCE)",
            "Longitude (X)": "decimal degrees",
            "Latitude (Y)": "decimal degrees"
          },
          {},
          {
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "Start Date": "year",
            "End Date": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Mixed date systems (BCE/CE) in Start Date and End Date columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Mixed date systems (BCE/CE) in Start Date and End Date columns"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Primary Key values should be unique",
          "Start Date should be <= End Date when both present",
          "Latitude values should be between -90 and 90",
          "Longitude values should be between -180 and 180",
          "Select Bibliography column must be parsed to extract individual sources",
          "Sources are delimited by semicolons and periods",
          "Uniqueness should count each distinct source identifier once across entire dataset",
          "The 'Select Bibliography' column may contain multiple sources separated by semicolons."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Primary Key values should be unique",
            "Start Date should be <= End Date when both present",
            "Latitude values should be between -90 and 90",
            "Longitude values should be between -180 and 180"
          ],
          [
            "Select Bibliography column must be parsed to extract individual sources",
            "Sources are delimited by semicolons and periods",
            "Uniqueness should count each distinct source identifier once across entire dataset"
          ],
          [
            "The 'Select Bibliography' column may contain multiple sources separated by semicolons."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows with missing 'Select Bibliography' values",
          "Consider only rows with non-empty 'Select Bibliography' for source counting"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows with missing 'Select Bibliography' values",
            "Consider only rows with non-empty 'Select Bibliography' for source counting"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Primary Key values",
          "Validate coordinate ranges",
          "Check date consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Primary Key values",
            "Validate coordinate ranges",
            "Check date consistency"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Integer count of unique sources",
          "List of unique sources for verification",
          "Return single integer representing count of unique sources"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Integer count of unique sources",
            "List of unique sources for verification"
          ],
          [
            "Return single integer representing count of unique sources"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5466666666666667
  },
  "archeology-hard-1": {
    "m_q": {
      "target_metric": {
        "value": "Average Potassium (K) in ppm from the first and last time the study recorded people in the Maltese area, with linear interpolation between samples, rounded to 4 decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "Average Potassium (K) in ppm from the first and last time the study recorded people in the Maltese area, with linear interpolation between samples, rounded to 4 decimal places",
          "Average Potassium in ppm from the first and last time the study recorded people in the Maltese area, with linear interpolation between samples",
          "Average Potassium (ppm) from the first and last time the study recorded people in the Maltese area, linearly interpolated between samples."
        ]
      },
      "filters": {
        "value": [
          "Region = 'Maltese area' or Site contains 'Malta' or similar Maltese geographic identifiers",
          "Potassium (K) values are not null",
          "First and last chronological records for the Maltese area",
          "Filter radiocarbon_database_regional for Region == 'Maltese' or 'Malta'",
          "Identify first and last time periods (based on calibrated dates)",
          "Filter climateMeasurements for Age_ky corresponding to those time periods",
          "Data from the 'climateMeasurements.xlsx' file",
          "Data is limited to the Maltese area (Site starts with 'ODP 967')",
          "First and last recorded time in 'Age_ky' column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Region = 'Maltese area' or Site contains 'Malta' or similar Maltese geographic identifiers",
            "Potassium (K) values are not null",
            "First and last chronological records for the Maltese area"
          ],
          [
            "Filter radiocarbon_database_regional for Region == 'Maltese' or 'Malta'",
            "Identify first and last time periods (based on calibrated dates)",
            "Filter climateMeasurements for Age_ky corresponding to those time periods"
          ],
          [
            "Data from the 'climateMeasurements.xlsx' file",
            "Data is limited to the Maltese area (Site starts with 'ODP 967')",
            "First and last recorded time in 'Age_ky' column"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Region",
          "Site",
          "Age_ky (or equivalent age columns)",
          "Time period (first vs last)",
          "Maltese region"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Region",
            "Site",
            "Age_ky (or equivalent age columns)"
          ],
          [
            "Time period (first vs last)",
            "Maltese region"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What constitutes the 'Maltese area' in the radiocarbon database?",
          "What are the earliest and latest age records for Maltese sites?",
          "How to identify corresponding Potassium measurements for those time periods?",
          "How to perform linear interpolation of Potassium values between available measurements?",
          "Which rows in radiocarbon_database_regional correspond to the Maltese area?",
          "What are the first and last dates when people were recorded in the Maltese area?",
          "What Age_ky values in climateMeasurements correspond to these first and last dates?",
          "What is the Potassium (K column) value at the first date (with linear interpolation if needed)?",
          "What is the Potassium (K column) value at the last date (with linear interpolation if needed)?",
          "What is the average of these two Potassium values?",
          "How to convert K values to ppm if not already in ppm?",
          "Identify the first and last 'Age_ky' values where 'Site' starts with 'ODP 967'.",
          "For the first and last 'Age_ky' values, find the corresponding 'K' values.",
          "If the first and last 'Age_ky' values are not directly available in the dataset, perform linear interpolation to estimate the 'K' values at those specific 'Age_ky' values.",
          "Calculate the average of the interpolated 'K' values."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "What constitutes the 'Maltese area' in the radiocarbon database?",
            "What are the earliest and latest age records for Maltese sites?",
            "How to identify corresponding Potassium measurements for those time periods?",
            "How to perform linear interpolation of Potassium values between available measurements?"
          ],
          [
            "Which rows in radiocarbon_database_regional correspond to the Maltese area?",
            "What are the first and last dates when people were recorded in the Maltese area?",
            "What Age_ky values in climateMeasurements correspond to these first and last dates?",
            "What is the Potassium (K column) value at the first date (with linear interpolation if needed)?",
            "What is the Potassium (K column) value at the last date (with linear interpolation if needed)?",
            "What is the average of these two Potassium values?",
            "How to convert K values to ppm if not already in ppm?"
          ],
          [
            "Identify the first and last 'Age_ky' values where 'Site' starts with 'ODP 967'.",
            "For the first and last 'Age_ky' values, find the corresponding 'K' values.",
            "If the first and last 'Age_ky' values are not directly available in the dataset, perform linear interpolation to estimate the 'K' values at those specific 'Age_ky' values.",
            "Calculate the average of the interpolated 'K' values."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "climateMeasurements.xlsx",
          "radiocarbon_database_regional.xlsx"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "climateMeasurements.xlsx",
            "radiocarbon_database_regional.xlsx"
          ],
          [
            "radiocarbon_database_regional.xlsx",
            "climateMeasurements.xlsx"
          ],
          [
            "climateMeasurements.xlsx",
            "radiocarbon_database_regional.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple age columns in climateMeasurements.xlsx (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3) with unclear relationships",
          "Different time units: radiocarbon dates in years BP vs. Age_ky in thousands of years",
          "Site naming conventions may differ between files",
          "radiocarbon_database_regional uses Cal. BC dates while climateMeasurements uses Age_ky (thousands of years)",
          "Need to convert between BC dates and ky (Age_ky)",
          "Multiple Age_ky columns in climateMeasurements (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple age columns in climateMeasurements.xlsx (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3) with unclear relationships",
            "Different time units: radiocarbon dates in years BP vs. Age_ky in thousands of years",
            "Site naming conventions may differ between files"
          ],
          [
            "radiocarbon_database_regional uses Cal. BC dates while climateMeasurements uses Age_ky (thousands of years)",
            "Need to convert between BC dates and ky (Age_ky)",
            "Multiple Age_ky columns in climateMeasurements (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3)"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "k": "ppm (parts per million)",
          "al": "ppm",
          "si": "ppm",
          "s": "ppm",
          "ca": "ppm",
          "ti": "ppm",
          "mn": "ppm",
          "fe": "ppm",
          "rb": "ppm",
          "sr": "ppm",
          "zr": "ppm",
          "ba": "ppm",
          "age_ky": "thousand years",
          "date": "radiocarbon years BP",
          "error": "radiocarbon years",
          "\u03b4r": "years",
          "\u03b4r error": "years",
          "% marine": "percentage",
          "cal. bc 1 sigma": "Calibrated BC dates",
          "cal. bc 2 sigma": "Calibrated BC dates"
        },
        "confidence": 0.41666666666666663,
        "votes": [
          {
            "K": "ppm (parts per million)",
            "Al": "ppm",
            "Si": "ppm",
            "S": "ppm",
            "Ca": "ppm",
            "Ti": "ppm",
            "Mn": "ppm",
            "Fe": "ppm",
            "Rb": "ppm",
            "Sr": "ppm",
            "Zr": "ppm",
            "Ba": "ppm",
            "Age_ky": "thousand years",
            "date": "radiocarbon years BP",
            "error": "radiocarbon years",
            "\u0394R": "years",
            "\u0394R error": "years",
            "% marine": "percentage"
          },
          {
            "K": "units unclear, need to determine if already in ppm or needs conversion",
            "Age_ky": "thousands of years (kiloyears)",
            "date": "radiocarbon years BP",
            "Cal. BC 1 sigma": "Calibrated BC dates",
            "Cal. BC 2 sigma": "Calibrated BC dates"
          },
          {
            "K": "ppm",
            "Age_ky": "ky"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Potassium values range ~10,000 ppm which is reasonable for geological samples",
          "Age columns show different scales (Age_ky vs. radiocarbon dates)",
          "Multiple age columns in climate data may represent different dating methods",
          "Age_ky is in thousands of years, Cal. BC dates are in years BC",
          "Conversion needed: Age_ky = (BC_date + 1950) / 1000",
          "K column units need verification - may already be in ppm or require conversion"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Potassium values range ~10,000 ppm which is reasonable for geological samples",
            "Age columns show different scales (Age_ky vs. radiocarbon dates)",
            "Multiple age columns in climate data may represent different dating methods"
          ],
          [
            "Age_ky is in thousands of years, Cal. BC dates are in years BC",
            "Conversion needed: Age_ky = (BC_date + 1950) / 1000",
            "K column units need verification - may already be in ppm or require conversion"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Time scales differ: climate data uses Age_ky (thousand years) while radiocarbon data uses uncalibrated BP years",
          "Geographic scope mismatch: climate data appears to be from ODP 967 (Eastern Mediterranean) while radiocarbon data includes multiple regions",
          "Time scale mismatch: radiocarbon dates in BC vs Age_ky in thousands of years",
          "Potassium unit unclear in climateMeasurements dataset"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time scales differ: climate data uses Age_ky (thousand years) while radiocarbon data uses uncalibrated BP years",
            "Geographic scope mismatch: climate data appears to be from ODP 967 (Eastern Mediterranean) while radiocarbon data includes multiple regions"
          ],
          [
            "Time scale mismatch: radiocarbon dates in BC vs Age_ky in thousands of years",
            "Potassium unit unclear in climateMeasurements dataset"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "?",
          "NaN"
        ],
        "confidence": 0.8666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " ",
            "?",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            "",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 30.0,
        "confidence": 1.0,
        "votes": [
          30.0,
          30.0,
          30.0
        ]
      },
      "file_format": {
        "value": "xlsx",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "xlsx",
          "xlsx"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Potassium values must be positive numbers",
          "Age values must be chronological (increasing with depth)",
          "Maltese sites must be identifiable in radiocarbon database",
          "First and last records must have corresponding climate data",
          "Region must be 'Malta' or 'Maltese' or related variant",
          "Need valid calibrated dates to determine first and last occurrences",
          "K values must be non-null at interpolated time points",
          "Linear interpolation required between Age_ky samples",
          "Final answer must be rounded to 4 decimal places",
          "Potassium values should be non-negative.",
          "Age values should be non-negative."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Potassium values must be positive numbers",
            "Age values must be chronological (increasing with depth)",
            "Maltese sites must be identifiable in radiocarbon database",
            "First and last records must have corresponding climate data"
          ],
          [
            "Region must be 'Malta' or 'Maltese' or related variant",
            "Need valid calibrated dates to determine first and last occurrences",
            "K values must be non-null at interpolated time points",
            "Linear interpolation required between Age_ky samples",
            "Final answer must be rounded to 4 decimal places"
          ],
          [
            "Potassium values should be non-negative.",
            "Age values should be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter radiocarbon database for Maltese region sites",
          "Find minimum and maximum dates for Maltese sites",
          "Match those dates to closest Age_ky values in climate data",
          "Interpolate Potassium values if exact age match not available",
          "Extract earliest and latest dates from radiocarbon records for Maltese region",
          "Convert BC dates to Age_ky scale for matching with climate data",
          "Identify bracketing Age_ky values in climateMeasurements for interpolation",
          "Site starts with 'ODP 967'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter radiocarbon database for Maltese region sites",
            "Find minimum and maximum dates for Maltese sites",
            "Match those dates to closest Age_ky values in climate data",
            "Interpolate Potassium values if exact age match not available"
          ],
          [
            "Extract earliest and latest dates from radiocarbon records for Maltese region",
            "Convert BC dates to Age_ky scale for matching with climate data",
            "Identify bracketing Age_ky values in climateMeasurements for interpolation"
          ],
          [
            "Site starts with 'ODP 967'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for temporal autocorrelation in Potassium measurements",
          "Validate linear interpolation assumptions",
          "Assess representativeness of Maltese site coverage",
          "Linear interpolation of K values based on Age_ky"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for temporal autocorrelation in Potassium measurements",
            "Validate linear interpolation assumptions",
            "Assess representativeness of Maltese site coverage"
          ],
          [
            "Linear interpolation of K values based on Age_ky"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Result must be a single numeric value",
          "Rounded to 4 decimal places",
          "Include units (ppm) in interpretation",
          "Single scalar value",
          "Units: ppm (parts per million)",
          "The final answer should be rounded to 4 decimal places."
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Result must be a single numeric value",
            "Rounded to 4 decimal places",
            "Include units (ppm) in interpretation"
          ],
          [
            "Single scalar value",
            "Rounded to 4 decimal places",
            "Units: ppm (parts per million)"
          ],
          [
            "The final answer should be rounded to 4 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5836111111111111
  },
  "archeology-hard-12": {
    "m_q": {
      "target_metric": {
        "value": "Count of human conflicts between 800 and 1400 AD, attributed to modern countries",
        "confidence": 0.6666666666666666,
        "votes": [
          "Count of human conflicts between 800 and 1400 AD, attributed to modern countries",
          "Count of human conflicts between 800 and 1400 AD that lasted at least one year, attributed to modern countries",
          "Count of human conflicts between 800 and 1400 AD, attributed to modern countries"
        ]
      },
      "filters": {
        "value": [
          "StartYear >= 800",
          "EndYear <= 1400",
          "(EndYear - StartYear) >= 1",
          "StartYear >= 800 AND StartYear <= 1400",
          "Conflict duration (EndYear - StartYear) >= 1",
          "Conflict duration >= 1 year"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "StartYear >= 800",
            "EndYear <= 1400",
            "(EndYear - StartYear) >= 1"
          ],
          [
            "StartYear >= 800 AND StartYear <= 1400",
            "Conflict duration (EndYear - StartYear) >= 1"
          ],
          [
            "StartYear >= 800",
            "EndYear <= 1400",
            "Conflict duration >= 1 year"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "country",
          "modern_country",
          "Modern Country"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "country"
          ],
          [
            "modern_country"
          ],
          [
            "Modern Country"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "How to extract location information from conflict names?",
          "How to map historical locations to modern countries?",
          "How to handle conflicts spanning multiple years?",
          "How to attribute multi-location conflicts to countries?",
          "How many conflicts occurred between 800 and 1400 AD?",
          "Which conflicts lasted at least one year (EndYear - StartYear >= 1)?",
          "How to extract actor/location information from Conflict column?",
          "How to map historical entities (kingdoms, empires, regions) to modern countries?",
          "How to use worldcities.csv to map location names to modern countries?",
          "Should conflicts involving multiple countries be counted once or multiple times (per country)?",
          "How to handle conflicts where locations cannot be mapped to modern countries?",
          "How to determine the duration of a conflict?",
          "How to map historical conflict locations to modern countries?",
          "How to handle conflicts spanning multiple modern countries?",
          "How to define 'human conflict'?",
          "How to handle conflicts with missing location information?"
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "How to extract location information from conflict names?",
            "How to map historical locations to modern countries?",
            "How to handle conflicts spanning multiple years?",
            "How to attribute multi-location conflicts to countries?"
          ],
          [
            "How many conflicts occurred between 800 and 1400 AD?",
            "Which conflicts lasted at least one year (EndYear - StartYear >= 1)?",
            "How to extract actor/location information from Conflict column?",
            "How to map historical entities (kingdoms, empires, regions) to modern countries?",
            "How to use worldcities.csv to map location names to modern countries?",
            "Should conflicts involving multiple countries be counted once or multiple times (per country)?",
            "How to handle conflicts where locations cannot be mapped to modern countries?"
          ],
          [
            "How to determine the duration of a conflict?",
            "How to map historical conflict locations to modern countries?",
            "How to handle conflicts spanning multiple modern countries?",
            "How to define 'human conflict'?",
            "How to handle conflicts with missing location information?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "conflict_brecke.csv",
          "worldcities.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "conflict_brecke.csv",
            "worldcities.csv"
          ],
          [
            "conflict_brecke.csv",
            "worldcities.csv"
          ],
          [
            "conflict_brecke.csv",
            "worldcities.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Conflict column contains location information but no standardized location columns",
          "No direct join key between conflict data and modern cities data",
          "conflict_brecke.csv contains historical entity names in Conflict column, not standardized location fields",
          "worldcities.csv contains modern country names, requires mapping from historical to modern entities",
          "No direct join key exists between files - requires text parsing and fuzzy matching"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Conflict column contains location information but no standardized location columns",
            "No direct join key between conflict data and modern cities data"
          ],
          [
            "conflict_brecke.csv contains historical entity names in Conflict column, not standardized location fields",
            "worldcities.csv contains modern country names, requires mapping from historical to modern entities",
            "No direct join key exists between files - requires text parsing and fuzzy matching"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "startyear": "AD year",
          "endyear": "AD year",
          "fatalities": "number of deaths",
          "century": "century number",
          "decade": "decade number",
          "lat": "degrees (decimal latitude)",
          "lng": "degrees (decimal longitude)",
          "population": "count (persons)"
        },
        "confidence": 0.6666666666666665,
        "votes": [
          {
            "StartYear": "AD year",
            "EndYear": "AD year",
            "Fatalities": "number of deaths",
            "Century": "century number",
            "Decade": "decade number"
          },
          {
            "StartYear": "year (integer, AD)",
            "EndYear": "year (integer, AD)",
            "Fatalities": "count (float, with nulls)",
            "Century": "year (integer, century marker)",
            "Decade": "year (integer, decade marker)",
            "lat": "degrees (decimal latitude)",
            "lng": "degrees (decimal longitude)",
            "population": "count (persons)"
          },
          {
            "StartYear": "AD",
            "EndYear": "AD",
            "Fatalities": "number of deaths"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Conflict names contain location information but need parsing",
          "Historical locations need mapping to modern countries",
          "Some conflicts may span multiple modern countries",
          "Fatalities column has many missing values (nulls)",
          "Century and Decade columns appear to be year values, not century/decade numbers",
          "Historical year ranges may need validation (EndYear >= StartYear)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Conflict names contain location information but need parsing",
            "Historical locations need mapping to modern countries",
            "Some conflicts may span multiple modern countries"
          ],
          [
            "Fatalities column has many missing values (nulls)",
            "Century and Decade columns appear to be year values, not century/decade numbers",
            "Historical year ranges may need validation (EndYear >= StartYear)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "conflict_brecke.csv uses historical names, worldcities.csv uses modern country names",
          "Historical place names in conflict_brecke.csv may not match modern names in worldcities.csv",
          "Historical political entities (kingdoms, empires) need mapping to modern nation-states",
          "Multiple modern countries may correspond to one historical entity"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "conflict_brecke.csv uses historical names, worldcities.csv uses modern country names"
          ],
          [
            "Historical place names in conflict_brecke.csv may not match modern names in worldcities.csv",
            "Historical political entities (kingdoms, empires) need mapping to modern nation-states",
            "Multiple modern countries may correspond to one historical entity"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A",
            "null"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Conflict must have StartYear and EndYear",
          "Conflict must last at least 1 year (EndYear - StartYear >= 1)",
          "Time period limited to 800-1400 AD",
          "StartYear >= 800 AND StartYear <= 1400",
          "EndYear - StartYear >= 1",
          "Conflict must involve human actors (exclude natural disasters)",
          "Each conflict should be counted once per analysis",
          "Conflict duration must be at least 1 year (EndYear - StartYear >= 0)",
          "StartYear and EndYear must be integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Conflict must have StartYear and EndYear",
            "Conflict must last at least 1 year (EndYear - StartYear >= 1)",
            "Time period limited to 800-1400 AD"
          ],
          [
            "StartYear >= 800 AND StartYear <= 1400",
            "EndYear - StartYear >= 1",
            "Conflict must involve human actors (exclude natural disasters)",
            "Each conflict should be counted once per analysis"
          ],
          [
            "Conflict duration must be at least 1 year (EndYear - StartYear >= 0)",
            "StartYear and EndYear must be integers"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract location names from Conflict column",
          "Map historical locations to modern countries using worldcities.csv or external knowledge",
          "Calculate conflict_duration = EndYear - StartYear",
          "Filter conflicts where conflict_duration >= 1",
          "Extract location and actor names from Conflict column using text parsing",
          "Map extracted entities to modern countries using worldcities.csv and historical mapping logic",
          "ConflictDuration = EndYear - StartYear + 1",
          "ConflictDuration >= 1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract location names from Conflict column",
            "Map historical locations to modern countries using worldcities.csv or external knowledge"
          ],
          [
            "Calculate conflict_duration = EndYear - StartYear",
            "Filter conflicts where conflict_duration >= 1",
            "Extract location and actor names from Conflict column using text parsing",
            "Map extracted entities to modern countries using worldcities.csv and historical mapping logic"
          ],
          [
            "ConflictDuration = EndYear - StartYear + 1",
            "ConflictDuration >= 1"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate conflicts",
          "Validate year ranges (StartYear <= EndYear)",
          "Verify century/decade calculations match year values",
          "Validate all StartYear and EndYear values are within 800-1400 range",
          "Verify EndYear >= StartYear for all records",
          "Count unmapped conflicts (those that cannot be attributed to modern countries)"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Check for duplicate conflicts",
            "Validate year ranges (StartYear <= EndYear)",
            "Verify century/decade calculations match year values"
          ],
          [
            "Validate all StartYear and EndYear values are within 800-1400 range",
            "Check for duplicate conflicts",
            "Verify EndYear >= StartYear for all records",
            "Count unmapped conflicts (those that cannot be attributed to modern countries)"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Count per modern country",
          "Total count across all countries",
          "List of conflicts with attributed countries",
          "Output should be a table with modern_country and conflict_count columns",
          "Include count of total conflicts analyzed",
          "Document attribution methodology and confidence level",
          "Note conflicts that could not be mapped to modern countries",
          "Handle conflicts involving multiple countries appropriately (specify counting method)",
          "Table with columns: Modern Country, Number of Conflicts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count per modern country",
            "Total count across all countries",
            "List of conflicts with attributed countries"
          ],
          [
            "Output should be a table with modern_country and conflict_count columns",
            "Include count of total conflicts analyzed",
            "Document attribution methodology and confidence level",
            "Note conflicts that could not be mapped to modern countries",
            "Handle conflicts involving multiple countries appropriately (specify counting method)"
          ],
          [
            "Table with columns: Modern Country, Number of Conflicts"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6333333333333334
  },
  "archeology-hard-2": {
    "m_q": {
      "target_metric": {
        "value": "Percentage of years where the ODP 967 wet-dry index showed an increasing trend",
        "confidence": 0.3333333333333333,
        "votes": [
          "Percentage of years where the ODP 967 wet-dry index showed an increasing trend",
          "Percentage of years where the wet-dry index was increasing compared to the previous year",
          "Percentage of years where the wet-dry index increased compared to the previous year."
        ]
      },
      "filters": {
        "value": [
          "Focus on 'ODP 967 wet-dry index' column",
          "Consider only valid numeric values",
          "Handle potential duplicate age columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Focus on 'ODP 967 wet-dry index' column",
            "Consider only valid numeric values",
            "Handle potential duplicate age columns"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Age_ky.3"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Age_ky.3"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the time range covered by the data?",
          "How many distinct years are represented?",
          "For each year transition, is the wet-dry index increasing?",
          "What percentage of year-to-year transitions show increase?",
          "What is the temporal sequence of wet-dry index measurements ordered by age?",
          "For each consecutive pair of years, was the wet-dry index increasing?",
          "What is the total count of year-to-year transitions?",
          "What is the count of increasing transitions?",
          "What percentage of transitions were increasing?",
          "Calculate the year-over-year change in 'ODP 967 wet-dry index'.",
          "Identify the years where the 'ODP 967 wet-dry index' increased.",
          "Calculate the total number of years with available 'ODP 967 wet-dry index' data.",
          "Divide the number of years with an increasing 'ODP 967 wet-dry index' by the total number of years and multiply by 100 to get the percentage."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the time range covered by the data?",
            "How many distinct years are represented?",
            "For each year transition, is the wet-dry index increasing?",
            "What percentage of year-to-year transitions show increase?"
          ],
          [
            "What is the temporal sequence of wet-dry index measurements ordered by age?",
            "For each consecutive pair of years, was the wet-dry index increasing?",
            "What is the total count of year-to-year transitions?",
            "What is the count of increasing transitions?",
            "What percentage of transitions were increasing?"
          ],
          [
            "Calculate the year-over-year change in 'ODP 967 wet-dry index'.",
            "Identify the years where the 'ODP 967 wet-dry index' increased.",
            "Calculate the total number of years with available 'ODP 967 wet-dry index' data.",
            "Divide the number of years with an increasing 'ODP 967 wet-dry index' by the total number of years and multiply by 100 to get the percentage."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "climateMeasurements.xlsx"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "climateMeasurements.xlsx"
          ],
          [
            "climateMeasurements.xlsx"
          ],
          [
            "climateMeasurements.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple 'Age_ky' columns with suffixes (.1, .2, .3) - need to identify which one corresponds to wet-dry index measurements",
          "Unnamed columns (20, 24, 27) with unclear meaning",
          "Multiple Age_ky columns exist (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3)",
          "ODP 967 wet-dry index appears only once but multiple age columns suggest data structure complexity"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple 'Age_ky' columns with suffixes (.1, .2, .3) - need to identify which one corresponds to wet-dry index measurements",
            "Unnamed columns (20, 24, 27) with unclear meaning"
          ],
          [
            "Multiple Age_ky columns exist (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3)",
            "ODP 967 wet-dry index appears only once but multiple age columns suggest data structure complexity"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "age_ky": "thousands of years",
          "odp 967 wet-dry index": "unitless index",
          "depth below seafloor_m": "meters",
          "splice depth_m": "meters",
          "interval_cm": "centimeters",
          "age_ky.3": "kiloyears (thousands of years)"
        },
        "confidence": 0.611111111111111,
        "votes": [
          {
            "Age_ky": "thousands of years",
            "ODP 967 wet-dry index": "unitless index",
            "Depth below seafloor_m": "meters",
            "Splice depth_m": "meters",
            "Interval_cm": "centimeters"
          },
          {
            "Age_ky.3": "kiloyears (thousands of years)",
            "ODP 967 wet-dry index": "dimensionless index"
          },
          {
            "Age_ky": "thousands of years",
            "Depth below seafloor_m": "meters",
            "Splice depth_m": "meters",
            "Interval_cm": "centimeters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Age columns appear in different scales - need to verify consistency",
          "Wet-dry index values range from negative to positive values",
          "Age is in kiloyears (ky) which represents thousands of years, not individual years",
          "The question asks about 'years' but data is in kiloyears - interpretation needed"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age columns appear in different scales - need to verify consistency",
            "Wet-dry index values range from negative to positive values"
          ],
          [
            "Age is in kiloyears (ky) which represents thousands of years, not individual years",
            "The question asks about 'years' but data is in kiloyears - interpretation needed"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "NaN"
        ],
        "confidence": 0.75,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "NaN"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 30.0,
        "confidence": 1.0,
        "votes": [
          30.0,
          30.0,
          30.0
        ]
      },
      "file_format": {
        "value": "xlsx",
        "confidence": 0.6666666666666666,
        "votes": [
          "excel",
          "xlsx",
          "xlsx"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Age values should be monotonically increasing for temporal analysis",
          "Wet-dry index should be numeric",
          "Need to handle potential missing values in wet-dry index column",
          "Filter out rows where Age_ky.3 is null",
          "Filter out rows where ODP 967 wet-dry index is null",
          "Ensure chronological ordering by Age_ky.3",
          "The 'Age_ky' column should be used to represent the year.",
          "Missing values in 'ODP 967 wet-dry index' should be handled appropriately (e.g., removed or imputed) before calculating year-over-year changes."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age values should be monotonically increasing for temporal analysis",
            "Wet-dry index should be numeric",
            "Need to handle potential missing values in wet-dry index column"
          ],
          [
            "Filter out rows where Age_ky.3 is null",
            "Filter out rows where ODP 967 wet-dry index is null",
            "Ensure chronological ordering by Age_ky.3"
          ],
          [
            "The 'Age_ky' column should be used to represent the year.",
            "Missing values in 'ODP 967 wet-dry index' should be handled appropriately (e.g., removed or imputed) before calculating year-over-year changes."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows where 'ODP 967 wet-dry index' is null",
          "Use 'Age_ky.3' column that appears to correspond with wet-dry index measurements",
          "Consider only unique age values or aggregate by year",
          "Remove duplicate age values if they exist",
          "Handle potential non-sequential age measurements"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows where 'ODP 967 wet-dry index' is null",
            "Use 'Age_ky.3' column that appears to correspond with wet-dry index measurements",
            "Consider only unique age values or aggregate by year"
          ],
          [
            "Remove duplicate age values if they exist",
            "Handle potential non-sequential age measurements"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for monotonicity of age values",
          "Test for stationarity of wet-dry index time series",
          "Calculate year-over-year differences",
          "Calculate year-over-year change in wet-dry index",
          "Count transitions where change > 0",
          "Calculate percentage: (increasing_count / total_transitions) * 100"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for monotonicity of age values",
            "Test for stationarity of wet-dry index time series",
            "Calculate year-over-year differences"
          ],
          [
            "Calculate year-over-year change in wet-dry index",
            "Count transitions where change > 0",
            "Calculate percentage: (increasing_count / total_transitions) * 100"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Percentage rounded to 2 decimal places",
          "Scalar output between 0-100%",
          "Result must be a percentage",
          "Round to 2 decimal places",
          "Format as numeric value (e.g., 45.67)",
          "Percentage value rounded to 2 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage rounded to 2 decimal places",
            "Scalar output between 0-100%"
          ],
          [
            "Result must be a percentage",
            "Round to 2 decimal places",
            "Format as numeric value (e.g., 45.67)"
          ],
          [
            "Percentage value rounded to 2 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5680555555555556
  },
  "archeology-hard-5": {
    "m_q": {
      "target_metric": {
        "value": "Maximum aluminum value (Al column) from climate dataset, rounded to 4 decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "Maximum aluminum value (Al column) from climate dataset, rounded to 4 decimal places",
          "Maximum aluminum value in the climate dataset for the year(s) closest to the year of the most northern Neolithic sample in Malta, rounded to 4 decimal places",
          "Maximum aluminum value in the climate dataset in the closest year to the year of the most northern Neolithic sample in the Maltese dataset."
        ]
      },
      "filters": {
        "value": [
          "Culture = 'Neolithic'",
          "Region = 'Maltese' or Site contains 'Malta'",
          "Most northern sample (highest Latitude)",
          "Break ties by later year (date)",
          "Region == 'Malta' in radiocarbon dataset",
          "Culture == 'Neolithic' in radiocarbon dataset"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Culture = 'Neolithic'",
            "Region = 'Maltese' or Site contains 'Malta'",
            "Most northern sample (highest Latitude)",
            "Break ties by later year (date)"
          ],
          [
            "Region == 'Malta' in radiocarbon dataset",
            "Culture == 'Neolithic' in radiocarbon dataset"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Year (from date column)",
          "Latitude (to find most northern)",
          "Year/date (to break ties by later year)",
          "Age_ky in climate dataset (to find closest year match)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year (from date column)"
          ],
          [
            "Latitude (to find most northern)",
            "Year/date (to break ties by later year)",
            "Age_ky in climate dataset (to find closest year match)"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Find most northern Neolithic sample in Maltese dataset",
          "Extract its year (date)",
          "Find closest year(s) in climate dataset Age_ky column",
          "Calculate max Al value across those years",
          "Round to 4 decimal places",
          "Which samples in radiocarbon_database_regional.xlsx are from Malta region?",
          "Among Maltese samples, which ones have Culture == 'Neolithic'?",
          "What is the maximum Latitude among Maltese Neolithic samples?",
          "For samples at the most northern latitude, which year/date is the latest?",
          "What is the target year from the most northern Neolithic sample?",
          "How to convert/compare radiocarbon dates or calibrated BC dates to Age_ky in climate dataset?",
          "What year(s) in climate dataset are closest to the target year?",
          "What is the maximum Al (aluminum) value for those closest year(s)?",
          "How to round the result to 4 decimal places?",
          "Find the year of the most northern Neolithic sample in the radiocarbon_database_regional.xlsx dataset, breaking ties by considering the later year.",
          "Find the closest year(s) in the climateMeasurements.xlsx dataset to the year found in the previous step.",
          "Find the maximum aluminum value recorded in the climateMeasurements.xlsx dataset in the closest year(s).",
          "Round the maximum aluminum value to 4 decimal places."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Find most northern Neolithic sample in Maltese dataset",
            "Extract its year (date)",
            "Find closest year(s) in climate dataset Age_ky column",
            "Calculate max Al value across those years",
            "Round to 4 decimal places"
          ],
          [
            "Which samples in radiocarbon_database_regional.xlsx are from Malta region?",
            "Among Maltese samples, which ones have Culture == 'Neolithic'?",
            "What is the maximum Latitude among Maltese Neolithic samples?",
            "For samples at the most northern latitude, which year/date is the latest?",
            "What is the target year from the most northern Neolithic sample?",
            "How to convert/compare radiocarbon dates or calibrated BC dates to Age_ky in climate dataset?",
            "What year(s) in climate dataset are closest to the target year?",
            "What is the maximum Al (aluminum) value for those closest year(s)?",
            "How to round the result to 4 decimal places?"
          ],
          [
            "Find the year of the most northern Neolithic sample in the radiocarbon_database_regional.xlsx dataset, breaking ties by considering the later year.",
            "Find the closest year(s) in the climateMeasurements.xlsx dataset to the year found in the previous step.",
            "Find the maximum aluminum value recorded in the climateMeasurements.xlsx dataset in the closest year(s).",
            "Round the maximum aluminum value to 4 decimal places."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "radiocarbon_database_regional.xlsx",
          "climateMeasurements.xlsx"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "radiocarbon_database_regional.xlsx",
            "climateMeasurements.xlsx"
          ],
          [
            "radiocarbon_database_regional.xlsx",
            "climateMeasurements.xlsx"
          ],
          [
            "climateMeasurements.xlsx",
            "radiocarbon_database_regional.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different time units: radiocarbon uses years BP (date), climate uses kiloyears (Age_ky)",
          "No direct join columns between datasets",
          "Date representations differ: radiocarbon uses 'date' (radiocarbon years BP), 'Cal. BC 1 sigma', 'Cal. BC 2 sigma' columns vs climate uses 'Age_ky' (thousands of years)",
          "Multiple Age_ky columns in climate dataset (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3) - need to determine which to use"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different time units: radiocarbon uses years BP (date), climate uses kiloyears (Age_ky)",
            "No direct join columns between datasets"
          ],
          [
            "Date representations differ: radiocarbon uses 'date' (radiocarbon years BP), 'Cal. BC 1 sigma', 'Cal. BC 2 sigma' columns vs climate uses 'Age_ky' (thousands of years)",
            "Multiple Age_ky columns in climate dataset (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3) - need to determine which to use"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "date": "radiocarbon years BP",
          "age_ky": "kiloyears",
          "al": "concentration units (unspecified)",
          "latitude": "degrees",
          "longitude": "degrees",
          "cal. bc 1 sigma": "calibrated years BC",
          "cal. bc 2 sigma": "calibrated years BC"
        },
        "confidence": 0.7619047619047618,
        "votes": [
          {
            "date": "radiocarbon years BP",
            "Age_ky": "kiloyears",
            "Al": "concentration units (unspecified)",
            "Latitude": "degrees",
            "Longitude": "degrees"
          },
          {
            "date": "radiocarbon years before present (BP)",
            "Age_ky": "thousands of years (kiloyears)",
            "Cal. BC 1 sigma": "calibrated years BC",
            "Cal. BC 2 sigma": "calibrated years BC",
            "Al": "aluminum concentration (units not specified in data)",
            "Latitude": "degrees",
            "Longitude": "degrees"
          },
          {
            "Latitude": "degrees",
            "Longitude": "degrees",
            "Age_ky": "kilo years",
            "Al": "concentration"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Age_ky in climate data needs conversion to years for comparison with radiocarbon dates",
          "Multiple Age_ky columns (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3) with potentially different values",
          "Age_ky is in thousands of years while calibrated dates are in years BC - requires conversion",
          "Radiocarbon dates (BP) vs calibrated dates (BC) vs Age_ky scale alignment needed",
          "Need to determine if 'year' refers to calibrated BC date or Age_ky value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age_ky in climate data needs conversion to years for comparison with radiocarbon dates",
            "Multiple Age_ky columns (Age_ky, Age_ky.1, Age_ky.2, Age_ky.3) with potentially different values"
          ],
          [
            "Age_ky is in thousands of years while calibrated dates are in years BC - requires conversion",
            "Radiocarbon dates (BP) vs calibrated dates (BC) vs Age_ky scale alignment needed",
            "Need to determine if 'year' refers to calibrated BC date or Age_ky value"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Time scale mismatch: radiocarbon dates vs geological ages",
          "Which Age_ky column to use for year matching?",
          "Time scales differ fundamentally between datasets: radiocarbon/calibrated years vs Age_ky",
          "Climate dataset has multiple Age_ky columns that may represent different measurements or duplicates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time scale mismatch: radiocarbon dates vs geological ages",
            "Which Age_ky column to use for year matching?"
          ],
          [
            "Time scales differ fundamentally between datasets: radiocarbon/calibrated years vs Age_ky",
            "Climate dataset has multiple Age_ky columns that may represent different measurements or duplicates"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "?"
        ],
        "confidence": 0.9999999999999999,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "?",
            " "
          ],
          [
            "NA",
            "N/A",
            "",
            "?"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 30.0,
        "confidence": 1.0,
        "votes": [
          30.0,
          30.0,
          30.0
        ]
      },
      "file_format": {
        "value": "xlsx",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "xlsx",
          "xlsx"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Must filter for Neolithic culture",
          "Must filter for Maltese region/sites",
          "Must handle multiple closest years in climate data",
          "Must convert Age_ky to years for comparison",
          "Must handle ties in northernmost samples by taking later date",
          "Region must be 'Malta' or 'Maltese'",
          "Culture must be 'Neolithic'",
          "Only consider samples with valid Latitude values (not null)",
          "Only consider climate records with valid Al values (not null)",
          "Ties in most northern latitude are broken by taking later year",
          "If multiple years are equally close, take maximum aluminum across all of them",
          "Consider only Neolithic samples when finding the most northern sample.",
          "If there are multiple closest years in the climate dataset, take the maximum aluminum value across all of them.",
          "Round the final answer to 4 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Must filter for Neolithic culture",
            "Must filter for Maltese region/sites",
            "Must handle multiple closest years in climate data",
            "Must convert Age_ky to years for comparison",
            "Must handle ties in northernmost samples by taking later date"
          ],
          [
            "Region must be 'Malta' or 'Maltese'",
            "Culture must be 'Neolithic'",
            "Only consider samples with valid Latitude values (not null)",
            "Only consider climate records with valid Al values (not null)",
            "Ties in most northern latitude are broken by taking later year",
            "If multiple years are equally close, take maximum aluminum across all of them"
          ],
          [
            "Consider only Neolithic samples when finding the most northern sample.",
            "If there are multiple closest years in the climate dataset, take the maximum aluminum value across all of them.",
            "Round the final answer to 4 decimal places."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter radiocarbon data: Culture == 'Neolithic' AND (Region contains 'Malta' OR Site contains 'Malta')",
          "Filter climate data: Use Age_ky column (not Age_ky.1, Age_ky.2, Age_ky.3) for year matching",
          "most_northern_latitude = MAX(Latitude WHERE Region='Malta' AND Culture='Neolithic')",
          "target_samples = samples WHERE Latitude == most_northern_latitude",
          "target_year = MAX(year) from target_samples",
          "closest_years = years in climate dataset with minimum absolute difference from target_year",
          "Filter radiocarbon_database_regional.xlsx to include only samples where Culture is 'Neolithic'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter radiocarbon data: Culture == 'Neolithic' AND (Region contains 'Malta' OR Site contains 'Malta')",
            "Filter climate data: Use Age_ky column (not Age_ky.1, Age_ky.2, Age_ky.3) for year matching"
          ],
          [
            "most_northern_latitude = MAX(Latitude WHERE Region='Malta' AND Culture='Neolithic')",
            "target_samples = samples WHERE Latitude == most_northern_latitude",
            "target_year = MAX(year) from target_samples",
            "closest_years = years in climate dataset with minimum absolute difference from target_year"
          ],
          [
            "Filter radiocarbon_database_regional.xlsx to include only samples where Culture is 'Neolithic'."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Age_ky values in climate data",
          "Verify Al values are numeric and not missing"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Age_ky values in climate data",
            "Verify Al values are numeric and not missing"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Final answer must be rounded to 4 decimal places",
          "Must handle case where multiple climate years are equally close to target year",
          "Final answer must be rounded to exactly 4 decimal places",
          "Output should be a single numeric value",
          "Handle multiple closest years by taking maximum aluminum value across all",
          "The final answer must be a number rounded to 4 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Final answer must be rounded to 4 decimal places",
            "Must handle case where multiple climate years are equally close to target year"
          ],
          [
            "Final answer must be rounded to exactly 4 decimal places",
            "Output should be a single numeric value",
            "Handle multiple closest years by taking maximum aluminum value across all"
          ],
          [
            "The final answer must be a number rounded to 4 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6047619047619048
  },
  "archeology-hard-7": {
    "m_q": {
      "target_metric": {
        "value": "Count of modern cities with population > 100,000 located within 0.1 degrees (approximately 11 km) of ancient Roman-era cities",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of modern cities with population > 100,000 located within 0.1 degrees (approximately 11 km) of ancient Roman-era cities",
          "Count of modern cities with population > 100,000 that are within 0.1 degrees (both latitude and longitude) of any ancient Roman-era city",
          "Count of modern cities with population > 100k within 0.1 degrees of any ancient Roman city"
        ]
      },
      "filters": {
        "value": [
          "worldcities.population > 100000",
          "Distance between roman_cities coordinates and worldcities coordinates <= 0.1 degrees",
          "roman_cities.Start Date and End Date indicate Roman-era timeframe",
          "Modern cities with population > 100,000",
          "Ancient cities that existed during Roman era (overlapping date ranges with Roman period)",
          "Distance threshold: within 0.1 degrees in both latitude and longitude",
          "distance between worldcities (lat, lng) and roman_cities (Latitude (Y), Longitude (X)) <= 0.1 degrees"
        ],
        "confidence": 0.38095238095238093,
        "votes": [
          [
            "worldcities.population > 100000",
            "Distance between roman_cities coordinates and worldcities coordinates <= 0.1 degrees",
            "roman_cities.Start Date and End Date indicate Roman-era timeframe"
          ],
          [
            "Modern cities with population > 100,000",
            "Ancient cities that existed during Roman era (overlapping date ranges with Roman period)",
            "Distance threshold: within 0.1 degrees in both latitude and longitude"
          ],
          [
            "worldcities.population > 100000",
            "distance between worldcities (lat, lng) and roman_cities (Latitude (Y), Longitude (X)) <= 0.1 degrees"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What constitutes 'Roman-era' based on Start Date and End Date columns?",
          "How to handle missing population values in worldcities?",
          "Should we consider only exact matches between Modern Toponym and city/city_ascii or just geographic proximity?",
          "How to calculate geographic distance using latitude/longitude coordinates?",
          "What date range constitutes the Roman era for filtering ancient cities?",
          "How to interpret Start Date and End Date columns to determine if a city existed during Roman era?",
          "How to calculate spatial proximity using coordinate differences?",
          "How to handle missing population values in worldcities.csv?",
          "How to handle missing coordinates in roman_cities.csv?",
          "Should a modern city be counted once even if it matches multiple Roman cities?",
          "Calculate the distance between each modern city and each ancient Roman city.",
          "Filter modern cities with population greater than 100,000.",
          "Identify modern cities within 0.1 degrees of any ancient Roman city.",
          "Count the number of modern cities that meet the criteria."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What constitutes 'Roman-era' based on Start Date and End Date columns?",
            "How to handle missing population values in worldcities?",
            "Should we consider only exact matches between Modern Toponym and city/city_ascii or just geographic proximity?",
            "How to calculate geographic distance using latitude/longitude coordinates?"
          ],
          [
            "What date range constitutes the Roman era for filtering ancient cities?",
            "How to interpret Start Date and End Date columns to determine if a city existed during Roman era?",
            "How to calculate spatial proximity using coordinate differences?",
            "How to handle missing population values in worldcities.csv?",
            "How to handle missing coordinates in roman_cities.csv?",
            "Should a modern city be counted once even if it matches multiple Roman cities?"
          ],
          [
            "Calculate the distance between each modern city and each ancient Roman city.",
            "Filter modern cities with population greater than 100,000.",
            "Identify modern cities within 0.1 degrees of any ancient Roman city.",
            "Count the number of modern cities that meet the criteria."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "roman_cities.csv",
          "worldcities.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "roman_cities.csv",
            "worldcities.csv"
          ],
          [
            "roman_cities.csv",
            "worldcities.csv"
          ],
          [
            "roman_cities.csv",
            "worldcities.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different coordinate column names: roman_cities uses 'Longitude (X)' and 'Latitude (Y)' while worldcities uses 'lng' and 'lat'",
          "Different naming conventions for modern cities: roman_cities.Modern Toponym vs worldcities.city/city_ascii",
          "Coordinate column names differ: 'Longitude (X)' vs 'lng', 'Latitude (Y)' vs 'lat'",
          "No direct join key; requires spatial distance calculation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different coordinate column names: roman_cities uses 'Longitude (X)' and 'Latitude (Y)' while worldcities uses 'lng' and 'lat'",
            "Different naming conventions for modern cities: roman_cities.Modern Toponym vs worldcities.city/city_ascii"
          ],
          [
            "Coordinate column names differ: 'Longitude (X)' vs 'lng', 'Latitude (Y)' vs 'lat'",
            "No direct join key; requires spatial distance calculation"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "longitude (x)": "degrees",
          "latitude (y)": "degrees",
          "lng": "degrees",
          "lat": "degrees",
          "population": "people",
          "start date": "year BCE/CE",
          "end date": "year BCE/CE"
        },
        "confidence": 0.9047619047619049,
        "votes": [
          {
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "lng": "degrees",
            "lat": "degrees",
            "population": "people",
            "Start Date": "year BCE/CE",
            "End Date": "year BCE/CE"
          },
          {
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "lng": "degrees",
            "lat": "degrees",
            "population": "persons",
            "Start Date": "year (negative for BCE, positive for CE)",
            "End Date": "year (negative for BCE, positive for CE)"
          },
          {
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "lat": "degrees",
            "lng": "degrees",
            "population": "people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Distance threshold of 0.1 degrees varies with latitude (approximately 11 km at equator, less at higher latitudes)",
          "Population values in worldcities may be estimates with varying precision",
          "Roman-era dates may have different calendar systems",
          "Distance threshold is 0.1 degrees, which varies in actual distance depending on latitude (approximately 11 km at equator for longitude)",
          "Start Date and End Date use negative values for BCE dates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Distance threshold of 0.1 degrees varies with latitude (approximately 11 km at equator, less at higher latitudes)",
            "Population values in worldcities may be estimates with varying precision",
            "Roman-era dates may have different calendar systems"
          ],
          [
            "Distance threshold is 0.1 degrees, which varies in actual distance depending on latitude (approximately 11 km at equator for longitude)",
            "Start Date and End Date use negative values for BCE dates"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Geographic coordinate systems may differ (WGS84 assumed but not confirmed)",
          "Time period definitions: Roman-era vs modern city data from different time periods",
          "Roman cities use negative years for BCE, modern cities have no temporal data",
          "Population in worldcities is float64 and may contain NaN values",
          "End Date in roman_cities.csv is float64 with potential NaN values indicating cities that continued"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Geographic coordinate systems may differ (WGS84 assumed but not confirmed)",
            "Time period definitions: Roman-era vs modern city data from different time periods"
          ],
          [
            "Roman cities use negative years for BCE, modern cities have no temporal data",
            "Population in worldcities is float64 and may contain NaN values",
            "End Date in roman_cities.csv is float64 with potential NaN values indicating cities that continued"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "Unknown",
          "NA",
          "N/A",
          "NaN"
        ],
        "confidence": 0.7333333333333333,
        "votes": [
          [
            "",
            "Unknown",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 0.9230769230769231,
        "votes": [
          12.0,
          12.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Geographic coordinates must be valid (lat: -90 to 90, lon: -180 to 180)",
          "Population must be positive for filtering",
          "Distance calculation requires both coordinate pairs",
          "Roman-era cities should have valid date ranges",
          "Population threshold: population > 100,000",
          "Spatial proximity: abs(lat_modern - lat_roman) <= 0.1 AND abs(lng_modern - lng_roman) <= 0.1",
          "Exclude records with missing coordinates",
          "Exclude records with missing or invalid population values",
          "Distance calculation must use appropriate formula (e.g., Haversine) for spherical coordinates.",
          "Ensure consistent coordinate systems (WGS84) for both datasets."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Geographic coordinates must be valid (lat: -90 to 90, lon: -180 to 180)",
            "Population must be positive for filtering",
            "Distance calculation requires both coordinate pairs",
            "Roman-era cities should have valid date ranges"
          ],
          [
            "Population threshold: population > 100,000",
            "Spatial proximity: abs(lat_modern - lat_roman) <= 0.1 AND abs(lng_modern - lng_roman) <= 0.1",
            "Exclude records with missing coordinates",
            "Exclude records with missing or invalid population values"
          ],
          [
            "Distance calculation must use appropriate formula (e.g., Haversine) for spherical coordinates.",
            "Ensure consistent coordinate systems (WGS84) for both datasets."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "ABS(roman_cities.Latitude (Y) - worldcities.lat) <= 0.1",
          "ABS(roman_cities.Longitude (X) - worldcities.lng) <= 0.1",
          "worldcities.population > 100000",
          "roman_cities.Start Date <= 476 AND roman_cities.End Date >= -27 (Roman Empire timeframe)",
          "Filter roman_cities to those existing during Roman era (approximately 753 BCE to 476 CE, or -753 to 476)",
          "A Roman city existed during Roman era if: (Start Date <= 476) AND (End Date >= -753 OR End Date is NULL)",
          "Filter worldcities where population > 100000 and population is not null",
          "modern_city_within_0.1_degrees = distance(worldcities.lat, worldcities.lng, roman_cities.Latitude (Y), roman_cities.Longitude (X)) <= 0.1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "ABS(roman_cities.Latitude (Y) - worldcities.lat) <= 0.1",
            "ABS(roman_cities.Longitude (X) - worldcities.lng) <= 0.1",
            "worldcities.population > 100000",
            "roman_cities.Start Date <= 476 AND roman_cities.End Date >= -27 (Roman Empire timeframe)"
          ],
          [
            "Filter roman_cities to those existing during Roman era (approximately 753 BCE to 476 CE, or -753 to 476)",
            "A Roman city existed during Roman era if: (Start Date <= 476) AND (End Date >= -753 OR End Date is NULL)",
            "Filter worldcities where population > 100000 and population is not null"
          ],
          [
            "modern_city_within_0.1_degrees = distance(worldcities.lat, worldcities.lng, roman_cities.Latitude (Y), roman_cities.Longitude (X)) <= 0.1"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for coordinate outliers",
          "Validate population distribution",
          "Test distance calculation accuracy",
          "Verify date range consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for coordinate outliers",
            "Validate population distribution",
            "Test distance calculation accuracy",
            "Verify date range consistency"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Should include validation of proximity calculation method",
          "May need to document which cities matched for verification",
          "Return a single integer count",
          "Count distinct modern cities (avoid double-counting if one modern city matches multiple Roman cities)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count",
            "Should include validation of proximity calculation method",
            "May need to document which cities matched for verification"
          ],
          [
            "Return a single integer count",
            "Count distinct modern cities (avoid double-counting if one modern city matches multiple Roman cities)"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5971062271062272
  },
  "archeology-hard-9": {
    "m_q": {
      "target_metric": {
        "value": "Correlation coefficient (to 6 decimal places) between ancient Roman city rank and modern city population for cities with population > 1,000,000",
        "confidence": 0.3333333333333333,
        "votes": [
          "Correlation coefficient (to 6 decimal places) between ancient Roman city rank and modern city population for cities with population > 1,000,000",
          "Correlation coefficient between the rank of ancient Roman cities and the population of their corresponding modern cities, rounded to 6 decimal places",
          "Pearson correlation between the rank of ancient Roman cities and the population of their corresponding modern cities with a population of over one million, rounded to 6 decimal places."
        ]
      },
      "filters": {
        "value": [
          "Barrington Atlas Rank not null",
          "population > 1000000",
          "distance between ancient and modern city coordinates < 0.1 degrees",
          "Modern cities with population > 1,000,000",
          "Ancient cities within 0.1 degrees distance of modern cities",
          "If multiple ancient cities match, take the last sample in the data",
          "Distance between ancient and modern cities < 0.1 degrees",
          "Take the last sample in the data if there are multiple ancient cities"
        ],
        "confidence": 0.375,
        "votes": [
          [
            "Barrington Atlas Rank not null",
            "population > 1000000",
            "distance between ancient and modern city coordinates < 0.1 degrees"
          ],
          [
            "Modern cities with population > 1,000,000",
            "Ancient cities within 0.1 degrees distance of modern cities",
            "If multiple ancient cities match, take the last sample in the data"
          ],
          [
            "Modern cities with population > 1,000,000",
            "Distance between ancient and modern cities < 0.1 degrees",
            "Take the last sample in the data if there are multiple ancient cities"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Ancient Toponym",
          "Modern Toponym"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Ancient Toponym",
            "Modern Toponym"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to handle 'or' in Barrington Atlas Rank? (take average)",
          "How to match ancient to modern cities? (distance < 0.1 degrees)",
          "How to handle multiple ancient cities? (take last sample)",
          "How to calculate correlation? (Pearson correlation)",
          "How to parse 'Barrington Atlas Rank' values containing 'or' to compute average?",
          "How to compute distance between ancient city coordinates (Longitude (X), Latitude (Y)) and modern city coordinates (lng, lat)?",
          "Which ancient-modern city pairs are within 0.1 degrees distance?",
          "Which modern cities have population > 1,000,000?",
          "If multiple ancient cities match the same modern city, which is the last sample?",
          "What is the Pearson correlation between rank and population for matched pairs?",
          "Determine the rank of each ancient Roman city, averaging ranks if 'or' is present.",
          "Find the modern city corresponding to each ancient Roman city based on proximity.",
          "Filter modern cities to include only those with a population greater than 1,000,000.",
          "Calculate the Pearson correlation between the ancient city ranks and modern city populations."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How to handle 'or' in Barrington Atlas Rank? (take average)",
            "How to match ancient to modern cities? (distance < 0.1 degrees)",
            "How to handle multiple ancient cities? (take last sample)",
            "How to calculate correlation? (Pearson correlation)"
          ],
          [
            "How to parse 'Barrington Atlas Rank' values containing 'or' to compute average?",
            "How to compute distance between ancient city coordinates (Longitude (X), Latitude (Y)) and modern city coordinates (lng, lat)?",
            "Which ancient-modern city pairs are within 0.1 degrees distance?",
            "Which modern cities have population > 1,000,000?",
            "If multiple ancient cities match the same modern city, which is the last sample?",
            "What is the Pearson correlation between rank and population for matched pairs?"
          ],
          [
            "Determine the rank of each ancient Roman city, averaging ranks if 'or' is present.",
            "Find the modern city corresponding to each ancient Roman city based on proximity.",
            "Filter modern cities to include only those with a population greater than 1,000,000.",
            "Calculate the Pearson correlation between the ancient city ranks and modern city populations."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "roman_cities.csv",
          "worldcities.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "roman_cities.csv",
            "worldcities.csv"
          ],
          [
            "roman_cities.csv",
            "worldcities.csv"
          ],
          [
            "roman_cities.csv",
            "worldcities.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "roman_cities.csv uses 'Longitude (X)' and 'Latitude (Y)' while worldcities.csv uses 'lng' and 'lat'",
          "roman_cities.csv has 'Modern Toponym' while worldcities.csv has 'city' and 'city_ascii'",
          "Coordinate column names differ: 'Longitude (X)' vs 'lng', 'Latitude (Y)' vs 'lat'",
          "City name columns differ: 'Ancient Toponym', 'Modern Toponym' vs 'city', 'city_ascii'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "roman_cities.csv uses 'Longitude (X)' and 'Latitude (Y)' while worldcities.csv uses 'lng' and 'lat'",
            "roman_cities.csv has 'Modern Toponym' while worldcities.csv has 'city' and 'city_ascii'"
          ],
          [
            "Coordinate column names differ: 'Longitude (X)' vs 'lng', 'Latitude (Y)' vs 'lat'",
            "City name columns differ: 'Ancient Toponym', 'Modern Toponym' vs 'city', 'city_ascii'"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "barrington atlas rank": "ordinal rank (1=highest, 3=lowest)",
          "population": "number of people",
          "longitude (x)": "degrees",
          "latitude (y)": "degrees",
          "lng": "degrees",
          "lat": "degrees",
          "start date": "year",
          "end date": "year"
        },
        "confidence": 0.8333333333333333,
        "votes": [
          {
            "Barrington Atlas Rank": "ordinal rank (1=highest, 3=lowest)",
            "population": "number of people",
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "lng": "degrees",
            "lat": "degrees"
          },
          {
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "lng": "degrees",
            "lat": "degrees",
            "population": "count",
            "Barrington Atlas Rank": "ordinal_rank",
            "Start Date": "year",
            "End Date": "year"
          },
          {
            "Barrington Atlas Rank": "rank",
            "population": "people",
            "Longitude (X)": "degrees",
            "Latitude (Y)": "degrees",
            "lat": "degrees",
            "lng": "degrees"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Population values are very large (millions)",
          "Rank values are small (1-3)",
          "Coordinate values are in degrees",
          "Population values are in raw counts (e.g., 37732000.0)",
          "Distance threshold is 0.1 degrees (approximately 11 km at equator)",
          "Rank values may contain 'or' requiring averaging"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Population values are very large (millions)",
            "Rank values are small (1-3)",
            "Coordinate values are in degrees"
          ],
          [
            "Population values are in raw counts (e.g., 37732000.0)",
            "Distance threshold is 0.1 degrees (approximately 11 km at equator)",
            "Rank values may contain 'or' requiring averaging"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Coordinate systems may differ (WGS84 assumed)",
          "Ancient city names may not match modern city names exactly",
          "Ancient cities may have different toponyms than modern cities despite geographic proximity",
          "Multiple ancient cities may map to the same modern city location"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Coordinate systems may differ (WGS84 assumed)",
            "Ancient city names may not match modern city names exactly"
          ],
          [
            "Ancient cities may have different toponyms than modern cities despite geographic proximity",
            "Multiple ancient cities may map to the same modern city location"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "Unknown",
          "NA",
          "N/A",
          "NaN"
        ],
        "confidence": 0.7333333333333333,
        "votes": [
          [
            "",
            "Unknown",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Distance calculation: sqrt((lng1-lng2)^2 + (lat1-lat2)^2) < 0.1",
          "Rank parsing: if 'or' exists, take average of two numbers",
          "Multiple ancient cities: take last sample in data",
          "Population filter: > 1000000",
          "Correlation: Pearson correlation coefficient",
          "Rounding: to 6 decimal places",
          "Distance between ancient and modern city must be < 0.1 degrees",
          "Modern city population must be > 1,000,000",
          "If 'Barrington Atlas Rank' contains 'or', compute average of the two numbers",
          "If multiple ancient cities match same modern city, select last sample from data",
          "Correlation must be rounded to exactly 6 decimal places",
          "Calculate distance between ancient and modern cities using longitude and latitude.",
          "Handle missing values in 'Barrington Atlas Rank'.",
          "Ensure that the population is a valid number."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Distance calculation: sqrt((lng1-lng2)^2 + (lat1-lat2)^2) < 0.1",
            "Rank parsing: if 'or' exists, take average of two numbers",
            "Multiple ancient cities: take last sample in data",
            "Population filter: > 1000000",
            "Correlation: Pearson correlation coefficient",
            "Rounding: to 6 decimal places"
          ],
          [
            "Distance between ancient and modern city must be < 0.1 degrees",
            "Modern city population must be > 1,000,000",
            "If 'Barrington Atlas Rank' contains 'or', compute average of the two numbers",
            "If multiple ancient cities match same modern city, select last sample from data",
            "Correlation must be rounded to exactly 6 decimal places"
          ],
          [
            "Calculate distance between ancient and modern cities using longitude and latitude.",
            "Handle missing values in 'Barrington Atlas Rank'.",
            "Ensure that the population is a valid number."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows where Barrington Atlas Rank is null",
          "Filter out rows where population <= 1000000",
          "Filter out rows where distance >= 0.1 degrees",
          "Calculate Euclidean distance: sqrt((lng - Longitude(X))^2 + (lat - Latitude(Y))^2) < 0.1",
          "Filter worldcities where population > 1000000",
          "Parse rank values to handle 'or' format and convert to numeric average",
          "Remove rows where rank is null or cannot be parsed",
          "Distance between ancient and modern cities < 0.1 degrees",
          "Modern cities with population > 1,000,000"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows where Barrington Atlas Rank is null",
            "Filter out rows where population <= 1000000",
            "Filter out rows where distance >= 0.1 degrees"
          ],
          [
            "Calculate Euclidean distance: sqrt((lng - Longitude(X))^2 + (lat - Latitude(Y))^2) < 0.1",
            "Filter worldcities where population > 1000000",
            "Parse rank values to handle 'or' format and convert to numeric average",
            "Remove rows where rank is null or cannot be parsed"
          ],
          [
            "Distance between ancient and modern cities < 0.1 degrees",
            "Modern cities with population > 1,000,000"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Pearson correlation test",
          "Check for normality of rank and population distributions",
          "Pearson correlation coefficient between parsed rank values and population values",
          "Pearson correlation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pearson correlation test",
            "Check for normality of rank and population distributions"
          ],
          [
            "Pearson correlation coefficient between parsed rank values and population values"
          ],
          [
            "Pearson correlation"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single correlation coefficient",
          "Rounded to 6 decimal places",
          "No additional text or formatting",
          "Output must be a single numeric value",
          "Value must be rounded to exactly 6 decimal places",
          "Format: X.XXXXXX",
          "Round the correlation to 6 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single correlation coefficient",
            "Rounded to 6 decimal places",
            "No additional text or formatting"
          ],
          [
            "Output must be a single numeric value",
            "Value must be rounded to exactly 6 decimal places",
            "Format: X.XXXXXX"
          ],
          [
            "Round the correlation to 6 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.61375
  },
  "astronomy-easy-1": {
    "m_q": {
      "target_metric": {
        "value": "Mean Absolute Error (MAE) of NOAA SWPC's 3-day AP forecast made on March 9, 2025 for the period March 10-12, 2025",
        "confidence": 0.3333333333333333,
        "votes": [
          "Mean Absolute Error (MAE) of NOAA SWPC's 3-day AP forecast made on March 9, 2025 for the period March 10-12, 2025",
          "Mean Absolute Error (MAE) between NOAA SWPC's 3-day forecast of AP index issued on March 9, 2025 and actual AP values for March 10-12, 2025",
          "Mean Absolute Error (MAE) of NOAA SWPC's 3-day AP forecast from March 9, 2025, for the period March 10-12, 2025"
        ]
      },
      "filters": {
        "value": [
          "Date range: March 10-12, 2025",
          "Forecast issued date: March 9, 2025",
          "Forecast issued on March 9, 2025",
          "Forecast period: March 10-12, 2025",
          "3-day forecast only"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date range: March 10-12, 2025",
            "Forecast issued date: March 9, 2025"
          ],
          [
            "Forecast issued on March 9, 2025",
            "Forecast period: March 10-12, 2025",
            "3-day forecast only"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "forecast_day"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "forecast_day"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the forecasted AP values for March 10-12, 2025 from the March 9 forecast?",
          "What are the actual AP values for March 10-12, 2025?",
          "How to calculate MAE from forecast vs actual AP values?",
          "What are the forecasted AP values for March 10, 11, and 12, 2025 from the March 9 forecast?",
          "What are the actual AP values for March 10, 11, and 12, 2025?",
          "What is the absolute error for each day (March 10, 11, 12)?",
          "What is the mean of the three absolute errors?",
          "Extract the AP forecast values for March 10, 11, and 12, 2025 from the '0309geomag_forecast.txt' file.",
          "Determine the actual AP values for March 10, 11, and 12, 2025.  Since these are forecasts, we need to assume that the actual values are in the forecasts issued on March 10, 11, and 12 respectively. Extract the AP forecast values for March 10 from '0310geomag_forecast.txt', March 11 from '0311geomag_forecast.txt', and March 12 from '0312geomag_forecast.txt'.",
          "Calculate the absolute error between the forecasted and actual AP values for each of the three days.",
          "Calculate the mean of the three absolute errors."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What are the forecasted AP values for March 10-12, 2025 from the March 9 forecast?",
            "What are the actual AP values for March 10-12, 2025?",
            "How to calculate MAE from forecast vs actual AP values?"
          ],
          [
            "What are the forecasted AP values for March 10, 11, and 12, 2025 from the March 9 forecast?",
            "What are the actual AP values for March 10, 11, and 12, 2025?",
            "What is the absolute error for each day (March 10, 11, 12)?",
            "What is the mean of the three absolute errors?"
          ],
          [
            "Extract the AP forecast values for March 10, 11, and 12, 2025 from the '0309geomag_forecast.txt' file.",
            "Determine the actual AP values for March 10, 11, and 12, 2025.  Since these are forecasts, we need to assume that the actual values are in the forecasts issued on March 10, 11, and 12 respectively. Extract the AP forecast values for March 10 from '0310geomag_forecast.txt', March 11 from '0311geomag_forecast.txt', and March 12 from '0312geomag_forecast.txt'.",
            "Calculate the absolute error between the forecasted and actual AP values for each of the three days.",
            "Calculate the mean of the three absolute errors."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "0309geomag_forecast.txt",
          "0311geomag_forecast.txt",
          "0312geomag_forecast.txt",
          "0313geomag_forecast.txt"
        ],
        "confidence": 0.9166666666666666,
        "votes": [
          [
            "0309geomag_forecast.txt",
            "0311geomag_forecast.txt",
            "0312geomag_forecast.txt",
            "0313geomag_forecast.txt"
          ],
          [
            "0309geomag_forecast.txt",
            "0311geomag_forecast.txt",
            "0312geomag_forecast.txt",
            "0313geomag_forecast.txt"
          ],
          [
            "0309geomag_forecast.txt",
            "0311geomag_forecast.txt",
            "0312geomag_forecast.txt"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All files have identical column names but different data values",
          "Column names are not descriptive (':Product:' and filename columns)",
          "File structure is non-standard with unclear column headers",
          "Data appears to be in fixed-format text rather than standard CSV",
          "Column names ':Product:' and filename do not represent actual data fields"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All files have identical column names but different data values",
            "Column names are not descriptive (':Product:' and filename columns)"
          ],
          [
            "File structure is non-standard with unclear column headers",
            "Data appears to be in fixed-format text rather than standard CSV",
            "Column names ':Product:' and filename do not represent actual data fields"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "forecast_mar9": "AP index",
          "actual_mar11": "AP index",
          "actual_mar12": "AP index",
          "actual_mar13": "AP index",
          "ap": "dimensionless geomagnetic index",
          "forecast_date": "date (UTC)",
          "target_date": "date (UTC)",
          "ap_value_0309": "unitless AP index",
          "ap_value_0311": "unitless AP index",
          "ap_value_0312": "unitless AP index"
        },
        "confidence": 0.33333333333333337,
        "votes": [
          {
            "forecast_mar9": "AP index",
            "actual_mar11": "AP index",
            "actual_mar12": "AP index",
            "actual_mar13": "AP index"
          },
          {
            "AP": "dimensionless geomagnetic index",
            "forecast_date": "date (UTC)",
            "target_date": "date (UTC)"
          },
          {
            "AP_Value_0309": "unitless AP index",
            "AP_Value_0311": "unitless AP index",
            "AP_Value_0312": "unitless AP index"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "AP values appear to be in standard AP index units (typically 0-400)",
          "AP values appear to be in raw index form (0-400 scale)",
          "Need to verify if values like '025-020-012' represent three sequential day forecasts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "AP values appear to be in standard AP index units (typically 0-400)"
          ],
          [
            "AP values appear to be in raw index form (0-400 scale)",
            "Need to verify if values like '025-020-012' represent three sequential day forecasts"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Need to verify that all files use same AP measurement scale and methodology",
          "Files from different dates may contain forecasts vs actuals",
          "Later files (0311, 0312, 0313) may contain actual observed values for March 10-12"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need to verify that all files use same AP measurement scale and methodology"
          ],
          [
            "Files from different dates may contain forecasts vs actuals",
            "Later files (0311, 0312, 0313) may contain actual observed values for March 10-12"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 0.6666666666666666,
        "votes": [
          true,
          false,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            " ",
            "NA",
            "N/A"
          ],
          [
            "",
            " "
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Forecast period must be exactly March 10-12, 2025",
          "Forecast must be from March 9, 2025 issuance",
          "AP values must be non-negative integers",
          "Forecast must be from March 9, 2025 (0309geomag_forecast.txt)",
          "Target period must be March 10-12, 2025",
          "Must extract 3-day forecast values (025-020-012 format)",
          "Actual values must be obtained from subsequent daily reports",
          "MAE = mean(|forecast - actual|) for all three days",
          "The AP forecast values for March 10, 11, and 12, 2025 are located in specific rows of the '0309geomag_forecast.txt' file. The exact row numbers need to be determined by inspecting the file structure.",
          "The AP values for March 10, 11, and 12 are embedded in the 'Mar' row, and need to be extracted by splitting the string."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Forecast period must be exactly March 10-12, 2025",
            "Forecast must be from March 9, 2025 issuance",
            "AP values must be non-negative integers"
          ],
          [
            "Forecast must be from March 9, 2025 (0309geomag_forecast.txt)",
            "Target period must be March 10-12, 2025",
            "Must extract 3-day forecast values (025-020-012 format)",
            "Actual values must be obtained from subsequent daily reports",
            "MAE = mean(|forecast - actual|) for all three days"
          ],
          [
            "The AP forecast values for March 10, 11, and 12, 2025 are located in specific rows of the '0309geomag_forecast.txt' file. The exact row numbers need to be determined by inspecting the file structure.",
            "The AP values for March 10, 11, and 12 are embedded in the 'Mar' row, and need to be extracted by splitting the string."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract only rows containing AP forecast/actual values",
          "Filter out metadata rows (UTC, month names, blank rows)",
          "Extract forecast values from row containing 'Mar' and hyphenated values in 0309geomag_forecast.txt",
          "Extract actual AP values for March 10, 11, 12 from files 0311, 0312, 0313 respectively",
          "Parse hyphen-separated values to individual daily forecasts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract only rows containing AP forecast/actual values",
            "Filter out metadata rows (UTC, month names, blank rows)"
          ],
          [
            "Extract forecast values from row containing 'Mar' and hyphenated values in 0309geomag_forecast.txt",
            "Extract actual AP values for March 10, 11, 12 from files 0311, 0312, 0313 respectively",
            "Parse hyphen-separated values to individual daily forecasts"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in forecast/actual pairs",
          "Validate AP value ranges (typically 0-400)",
          "Calculate absolute error for each day: |forecast_day - actual_day|",
          "Calculate MAE: (|error_Mar10| + |error_Mar11| + |error_Mar12|) / 3"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in forecast/actual pairs",
            "Validate AP value ranges (typically 0-400)"
          ],
          [
            "Calculate absolute error for each day: |forecast_day - actual_day|",
            "Calculate MAE: (|error_Mar10| + |error_Mar11| + |error_Mar12|) / 3"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "MAE should be calculated as: MAE = mean(|forecast - actual|) for March 10-12",
          "Result should be a single numeric value with appropriate precision",
          "Return single scalar MAE value",
          "MAE should be in same units as AP index",
          "Round to appropriate precision (typically 1-2 decimal places)",
          "The final output should be a single numerical value representing the MAE."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "MAE should be calculated as: MAE = mean(|forecast - actual|) for March 10-12",
            "Result should be a single numeric value with appropriate precision"
          ],
          [
            "Return single scalar MAE value",
            "MAE should be in same units as AP index",
            "Round to appropriate precision (typically 1-2 decimal places)"
          ],
          [
            "The final output should be a single numerical value representing the MAE."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5791666666666668
  },
  "astronomy-easy-2": {
    "m_q": {
      "target_metric": {
        "value": "ratio of peak atmospheric mass density (Orbit Mean Density in kg/m^3) between two time periods: March 14-17, 2014 vs July 18-21, 2018",
        "confidence": 0.3333333333333333,
        "votes": [
          "ratio of peak atmospheric mass density (Orbit Mean Density in kg/m^3) between two time periods: March 14-17, 2014 vs July 18-21, 2018",
          "Ratio of peak atmospheric mass density (March 2014 / July 2018)",
          "Ratio of peak atmospheric mass density experienced by Swarm A satellite during March 14th-17th 2014 vs July 18th-21st 2018"
        ]
      },
      "filters": {
        "value": [
          "Timestamp between 2014-03-14 and 2014-03-17",
          "Timestamp between 2018-07-18 and 2018-07-21",
          "Timestamp between 2014-03-14 and 2014-03-17 (inclusive)",
          "Timestamp between 2018-07-18 and 2018-07-21 (inclusive)",
          "Timestamp between March 14th 2014 and March 17th 2014",
          "Timestamp between July 18th 2018 and July 21st 2018"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Timestamp between 2014-03-14 and 2014-03-17",
            "Timestamp between 2018-07-18 and 2018-07-21"
          ],
          [
            "Timestamp between 2014-03-14 and 2014-03-17 (inclusive)",
            "Timestamp between 2018-07-18 and 2018-07-21 (inclusive)"
          ],
          [
            "Timestamp between March 14th 2014 and March 17th 2014",
            "Timestamp between July 18th 2018 and July 21st 2018"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "year",
          "period"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "year",
            "period"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the maximum Orbit Mean Density during March 14-17, 2014?",
          "What is the maximum Orbit Mean Density during July 18-21, 2018?",
          "What is the ratio: (peak 2014)/(peak 2018)?",
          "What is the maximum value of 'Orbit Mean Density (kg/m^3)' in March 14-17, 2014?",
          "What is the maximum value of 'Orbit Mean Density (kg/m^3)' in July 18-21, 2018?",
          "What is the ratio of the 2014 peak to the 2018 peak?",
          "Find the peak 'Orbit Mean Density (kg/m^3)' during March 14th-17th 2014",
          "Find the peak 'Orbit Mean Density (kg/m^3)' during July 18th-21st 2018",
          "Calculate the ratio of the two peak densities"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the maximum Orbit Mean Density during March 14-17, 2014?",
            "What is the maximum Orbit Mean Density during July 18-21, 2018?",
            "What is the ratio: (peak 2014)/(peak 2018)?"
          ],
          [
            "What is the maximum value of 'Orbit Mean Density (kg/m^3)' in March 14-17, 2014?",
            "What is the maximum value of 'Orbit Mean Density (kg/m^3)' in July 18-21, 2018?",
            "What is the ratio of the 2014 peak to the 2018 peak?"
          ],
          [
            "Find the peak 'Orbit Mean Density (kg/m^3)' during March 14th-17th 2014",
            "Find the peak 'Orbit Mean Density (kg/m^3)' during July 18th-21st 2018",
            "Calculate the ratio of the two peak densities"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "swarma-wu016-20140314_to_20140317.csv",
          "swarma-wu545-20180718_to_20180721.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "swarma-wu016-20140314_to_20140317.csv",
            "swarma-wu545-20180718_to_20180721.csv"
          ],
          [
            "swarma-wu016-20140314_to_20140317.csv",
            "swarma-wu545-20180718_to_20180721.csv"
          ],
          [
            "swarma-wu016-20140314_to_20140317.csv",
            "swarma-wu545-20180718_to_20180721.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Both files have identical column names and dtypes, no schema conflicts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Both files have identical column names and dtypes, no schema conflicts"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "timestamp": "datetime",
          "orbit mean density (kg/m^3)": "kg/m^3"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Timestamp": "datetime",
            "Orbit Mean Density (kg/m^3)": "kg/m^3"
          },
          {
            "Orbit Mean Density (kg/m^3)": "kg/m^3",
            "Timestamp": "ISO 8601 datetime string"
          },
          {
            "Orbit Mean Density (kg/m^3)": "kg/m^3",
            "Timestamp": "datetime"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Density values are extremely small (e-12 to e-13 scale), ensure numerical precision in calculations",
          "Density values are in scientific notation with magnitudes around 1e-12 to 1e-13",
          "2014 density values (~1.17e-12 kg/m^3) are approximately 10x higher than 2018 values (~1.5e-13 kg/m^3)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Density values are extremely small (e-12 to e-13 scale), ensure numerical precision in calculations"
          ],
          [
            "Density values are in scientific notation with magnitudes around 1e-12 to 1e-13",
            "2014 density values (~1.17e-12 kg/m^3) are approximately 10x higher than 2018 values (~1.5e-13 kg/m^3)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Values in 2014 file are ~10x larger than 2018 file (e-12 vs e-13), this appears to be real physical variation rather than measurement conflict",
          "Significant magnitude difference between 2014 and 2018 density measurements suggests different atmospheric conditions or orbital parameters"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Values in 2014 file are ~10x larger than 2018 file (e-12 vs e-13), this appears to be real physical variation rather than measurement conflict"
          ],
          [
            "Significant magnitude difference between 2014 and 2018 density measurements suggests different atmospheric conditions or orbital parameters"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Density values must be positive",
          "Timestamps are at 10-minute intervals",
          "Both periods have exactly 4 days of data (96 hours = 576 ten-minute intervals, but files show 433 rows)",
          "Timestamp must be parseable as datetime",
          "Orbit Mean Density (kg/m^3) must be positive non-zero float",
          "First dataset must contain data only from 2014-03-14 to 2014-03-17",
          "Second dataset must contain data only from 2018-07-18 to 2018-07-21",
          "Timestamp column must be parsed as datetime objects",
          "Ensure that the date ranges are correctly filtered"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Density values must be positive",
            "Timestamps are at 10-minute intervals",
            "Both periods have exactly 4 days of data (96 hours = 576 ten-minute intervals, but files show 433 rows)"
          ],
          [
            "Timestamp must be parseable as datetime",
            "Orbit Mean Density (kg/m^3) must be positive non-zero float",
            "First dataset must contain data only from 2014-03-14 to 2014-03-17",
            "Second dataset must contain data only from 2018-07-18 to 2018-07-21"
          ],
          [
            "Timestamp column must be parsed as datetime objects",
            "Ensure that the date ranges are correctly filtered"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude any rows with non-positive density values",
          "Ensure timestamps are within specified date ranges",
          "Extract maximum density from March 2014 dataset",
          "Extract maximum density from July 2018 dataset"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude any rows with non-positive density values",
            "Ensure timestamps are within specified date ranges"
          ],
          [
            "Extract maximum density from March 2014 dataset",
            "Extract maximum density from July 2018 dataset"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in density measurements",
          "Verify temporal continuity in timestamps"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in density measurements",
            "Verify temporal continuity in timestamps"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Ratio should be reported as a single numeric value",
          "Consider reporting both peak values alongside the ratio",
          "Specify which period's peak is numerator vs denominator",
          "Output should be a single scalar ratio value",
          "Ratio format: peak_density_2014 / peak_density_2018",
          "Result should be dimensionless (units cancel out)",
          "The ratio should be a single numerical value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Ratio should be reported as a single numeric value",
            "Consider reporting both peak values alongside the ratio",
            "Specify which period's peak is numerator vs denominator"
          ],
          [
            "Output should be a single scalar ratio value",
            "Ratio format: peak_density_2014 / peak_density_2018",
            "Result should be dimensionless (units cancel out)"
          ],
          [
            "The ratio should be a single numerical value"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6166666666666667
  },
  "astronomy-easy-3": {
    "m_q": {
      "target_metric": {
        "value": "average atmospheric density (kg/m^3) measured between 450km and 500km altitude at 00:00 of each day in 2015",
        "confidence": 0.3333333333333333,
        "votes": [
          "average atmospheric density (kg/m^3) measured between 450km and 500km altitude at 00:00 of each day in 2015",
          "average atmospheric density measured between 450km and 500km altitude during 2015 for data points at 00:00 of each day",
          ""
        ]
      },
      "filters": {
        "value": [
          "Timestamp at 00:00:00",
          "Year = 2015",
          "Altitude between 450km and 500km",
          "Exclude n/a values",
          "Exclude 9.99E32 values",
          "Altitude (km) >= 450 AND Altitude (km) <= 500",
          "Timestamp year = 2015",
          "Timestamp time = 00:00:00",
          "Orbit Mean Density (kg/m^3) != 9.99E32",
          "Orbit Mean Density (kg/m^3) IS NOT NULL"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Timestamp at 00:00:00",
            "Year = 2015",
            "Altitude between 450km and 500km",
            "Exclude n/a values",
            "Exclude 9.99E32 values"
          ],
          [
            "Altitude (km) >= 450 AND Altitude (km) <= 500",
            "Timestamp year = 2015",
            "Timestamp time = 00:00:00",
            "Orbit Mean Density (kg/m^3) != 9.99E32",
            "Orbit Mean Density (kg/m^3) IS NOT NULL"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Date (daily)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date (daily)"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          ""
        ]
      },
      "sub_questions": {
        "value": [
          "Identify valid data points at 00:00 each day",
          "Filter by altitude range",
          "Clean density measurements",
          "Calculate daily averages",
          "Compute overall average across 2015",
          "Which file IDs in initial states file correspond to dates in 2015?",
          "Which initial state records have altitude between 450-500km at 00:00:00?",
          "Which satellite density files contain measurements for these dates?",
          "How to join initial states with density measurements by timestamp?",
          "What are the valid density values after cleaning sentinel values?",
          "What is the average of the cleaned density measurements?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Identify valid data points at 00:00 each day",
            "Filter by altitude range",
            "Clean density measurements",
            "Calculate daily averages",
            "Compute overall average across 2015"
          ],
          [
            "Which file IDs in initial states file correspond to dates in 2015?",
            "Which initial state records have altitude between 450-500km at 00:00:00?",
            "Which satellite density files contain measurements for these dates?",
            "How to join initial states with density measurements by timestamp?",
            "What are the valid density values after cleaning sentinel values?",
            "What is the average of the cleaned density measurements?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "wu001_to_wu715-initial_states.csv",
          "swarma-wu113-20141230_to_20150102.csv",
          "swarma-wu114-20150102_to_20150105.csv",
          "swarma-wu115-20150105_to_20150108.csv",
          "swarma-wu116-20150108_to_20150111.csv",
          "swarma-wu117-20150111_to_20150114.csv",
          "swarma-wu118-20150114_to_20150117.csv",
          "swarma-wu119-20150117_to_20150120.csv",
          "swarma-wu120-20150120_to_20150123.csv",
          "swarma-wu121-20150123_to_20150126.csv",
          "swarma-wu122-20150126_to_20150129.csv",
          "swarma-wu123-20150129_to_20150201.csv",
          "swarma-wu124-20150201_to_20150204.csv",
          "swarma-wu125-20150204_to_20150207.csv",
          "swarma-wu126-20150207_to_20150210.csv",
          "swarma-wu127-20150210_to_20150213.csv",
          "swarma-wu128-20150213_to_20150216.csv",
          "swarma-wu129-20150216_to_20150219.csv",
          "swarma-wu130-20150219_to_20150222.csv",
          "swarma-wu131-20150222_to_20150225.csv",
          "swarma-wu132-20150225_to_20150228.csv",
          "swarma-wu133-20150228_to_20150303.csv",
          "swarma-wu134-20150303_to_20150306.csv",
          "swarma-wu135-20150306_to_20150309.csv",
          "swarma-wu136-20150309_to_20150312.csv",
          "swarma-wu137-20150312_to_20150315.csv",
          "swarma-wu138-20150315_to_20150318.csv",
          "swarma-wu139-20150318_to_20150321.csv",
          "swarma-wu140-20150321_to_20150324.csv",
          "swarma-wu141-20150324_to_20150327.csv",
          "swarma-wu142-20150327_to_20150330.csv",
          "swarma-wu143-20150330_to_20150402.csv",
          "swarma-wu144-20150402_to_20150405.csv",
          "swarma-wu145-20150405_to_20150408.csv",
          "swarma-wu146-20150408_to_20150411.csv",
          "swarma-wu147-20150411_to_20150414.csv",
          "swarma-wu148-20150414_to_20150417.csv",
          "swarma-wu149-20150417_to_20150420.csv",
          "swarma-wu150-20150420_to_20150423.csv",
          "swarma-wu151-20150423_to_20150426.csv",
          "swarma-wu152-20150426_to_20150429.csv",
          "swarma-wu153-20150429_to_20150502.csv",
          "swarma-wu154-20150502_to_20150505.csv",
          "swarma-wu155-20150505_to_20150508.csv",
          "swarma-wu156-20150508_to_20150511.csv",
          "swarma-wu157-20150511_to_20150514.csv",
          "swarma-wu158-20150514_to_20150517.csv",
          "swarma-wu159-20150517_to_20150520.csv",
          "swarma-wu160-20150520_to_20150523.csv",
          "swarma-wu161-20150523_to_20150526.csv",
          "swarma-wu162-20150526_to_20150529.csv",
          "swarma-wu163-20150529_to_20150601.csv",
          "swarma-wu164-20150601_to_20150604.csv",
          "swarma-wu165-20150604_to_20150607.csv",
          "swarma-wu166-20150607_to_20150610.csv",
          "swarma-wu167-20150610_to_20150613.csv",
          "swarma-wu168-20150613_to_20150616.csv",
          "swarma-wu169-20150616_to_20150619.csv",
          "swarma-wu170-20150619_to_20150622.csv",
          "swarma-wu171-20150622_to_20150625.csv",
          "swarma-wu172-20150625_to_20150628.csv",
          "swarma-wu173-20150628_to_20150701.csv",
          "swarma-wu174-20150701_to_20150704.csv",
          "swarma-wu175-20150704_to_20150707.csv",
          "swarma-wu176-20150707_to_20150710.csv",
          "swarma-wu177-20150710_to_20150713.csv",
          "swarma-wu178-20150713_to_20150716.csv",
          "swarma-wu179-20150716_to_20150719.csv",
          "swarma-wu180-20150719_to_20150722.csv",
          "swarma-wu181-20150722_to_20150725.csv",
          "swarma-wu182-20150725_to_20150728.csv",
          "swarma-wu183-20150728_to_20150731.csv",
          "swarma-wu184-20150731_to_20150803.csv",
          "swarma-wu185-20150803_to_20150806.csv",
          "swarma-wu186-20150806_to_20150809.csv",
          "swarma-wu187-20150809_to_20150812.csv",
          "swarma-wu188-20150812_to_20150815.csv",
          "swarma-wu189-20150815_to_20150818.csv",
          "swarma-wu190-20150818_to_20150821.csv",
          "swarma-wu191-20150821_to_20150824.csv",
          "swarma-wu192-20150824_to_20150827.csv",
          "swarma-wu193-20150827_to_20150830.csv",
          "swarma-wu194-20150830_to_20150902.csv",
          "swarma-wu195-20150902_to_20150905.csv",
          "swarma-wu196-20150905_to_20150908.csv",
          "swarma-wu197-20150908_to_20150911.csv",
          "swarma-wu198-20150911_to_20150914.csv",
          "swarma-wu199-20150914_to_20150917.csv",
          "swarma-wu200-20150917_to_20150920.csv",
          "swarma-wu201-20150920_to_20150923.csv",
          "swarma-wu202-20150923_to_20150926.csv",
          "swarma-wu203-20150926_to_20150929.csv",
          "swarma-wu204-20150929_to_20151002.csv",
          "swarma-wu205-20151002_to_20151005.csv",
          "swarma-wu206-20151005_to_20151008.csv",
          "swarma-wu207-20151008_to_20151011.csv",
          "swarma-wu208-20151011_to_20151014.csv",
          "swarma-wu209-20151014_to_20151017.csv",
          "swarma-wu210-20151017_to_20151020.csv",
          "swarma-wu211-20151020_to_20151023.csv",
          "swarma-wu212-20151023_to_20151026.csv",
          "swarma-wu213-20151026_to_20151029.csv",
          "swarma-wu214-20151029_to_20151101.csv",
          "swarma-wu215-20151101_to_20151104.csv",
          "swarma-wu216-20151104_to_20151107.csv",
          "swarma-wu217-20151107_to_20151110.csv",
          "swarma-wu218-20151110_to_20151113.csv",
          "swarma-wu219-20151113_to_20151116.csv",
          "swarma-wu220-20151116_to_20151119.csv",
          "swarma-wu221-20151119_to_20151122.csv",
          "swarma-wu222-20151122_to_20151125.csv",
          "swarma-wu223-20151125_to_20151128.csv",
          "swarma-wu224-20151128_to_20151201.csv",
          "swarma-wu225-20151201_to_20151204.csv",
          "swarma-wu226-20151204_to_20151207.csv",
          "swarma-wu227-20151207_to_20151210.csv",
          "swarma-wu228-20151210_to_20151213.csv",
          "swarma-wu229-20151213_to_20151216.csv",
          "swarma-wu230-20151216_to_20151219.csv",
          "swarma-wu231-20151219_to_20151222.csv",
          "swarma-wu232-20151222_to_20151225.csv",
          "swarma-wu233-20151225_to_20151228.csv",
          "swarma-wu234-20151228_to_20151231.csv",
          "swarma-wu235-20151231_to_20160103.csv"
        ],
        "confidence": 0.666666666666667,
        "votes": [
          [
            "wu001_to_wu715-initial_states.csv",
            "swarma-wu113-20141230_to_20150102.csv",
            "swarma-wu114-20150102_to_20150105.csv",
            "swarma-wu115-20150105_to_20150108.csv",
            "swarma-wu116-20150108_to_20150111.csv",
            "swarma-wu117-20150111_to_20150114.csv",
            "swarma-wu118-20150114_to_20150117.csv",
            "swarma-wu119-20150117_to_20150120.csv",
            "swarma-wu120-20150120_to_20150123.csv",
            "swarma-wu121-20150123_to_20150126.csv",
            "swarma-wu122-20150126_to_20150129.csv",
            "swarma-wu123-20150129_to_20150201.csv",
            "swarma-wu124-20150201_to_20150204.csv",
            "swarma-wu125-20150204_to_20150207.csv",
            "swarma-wu126-20150207_to_20150210.csv",
            "swarma-wu127-20150210_to_20150213.csv",
            "swarma-wu128-20150213_to_20150216.csv",
            "swarma-wu129-20150216_to_20150219.csv",
            "swarma-wu130-20150219_to_20150222.csv",
            "swarma-wu131-20150222_to_20150225.csv",
            "swarma-wu132-20150225_to_20150228.csv",
            "swarma-wu133-20150228_to_20150303.csv",
            "swarma-wu134-20150303_to_20150306.csv",
            "swarma-wu135-20150306_to_20150309.csv",
            "swarma-wu136-20150309_to_20150312.csv",
            "swarma-wu137-20150312_to_20150315.csv",
            "swarma-wu138-20150315_to_20150318.csv",
            "swarma-wu139-20150318_to_20150321.csv",
            "swarma-wu140-20150321_to_20150324.csv",
            "swarma-wu141-20150324_to_20150327.csv",
            "swarma-wu142-20150327_to_20150330.csv",
            "swarma-wu143-20150330_to_20150402.csv",
            "swarma-wu144-20150402_to_20150405.csv",
            "swarma-wu145-20150405_to_20150408.csv",
            "swarma-wu146-20150408_to_20150411.csv",
            "swarma-wu147-20150411_to_20150414.csv",
            "swarma-wu148-20150414_to_20150417.csv",
            "swarma-wu149-20150417_to_20150420.csv",
            "swarma-wu150-20150420_to_20150423.csv",
            "swarma-wu151-20150423_to_20150426.csv",
            "swarma-wu152-20150426_to_20150429.csv",
            "swarma-wu153-20150429_to_20150502.csv",
            "swarma-wu154-20150502_to_20150505.csv",
            "swarma-wu155-20150505_to_20150508.csv",
            "swarma-wu156-20150508_to_20150511.csv",
            "swarma-wu157-20150511_to_20150514.csv",
            "swarma-wu158-20150514_to_20150517.csv",
            "swarma-wu159-20150517_to_20150520.csv",
            "swarma-wu160-20150520_to_20150523.csv",
            "swarma-wu161-20150523_to_20150526.csv",
            "swarma-wu162-20150526_to_20150529.csv",
            "swarma-wu163-20150529_to_20150601.csv",
            "swarma-wu164-20150601_to_20150604.csv",
            "swarma-wu165-20150604_to_20150607.csv",
            "swarma-wu166-20150607_to_20150610.csv",
            "swarma-wu167-20150610_to_20150613.csv",
            "swarma-wu168-20150613_to_20150616.csv",
            "swarma-wu169-20150616_to_20150619.csv",
            "swarma-wu170-20150619_to_20150622.csv",
            "swarma-wu171-20150622_to_20150625.csv",
            "swarma-wu172-20150625_to_20150628.csv",
            "swarma-wu173-20150628_to_20150701.csv",
            "swarma-wu174-20150701_to_20150704.csv",
            "swarma-wu175-20150704_to_20150707.csv",
            "swarma-wu176-20150707_to_20150710.csv",
            "swarma-wu177-20150710_to_20150713.csv",
            "swarma-wu178-20150713_to_20150716.csv",
            "swarma-wu179-20150716_to_20150719.csv",
            "swarma-wu180-20150719_to_20150722.csv",
            "swarma-wu181-20150722_to_20150725.csv",
            "swarma-wu182-20150725_to_20150728.csv",
            "swarma-wu183-20150728_to_20150731.csv",
            "swarma-wu184-20150731_to_20150803.csv",
            "swarma-wu185-20150803_to_20150806.csv",
            "swarma-wu186-20150806_to_20150809.csv",
            "swarma-wu187-20150809_to_20150812.csv",
            "swarma-wu188-20150812_to_20150815.csv",
            "swarma-wu189-20150815_to_20150818.csv",
            "swarma-wu190-20150818_to_20150821.csv",
            "swarma-wu191-20150821_to_20150824.csv",
            "swarma-wu192-20150824_to_20150827.csv",
            "swarma-wu193-20150827_to_20150830.csv",
            "swarma-wu194-20150830_to_20150902.csv",
            "swarma-wu195-20150902_to_20150905.csv",
            "swarma-wu196-20150905_to_20150908.csv",
            "swarma-wu197-20150908_to_20150911.csv",
            "swarma-wu198-20150911_to_20150914.csv",
            "swarma-wu199-20150914_to_20150917.csv",
            "swarma-wu200-20150917_to_20150920.csv",
            "swarma-wu201-20150920_to_20150923.csv",
            "swarma-wu202-20150923_to_20150926.csv",
            "swarma-wu203-20150926_to_20150929.csv",
            "swarma-wu204-20150929_to_20151002.csv",
            "swarma-wu205-20151002_to_20151005.csv",
            "swarma-wu206-20151005_to_20151008.csv",
            "swarma-wu207-20151008_to_20151011.csv",
            "swarma-wu208-20151011_to_20151014.csv",
            "swarma-wu209-20151014_to_20151017.csv",
            "swarma-wu210-20151017_to_20151020.csv",
            "swarma-wu211-20151020_to_20151023.csv",
            "swarma-wu212-20151023_to_20151026.csv",
            "swarma-wu213-20151026_to_20151029.csv",
            "swarma-wu214-20151029_to_20151101.csv",
            "swarma-wu215-20151101_to_20151104.csv",
            "swarma-wu216-20151104_to_20151107.csv",
            "swarma-wu217-20151107_to_20151110.csv",
            "swarma-wu218-20151110_to_20151113.csv",
            "swarma-wu219-20151113_to_20151116.csv",
            "swarma-wu220-20151116_to_20151119.csv",
            "swarma-wu221-20151119_to_20151122.csv",
            "swarma-wu222-20151122_to_20151125.csv",
            "swarma-wu223-20151125_to_20151128.csv",
            "swarma-wu224-20151128_to_20151201.csv",
            "swarma-wu225-20151201_to_20151204.csv",
            "swarma-wu226-20151204_to_20151207.csv",
            "swarma-wu227-20151207_to_20151210.csv",
            "swarma-wu228-20151210_to_20151213.csv",
            "swarma-wu229-20151213_to_20151216.csv",
            "swarma-wu230-20151216_to_20151219.csv",
            "swarma-wu231-20151219_to_20151222.csv",
            "swarma-wu232-20151222_to_20151225.csv",
            "swarma-wu233-20151225_to_20151228.csv",
            "swarma-wu234-20151228_to_20151231.csv",
            "swarma-wu235-20151231_to_20160103.csv"
          ],
          [
            "wu001_to_wu715-initial_states.csv",
            "swarma-wu113-20141230_to_20150102.csv",
            "swarma-wu114-20150102_to_20150105.csv",
            "swarma-wu115-20150105_to_20150108.csv",
            "swarma-wu116-20150108_to_20150111.csv",
            "swarma-wu117-20150111_to_20150114.csv",
            "swarma-wu118-20150114_to_20150117.csv",
            "swarma-wu119-20150117_to_20150120.csv",
            "swarma-wu120-20150120_to_20150123.csv",
            "swarma-wu121-20150123_to_20150126.csv",
            "swarma-wu122-20150126_to_20150129.csv",
            "swarma-wu123-20150129_to_20150201.csv",
            "swarma-wu124-20150201_to_20150204.csv",
            "swarma-wu125-20150204_to_20150207.csv",
            "swarma-wu126-20150207_to_20150210.csv",
            "swarma-wu127-20150210_to_20150213.csv",
            "swarma-wu128-20150213_to_20150216.csv",
            "swarma-wu129-20150216_to_20150219.csv",
            "swarma-wu130-20150219_to_20150222.csv",
            "swarma-wu131-20150222_to_20150225.csv",
            "swarma-wu132-20150225_to_20150228.csv",
            "swarma-wu133-20150228_to_20150303.csv",
            "swarma-wu134-20150303_to_20150306.csv",
            "swarma-wu135-20150306_to_20150309.csv",
            "swarma-wu136-20150309_to_20150312.csv",
            "swarma-wu137-20150312_to_20150315.csv",
            "swarma-wu138-20150315_to_20150318.csv",
            "swarma-wu139-20150318_to_20150321.csv",
            "swarma-wu140-20150321_to_20150324.csv",
            "swarma-wu141-20150324_to_20150327.csv",
            "swarma-wu142-20150327_to_20150330.csv",
            "swarma-wu143-20150330_to_20150402.csv",
            "swarma-wu144-20150402_to_20150405.csv",
            "swarma-wu145-20150405_to_20150408.csv",
            "swarma-wu146-20150408_to_20150411.csv",
            "swarma-wu147-20150411_to_20150414.csv",
            "swarma-wu148-20150414_to_20150417.csv",
            "swarma-wu149-20150417_to_20150420.csv",
            "swarma-wu150-20150420_to_20150423.csv",
            "swarma-wu151-20150423_to_20150426.csv",
            "swarma-wu152-20150426_to_20150429.csv",
            "swarma-wu153-20150429_to_20150502.csv",
            "swarma-wu154-20150502_to_20150505.csv",
            "swarma-wu155-20150505_to_20150508.csv",
            "swarma-wu156-20150508_to_20150511.csv",
            "swarma-wu157-20150511_to_20150514.csv",
            "swarma-wu158-20150514_to_20150517.csv",
            "swarma-wu159-20150517_to_20150520.csv",
            "swarma-wu160-20150520_to_20150523.csv",
            "swarma-wu161-20150523_to_20150526.csv",
            "swarma-wu162-20150526_to_20150529.csv",
            "swarma-wu163-20150529_to_20150601.csv",
            "swarma-wu164-20150601_to_20150604.csv",
            "swarma-wu165-20150604_to_20150607.csv",
            "swarma-wu166-20150607_to_20150610.csv",
            "swarma-wu167-20150610_to_20150613.csv",
            "swarma-wu168-20150613_to_20150616.csv",
            "swarma-wu169-20150616_to_20150619.csv",
            "swarma-wu170-20150619_to_20150622.csv",
            "swarma-wu171-20150622_to_20150625.csv",
            "swarma-wu172-20150625_to_20150628.csv",
            "swarma-wu173-20150628_to_20150701.csv",
            "swarma-wu174-20150701_to_20150704.csv",
            "swarma-wu175-20150704_to_20150707.csv",
            "swarma-wu176-20150707_to_20150710.csv",
            "swarma-wu177-20150710_to_20150713.csv",
            "swarma-wu178-20150713_to_20150716.csv",
            "swarma-wu179-20150716_to_20150719.csv",
            "swarma-wu180-20150719_to_20150722.csv",
            "swarma-wu181-20150722_to_20150725.csv",
            "swarma-wu182-20150725_to_20150728.csv",
            "swarma-wu183-20150728_to_20150731.csv",
            "swarma-wu184-20150731_to_20150803.csv",
            "swarma-wu185-20150803_to_20150806.csv",
            "swarma-wu186-20150806_to_20150809.csv",
            "swarma-wu187-20150809_to_20150812.csv",
            "swarma-wu188-20150812_to_20150815.csv",
            "swarma-wu189-20150815_to_20150818.csv",
            "swarma-wu190-20150818_to_20150821.csv",
            "swarma-wu191-20150821_to_20150824.csv",
            "swarma-wu192-20150824_to_20150827.csv",
            "swarma-wu193-20150827_to_20150830.csv",
            "swarma-wu194-20150830_to_20150902.csv",
            "swarma-wu195-20150902_to_20150905.csv",
            "swarma-wu196-20150905_to_20150908.csv",
            "swarma-wu197-20150908_to_20150911.csv",
            "swarma-wu198-20150911_to_20150914.csv",
            "swarma-wu199-20150914_to_20150917.csv",
            "swarma-wu200-20150917_to_20150920.csv",
            "swarma-wu201-20150920_to_20150923.csv",
            "swarma-wu202-20150923_to_20150926.csv",
            "swarma-wu203-20150926_to_20150929.csv",
            "swarma-wu204-20150929_to_20151002.csv",
            "swarma-wu205-20151002_to_20151005.csv",
            "swarma-wu206-20151005_to_20151008.csv",
            "swarma-wu207-20151008_to_20151011.csv",
            "swarma-wu208-20151011_to_20151014.csv",
            "swarma-wu209-20151014_to_20151017.csv",
            "swarma-wu210-20151017_to_20151020.csv",
            "swarma-wu211-20151020_to_20151023.csv",
            "swarma-wu212-20151023_to_20151026.csv",
            "swarma-wu213-20151026_to_20151029.csv",
            "swarma-wu214-20151029_to_20151101.csv",
            "swarma-wu215-20151101_to_20151104.csv",
            "swarma-wu216-20151104_to_20151107.csv",
            "swarma-wu217-20151107_to_20151110.csv",
            "swarma-wu218-20151110_to_20151113.csv",
            "swarma-wu219-20151113_to_20151116.csv",
            "swarma-wu220-20151116_to_20151119.csv",
            "swarma-wu221-20151119_to_20151122.csv",
            "swarma-wu222-20151122_to_20151125.csv",
            "swarma-wu223-20151125_to_20151128.csv",
            "swarma-wu224-20151128_to_20151201.csv",
            "swarma-wu225-20151201_to_20151204.csv",
            "swarma-wu226-20151204_to_20151207.csv",
            "swarma-wu227-20151207_to_20151210.csv",
            "swarma-wu228-20151210_to_20151213.csv",
            "swarma-wu229-20151213_to_20151216.csv",
            "swarma-wu230-20151216_to_20151219.csv",
            "swarma-wu231-20151219_to_20151222.csv",
            "swarma-wu232-20151222_to_20151225.csv",
            "swarma-wu233-20151225_to_20151228.csv",
            "swarma-wu234-20151228_to_20151231.csv",
            "swarma-wu235-20151231_to_20160103.csv"
          ],
          []
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple density files with same column structure but different date ranges",
          "Initial states file has different schema than density files",
          "Initial states file has Altitude (km) but density files do not",
          "Density files have Orbit Mean Density (kg/m^3) but initial states file does not"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple density files with same column structure but different date ranges",
            "Initial states file has different schema than density files"
          ],
          [
            "Initial states file has Altitude (km) but density files do not",
            "Density files have Orbit Mean Density (kg/m^3) but initial states file does not"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "semi-major axis (km)": "km",
          "altitude (km)": "km",
          "orbit mean density (kg/m^3)": "kg/m^3",
          "inclination (deg)": "degrees",
          "raan (deg)": "degrees",
          "argument of perigee (deg)": "degrees",
          "true anomaly (deg)": "degrees",
          "latitude (deg)": "degrees",
          "longitude (deg)": "degrees"
        },
        "confidence": 0.40740740740740744,
        "votes": [
          {
            "Semi-major Axis (km)": "km",
            "Altitude (km)": "km",
            "Orbit Mean Density (kg/m^3)": "kg/m^3",
            "Inclination (deg)": "degrees",
            "RAAN (deg)": "degrees",
            "Argument of Perigee (deg)": "degrees",
            "True Anomaly (deg)": "degrees",
            "Latitude (deg)": "degrees",
            "Longitude (deg)": "degrees"
          },
          {
            "Altitude (km)": "kilometers",
            "Orbit Mean Density (kg/m^3)": "kilograms per cubic meter"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Density values in scientific notation (e-12 to e-13 scale)",
          "Altitude values in km, need to filter 450-500km range",
          "Altitude values in kilometers range 450-500",
          "Density values in scientific notation (e.g., 1.5e-12)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Density values in scientific notation (e-12 to e-13 scale)",
            "Altitude values in km, need to filter 450-500km range"
          ],
          [
            "Altitude values in kilometers range 450-500",
            "Density values in scientific notation (e.g., 1.5e-12)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Density files have 10-minute intervals, need to extract only 00:00 timestamps",
          "Some density files have missing values (empty cells) that need cleaning",
          "Altitude only available in initial states file, density only in satellite measurement files",
          "Need to match timestamps exactly between files to correlate altitude with density"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Density files have 10-minute intervals, need to extract only 00:00 timestamps",
            "Some density files have missing values (empty cells) that need cleaning"
          ],
          [
            "Altitude only available in initial states file, density only in satellite measurement files",
            "Need to match timestamps exactly between files to correlate altitude with density"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "9.99E32"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "9.99E32"
          ],
          [
            "",
            "n/a",
            "9.99E32",
            "9.99e32"
          ],
          []
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 0.33,
        "votes": [
          11.0,
          2.0,
          0.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only use data from 2015",
          "Only use measurements at exactly 00:00:00",
          "Altitude must be between 450km and 500km inclusive",
          "Density values must be valid numbers (not sentinel values)",
          "Altitude must be between 450 and 500 km inclusive",
          "Year must be 2015",
          "Time must be exactly 00:00:00",
          "Density values must not be 9.99E32",
          "Density values must not be null or empty"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only use data from 2015",
            "Only use measurements at exactly 00:00:00",
            "Altitude must be between 450km and 500km inclusive",
            "Density values must be valid numbers (not sentinel values)"
          ],
          [
            "Altitude must be between 450 and 500 km inclusive",
            "Year must be 2015",
            "Time must be exactly 00:00:00",
            "Density values must not be 9.99E32",
            "Density values must not be null or empty"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Extract date part from Timestamp",
          "Filter by year = 2015",
          "Filter by time = 00:00:00",
          "Filter altitude range",
          "Remove density values equal to 9.99E32",
          "Remove empty/missing density values",
          "Extract year from Timestamp column = 2015",
          "Extract time from Timestamp column = 00:00:00",
          "Filter Orbit Mean Density != 9.99E32 and IS NOT NULL"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract date part from Timestamp",
            "Filter by year = 2015",
            "Filter by time = 00:00:00",
            "Filter altitude range",
            "Remove density values equal to 9.99E32",
            "Remove empty/missing density values"
          ],
          [
            "Extract year from Timestamp column = 2015",
            "Extract time from Timestamp column = 00:00:00",
            "Filter Orbit Mean Density != 9.99E32 and IS NOT NULL"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in density measurements",
          "Verify normal distribution of density values",
          "Test for seasonal patterns in 2015 data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in density measurements",
            "Verify normal distribution of density values",
            "Test for seasonal patterns in 2015 data"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single average value in kg/m^3",
          "Scientific notation acceptable",
          "Include confidence interval if possible",
          "Return single scalar value representing average density",
          "Unit should be kg/m^3",
          "Precision should match input data (scientific notation with appropriate significant figures)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single average value in kg/m^3",
            "Scientific notation acceptable",
            "Include confidence interval if possible"
          ],
          [
            "Return single scalar value representing average density",
            "Unit should be kg/m^3",
            "Precision should match input data (scientific notation with appropriate significant figures)"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5202037037037038
  },
  "astronomy-easy-4": {
    "m_q": {
      "target_metric": {
        "value": "solar activity cycles period and top five years of minimum and maximum activity",
        "confidence": 0.3333333333333333,
        "votes": [
          "solar activity cycles period and top five years of minimum and maximum activity",
          "Solar activity cycle period (in years), top 5 years of maximum sunspot activity, and top 5 years of minimum sunspot activity between 1960 and 2020",
          "Solar activity cycle period and top five years of minimum and maximum activity based on sunspot numbers between 1960 and 2020."
        ]
      },
      "filters": {
        "value": [
          "year between 1960 and 2020",
          "peaks with prominence >= 20 and distance >= 5",
          "troughs with similar prominence and distance rules",
          "Year >= 1960",
          "Year <= 2020",
          "Years between 1960 and 2020",
          "Minimums with prominence >= 20 and distance >= 5"
        ],
        "confidence": 0.38095238095238093,
        "votes": [
          [
            "year between 1960 and 2020",
            "peaks with prominence >= 20 and distance >= 5",
            "troughs with similar prominence and distance rules"
          ],
          [
            "Year >= 1960",
            "Year <= 2020"
          ],
          [
            "Years between 1960 and 2020",
            "Peaks with prominence >= 20 and distance >= 5",
            "Minimums with prominence >= 20 and distance >= 5"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "year"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "calculate approximate period of solar cycles",
          "identify top 5 maximum activity years",
          "identify top 5 minimum activity years",
          "What is the average period between consecutive solar activity cycles?",
          "Which 5 years had the highest sunspot numbers (peaks) with prominence >= 20 and distance >= 5?",
          "Which 5 years had the lowest sunspot numbers (troughs) with prominence >= 20 and distance >= 5?",
          "How are peaks and troughs identified using prominence and distance parameters?",
          "Determine the yearly mean sunspot numbers.",
          "Identify peaks (maximums) in sunspot numbers with prominence >= 20 and distance >= 5.",
          "Identify minimums in sunspot numbers with prominence >= 20 and distance >= 5.",
          "Calculate the period of solar activity cycles based on the identified peaks/minimums.",
          "Rank the years of minimum and maximum activity based on sunspot numbers.",
          "Select the top five years of minimum and maximum activity."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "calculate approximate period of solar cycles",
            "identify top 5 maximum activity years",
            "identify top 5 minimum activity years"
          ],
          [
            "What is the average period between consecutive solar activity cycles?",
            "Which 5 years had the highest sunspot numbers (peaks) with prominence >= 20 and distance >= 5?",
            "Which 5 years had the lowest sunspot numbers (troughs) with prominence >= 20 and distance >= 5?",
            "How are peaks and troughs identified using prominence and distance parameters?"
          ],
          [
            "Determine the yearly mean sunspot numbers.",
            "Identify peaks (maximums) in sunspot numbers with prominence >= 20 and distance >= 5.",
            "Identify minimums in sunspot numbers with prominence >= 20 and distance >= 5.",
            "Calculate the period of solar activity cycles based on the identified peaks/minimums.",
            "Rank the years of minimum and maximum activity based on sunspot numbers.",
            "Select the top five years of minimum and maximum activity."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "SN_y_tot_V2.0.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "SN_y_tot_V2.0.csv"
          ],
          [
            "SN_y_tot_V2.0.csv"
          ],
          [
            "SN_y_tot_V2.0.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "single column contains multiple data fields separated by semicolons",
          "Single column contains multiple fields separated by semicolons instead of proper CSV structure"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "single column contains multiple data fields separated by semicolons"
          ],
          [
            "Single column contains multiple fields separated by semicolons instead of proper CSV structure"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "decimal year",
          "sunspot_number": "yearly mean",
          "uncertainty": "standard deviation",
          "observations": "count",
          "definitive_provisional_flag": "categorical",
          "mean_sunspot_number": "yearly mean sunspot number (dimensionless count)",
          "std_deviation": "standard deviation",
          "num_observations": "number of observations",
          "definitive_provisional": "flag (1=definitive, 0=provisional)",
          "data": "Yearly mean sunspot number"
        },
        "confidence": 0.3666666666666667,
        "votes": [
          {
            "year": "decimal year",
            "sunspot_number": "yearly mean",
            "uncertainty": "standard deviation",
            "observations": "count",
            "definitive_provisional_flag": "categorical"
          },
          {
            "year": "decimal year (mid-year)",
            "mean_sunspot_number": "yearly mean sunspot number (dimensionless count)",
            "std_deviation": "standard deviation",
            "num_observations": "number of observations",
            "definitive_provisional": "flag (1=definitive, 0=provisional)"
          },
          {
            "data": "Yearly mean sunspot number"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "year values are decimal (e.g., 1700.5)",
          "negative sunspot numbers may indicate missing data",
          "Year is given as decimal (e.g., 1700.5) representing mid-year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "year values are decimal (e.g., 1700.5)",
            "negative sunspot numbers may indicate missing data"
          ],
          [
            "Year is given as decimal (e.g., 1700.5) representing mid-year"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": false,
        "confidence": 1.0,
        "votes": [
          false,
          false,
          false
        ]
      },
      "delimiter": {
        "value": ";",
        "confidence": 1.0,
        "votes": [
          ";",
          ";",
          ";"
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "-1.0",
          "-1"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "-1.0",
            "-1"
          ],
          [
            "-1",
            "-1.0"
          ],
          [
            "-1"
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "year range: 1700.5 to 2023.5 (based on 324 rows)",
          "sunspot numbers should be non-negative for valid observations",
          "flag values likely indicate data quality",
          "Year >= 1960 AND Year <= 2020",
          "Peaks must have prominence >= 20",
          "Peaks must have distance >= 5 years from neighboring peaks",
          "Troughs (minima) must have prominence >= 20",
          "Troughs must have distance >= 5 years from neighboring troughs",
          "Year must be between 1960 and 2020.",
          "Prominence of peaks must be >= 20.",
          "Distance between peaks must be >= 5.",
          "Prominence of minimums must be >= 20.",
          "Distance between minimums must be >= 5."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "year range: 1700.5 to 2023.5 (based on 324 rows)",
            "sunspot numbers should be non-negative for valid observations",
            "flag values likely indicate data quality"
          ],
          [
            "Year >= 1960 AND Year <= 2020",
            "Peaks must have prominence >= 20",
            "Peaks must have distance >= 5 years from neighboring peaks",
            "Troughs (minima) must have prominence >= 20",
            "Troughs must have distance >= 5 years from neighboring troughs"
          ],
          [
            "Year must be between 1960 and 2020.",
            "Prominence of peaks must be >= 20.",
            "Distance between peaks must be >= 5.",
            "Prominence of minimums must be >= 20.",
            "Distance between minimums must be >= 5."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "filter to years 1960-2020",
          "exclude rows with negative sunspot numbers or uncertainty values",
          "Convert decimal year (e.g., 1960.5) to integer year (1960) for filtering",
          "Filter out rows with sentinel values (-1, -1.0) if they affect analysis",
          "Year >= 1960",
          "Year <= 2020"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "filter to years 1960-2020",
            "exclude rows with negative sunspot numbers or uncertainty values"
          ],
          [
            "Convert decimal year (e.g., 1960.5) to integer year (1960) for filtering",
            "Filter out rows with sentinel values (-1, -1.0) if they affect analysis"
          ],
          [
            "Year >= 1960",
            "Year <= 2020"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "peak detection with prominence and distance thresholds",
          "cycle period calculation using Fourier analysis or peak-to-peak distances",
          "Peak detection algorithm with prominence=20 and distance=5 on sunspot numbers",
          "Trough detection algorithm with prominence=20 and distance=5 on inverted sunspot numbers",
          "Calculate mean period between consecutive peaks or troughs to determine solar cycle length"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "peak detection with prominence and distance thresholds",
            "cycle period calculation using Fourier analysis or peak-to-peak distances"
          ],
          [
            "Peak detection algorithm with prominence=20 and distance=5 on sunspot numbers",
            "Trough detection algorithm with prominence=20 and distance=5 on inverted sunspot numbers",
            "Calculate mean period between consecutive peaks or troughs to determine solar cycle length"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "list of top 5 maximum years with values",
          "list of top 5 minimum years with values",
          "average cycle period calculation",
          "Report approximate solar cycle period in years",
          "List top 5 years of maximum activity with their sunspot numbers",
          "List top 5 years of minimum activity with their sunspot numbers",
          "Results should be ranked by sunspot number magnitude",
          "Table showing top 5 years of minimum activity with corresponding sunspot numbers.",
          "Table showing top 5 years of maximum activity with corresponding sunspot numbers.",
          "Approximate period of solar activity cycles."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "list of top 5 maximum years with values",
            "list of top 5 minimum years with values",
            "average cycle period calculation"
          ],
          [
            "Report approximate solar cycle period in years",
            "List top 5 years of maximum activity with their sunspot numbers",
            "List top 5 years of minimum activity with their sunspot numbers",
            "Results should be ranked by sunspot number magnitude"
          ],
          [
            "Table showing top 5 years of minimum activity with corresponding sunspot numbers.",
            "Table showing top 5 years of maximum activity with corresponding sunspot numbers.",
            "Approximate period of solar activity cycles."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5790476190476191
  },
  "astronomy-easy-5": {
    "m_q": {
      "target_metric": {
        "value": "total count of satellite major altitude changes (change > 1000m within 12h) for satellite 48445 during 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "total count of satellite major altitude changes (change > 1000m within 12h) for satellite 48445 during 2024",
          "Total count of major altitude changes (>1000m within 12h) for satellite 48445 during 2024",
          "Total count of satellite major altitude changes (change of altitude > 1000m within 12h) for satellite 48445 during 2024"
        ]
      },
      "filters": {
        "value": [
          "epoch_year == 24",
          "norad_id == '48445'",
          "altitude_change > 1000",
          "time_difference <= 12 hours",
          "satellite norad_id == 48445",
          "epoch_year == 24 (2024)",
          "altitude change > 1000m within 12-hour window",
          "year = 2024",
          "norad_id = 48445",
          "altitude change > 1000m within 12h"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "epoch_year == 24",
            "norad_id == '48445'",
            "altitude_change > 1000",
            "time_difference <= 12 hours"
          ],
          [
            "satellite norad_id == 48445",
            "epoch_year == 24 (2024)",
            "altitude change > 1000m within 12-hour window"
          ],
          [
            "year = 2024",
            "norad_id = 48445",
            "altitude change > 1000m within 12h"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "norad_id"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "norad_id"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to convert TLE orbital elements to ITRF XYZ coordinates?",
          "How to calculate altitude from ITRF XYZ coordinates?",
          "How to identify consecutive TLE epochs within 12 hours?",
          "How to compute altitude differences between consecutive valid epochs?",
          "How to count altitude changes exceeding 1000m threshold?",
          "How to convert TLE orbital elements to altitude using skifield's itrf_xyz?",
          "What is the time ordering of TLE records based on epoch_year and epoch_day?",
          "How to compute altitude from orbital elements (mean_motion, eccentricity, inclination, etc.)?",
          "How to identify consecutive TLE measurements within 12-hour windows?",
          "What threshold defines a major altitude change (>1000m)?",
          "Are there duplicate TLE entries with same epoch but different rev_number?",
          "Calculate altitude from TLE data using skifield's itrf_xyz.",
          "Calculate the time difference between consecutive TLE entries.",
          "Calculate the altitude difference between TLE entries within a 12-hour window.",
          "Count the number of altitude changes greater than 1000m."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "How to convert TLE orbital elements to ITRF XYZ coordinates?",
            "How to calculate altitude from ITRF XYZ coordinates?",
            "How to identify consecutive TLE epochs within 12 hours?",
            "How to compute altitude differences between consecutive valid epochs?",
            "How to count altitude changes exceeding 1000m threshold?"
          ],
          [
            "How to convert TLE orbital elements to altitude using skifield's itrf_xyz?",
            "What is the time ordering of TLE records based on epoch_year and epoch_day?",
            "How to compute altitude from orbital elements (mean_motion, eccentricity, inclination, etc.)?",
            "How to identify consecutive TLE measurements within 12-hour windows?",
            "What threshold defines a major altitude change (>1000m)?",
            "Are there duplicate TLE entries with same epoch but different rev_number?"
          ],
          [
            "Calculate altitude from TLE data using skifield's itrf_xyz.",
            "Calculate the time difference between consecutive TLE entries.",
            "Calculate the altitude difference between TLE entries within a 12-hour window.",
            "Count the number of altitude changes greater than 1000m."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "48445.tle"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "48445.tle"
          ],
          [
            "48445.tle"
          ],
          [
            "48445.tle"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple rows with identical epoch_year and epoch_day values but different rev_number (e.g., rows 1-2, 8-9, 16-17)",
          "Potential duplicate TLE entries for same timestamp"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple rows with identical epoch_year and epoch_day values but different rev_number (e.g., rows 1-2, 8-9, 16-17)",
            "Potential duplicate TLE entries for same timestamp"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "epoch_year": "two-digit year (24 = 2024)",
          "epoch_day": "day of year with fractional day",
          "inclination": "degrees",
          "raan": "degrees",
          "eccentricity": "unitless",
          "arg_perigee": "degrees",
          "mean_anomaly": "degrees",
          "mean_motion": "revolutions per day",
          "rev_number": "orbit revolution count",
          "altitude": "meters (to be derived)",
          "time_difference": "hours (for 12h window calculation)"
        },
        "confidence": 0.7272727272727272,
        "votes": [
          {
            "epoch_year": "two-digit year (24 = 2024)",
            "epoch_day": "day of year with fractional day",
            "inclination": "degrees",
            "raan": "degrees",
            "eccentricity": "unitless",
            "arg_perigee": "degrees",
            "mean_anomaly": "degrees",
            "mean_motion": "revolutions per day",
            "rev_number": "orbit revolution count"
          },
          {
            "epoch_day": "fractional day of year",
            "inclination": "degrees",
            "raan": "degrees (right ascension of ascending node)",
            "eccentricity": "dimensionless",
            "arg_perigee": "degrees (argument of perigee)",
            "mean_anomaly": "degrees",
            "mean_motion": "revolutions per day",
            "altitude": "meters (to be derived)",
            "time_difference": "hours (for 12h window calculation)"
          },
          {
            "inclination": "degrees",
            "raan": "degrees",
            "eccentricity": "unitless",
            "arg_perigee": "degrees",
            "mean_anomaly": "degrees",
            "mean_motion": "revolutions per day"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "epoch_year uses two-digit format (24 = 2024)",
          "epoch_day includes fractional days (e.g., 1.44338999 = day 1 + fractional day)",
          "mean_motion values around 15.06 rev/day correspond to ~96 minute orbits",
          "eccentricity appears to be scaled (e.g., 0.0001342 likely represents actual eccentricity)",
          "epoch_day is fractional day of year requiring conversion to absolute timestamp",
          "mean_motion in revs/day needs conversion to orbital period and altitude",
          "Eccentricity is a very small number, potential for precision issues."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "epoch_year uses two-digit format (24 = 2024)",
            "epoch_day includes fractional days (e.g., 1.44338999 = day 1 + fractional day)",
            "mean_motion values around 15.06 rev/day correspond to ~96 minute orbits"
          ],
          [
            "eccentricity appears to be scaled (e.g., 0.0001342 likely represents actual eccentricity)",
            "epoch_day is fractional day of year requiring conversion to absolute timestamp",
            "mean_motion in revs/day needs conversion to orbital period and altitude"
          ],
          [
            "Eccentricity is a very small number, potential for precision issues."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "No external data sources provided - requires skifield's itrf_xyz library for coordinate conversion"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No external data sources provided - requires skifield's itrf_xyz library for coordinate conversion"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "TLE epochs must be sorted chronologically for time series analysis",
          "Altitude calculation requires Earth radius assumption (typically 6371km)",
          "Time differences must account for day-of-year rollover in 2024",
          "Only consider consecutive epochs within 12 hours for valid altitude change calculation",
          "Exclude duplicate epoch entries before analysis",
          "norad_id must equal 48445",
          "epoch_year must equal 24 (representing 2024)",
          "Altitude change must exceed 1000 meters",
          "Time window must be within 12 hours between consecutive measurements",
          "TLE records must be sorted chronologically by epoch before analysis",
          "epoch_year must be 24 for the year 2024",
          "norad_id must be 48445"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "TLE epochs must be sorted chronologically for time series analysis",
            "Altitude calculation requires Earth radius assumption (typically 6371km)",
            "Time differences must account for day-of-year rollover in 2024",
            "Only consider consecutive epochs within 12 hours for valid altitude change calculation",
            "Exclude duplicate epoch entries before analysis"
          ],
          [
            "norad_id must equal 48445",
            "epoch_year must equal 24 (representing 2024)",
            "Altitude change must exceed 1000 meters",
            "Time window must be within 12 hours between consecutive measurements",
            "TLE records must be sorted chronologically by epoch before analysis"
          ],
          [
            "epoch_year must be 24 for the year 2024",
            "norad_id must be 48445"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove duplicate rows with identical epoch_year and epoch_day",
          "Filter to only 2024 data (epoch_year == 24)",
          "Calculate time differences between consecutive valid epochs",
          "Filter to epoch pairs with time_difference <= 12 hours",
          "Convert epoch_year and epoch_day to absolute datetime for 2024",
          "Calculate altitude from TLE orbital parameters using skifield's itrf_xyz method",
          "Compute time differences between consecutive TLE records",
          "Identify pairs of records where time_diff <= 12 hours AND altitude_diff > 1000m",
          "Handle duplicate TLE entries at same epoch appropriately",
          "epoch_year = 24",
          "time difference between consecutive TLE entries <= 12 hours",
          "altitude difference > 1000m"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove duplicate rows with identical epoch_year and epoch_day",
            "Filter to only 2024 data (epoch_year == 24)",
            "Calculate time differences between consecutive valid epochs",
            "Filter to epoch pairs with time_difference <= 12 hours"
          ],
          [
            "Convert epoch_year and epoch_day to absolute datetime for 2024",
            "Calculate altitude from TLE orbital parameters using skifield's itrf_xyz method",
            "Compute time differences between consecutive TLE records",
            "Identify pairs of records where time_diff <= 12 hours AND altitude_diff > 1000m",
            "Handle duplicate TLE entries at same epoch appropriately"
          ],
          [
            "epoch_year = 24",
            "time difference between consecutive TLE entries <= 12 hours",
            "altitude difference > 1000m"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for monotonic increasing epoch_day within each year",
          "Validate altitude calculations against expected LEO range (~400-2000km)",
          "Test for outliers in orbital elements (eccentricity should be ~0 for near-circular orbits)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for monotonic increasing epoch_day within each year",
            "Validate altitude calculations against expected LEO range (~400-2000km)",
            "Test for outliers in orbital elements (eccentricity should be ~0 for near-circular orbits)"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count of major altitude changes",
          "Document altitude calculation method using skifield's itrf_xyz",
          "Specify time window criteria (12 hours)",
          "State altitude change threshold (1000 meters)",
          "Output should be a single integer count",
          "Count represents number of major altitude change events during 2024",
          "Major altitude change defined as >1000m within 12-hour period",
          "Integer representing the count of major altitude changes."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count of major altitude changes",
            "Document altitude calculation method using skifield's itrf_xyz",
            "Specify time window criteria (12 hours)",
            "State altitude change threshold (1000 meters)"
          ],
          [
            "Output should be a single integer count",
            "Count represents number of major altitude change events during 2024",
            "Major altitude change defined as >1000m within 12-hour period"
          ],
          [
            "Integer representing the count of major altitude changes."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6141414141414142
  },
  "astronomy-easy-6": {
    "m_q": {
      "target_metric": {
        "value": "average rate of semi-major axis decay (km/day) for two periods: quiet period (May 1-4, 2024) and storm period (May 10-13, 2024)",
        "confidence": 0.3333333333333333,
        "votes": [
          "average rate of semi-major axis decay (km/day) for two periods: quiet period (May 1-4, 2024) and storm period (May 10-13, 2024)",
          "Average rate of semi-major axis decay (km/day) for two periods: (1) quiet period May 1-4, 2024, (2) Gannon storm May 10-13, 2024",
          "Average rate of semi-major axis decay (km/day) for Starlink satellite 58214 during the Gannon storm (May 10-13, 2024) and the preceding quiet period (May 1-4, 2024)"
        ]
      },
      "filters": {
        "value": [
          "EPOCH between 2024-05-01 and 2024-05-04 for quiet period",
          "EPOCH between 2024-05-10 and 2024-05-13 for storm period",
          "NORAD_CAT_ID = 58214",
          "EPOCH >= 2024-05-01 AND EPOCH <= 2024-05-04 for quiet period",
          "EPOCH >= 2024-05-10 AND EPOCH <= 2024-05-13 for storm period",
          "NORAD_CAT_ID == 58214",
          "Time period: May 1-4, 2024 (quiet) and May 10-13, 2024 (storm)",
          "Satellite: NORAD_CAT_ID = 58214"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "EPOCH between 2024-05-01 and 2024-05-04 for quiet period",
            "EPOCH between 2024-05-10 and 2024-05-13 for storm period",
            "NORAD_CAT_ID = 58214"
          ],
          [
            "EPOCH >= 2024-05-01 AND EPOCH <= 2024-05-04 for quiet period",
            "EPOCH >= 2024-05-10 AND EPOCH <= 2024-05-13 for storm period",
            "NORAD_CAT_ID == 58214"
          ],
          [
            "Time period: May 1-4, 2024 (quiet) and May 10-13, 2024 (storm)",
            "Satellite: NORAD_CAT_ID = 58214"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "period_type (quiet/storm)",
          "time_period (quiet vs storm)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "period_type (quiet/storm)"
          ],
          [
            "time_period (quiet vs storm)"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "pair of numbers (average_quiet_rate_km_day, average_storm_rate_km_day)",
        "confidence": 0.3333333333333333,
        "votes": [
          "pair of numbers (average_quiet_rate_km_day, average_storm_rate_km_day)",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate semi-major axis from MEAN_MOTION using Kepler's law",
          "Compute daily decay rate from semi-major axis changes over time",
          "Average decay rates separately for quiet and storm periods",
          "Parse EPOCH column to datetime for filtering by date ranges",
          "Calculate semi-major axis from MEAN_MOTION using Kepler's law: a = (mu/(2*pi*n)^2)^(1/3) where n is mean motion in rad/s",
          "Order TLE records by EPOCH timestamp within each period",
          "Compute semi-major axis changes between consecutive TLE observations",
          "Calculate time differences in days between consecutive observations",
          "Compute decay rates as delta_a / delta_t for each consecutive pair",
          "Average the decay rates over the quiet period (May 1-4)",
          "Average the decay rates over the storm period (May 10-13)",
          "Format output as tuple (average_quiet_rate_km_day, average_storm_rate_km_day)",
          "Calculate the semi-major axis from MEAN_MOTION using Kepler's law for each TLE.",
          "Calculate the rate of semi-major axis decay (km/day) for each day in the quiet and storm periods.",
          "Average the daily decay rates for the quiet and storm periods separately."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Calculate semi-major axis from MEAN_MOTION using Kepler's law",
            "Compute daily decay rate from semi-major axis changes over time",
            "Average decay rates separately for quiet and storm periods"
          ],
          [
            "Parse EPOCH column to datetime for filtering by date ranges",
            "Calculate semi-major axis from MEAN_MOTION using Kepler's law: a = (mu/(2*pi*n)^2)^(1/3) where n is mean motion in rad/s",
            "Order TLE records by EPOCH timestamp within each period",
            "Compute semi-major axis changes between consecutive TLE observations",
            "Calculate time differences in days between consecutive observations",
            "Compute decay rates as delta_a / delta_t for each consecutive pair",
            "Average the decay rates over the quiet period (May 1-4)",
            "Average the decay rates over the storm period (May 10-13)",
            "Format output as tuple (average_quiet_rate_km_day, average_storm_rate_km_day)"
          ],
          [
            "Calculate the semi-major axis from MEAN_MOTION using Kepler's law for each TLE.",
            "Calculate the rate of semi-major axis decay (km/day) for each day in the quiet and storm periods.",
            "Average the daily decay rates for the quiet and storm periods separately."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "58214_quiet.csv",
          "58214_storm.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "58214_quiet.csv",
            "58214_storm.csv"
          ],
          [
            "58214_quiet.csv",
            "58214_storm.csv"
          ],
          [
            "58214_storm.csv",
            "58214_quiet.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Both files have identical column structures and dtypes",
          "Both files have identical schema, no conflicts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Both files have identical column structures and dtypes"
          ],
          [
            "Both files have identical schema, no conflicts"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "mean_motion": "revolutions per day",
          "semimajor_axis": "km",
          "epoch": "UTC datetime",
          "mean_motion_dot": "revolutions per day squared",
          "period": "minutes",
          "apoapsis": "km",
          "periapsis": "km",
          "mu": "398600.4418 km^3/s^2 (Earth gravitational parameter)",
          "earth_radius": "6371.0 km",
          "target_output": "km/day (decay rate)",
          "eccentricity": "unitless",
          "inclination": "degrees",
          "ra_of_asc_node": "degrees",
          "arg_of_pericenter": "degrees",
          "mean_anomaly": "degrees",
          "bstar": "1/Earth radii",
          "mean_motion_ddot": "revolutions/day^3"
        },
        "confidence": 0.5098039215686273,
        "votes": [
          {
            "MEAN_MOTION": "revolutions per day",
            "SEMIMAJOR_AXIS": "km",
            "EPOCH": "UTC datetime",
            "MEAN_MOTION_DOT": "revolutions per day squared",
            "PERIOD": "minutes",
            "APOAPSIS": "km",
            "PERIAPSIS": "km"
          },
          {
            "EPOCH": "UTC datetime string",
            "MEAN_MOTION": "revolutions per day",
            "SEMIMAJOR_AXIS": "km (pre-calculated in data)",
            "mu": "398600.4418 km^3/s^2 (Earth gravitational parameter)",
            "earth_radius": "6371.0 km",
            "target_output": "km/day (decay rate)"
          },
          {
            "MEAN_MOTION": "revolutions/day",
            "ECCENTRICITY": "unitless",
            "INCLINATION": "degrees",
            "RA_OF_ASC_NODE": "degrees",
            "ARG_OF_PERICENTER": "degrees",
            "MEAN_ANOMALY": "degrees",
            "BSTAR": "1/Earth radii",
            "MEAN_MOTION_DOT": "revolutions/day^2",
            "MEAN_MOTION_DDOT": "revolutions/day^3",
            "SEMIMAJOR_AXIS": "km",
            "PERIOD": "minutes",
            "APOAPSIS": "km",
            "PERIAPSIS": "km"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "MEAN_MOTION_DOT values are very small (e-05 to e-03)",
          "BSTAR values are negative and small (e-03 to e-04)",
          "ECCENTRICITY values are very small (e-04 to e-03)",
          "MEAN_MOTION is in revolutions/day, needs conversion to rad/s: n_rad_s = MEAN_MOTION * 2*pi / 86400",
          "Time differences must be computed in days from EPOCH timestamps",
          "Semi-major axis calculated from Kepler's law: a = (mu/(n^2))^(1/3) where n is in rad/s"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "MEAN_MOTION_DOT values are very small (e-05 to e-03)",
            "BSTAR values are negative and small (e-03 to e-04)",
            "ECCENTRICITY values are very small (e-04 to e-03)"
          ],
          [
            "MEAN_MOTION is in revolutions/day, needs conversion to rad/s: n_rad_s = MEAN_MOTION * 2*pi / 86400",
            "Time differences must be computed in days from EPOCH timestamps",
            "Semi-major axis calculated from Kepler's law: a = (mu/(n^2))^(1/3) where n is in rad/s"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Time periods are different but data structure is consistent"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time periods are different but data structure is consistent"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "0.0"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "0.0"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 39.0,
        "confidence": 1.0,
        "votes": [
          39.0,
          39.0,
          39.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "mu = 398600.4418 km^3/s^2 must be used for Kepler's law",
          "earth radius = 6371.0 km",
          "Semi-major axis must be calculated from MEAN_MOTION: a = (mu/(n*2\u03c0/86400)^2)^(1/3) where n is MEAN_MOTION in rev/day",
          "Decay rate = \u0394a/\u0394t where \u0394a is change in semi-major axis and \u0394t is time difference in days",
          "Use only NORAD_CAT_ID 58214 (Starlink-30805)",
          "Quiet period: May 1-4, 2024 (inclusive)",
          "Storm period: May 10-13, 2024 (inclusive)",
          "Must use mu = 398600.4418 km^3/s^2",
          "Must use earth_radius = 6371.0 km",
          "Must calculate semi-major axis using Kepler's law from MEAN_MOTION, not use pre-calculated SEMIMAJOR_AXIS column",
          "mu = 398600.4418 km^3/s^2",
          "Kepler's Law: T^2 = (4*pi^2/mu) * a^3, where T is period and a is semi-major axis.  Period needs to be converted to seconds from MEAN_MOTION."
        ],
        "confidence": 0.3611111111111111,
        "votes": [
          [
            "mu = 398600.4418 km^3/s^2 must be used for Kepler's law",
            "earth radius = 6371.0 km",
            "Semi-major axis must be calculated from MEAN_MOTION: a = (mu/(n*2\u03c0/86400)^2)^(1/3) where n is MEAN_MOTION in rev/day",
            "Decay rate = \u0394a/\u0394t where \u0394a is change in semi-major axis and \u0394t is time difference in days"
          ],
          [
            "Use only NORAD_CAT_ID 58214 (Starlink-30805)",
            "Quiet period: May 1-4, 2024 (inclusive)",
            "Storm period: May 10-13, 2024 (inclusive)",
            "Must use mu = 398600.4418 km^3/s^2",
            "Must use earth_radius = 6371.0 km",
            "Must calculate semi-major axis using Kepler's law from MEAN_MOTION, not use pre-calculated SEMIMAJOR_AXIS column"
          ],
          [
            "mu = 398600.4418 km^3/s^2",
            "Earth radius = 6371.0 km",
            "Kepler's Law: T^2 = (4*pi^2/mu) * a^3, where T is period and a is semi-major axis.  Period needs to be converted to seconds from MEAN_MOTION."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove duplicate EPOCH timestamps",
          "Ensure chronological ordering within each period",
          "Filter to only include valid MEAN_MOTION values > 0",
          "Extract date from EPOCH string and filter by date range",
          "Sort records chronologically within each period for sequential decay calculation",
          "Quiet period: EPOCH between 2024-05-01 and 2024-05-04",
          "Storm period: EPOCH between 2024-05-10 and 2024-05-13"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove duplicate EPOCH timestamps",
            "Ensure chronological ordering within each period",
            "Filter to only include valid MEAN_MOTION values > 0"
          ],
          [
            "Extract date from EPOCH string and filter by date range",
            "Sort records chronologically within each period for sequential decay calculation"
          ],
          [
            "Quiet period: EPOCH between 2024-05-01 and 2024-05-04",
            "Storm period: EPOCH between 2024-05-10 and 2024-05-13"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Linear regression of semi-major axis vs time for each period",
          "Check for outliers in MEAN_MOTION values",
          "Verify temporal coverage within specified date ranges"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Linear regression of semi-major axis vs time for each period",
            "Check for outliers in MEAN_MOTION values",
            "Verify temporal coverage within specified date ranges"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Pair of numbers with units km/day",
          "Precision appropriate for orbital decay rates (likely 4-6 decimal places)",
          "Output as tuple of two numbers: (average_quiet_rate_km_day, average_storm_rate_km_day)",
          "Both values should be in km/day units",
          "Decay rate is negative (decreasing semi-major axis) so values likely negative",
          "(average_quiet_rate_km_day, average_storm_rate_km_day)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pair of numbers with units km/day",
            "Precision appropriate for orbital decay rates (likely 4-6 decimal places)"
          ],
          [
            "Output as tuple of two numbers: (average_quiet_rate_km_day, average_storm_rate_km_day)",
            "Both values should be in km/day units",
            "Decay rate is negative (decreasing semi-major axis) so values likely negative"
          ],
          [
            "(average_quiet_rate_km_day, average_storm_rate_km_day)"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.568545751633987
  },
  "astronomy-hard-10": {
    "m_q": {
      "target_metric": {
        "value": "Pearson correlation coefficient between Swarm-A satellite's hourly altitude change (change_altitude per hour) and all available variables from OMNI2 and Sat_Density datasets",
        "confidence": 0.3333333333333333,
        "votes": [
          "Pearson correlation coefficient between Swarm-A satellite's hourly altitude change (change_altitude per hour) and all available variables from OMNI2 and Sat_Density datasets",
          "Variable with strongest Pearson correlation (positive or negative) with hourly altitude change of Swarm-A satellite",
          "Pearson correlation between OMNI2 and Sat_Density variables and Swarm-A's hourly altitude change for the period 2018-10-01 to 2018-10-10, identifying the variable with the strongest correlation and reporting its name and correlation value."
        ]
      },
      "filters": {
        "value": [
          "Timestamp between 2018-10-01 and 2018-10-10 inclusive",
          "Earth radius = 6371.0 km",
          "Satellite_id = 'L47' (Swarm-A)",
          "Date range: 2018-10-01 to 2018-10-10 (inclusive)",
          "Satellite: Swarm-A (L47)",
          "Variables from: OMNI2 (solar wind, IMF, geomagnetic indices, proton flux) and Sat_Density",
          "Date range: 2018-10-01 to 2018-10-10"
        ],
        "confidence": 0.380952380952381,
        "votes": [
          [
            "Timestamp between 2018-10-01 and 2018-10-10 inclusive",
            "Earth radius = 6371.0 km",
            "Satellite_id = 'L47' (Swarm-A)"
          ],
          [
            "Date range: 2018-10-01 to 2018-10-10 (inclusive)",
            "Satellite: Swarm-A (L47)",
            "Variables from: OMNI2 (solar wind, IMF, geomagnetic indices, proton flux) and Sat_Density"
          ],
          [
            "Date range: 2018-10-01 to 2018-10-10",
            "Satellite: Swarm-A (L47)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Timestamp (hourly resolution)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Timestamp (hourly resolution)"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Compute altitude from SP3 position data (x_km, y_km, z_km) using formula: altitude = sqrt(x^2 + y^2 + z^2) - 6371.0",
          "Calculate hourly altitude change as difference between consecutive hourly altitude values",
          "Extract OMNI2 variables for solar wind parameters, IMF measurements, geomagnetic indices, and proton flux metrics",
          "Extract Sat_Density mean atmospheric density values",
          "Compute Pearson correlations between hourly altitude change and each variable",
          "Identify variable with strongest absolute correlation magnitude",
          "Format output as [variable_name, correlation_value] with correlation rounded to 3 decimal places",
          "How to compute altitude from SP3 x_km, y_km, z_km coordinates using Earth radius 6371.0 km?",
          "How to compute hourly altitude change from POD SP3 data?",
          "How to align hourly altitude changes with OMNI2 hourly data?",
          "How to align hourly altitude changes with Sat_Density data (10-minute resolution)?",
          "Which OMNI2 columns are valid variables (excluding metadata, IDs, counts)?",
          "How to handle missing/sentinel values in correlation computation?",
          "How to compute Pearson correlation for each variable against altitude change?",
          "How to identify the variable with maximum absolute correlation value?",
          "Calculate the hourly altitude change of Swarm-A from SP3 files.",
          "Merge OMNI2 and Sat_Density data with the calculated altitude changes based on timestamp.",
          "Calculate the Pearson correlation between each OMNI2 and Sat_Density variable and the hourly altitude change.",
          "Identify the variable with the strongest Pearson correlation (positive or negative).",
          "Report the variable name and its correlation value (rounded to 3 decimal places)."
        ],
        "confidence": 0.3333333333333332,
        "votes": [
          [
            "Compute altitude from SP3 position data (x_km, y_km, z_km) using formula: altitude = sqrt(x^2 + y^2 + z^2) - 6371.0",
            "Calculate hourly altitude change as difference between consecutive hourly altitude values",
            "Extract OMNI2 variables for solar wind parameters, IMF measurements, geomagnetic indices, and proton flux metrics",
            "Extract Sat_Density mean atmospheric density values",
            "Compute Pearson correlations between hourly altitude change and each variable",
            "Identify variable with strongest absolute correlation magnitude",
            "Format output as [variable_name, correlation_value] with correlation rounded to 3 decimal places"
          ],
          [
            "How to compute altitude from SP3 x_km, y_km, z_km coordinates using Earth radius 6371.0 km?",
            "How to compute hourly altitude change from POD SP3 data?",
            "How to align hourly altitude changes with OMNI2 hourly data?",
            "How to align hourly altitude changes with Sat_Density data (10-minute resolution)?",
            "Which OMNI2 columns are valid variables (excluding metadata, IDs, counts)?",
            "How to handle missing/sentinel values in correlation computation?",
            "How to compute Pearson correlation for each variable against altitude change?",
            "How to identify the variable with maximum absolute correlation value?"
          ],
          [
            "Calculate the hourly altitude change of Swarm-A from SP3 files.",
            "Merge OMNI2 and Sat_Density data with the calculated altitude changes based on timestamp.",
            "Calculate the Pearson correlation between each OMNI2 and Sat_Density variable and the hourly altitude change.",
            "Identify the variable with the strongest Pearson correlation (positive or negative).",
            "Report the variable name and its correlation value (rounded to 3 decimal places)."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "omni2-wu590-20181001_to_20181130.csv",
          "swarma-wu574-20181013_to_20181016.csv",
          "SW_OPER_SP3ACOM_2__20181001T235942_20181002T235942_0201.sp3",
          "SW_OPER_SP3ACOM_2__20181002T235942_20181003T235942_0201.sp3",
          "SW_OPER_SP3ACOM_2__20181003T235942_20181004T235942_0201.sp3",
          "SW_OPER_SP3ACOM_2__20181004T235942_20181005T235942_0201.sp3",
          "SW_OPER_SP3ACOM_2__20181005T235942_20181006T235942_0201.sp3",
          "SW_OPER_SP3ACOM_2__20181006T235942_20181007T235942_0201.sp3",
          "SW_OPER_SP3ACOM_2__20181007T235942_20181008T235942_0201.sp3",
          "SW_OPER_SP3ACOM_2__20181008T235942_20181009T235942_0201.sp3",
          "SW_OPER_SP3ACOM_2__20181009T235942_20181010T235942_0201.sp3"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "omni2-wu590-20181001_to_20181130.csv",
            "swarma-wu574-20181013_to_20181016.csv",
            "SW_OPER_SP3ACOM_2__20181001T235942_20181002T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181002T235942_20181003T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181003T235942_20181004T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181004T235942_20181005T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181005T235942_20181006T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181006T235942_20181007T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181007T235942_20181008T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181008T235942_20181009T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181009T235942_20181010T235942_0201.sp3"
          ],
          [
            "omni2-wu590-20181001_to_20181130.csv",
            "swarma-wu574-20181013_to_20181016.csv",
            "SW_OPER_SP3ACOM_2__20181001T235942_20181002T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181002T235942_20181003T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181003T235942_20181004T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181004T235942_20181005T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181005T235942_20181006T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181006T235942_20181007T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181007T235942_20181008T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181008T235942_20181009T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181009T235942_20181010T235942_0201.sp3"
          ],
          [
            "omni2-wu590-20181001_to_20181130.csv",
            "swarma-wu574-20181013_to_20181016.csv",
            "SW_OPER_SP3ACOM_2__20181001T235942_20181002T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181002T235942_20181003T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181003T235942_20181004T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181004T235942_20181005T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181005T235942_20181006T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181006T235942_20181007T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181007T235942_20181008T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181008T235942_20181009T235942_0201.sp3",
            "SW_OPER_SP3ACOM_2__20181009T235942_20181010T235942_0201.sp3"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "SP3 files have 10-second resolution while OMNI2 has hourly resolution",
          "Sat_Density data starts from 2018-10-13, missing first 12 days of analysis period",
          "SP3 files need aggregation to hourly resolution",
          "OMNI2 has 57 columns with various measurement types",
          "OMNI2 has hourly resolution, Sat_Density has 10-minute resolution",
          "SP3 files have 10-second resolution requiring aggregation to hourly",
          "Sat_Density only covers 2018-10-13 to 2018-10-16, partial overlap with analysis period"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "SP3 files have 10-second resolution while OMNI2 has hourly resolution",
            "Sat_Density data starts from 2018-10-13, missing first 12 days of analysis period",
            "SP3 files need aggregation to hourly resolution",
            "OMNI2 has 57 columns with various measurement types"
          ],
          [
            "OMNI2 has hourly resolution, Sat_Density has 10-minute resolution",
            "SP3 files have 10-second resolution requiring aggregation to hourly",
            "Sat_Density only covers 2018-10-13 to 2018-10-16, partial overlap with analysis period"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "scalar_b_nt": "nT",
          "vector_b_magnitude_nt": "nT",
          "sw_plasma_temperature_k": "K",
          "sw_proton_density_n_cm3": "N/cm\u00b3",
          "sw_plasma_speed_km_s": "km/s",
          "flow_pressure": "nPa",
          "dst_index_nt": "nT",
          "ap_index_nt": "nT",
          "ae_index_nt": "nT",
          "proton_flux_>1_mev": "pfu",
          "x_km": "km",
          "y_km": "km",
          "z_km": "km",
          "orbit mean density (kg/m^3)": "kg/m\u00b3",
          "change_altitude": "km/hour",
          "altitude": "kilometers",
          "al_index_nt": "nanoTesla",
          "au_index_nt": "nanoTesla",
          "earth_radius": "6371.0 kilometers",
          "bx_nt_gse_gsm": "nT",
          "by_nt_gse": "nT",
          "bz_nt_gse": "nT",
          "by_nt_gsm": "nT",
          "bz_nt_gsm": "nT",
          "rms_magnitude_nt": "nT",
          "rms_field_vector_nt": "nT",
          "rms_bx_gse_nt": "nT",
          "rms_by_gse_nt": "nT",
          "rms_bz_gse_nt": "nT",
          "e_electric_field": "unitless",
          "pc_index": "unitless",
          "proton_flux_>2_mev": "unitless",
          "proton_flux_>4_mev": "unitless",
          "proton_flux_>10_mev": "unitless",
          "proton_flux_>30_mev": "unitless",
          "proton_flux_>60_mev": "unitless",
          "clock_microsec": "microseconds",
          "sat_density": "kg/m^3"
        },
        "confidence": 0.5877192982456134,
        "votes": [
          {
            "Scalar_B_nT": "nT",
            "Vector_B_Magnitude_nT": "nT",
            "SW_Plasma_Temperature_K": "K",
            "SW_Proton_Density_N_cm3": "N/cm\u00b3",
            "SW_Plasma_Speed_km_s": "km/s",
            "Flow_pressure": "nPa",
            "Dst_index_nT": "nT",
            "ap_index_nT": "nT",
            "AE_index_nT": "nT",
            "Proton_flux_>1_Mev": "pfu",
            "x_km": "km",
            "y_km": "km",
            "z_km": "km",
            "Orbit Mean Density (kg/m^3)": "kg/m\u00b3",
            "change_altitude": "km/hour"
          },
          {
            "x_km": "kilometers",
            "y_km": "kilometers",
            "z_km": "kilometers",
            "altitude": "kilometers",
            "change_altitude": "kilometers per hour",
            "Scalar_B_nT": "nanoTesla",
            "Vector_B_Magnitude_nT": "nanoTesla",
            "SW_Plasma_Temperature_K": "Kelvin",
            "SW_Proton_Density_N_cm3": "number per cubic centimeter",
            "SW_Plasma_Speed_km_s": "kilometers per second",
            "Dst_index_nT": "nanoTesla",
            "ap_index_nT": "nanoTesla",
            "AE_index_nT": "nanoTesla",
            "AL_index_nT": "nanoTesla",
            "AU_index_nT": "nanoTesla",
            "Proton_flux_>1_Mev": "particles/(cm^2 s sr)",
            "Orbit Mean Density (kg/m^3)": "kilograms per cubic meter",
            "earth_radius": "6371.0 kilometers"
          },
          {
            "Scalar_B_nT": "nT",
            "Vector_B_Magnitude_nT": "nT",
            "BX_nT_GSE_GSM": "nT",
            "BY_nT_GSE": "nT",
            "BZ_nT_GSE": "nT",
            "BY_nT_GSM": "nT",
            "BZ_nT_GSM": "nT",
            "RMS_magnitude_nT": "nT",
            "RMS_field_vector_nT": "nT",
            "RMS_BX_GSE_nT": "nT",
            "RMS_BY_GSE_nT": "nT",
            "RMS_BZ_GSE_nT": "nT",
            "SW_Plasma_Temperature_K": "K",
            "SW_Proton_Density_N_cm3": "N/cm3",
            "SW_Plasma_Speed_km_s": "km/s",
            "Flow_pressure": "unitless",
            "E_electric_field": "unitless",
            "Dst_index_nT": "nT",
            "ap_index_nT": "nT",
            "AE_index_nT": "nT",
            "AL_index_nT": "nT",
            "AU_index_nT": "nT",
            "pc_index": "unitless",
            "Proton_flux_>1_Mev": "unitless",
            "Proton_flux_>2_Mev": "unitless",
            "Proton_flux_>4_Mev": "unitless",
            "Proton_flux_>10_Mev": "unitless",
            "Proton_flux_>30_Mev": "unitless",
            "Proton_flux_>60_Mev": "unitless",
            "x_km": "km",
            "y_km": "km",
            "z_km": "km",
            "clock_microsec": "microseconds",
            "Sat_Density": "kg/m^3"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Proton flux columns contain sentinel values 999999.99 and 99999.99",
          "Clock_microsec column contains 999999.999999 sentinel values",
          "Sat_Density values are extremely small (e-13 scale)",
          "Altitude changes expected to be small (meters/hour)",
          "SP3 coordinates in km need conversion to altitude above Earth surface",
          "Time resolution mismatch: 10-second SP3 data vs hourly OMNI2 vs 10-minute Sat_Density"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Proton flux columns contain sentinel values 999999.99 and 99999.99",
            "Clock_microsec column contains 999999.999999 sentinel values",
            "Sat_Density values are extremely small (e-13 scale)",
            "Altitude changes expected to be small (meters/hour)"
          ],
          [
            "SP3 coordinates in km need conversion to altitude above Earth surface",
            "Time resolution mismatch: 10-second SP3 data vs hourly OMNI2 vs 10-minute Sat_Density"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Time alignment: SP3 at 10-second intervals vs OMNI2 at hourly intervals",
          "Sat_Density has irregular sampling (not every 10 minutes has data)",
          "SP3 files cover different 24-hour periods with overlaps at boundaries",
          "Sat_Density timestamp range does not cover full analysis period (only 2018-10-13 to 2018-10-16)",
          "SP3 files span date boundaries differently than calendar dates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time alignment: SP3 at 10-second intervals vs OMNI2 at hourly intervals",
            "Sat_Density has irregular sampling (not every 10 minutes has data)",
            "SP3 files cover different 24-hour periods with overlaps at boundaries"
          ],
          [
            "Sat_Density timestamp range does not cover full analysis period (only 2018-10-13 to 2018-10-16)",
            "SP3 files span date boundaries differently than calendar dates"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "999999.99",
          "99999.99",
          "999999.999999",
          "",
          "-1",
          "NA",
          "9999.99",
          "999.99",
          "N/A"
        ],
        "confidence": 0.6666666666666665,
        "votes": [
          [
            "999999.99",
            "99999.99",
            "999999.999999",
            "",
            "-1",
            "NA"
          ],
          [
            "999999.99",
            "99999.99",
            "9999.99",
            "999.99",
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            "",
            "999999.99",
            "999999.999999",
            "99999.99"
          ]
        ]
      },
      "expected_columns": {
        "value": 57.0,
        "confidence": 0.33,
        "votes": [
          0.0,
          0.0,
          57.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Analysis period: 2018-10-01 to 2018-10-10 (10 days)",
          "Earth radius fixed at 6371.0 km",
          "Only Swarm-A satellite (L47)",
          "Hourly altitude change computation requires consecutive hourly altitude values",
          "Correlation requires complete data pairs (no missing values)",
          "Date range must be 2018-10-01 00:00:00 to 2018-10-10 23:59:59",
          "Use Earth radius = 6371.0 km for altitude calculation",
          "Altitude = sqrt(x_km^2 + y_km^2 + z_km^2) - 6371.0",
          "Hourly altitude change = altitude(hour_t) - altitude(hour_t-1)",
          "Only include OMNI2 variables: solar wind parameters, IMF measurements, geomagnetic indices, proton flux",
          "Exclude OMNI2 metadata columns: YEAR, DOY, Bartels_rotation_number, ID columns, num_points columns, Flux_FLAG",
          "Handle missing values by excluding pairs with sentinel values from correlation",
          "Sat_Density only available 2018-10-13 to 2018-10-16, limited correlation window",
          "Earth radius is 6371.0 km.",
          "Altitude change should be calculated in km/hour.",
          "Correlation should be Pearson correlation.",
          "The satellite ID for Swarm-A is L47."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Analysis period: 2018-10-01 to 2018-10-10 (10 days)",
            "Earth radius fixed at 6371.0 km",
            "Only Swarm-A satellite (L47)",
            "Hourly altitude change computation requires consecutive hourly altitude values",
            "Correlation requires complete data pairs (no missing values)"
          ],
          [
            "Date range must be 2018-10-01 00:00:00 to 2018-10-10 23:59:59",
            "Use Earth radius = 6371.0 km for altitude calculation",
            "Altitude = sqrt(x_km^2 + y_km^2 + z_km^2) - 6371.0",
            "Hourly altitude change = altitude(hour_t) - altitude(hour_t-1)",
            "Only include OMNI2 variables: solar wind parameters, IMF measurements, geomagnetic indices, proton flux",
            "Exclude OMNI2 metadata columns: YEAR, DOY, Bartels_rotation_number, ID columns, num_points columns, Flux_FLAG",
            "Handle missing values by excluding pairs with sentinel values from correlation",
            "Sat_Density only available 2018-10-13 to 2018-10-16, limited correlation window"
          ],
          [
            "Earth radius is 6371.0 km.",
            "Altitude change should be calculated in km/hour.",
            "Correlation should be Pearson correlation.",
            "The satellite ID for Swarm-A is L47."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter OMNI2 data to 2018-10-01 to 2018-10-10",
          "Filter SP3 data to satellite_id = 'L47'",
          "Remove sentinel values from proton flux columns",
          "Handle missing Sat_Density values for 2018-10-01 to 2018-10-12",
          "Aggregate SP3 10-second data to hourly altitude values",
          "Filter SP3 data to satellite_id = 'L47' (Swarm-A)",
          "Filter OMNI2 data to dates 2018-10-01 to 2018-10-10",
          "Aggregate SP3 10-second data to hourly averages for altitude",
          "Remove hourly periods with insufficient SP3 data points",
          "Exclude variable-hour pairs where either value is a sentinel",
          "Filter OMNI2 data for the period 2018-10-01 to 2018-10-10.",
          "Filter SP3 data for satellite L47 and the period 2018-10-01 to 2018-10-10."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter OMNI2 data to 2018-10-01 to 2018-10-10",
            "Filter SP3 data to satellite_id = 'L47'",
            "Remove sentinel values from proton flux columns",
            "Handle missing Sat_Density values for 2018-10-01 to 2018-10-12",
            "Aggregate SP3 10-second data to hourly altitude values"
          ],
          [
            "Filter SP3 data to satellite_id = 'L47' (Swarm-A)",
            "Filter OMNI2 data to dates 2018-10-01 to 2018-10-10",
            "Aggregate SP3 10-second data to hourly averages for altitude",
            "Remove hourly periods with insufficient SP3 data points",
            "Exclude variable-hour pairs where either value is a sentinel"
          ],
          [
            "Filter OMNI2 data for the period 2018-10-01 to 2018-10-10.",
            "Filter SP3 data for satellite L47 and the period 2018-10-01 to 2018-10-10."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Pearson correlation for each variable vs hourly altitude change",
          "Check for normality of variables before correlation",
          "Handle potential outliers in proton flux data",
          "Pearson correlation coefficient between each OMNI2/Sat_Density variable and hourly altitude change",
          "Select variable with maximum absolute correlation value",
          "Pearson correlation test"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pearson correlation for each variable vs hourly altitude change",
            "Check for normality of variables before correlation",
            "Handle potential outliers in proton flux data"
          ],
          [
            "Pearson correlation coefficient between each OMNI2/Sat_Density variable and hourly altitude change",
            "Select variable with maximum absolute correlation value"
          ],
          [
            "Pearson correlation test"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "List format: [variable_name, correlation_value]",
          "Correlation rounded to 3 decimal places",
          "Report strongest correlation by absolute magnitude",
          "Include both positive and negative correlations",
          "Report variable name with strongest correlation",
          "Report correlation value rounded to 3 decimal places",
          "Output as list format",
          "Include both positive and negative correlations in comparison",
          "Use absolute value to determine 'strongest' correlation",
          "Output a list containing the variable name and its Pearson correlation value (to 3 decimal places) with the hourly altitude change."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "List format: [variable_name, correlation_value]",
            "Correlation rounded to 3 decimal places",
            "Report strongest correlation by absolute magnitude",
            "Include both positive and negative correlations"
          ],
          [
            "Report variable name with strongest correlation",
            "Report correlation value rounded to 3 decimal places",
            "Output as list format",
            "Include both positive and negative correlations in comparison",
            "Use absolute value to determine 'strongest' correlation"
          ],
          [
            "Output a list containing the variable name and its Pearson correlation value (to 3 decimal places) with the hourly altitude change."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5649335839598998
  },
  "astronomy-hard-11": {
    "m_q": {
      "target_metric": {
        "value": "RMSE between NRLMSISE-00 predicted neutral density and Swarm-B measured neutral density for all 2024 observations",
        "confidence": 0.3333333333333333,
        "votes": [
          "RMSE between NRLMSISE-00 predicted neutral density and Swarm-B measured neutral density for all 2024 observations",
          "Root Mean Square Error (RMSE) between NRLMSISE-00 predicted neutral density and Swarm-B measured neutral density for entire year 2024",
          ""
        ]
      },
      "filters": {
        "value": [
          "Time range: 2024-01-01 to 2024-12-31",
          "Swarm-B satellite data only",
          "OMNI2 data for 2024 only",
          "Year 2024",
          "Valid (non-missing) OMNI2 space weather parameters",
          "Valid measured neutral density values from POD files"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Time range: 2024-01-01 to 2024-12-31",
            "Swarm-B satellite data only",
            "OMNI2 data for 2024 only"
          ],
          [
            "Year 2024",
            "Swarm-B satellite data only",
            "Valid (non-missing) OMNI2 space weather parameters",
            "Valid measured neutral density values from POD files"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "timestamp",
          "satellite_position_parameters",
          "timestamp (date and time)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "timestamp",
            "satellite_position_parameters"
          ],
          [
            "timestamp (date and time)"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar (single RMSE value)",
        "confidence": 0.3333333333333333,
        "votes": [
          "scalar (single RMSE value)",
          "scalar",
          ""
        ]
      },
      "sub_questions": {
        "value": [
          "Extract F10.7, F10.7A, daily Ap, and 3-hour Ap vector from OMNI2 data",
          "Prepare NRLMSISE-00 model inputs from OMNI2 data",
          "Run NRLMSISE-00 model for each Swarm-B observation timestamp",
          "Extract measured neutral density from Swarm-B POD files",
          "Calculate RMSE between predicted and measured densities",
          "What are the required OMNI2 columns for F10.7, F10.7A, daily Ap, and 3-hour Ap indices?",
          "How to construct the 3-hour Ap vector (seven values: current + six previous) from OMNI2 hourly data?",
          "What are the Swarm-B position coordinates (altitude, latitude, longitude) and timestamp columns?",
          "What is the measured neutral density column in Swarm-B POD files?",
          "How to interpolate OMNI2 hourly data to match Swarm-B 30-second measurement timestamps?",
          "How to calculate F10.7A (81-day centered average) from daily F10.7 values?",
          "What is the NRLMSISE-00 model interface and required input parameters?",
          "How to handle missing or fill values (999999.99, 99999.99, 999.9) in OMNI2 data?",
          "What are the units of measured and predicted density?",
          "How to aggregate predictions and measurements to compute yearly RMSE?"
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Extract F10.7, F10.7A, daily Ap, and 3-hour Ap vector from OMNI2 data",
            "Prepare NRLMSISE-00 model inputs from OMNI2 data",
            "Run NRLMSISE-00 model for each Swarm-B observation timestamp",
            "Extract measured neutral density from Swarm-B POD files",
            "Calculate RMSE between predicted and measured densities"
          ],
          [
            "What are the required OMNI2 columns for F10.7, F10.7A, daily Ap, and 3-hour Ap indices?",
            "How to construct the 3-hour Ap vector (seven values: current + six previous) from OMNI2 hourly data?",
            "What are the Swarm-B position coordinates (altitude, latitude, longitude) and timestamp columns?",
            "What is the measured neutral density column in Swarm-B POD files?",
            "How to interpolate OMNI2 hourly data to match Swarm-B 30-second measurement timestamps?",
            "How to calculate F10.7A (81-day centered average) from daily F10.7 values?",
            "What is the NRLMSISE-00 model interface and required input parameters?",
            "How to handle missing or fill values (999999.99, 99999.99, 999.9) in OMNI2 data?",
            "What are the units of measured and predicted density?",
            "How to aggregate predictions and measurements to compute yearly RMSE?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "SB_DNS_POD_2024_01_v02.txt",
          "SB_DNS_POD_2024_02_v02.txt",
          "SB_DNS_POD_2024_03_v02.txt",
          "SB_DNS_POD_2024_04_v02.txt",
          "SB_DNS_POD_2024_05_v02.txt",
          "SB_DNS_POD_2024_06_v02.txt",
          "SB_DNS_POD_2024_07_v02.txt",
          "SB_DNS_POD_2024_08_v02.txt",
          "SB_DNS_POD_2024_09_v02.txt",
          "SB_DNS_POD_2024_10_v02.txt",
          "SB_DNS_POD_2024_11_v02.txt",
          "SB_DNS_POD_2024_12_v02.txt",
          "omni2_2024.dat",
          "omni2_2023.dat",
          "omni2.text"
        ],
        "confidence": 0.6666666666666665,
        "votes": [
          [
            "SB_DNS_POD_2024_01_v02.txt",
            "SB_DNS_POD_2024_02_v02.txt",
            "SB_DNS_POD_2024_03_v02.txt",
            "SB_DNS_POD_2024_04_v02.txt",
            "SB_DNS_POD_2024_05_v02.txt",
            "SB_DNS_POD_2024_06_v02.txt",
            "SB_DNS_POD_2024_07_v02.txt",
            "SB_DNS_POD_2024_08_v02.txt",
            "SB_DNS_POD_2024_09_v02.txt",
            "SB_DNS_POD_2024_10_v02.txt",
            "SB_DNS_POD_2024_11_v02.txt",
            "SB_DNS_POD_2024_12_v02.txt",
            "omni2_2024.dat",
            "omni2_2023.dat",
            "omni2.text"
          ],
          [
            "omni2_2024.dat",
            "omni2_2023.dat",
            "SB_DNS_POD_2024_01_v02.txt",
            "SB_DNS_POD_2024_02_v02.txt",
            "SB_DNS_POD_2024_03_v02.txt",
            "SB_DNS_POD_2024_04_v02.txt",
            "SB_DNS_POD_2024_05_v02.txt",
            "SB_DNS_POD_2024_06_v02.txt",
            "SB_DNS_POD_2024_07_v02.txt",
            "SB_DNS_POD_2024_08_v02.txt",
            "SB_DNS_POD_2024_09_v02.txt",
            "SB_DNS_POD_2024_10_v02.txt",
            "SB_DNS_POD_2024_11_v02.txt",
            "SB_DNS_POD_2024_12_v02.txt",
            "omni2.text"
          ],
          []
        ]
      },
      "schema_conflicts": {
        "value": [
          "Swarm-B files have date and time in separate columns (col0 and col1), OMNI2 has combined datetime",
          "Swarm-B column names are generic (col0-col8), OMNI2 column names are numeric codes",
          "OMNI2 data format requires reference to omni2.text for column interpretation",
          "OMNI2 files have 55 columns with no proper header row; column names are generic (integers and floats)",
          "Swarm-B POD files have 9 columns with no header; first three columns are date, time, UTC marker",
          "OMNI2 column positions must be mapped from specification in omni2.text",
          "Swarm-B columns need identification: date, time, UTC, unknown1, latitude, longitude, unknown2, unknown3, neutral_density",
          "OMNI2 data is hourly; Swarm-B data is 30-second resolution requiring temporal interpolation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Swarm-B files have date and time in separate columns (col0 and col1), OMNI2 has combined datetime",
            "Swarm-B column names are generic (col0-col8), OMNI2 column names are numeric codes",
            "OMNI2 data format requires reference to omni2.text for column interpretation"
          ],
          [
            "OMNI2 files have 55 columns with no proper header row; column names are generic (integers and floats)",
            "Swarm-B POD files have 9 columns with no header; first three columns are date, time, UTC marker",
            "OMNI2 column positions must be mapped from specification in omni2.text",
            "Swarm-B columns need identification: date, time, UTC, unknown1, latitude, longitude, unknown2, unknown3, neutral_density",
            "OMNI2 data is hourly; Swarm-B data is 30-second resolution requiring temporal interpolation"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "col3": "meters (altitude)",
          "col4": "degrees (latitude)",
          "col5": "degrees (longitude)",
          "col6": "hours (local solar time)",
          "col7": "degrees (solar zenith angle)",
          "col8": "kg/m\u00b3 (neutral density)",
          "f10.7": "solar flux unit (sfu)",
          "ap": "nT (geomagnetic index)",
          "neutral_density_measured": "kg/m^3 (scientific notation)",
          "f10.7_index": "10^-22 W/(m^2 Hz) or sfu",
          "ap_index": "dimensionless geomagnetic index",
          "latitude": "degrees",
          "longitude": "degrees",
          "altitude": "km (likely column 'unknown_param2')",
          "time": "UTC seconds or HH:MM:SS.mmm",
          "date": "YYYY-MM-DD"
        },
        "confidence": 0.33333333333333326,
        "votes": [
          {
            "col3": "meters (altitude)",
            "col4": "degrees (latitude)",
            "col5": "degrees (longitude)",
            "col6": "hours (local solar time)",
            "col7": "degrees (solar zenith angle)",
            "col8": "kg/m\u00b3 (neutral density)",
            "F10.7": "solar flux unit (sfu)",
            "Ap": "nT (geomagnetic index)"
          },
          {
            "neutral_density_measured": "kg/m^3 (scientific notation)",
            "F10.7_index": "10^-22 W/(m^2 Hz) or sfu",
            "Ap_index": "dimensionless geomagnetic index",
            "latitude": "degrees",
            "longitude": "degrees",
            "altitude": "km (likely column 'unknown_param2')",
            "time": "UTC seconds or HH:MM:SS.mmm",
            "date": "YYYY-MM-DD"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Neutral density values in scientific notation (e.g., 0.30634757E-12)",
          "OMNI2 data contains placeholder values (999999.99, 99999.99) that need filtering",
          "Swarm-B time resolution is 30 seconds, OMNI2 is hourly",
          "Neutral density in scientific notation (e.g., 0.30634757E-12) needs proper parsing",
          "OMNI2 F10.7 index may need scaling factor verification",
          "OMNI2 uses day-of-year (1-366) instead of month-day format",
          "Swarm-B altitude column identification needed (likely column index 6 with value ~22 km suggests wrong unit or different parameter)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Neutral density values in scientific notation (e.g., 0.30634757E-12)",
            "OMNI2 data contains placeholder values (999999.99, 99999.99) that need filtering",
            "Swarm-B time resolution is 30 seconds, OMNI2 is hourly"
          ],
          [
            "Neutral density in scientific notation (e.g., 0.30634757E-12) needs proper parsing",
            "OMNI2 F10.7 index may need scaling factor verification",
            "OMNI2 uses day-of-year (1-366) instead of month-day format",
            "Swarm-B altitude column identification needed (likely column index 6 with value ~22 km suggests wrong unit or different parameter)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Time alignment: Swarm-B 30-second data vs OMNI2 hourly data requires temporal interpolation",
          "Geographic coordinates vs solar/magnetic coordinates for NRLMSISE-00 inputs",
          "Temporal resolution mismatch: OMNI2 hourly vs Swarm-B 30-second",
          "OMNI2 covers full days; Swarm-B has gaps in coverage",
          "Missing value indicators differ: OMNI2 uses 999999.99, 99999.99, 999.9; Swarm-B format unclear",
          "Need 2023 OMNI2 data (last ~40 days) to compute 81-day centered F10.7A for early January 2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time alignment: Swarm-B 30-second data vs OMNI2 hourly data requires temporal interpolation",
            "Geographic coordinates vs solar/magnetic coordinates for NRLMSISE-00 inputs"
          ],
          [
            "Temporal resolution mismatch: OMNI2 hourly vs Swarm-B 30-second",
            "OMNI2 covers full days; Swarm-B has gaps in coverage",
            "Missing value indicators differ: OMNI2 uses 999999.99, 99999.99, 999.9; Swarm-B format unclear",
            "Need 2023 OMNI2 data (last ~40 days) to compute 81-day centered F10.7A for early January 2024"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": false,
        "confidence": 0.6666666666666666,
        "votes": [
          false,
          false,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "999999.99",
          "99999.99",
          "999.9",
          "-9",
          "",
          "9999.99",
          "9999.9"
        ],
        "confidence": 0.4761904761904763,
        "votes": [
          [
            "999999.99",
            "99999.99",
            "999.9",
            "-9",
            ""
          ],
          [
            "999999.99",
            "99999.99",
            "999.9",
            "9999.99",
            "9999.9"
          ],
          []
        ]
      },
      "expected_columns": {
        "value": 55.0,
        "confidence": 0.33,
        "votes": [
          9.0,
          55.0,
          0.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Must use only OMNI2 data for space weather inputs (no external feeds)",
          "3-hour Ap vector must be constructed according to NRLMSISE-00 specification",
          "F10.7A (81-day average) must be calculated from OMNI2 F10.7 data",
          "Daily Ap must be derived from OMNI2 data",
          "Time range limited to 2024",
          "Use only OMNI2 data for space weather inputs - no external feeds",
          "Calculate F10.7A as 81-day centered average from OMNI2 F10.7 values",
          "Construct 3-hour Ap vector according to NRLMSISE-00 specification: [current, -3hr, -6hr, -9hr, -12-33hr avg, -36-57hr avg, -57-59hr avg]",
          "Use daily Ap from OMNI2 for the daily magnetic index parameter",
          "Match NRLMSISE-00 predictions to Swarm-B measurement timestamps via interpolation",
          "Report single RMSE value over entire 2024 calendar year",
          "Exclude records with OMNI2 missing values (999999.99, 99999.99, 999.9)",
          "Exclude records with invalid Swarm-B neutral density measurements"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Must use only OMNI2 data for space weather inputs (no external feeds)",
            "3-hour Ap vector must be constructed according to NRLMSISE-00 specification",
            "F10.7A (81-day average) must be calculated from OMNI2 F10.7 data",
            "Daily Ap must be derived from OMNI2 data",
            "Time range limited to 2024"
          ],
          [
            "Use only OMNI2 data for space weather inputs - no external feeds",
            "Calculate F10.7A as 81-day centered average from OMNI2 F10.7 values",
            "Construct 3-hour Ap vector according to NRLMSISE-00 specification: [current, -3hr, -6hr, -9hr, -12-33hr avg, -36-57hr avg, -57-59hr avg]",
            "Use daily Ap from OMNI2 for the daily magnetic index parameter",
            "Match NRLMSISE-00 predictions to Swarm-B measurement timestamps via interpolation",
            "Report single RMSE value over entire 2024 calendar year",
            "Exclude records with OMNI2 missing values (999999.99, 99999.99, 999.9)",
            "Exclude records with invalid Swarm-B neutral density measurements"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out OMNI2 placeholder values (999999.99, etc.)",
          "Ensure time alignment between Swarm-B observations and OMNI2 data",
          "Validate geographic coordinate ranges (-90 to 90\u00b0 latitude, -180 to 180\u00b0 longitude)",
          "Valid F10.7 index (< 999)",
          "Valid Ap indices (< 999)",
          "Valid Swarm-B position data (latitude in [-90, 90], longitude in [-180, 360])",
          "Valid neutral density measurements (positive values, not null)",
          "Sufficient historical OMNI2 data for 81-day F10.7A window (need ~40 days from 2023)",
          "Sufficient historical OMNI2 data for 3-hour Ap vector lookback (~59 hours)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out OMNI2 placeholder values (999999.99, etc.)",
            "Ensure time alignment between Swarm-B observations and OMNI2 data",
            "Validate geographic coordinate ranges (-90 to 90\u00b0 latitude, -180 to 180\u00b0 longitude)"
          ],
          [
            "Valid F10.7 index (< 999)",
            "Valid Ap indices (< 999)",
            "Valid Swarm-B position data (latitude in [-90, 90], longitude in [-180, 360])",
            "Valid neutral density measurements (positive values, not null)",
            "Sufficient historical OMNI2 data for 81-day F10.7A window (need ~40 days from 2023)",
            "Sufficient historical OMNI2 data for 3-hour Ap vector lookback (~59 hours)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in time series",
          "Validate neutral density value ranges (positive, physically plausible)",
          "Test temporal consistency of OMNI2 indices",
          "RMSE calculation: sqrt(mean((predicted - measured)^2))",
          "Verify prediction-measurement pair count matches expected ~2.9M records (365 days * 24 hr * 60 min * 2 per min)",
          "Check for systematic bias (mean error)",
          "Verify density value ranges are physical (> 0, typical range 1e-13 to 1e-11 kg/m^3 at ~500 km)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in time series",
            "Validate neutral density value ranges (positive, physically plausible)",
            "Test temporal consistency of OMNI2 indices"
          ],
          [
            "RMSE calculation: sqrt(mean((predicted - measured)^2))",
            "Verify prediction-measurement pair count matches expected ~2.9M records (365 days * 24 hr * 60 min * 2 per min)",
            "Check for systematic bias (mean error)",
            "Verify density value ranges are physical (> 0, typical range 1e-13 to 1e-11 kg/m^3 at ~500 km)"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "RMSE reported as single numeric value",
          "Comparison should include all valid 2024 observations",
          "NRLMSISE-00 must be run with correct input parameter ordering",
          "Report RMSE as single scalar value with units (kg/m^3)",
          "Include number of valid prediction-measurement pairs used",
          "Document any data gaps or excluded periods",
          "Specify NRLMSISE-00 model version used",
          "Report temporal coverage (start and end timestamps)",
          "Include summary statistics: mean predicted density, mean measured density, standard deviations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "RMSE reported as single numeric value",
            "Comparison should include all valid 2024 observations",
            "NRLMSISE-00 must be run with correct input parameter ordering"
          ],
          [
            "Report RMSE as single scalar value with units (kg/m^3)",
            "Include number of valid prediction-measurement pairs used",
            "Document any data gaps or excluded periods",
            "Specify NRLMSISE-00 model version used",
            "Report temporal coverage (start and end timestamps)",
            "Include summary statistics: mean predicted density, mean measured density, standard deviations"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.4764206349206351
  },
  "astronomy-hard-12": {
    "m_q": {
      "target_metric": {
        "value": "Mean geopotential energy per unit mass (J/kg) for Swarm-A satellite from September 2 to 29, 2019",
        "confidence": 0.3333333333333333,
        "votes": [
          "Mean geopotential energy per unit mass (J/kg) for Swarm-A satellite from September 2 to 29, 2019",
          "Mean geopotential energy per unit mass (J/kg) for Swarm-A satellite from September 2-29, 2019",
          "Mean geopotential energy per unit mass (J/kg) experienced by Swarm-A from September 2 to 29, 2019"
        ]
      },
      "filters": {
        "value": [
          "Time range: 2019-09-02 to 2019-09-29",
          "Satellite: Swarm-A only",
          "Date range: September 2, 2019 to September 29, 2019",
          "Satellite: Swarm-A",
          "Date range: September 2 to 29, 2019"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Time range: 2019-09-02 to 2019-09-29",
            "Satellite: Swarm-A only"
          ],
          [
            "Date range: September 2, 2019 to September 29, 2019",
            "Satellite: Swarm-A"
          ],
          [
            "Date range: September 2 to 29, 2019",
            "Satellite: Swarm-A"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "timepoint"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "timepoint"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Extract satellite positions from SP3 files",
          "Interpolate geopotential field from mock_tiegcm_grid_sept2019.npz",
          "Compute geopotential energy at each timepoint",
          "Average energy values over time period",
          "Where are the SP3 files containing Swarm-A orbital data for September 2-29, 2019?",
          "How to extract geodetic position (latitude, longitude, altitude) from SP3 files?",
          "How to define the mock geopotential field using lat_grid, lon_grid, alt_grid?",
          "What is the geopotential function to interpolate from the grid?",
          "How to convert satellite positions to match grid coordinate system?",
          "How to compute geopotential energy per unit mass at each timepoint?",
          "How to average the energy values across all timepoints in the period?",
          "Determine the geodetic position (latitude, longitude, altitude) of Swarm-A at each timepoint using SP3 files (not provided, assuming this is a pre-processing step).",
          "Interpolate the geopotential field at each timepoint based on the satellite's geodetic position using the mock_tiegcm_grid_sept2019.npz data.",
          "Calculate the geopotential energy per unit mass at each timepoint.",
          "Average the geopotential energy per unit mass over the specified time period."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Extract satellite positions from SP3 files",
            "Interpolate geopotential field from mock_tiegcm_grid_sept2019.npz",
            "Compute geopotential energy at each timepoint",
            "Average energy values over time period"
          ],
          [
            "Where are the SP3 files containing Swarm-A orbital data for September 2-29, 2019?",
            "How to extract geodetic position (latitude, longitude, altitude) from SP3 files?",
            "How to define the mock geopotential field using lat_grid, lon_grid, alt_grid?",
            "What is the geopotential function to interpolate from the grid?",
            "How to convert satellite positions to match grid coordinate system?",
            "How to compute geopotential energy per unit mass at each timepoint?",
            "How to average the energy values across all timepoints in the period?"
          ],
          [
            "Determine the geodetic position (latitude, longitude, altitude) of Swarm-A at each timepoint using SP3 files (not provided, assuming this is a pre-processing step).",
            "Interpolate the geopotential field at each timepoint based on the satellite's geodetic position using the mock_tiegcm_grid_sept2019.npz data.",
            "Calculate the geopotential energy per unit mass at each timepoint.",
            "Average the geopotential energy per unit mass over the specified time period."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "mock_tiegcm_grid_sept2019.npz",
          "SP3 orbital data files (not provided in sample)",
          "SP3 files (not provided, required for Swarm-A precise orbital data)"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "mock_tiegcm_grid_sept2019.npz",
            "SP3 orbital data files (not provided in sample)"
          ],
          [
            "mock_tiegcm_grid_sept2019.npz",
            "SP3 files (not provided, required for Swarm-A precise orbital data)"
          ],
          [
            "mock_tiegcm_grid_sept2019.npz"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "SP3 file format not specified in data files",
          "Missing SP3 data source for satellite positions",
          "SP3 files not present in provided data - only grid definition available",
          "Missing geopotential values - only grid coordinates provided"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "SP3 file format not specified in data files",
            "Missing SP3 data source for satellite positions"
          ],
          [
            "SP3 files not present in provided data - only grid definition available",
            "Missing geopotential values - only grid coordinates provided"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "lat_grid": "degrees",
          "lon_grid": "degrees",
          "alt_grid": "km",
          "geopotential_energy": "J/kg",
          "earth_radius": "km",
          "g": "m/s\u00b2",
          "target_output": "J/kg"
        },
        "confidence": 0.7142857142857143,
        "votes": [
          {
            "lat_grid": "degrees",
            "lon_grid": "degrees",
            "alt_grid": "km",
            "geopotential_energy": "J/kg",
            "earth_radius": "km",
            "g": "m/s\u00b2"
          },
          {
            "lat_grid": "degrees",
            "lon_grid": "degrees",
            "alt_grid": "km (assumed based on earth radius context)",
            "earth_radius": "6371.0 km",
            "g": "9.80665 m/s^2",
            "target_output": "J/kg"
          },
          {
            "lat_grid": "degrees",
            "lon_grid": "degrees",
            "alt_grid": "km"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "alt_grid appears to be in km (100-500 range), but earth radius given as 6371.0 km - need consistent units for altitude calculations",
          "Altitude in grid appears to be in km, needs conversion to meters for energy calculation",
          "Earth radius given in km (6371.0), gravitational constant in m/s^2",
          "Need consistent unit system for geopotential energy calculation (SI units: meters, m/s^2)",
          "Altitude is in kilometers, but the geopotential energy calculation might require meters. Need to convert alt_grid to meters if necessary."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "alt_grid appears to be in km (100-500 range), but earth radius given as 6371.0 km - need consistent units for altitude calculations"
          ],
          [
            "Altitude in grid appears to be in km, needs conversion to meters for energy calculation",
            "Earth radius given in km (6371.0), gravitational constant in m/s^2",
            "Need consistent unit system for geopotential energy calculation (SI units: meters, m/s^2)"
          ],
          [
            "Altitude is in kilometers, but the geopotential energy calculation might require meters. Need to convert alt_grid to meters if necessary."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "SP3 files needed for satellite positions but not provided in data files",
          "SP3 orbital data units (typically in meters or km) must match grid altitude units",
          "Coordinate systems: geodetic (from SP3) vs grid coordinates (lat/lon/alt)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "SP3 files needed for satellite positions but not provided in data files"
          ],
          [
            "SP3 orbital data units (typically in meters or km) must match grid altitude units",
            "Coordinate systems: geodetic (from SP3) vs grid coordinates (lat/lon/alt)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 0.6666666666666666,
        "votes": [
          true,
          false,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 0.6666666666666666,
        "votes": [
          ",",
          "N/A",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 0.6666666666666666,
        "votes": [
          "utf-8",
          "binary",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.6,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 3.0,
        "confidence": 1.0,
        "votes": [
          3.0,
          3.0,
          3.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "npz (NumPy compressed array)",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Earth radius = 6371.0 km",
          "g = 9.80665 m/s\u00b2",
          "Time period: September 2-29, 2019",
          "Output rounded to 2 decimal places",
          "Date range must be September 2-29, 2019 (28 days)",
          "Satellite must be Swarm-A",
          "Gravitational acceleration g = 9.80665 m/s^2",
          "Output must be rounded to 2 decimal places",
          "g = 9.80665 m/s^2",
          "Output should be rounded to 2 decimal places"
        ],
        "confidence": 0.4,
        "votes": [
          [
            "Earth radius = 6371.0 km",
            "g = 9.80665 m/s\u00b2",
            "Time period: September 2-29, 2019",
            "Output rounded to 2 decimal places"
          ],
          [
            "Date range must be September 2-29, 2019 (28 days)",
            "Satellite must be Swarm-A",
            "Earth radius = 6371.0 km",
            "Gravitational acceleration g = 9.80665 m/s^2",
            "Output must be rounded to 2 decimal places"
          ],
          [
            "Earth radius = 6371.0 km",
            "g = 9.80665 m/s^2",
            "Output should be rounded to 2 decimal places"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter SP3 data to Swarm-A satellite only",
          "Filter SP3 data to specified date range",
          "Extract only Swarm-A records from SP3 files",
          "Filter timestamps to range [2019-09-02 00:00:00, 2019-09-29 23:59:59]",
          "Exclude invalid or missing position data points"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter SP3 data to Swarm-A satellite only",
            "Filter SP3 data to specified date range"
          ],
          [
            "Extract only Swarm-A records from SP3 files",
            "Filter timestamps to range [2019-09-02 00:00:00, 2019-09-29 23:59:59]",
            "Exclude invalid or missing position data points"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in alt_grid",
          "Validate interpolation accuracy for geopotential field",
          "Verify mean calculation across all valid timepoints",
          "Check for outliers in computed energy values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in alt_grid",
            "Validate interpolation accuracy for geopotential field"
          ],
          [
            "Verify mean calculation across all valid timepoints",
            "Check for outliers in computed energy values"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numeric scalar with 2 decimal places",
          "Units: J/kg",
          "Single scalar value in J/kg",
          "Rounded to exactly 2 decimal places",
          "Format: XXX.XX",
          "The final answer must be a single numerical value representing the mean geopotential energy per unit mass in J/kg."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric scalar with 2 decimal places",
            "Units: J/kg"
          ],
          [
            "Single scalar value in J/kg",
            "Rounded to exactly 2 decimal places",
            "Format: XXX.XX"
          ],
          [
            "The final answer must be a single numerical value representing the mean geopotential energy per unit mass in J/kg."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5168253968253969
  },
  "astronomy-hard-7": {
    "m_q": {
      "target_metric": {
        "value": "RMSE between predicted and observed Swarm Alpha atmospheric density values over a 4-hour forecast window",
        "confidence": 0.6666666666666666,
        "votes": [
          "RMSE between predicted and observed Swarm Alpha atmospheric density values over a 4-hour forecast window",
          "RMSE between predicted and observed Swarm Alpha atmospheric density values over a 4-hour forecast window",
          "RMSE between predicted and observed Swarm Alpha atmospheric density values over the 4-hour forecast window"
        ]
      },
      "filters": {
        "value": [
          "Training: OMNI/GOES data from 2016-10-22 to 2016-10-23, Density data from 2016-10-23 to 2016-10-24",
          "Evaluation: OMNI/GOES data from 2016-10-25 to 2016-10-26, Density data from 2016-10-29",
          "Use only OMNI2 variables: f10.7_index, Kp_index, Dst_index_nT",
          "Use only GOES variables: xrsb_flux_observed, xrsa_flux_observed",
          "16-hour context window for VAR(1) model",
          "4-hour forecast window for linear regression",
          "Training data: OMNI2/GOES from 2016-10-22 to 2016-10-23, Density from 2016-10-23 to 2016-10-24 (wu334)",
          "Evaluation data: OMNI2/GOES from 2016-10-25 to 2016-10-26, Density from 2016-10-29 (wu335)",
          "Use 16-hour context window for VAR(1) model",
          "Forecast 4 hours ahead using linear regression",
          "Training data from wu334 (OMNI/GOES: 2016-10-22 to 2016-10-23; Density: 2016-10-23 to 2016-10-24)",
          "Evaluation data from wu335 (OMNI/GOES: 2016-10-25 to 2016-10-26; Density: 2016-10-29)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Training: OMNI/GOES data from 2016-10-22 to 2016-10-23, Density data from 2016-10-23 to 2016-10-24",
            "Evaluation: OMNI/GOES data from 2016-10-25 to 2016-10-26, Density data from 2016-10-29",
            "Use only OMNI2 variables: f10.7_index, Kp_index, Dst_index_nT",
            "Use only GOES variables: xrsb_flux_observed, xrsa_flux_observed",
            "16-hour context window for VAR(1) model",
            "4-hour forecast window for linear regression"
          ],
          [
            "Training data: OMNI2/GOES from 2016-10-22 to 2016-10-23, Density from 2016-10-23 to 2016-10-24 (wu334)",
            "Evaluation data: OMNI2/GOES from 2016-10-25 to 2016-10-26, Density from 2016-10-29 (wu335)",
            "Use 16-hour context window for VAR(1) model",
            "Forecast 4 hours ahead using linear regression"
          ],
          [
            "Training data from wu334 (OMNI/GOES: 2016-10-22 to 2016-10-23; Density: 2016-10-23 to 2016-10-24)",
            "Evaluation data from wu335 (OMNI/GOES: 2016-10-25 to 2016-10-26; Density: 2016-10-29)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Timestamp"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Timestamp"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to align 16-hour OMNI/GOES windows with 4-hour density forecasts?",
          "How to handle the single timestamp overlap between OMNI/GOES end and Density start?",
          "How to project time series forward using VAR(1) model?",
          "How to fit linear regression to predict next 4 hours of density?",
          "Extract the last 16 hours of OMNI2/GOES data from wu334 training period (2016-10-22 to 2016-10-23)",
          "Extract corresponding Swarm Alpha density data beginning at the end of OMNI2/GOES window (2016-10-23 to 2016-10-24)",
          "Fit VAR(1) model on the 16-hour context window using f10.7_index, Kp_index, Dst_index_nT, xrsb_flux_observed, xrsa_flux_observed",
          "Project the input time series 4 hours forward using the VAR(1) model",
          "Fit linear regression model mapping projected features to density values",
          "Extract the last 16 hours of OMNI2/GOES data from wu335 evaluation period (2016-10-25 to 2016-10-26)",
          "Project wu335 OMNI2/GOES features 4 hours forward using fitted VAR(1) model",
          "Predict density for 4-hour window using linear regression model",
          "Extract observed density from wu335 for 2016-10-29",
          "Calculate RMSE between predicted and observed density values",
          "Train a VAR(1) model using a 16-hour context window of OMNI2 (f10.7_index, Kp_index, Dst_index_nT) and GOES (xrsb_flux_observed, xrsa_flux_observed) variables to project the time series forward.",
          "Fit a linear regression model to predict the next 4 hours of Swarm Alpha's atmospheric density.",
          "Calculate the RMSE between the predicted and observed density values for the evaluation dataset."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "How to align 16-hour OMNI/GOES windows with 4-hour density forecasts?",
            "How to handle the single timestamp overlap between OMNI/GOES end and Density start?",
            "How to project time series forward using VAR(1) model?",
            "How to fit linear regression to predict next 4 hours of density?"
          ],
          [
            "Extract the last 16 hours of OMNI2/GOES data from wu334 training period (2016-10-22 to 2016-10-23)",
            "Extract corresponding Swarm Alpha density data beginning at the end of OMNI2/GOES window (2016-10-23 to 2016-10-24)",
            "Fit VAR(1) model on the 16-hour context window using f10.7_index, Kp_index, Dst_index_nT, xrsb_flux_observed, xrsa_flux_observed",
            "Project the input time series 4 hours forward using the VAR(1) model",
            "Fit linear regression model mapping projected features to density values",
            "Extract the last 16 hours of OMNI2/GOES data from wu335 evaluation period (2016-10-25 to 2016-10-26)",
            "Project wu335 OMNI2/GOES features 4 hours forward using fitted VAR(1) model",
            "Predict density for 4-hour window using linear regression model",
            "Extract observed density from wu335 for 2016-10-29",
            "Calculate RMSE between predicted and observed density values"
          ],
          [
            "Train a VAR(1) model using a 16-hour context window of OMNI2 (f10.7_index, Kp_index, Dst_index_nT) and GOES (xrsb_flux_observed, xrsa_flux_observed) variables to project the time series forward.",
            "Fit a linear regression model to predict the next 4 hours of Swarm Alpha's atmospheric density.",
            "Calculate the RMSE between the predicted and observed density values for the evaluation dataset."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "omni2-wu334-20160824_to_20161023.csv",
          "goes-wu334-20160824_to_20161023.csv",
          "swarma-wu334-20161023_to_20161026.csv",
          "omni2-wu335-20160827_to_20161026.csv",
          "goes-wu335-20160827_to_20161026.csv",
          "swarma-wu335-20161026_to_20161029.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "omni2-wu334-20160824_to_20161023.csv",
            "goes-wu334-20160824_to_20161023.csv",
            "swarma-wu334-20161023_to_20161026.csv",
            "omni2-wu335-20160827_to_20161026.csv",
            "goes-wu335-20160827_to_20161026.csv",
            "swarma-wu335-20161026_to_20161029.csv"
          ],
          [
            "omni2-wu334-20160824_to_20161023.csv",
            "goes-wu334-20160824_to_20161023.csv",
            "swarma-wu334-20161023_to_20161026.csv",
            "omni2-wu335-20160827_to_20161026.csv",
            "goes-wu335-20160827_to_20161026.csv",
            "swarma-wu335-20161026_to_20161029.csv"
          ],
          [
            "omni2-wu334-20160824_to_20161023.csv",
            "goes-wu334-20160824_to_20161023.csv",
            "swarma-wu334-20161023_to_20161026.csv",
            "omni2-wu335-20160827_to_20161026.csv",
            "goes-wu335-20160827_to_20161026.csv",
            "swarma-wu335-20161026_to_20161029.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "OMNI2 files have 57 columns, GOES files have 13 columns, Swarm files have 2 columns",
          "OMNI2 Timestamp appears hourly, GOES Timestamp appears minutely, Swarm Timestamp appears every 10 minutes",
          "OMNI2 data has hourly resolution while GOES data has minute-level resolution (needs resampling to hourly)",
          "Swarm Alpha density data has 10-minute resolution (needs resampling or alignment)",
          "Temporal alignment required between OMNI2/GOES end timestamp and Swarm Alpha density start timestamp"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "OMNI2 files have 57 columns, GOES files have 13 columns, Swarm files have 2 columns",
            "OMNI2 Timestamp appears hourly, GOES Timestamp appears minutely, Swarm Timestamp appears every 10 minutes"
          ],
          [
            "OMNI2 data has hourly resolution while GOES data has minute-level resolution (needs resampling to hourly)",
            "Swarm Alpha density data has 10-minute resolution (needs resampling or alignment)",
            "Temporal alignment required between OMNI2/GOES end timestamp and Swarm Alpha density start timestamp"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "f10.7_index": "solar flux unit (10^-22 W\u00b7m^-2\u00b7Hz^-1)",
          "kp_index": "unitless (0-9)",
          "dst_index_nt": "nanoTesla",
          "xrsb_flux_observed": "W/m^2",
          "xrsa_flux_observed": "W/m^2",
          "orbit mean density (kg/m^3)": "kg/m^3",
          "timestamp": "datetime"
        },
        "confidence": 0.9047619047619048,
        "votes": [
          {
            "f10.7_index": "solar flux unit (10^-22 W\u00b7m^-2\u00b7Hz^-1)",
            "Kp_index": "unitless (0-9)",
            "Dst_index_nT": "nanoTesla",
            "xrsb_flux_observed": "W/m^2",
            "xrsa_flux_observed": "W/m^2",
            "Orbit Mean Density (kg/m^3)": "kg/m^3"
          },
          {
            "f10.7_index": "solar flux units (10^-22 W/m^2/Hz)",
            "Kp_index": "dimensionless geomagnetic index",
            "Dst_index_nT": "nanotesla",
            "xrsb_flux_observed": "W/m^2",
            "xrsa_flux_observed": "W/m^2",
            "Orbit Mean Density (kg/m^3)": "kg/m^3",
            "Timestamp": "datetime"
          },
          {
            "f10.7_index": "sfu",
            "Kp_index": "unitless",
            "Dst_index_nT": "nT",
            "xrsb_flux_observed": "W/m^2",
            "xrsa_flux_observed": "W/m^2",
            "Orbit Mean Density (kg/m^3)": "kg/m^3"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "OMNI2 data is hourly, GOES data is minutely, Swarm data is every 10 minutes",
          "f10.7_index values ~79-85, Kp_index values 0-53, Dst_index_nT values -39 to -6",
          "xrs fluxes range from 1e-09 to ~1e-07",
          "Density values range from ~2.8e-13 to 6.8e-13",
          "xrsa_flux_observed and xrsb_flux_observed are in very small values (1e-9 to 1e-7 range), may need log transformation",
          "Density values are extremely small (1e-13 range), may need scaling for numerical stability in regression",
          "GOES data has minute resolution requiring aggregation to hourly to match OMNI2",
          "Swarm Alpha has 10-minute resolution requiring alignment to forecast timestamps"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "OMNI2 data is hourly, GOES data is minutely, Swarm data is every 10 minutes",
            "f10.7_index values ~79-85, Kp_index values 0-53, Dst_index_nT values -39 to -6",
            "xrs fluxes range from 1e-09 to ~1e-07",
            "Density values range from ~2.8e-13 to 6.8e-13"
          ],
          [
            "xrsa_flux_observed and xrsb_flux_observed are in very small values (1e-9 to 1e-7 range), may need log transformation",
            "Density values are extremely small (1e-13 range), may need scaling for numerical stability in regression",
            "GOES data has minute resolution requiring aggregation to hourly to match OMNI2",
            "Swarm Alpha has 10-minute resolution requiring alignment to forecast timestamps"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Different temporal resolutions require aggregation/resampling",
          "Training and evaluation periods have different date ranges",
          "GOES data has many more rows (86401) than OMNI2 (1441) for same time period",
          "OMNI2 hourly data vs GOES minute-level data temporal resolution mismatch",
          "Training period ends 2016-10-23 for OMNI/GOES and starts 2016-10-23 for density (overlap at single timestamp)",
          "Evaluation period ends 2016-10-26 for OMNI/GOES but density is from 2016-10-29 (3-day gap suggesting 4-hour forecast spans multiple days)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different temporal resolutions require aggregation/resampling",
            "Training and evaluation periods have different date ranges",
            "GOES data has many more rows (86401) than OMNI2 (1441) for same time period"
          ],
          [
            "OMNI2 hourly data vs GOES minute-level data temporal resolution mismatch",
            "Training period ends 2016-10-23 for OMNI/GOES and starts 2016-10-23 for density (overlap at single timestamp)",
            "Evaluation period ends 2016-10-26 for OMNI/GOES but density is from 2016-10-29 (3-day gap suggesting 4-hour forecast spans multiple days)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "999999.99",
          "99999.99",
          "-1",
          "9999.99",
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.4761904761904763,
        "votes": [
          [
            "999999.99",
            "99999.99",
            "-1"
          ],
          [
            "999999.99",
            "99999.99",
            "9999.99",
            "-1"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 57.0,
        "confidence": 0.33,
        "votes": [
          0.0,
          0.0,
          57.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "16-hour context window must be contiguous",
          "4-hour forecast window must immediately follow context window",
          "Single timestamp overlap between OMNI/GOES end and Density start",
          "All windows contain valid data (assumption)",
          "VAR(1) model requires stationary time series",
          "Linear regression requires aligned feature and target vectors",
          "Use exactly 16-hour context window for VAR(1) model training",
          "Forecast exactly 4 hours ahead",
          "Use only specified OMNI2 variables: f10.7_index, Kp_index, Dst_index_nT",
          "Use only specified GOES variables: xrsb_flux_observed, xrsa_flux_observed",
          "Training OMNI/GOES: 2016-10-22 to 2016-10-23; Training Density: 2016-10-23 to 2016-10-24",
          "Evaluation OMNI/GOES: 2016-10-25 to 2016-10-26; Evaluation Density: 2016-10-29",
          "VAR(1) model must be order 1 (use only 1 lag)",
          "OMNI/GOES data ends at a timestamp where Density begins (single timestamp overlap)",
          "Assume all windows contain valid data (no missing data handling required)",
          "The 16-hour context window must contain valid data.",
          "The 4-hour forecast window must contain valid data."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "16-hour context window must be contiguous",
            "4-hour forecast window must immediately follow context window",
            "Single timestamp overlap between OMNI/GOES end and Density start",
            "All windows contain valid data (assumption)",
            "VAR(1) model requires stationary time series",
            "Linear regression requires aligned feature and target vectors"
          ],
          [
            "Use exactly 16-hour context window for VAR(1) model training",
            "Forecast exactly 4 hours ahead",
            "Use only specified OMNI2 variables: f10.7_index, Kp_index, Dst_index_nT",
            "Use only specified GOES variables: xrsb_flux_observed, xrsa_flux_observed",
            "Training OMNI/GOES: 2016-10-22 to 2016-10-23; Training Density: 2016-10-23 to 2016-10-24",
            "Evaluation OMNI/GOES: 2016-10-25 to 2016-10-26; Evaluation Density: 2016-10-29",
            "VAR(1) model must be order 1 (use only 1 lag)",
            "OMNI/GOES data ends at a timestamp where Density begins (single timestamp overlap)",
            "Assume all windows contain valid data (no missing data handling required)"
          ],
          [
            "The 16-hour context window must contain valid data.",
            "The 4-hour forecast window must contain valid data."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract 16-hour windows ending at training period boundaries",
          "Extract corresponding 4-hour density windows starting immediately after",
          "Filter OMNI2 to only f10.7_index, Kp_index, Dst_index_nT columns",
          "Filter GOES to only xrsb_flux_observed, xrsa_flux_observed columns",
          "Filter omni2-wu334 to extract rows from 2016-10-22 00:00:00 to 2016-10-23 23:00:00",
          "Filter goes-wu334 to extract rows from 2016-10-22 00:00:00 to 2016-10-23 23:59:00 and aggregate to hourly",
          "Filter swarma-wu334 to extract rows from 2016-10-23 00:00:00 to 2016-10-24 23:50:00",
          "Filter omni2-wu335 to extract rows from 2016-10-25 00:00:00 to 2016-10-26 23:00:00",
          "Filter goes-wu335 to extract rows from 2016-10-25 00:00:00 to 2016-10-26 23:59:00 and aggregate to hourly",
          "Filter swarma-wu335 to extract rows from 2016-10-29 with 4-hour duration for evaluation",
          "Extract last 16 hours from training OMNI/GOES for VAR model context",
          "Extract corresponding 4 hours of density data immediately following the 16-hour context window",
          "Training data: Timestamp between 2016-10-22 00:00:00 and 2016-10-23 23:00:00 for OMNI2 and GOES, and 2016-10-23 00:00:00 and 2016-10-24 03:00:00 for Swarm Alpha density.",
          "Evaluation data: Timestamp between 2016-10-25 00:00:00 and 2016-10-26 23:00:00 for OMNI2 and GOES, and 2016-10-29 00:00:00 and 2016-10-29 03:00:00 for Swarm Alpha density."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract 16-hour windows ending at training period boundaries",
            "Extract corresponding 4-hour density windows starting immediately after",
            "Filter OMNI2 to only f10.7_index, Kp_index, Dst_index_nT columns",
            "Filter GOES to only xrsb_flux_observed, xrsa_flux_observed columns"
          ],
          [
            "Filter omni2-wu334 to extract rows from 2016-10-22 00:00:00 to 2016-10-23 23:00:00",
            "Filter goes-wu334 to extract rows from 2016-10-22 00:00:00 to 2016-10-23 23:59:00 and aggregate to hourly",
            "Filter swarma-wu334 to extract rows from 2016-10-23 00:00:00 to 2016-10-24 23:50:00",
            "Filter omni2-wu335 to extract rows from 2016-10-25 00:00:00 to 2016-10-26 23:00:00",
            "Filter goes-wu335 to extract rows from 2016-10-25 00:00:00 to 2016-10-26 23:59:00 and aggregate to hourly",
            "Filter swarma-wu335 to extract rows from 2016-10-29 with 4-hour duration for evaluation",
            "Extract last 16 hours from training OMNI/GOES for VAR model context",
            "Extract corresponding 4 hours of density data immediately following the 16-hour context window"
          ],
          [
            "Training data: Timestamp between 2016-10-22 00:00:00 and 2016-10-23 23:00:00 for OMNI2 and GOES, and 2016-10-23 00:00:00 and 2016-10-24 03:00:00 for Swarm Alpha density.",
            "Evaluation data: Timestamp between 2016-10-25 00:00:00 and 2016-10-26 23:00:00 for OMNI2 and GOES, and 2016-10-29 00:00:00 and 2016-10-29 03:00:00 for Swarm Alpha density."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check stationarity of OMNI/GOES variables for VAR modeling",
          "Test autocorrelation in density time series",
          "Evaluate linear regression assumptions (linearity, independence, homoscedasticity, normality)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check stationarity of OMNI/GOES variables for VAR modeling",
            "Test autocorrelation in density time series",
            "Evaluate linear regression assumptions (linearity, independence, homoscedasticity, normality)"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "RMSE value as scalar",
          "Comparison of predicted vs observed density over 4-hour window",
          "Timestamp alignment verification",
          "Report single RMSE value as the final metric",
          "RMSE formula: sqrt(mean((predicted_density - observed_density)^2))",
          "Predictions and observations must align temporally over 4-hour forecast window",
          "All intermediate model artifacts (VAR(1) coefficients, linear regression coefficients) should be computed but not reported",
          "Report the RMSE value with appropriate units."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "RMSE value as scalar",
            "Comparison of predicted vs observed density over 4-hour window",
            "Timestamp alignment verification"
          ],
          [
            "Report single RMSE value as the final metric",
            "RMSE formula: sqrt(mean((predicted_density - observed_density)^2))",
            "Predictions and observations must align temporally over 4-hour forecast window",
            "All intermediate model artifacts (VAR(1) coefficients, linear regression coefficients) should be computed but not reported"
          ],
          [
            "Report the RMSE value with appropriate units."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5855476190476191
  },
  "astronomy-hard-8": {
    "m_q": {
      "target_metric": {
        "value": "RMSE values for two single-variable linear regression models forecasting Swarm Alpha's along-track acceleration 3 hours ahead during May 11, 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "RMSE values for two single-variable linear regression models forecasting Swarm Alpha's along-track acceleration 3 hours ahead during May 11, 2024",
          "RMSE (Root Mean Square Error) for two separate single-variable linear regression models predicting Swarm Alpha along-track acceleration 3 hours ahead",
          "RMSE of two single-variable linear regression models predicting Swarm Alpha's along-track acceleration 3 hours ahead"
        ]
      },
      "filters": {
        "value": [
          "Date = May 11, 2024",
          "Forecast horizon = 3 hours ahead",
          "Model 1: OMNI Kp index only",
          "Model 2: OMNI solar wind dynamic pressure (Pdyn) only",
          "Date equals May 11, 2024 (day 132 of 2024)",
          "Satellite: Swarm Alpha",
          "Prediction horizon: 3 hours ahead",
          "Data limited to May 11, 2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date = May 11, 2024",
            "Forecast horizon = 3 hours ahead",
            "Model 1: OMNI Kp index only",
            "Model 2: OMNI solar wind dynamic pressure (Pdyn) only"
          ],
          [
            "Date equals May 11, 2024 (day 132 of 2024)",
            "Satellite: Swarm Alpha",
            "Prediction horizon: 3 hours ahead"
          ],
          [
            "Data limited to May 11, 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "model_type",
          "Model type (Model 1: Kp index input, Model 2: Pdyn input)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "model_type"
          ],
          [
            "Model type (Model 1: Kp index input, Model 2: Pdyn input)"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the RMSE for Model 1 (Kp input)?",
          "What is the RMSE for Model 2 (Pdyn input)?",
          "Which model has better predictive accuracy?",
          "What is the structure and temporal resolution of OMNI Kp index data for May 11, 2024?",
          "What is the structure and temporal resolution of OMNI solar wind dynamic pressure (Pdyn) data for May 11, 2024?",
          "What is the along-track acceleration data for Swarm Alpha on May 11, 2024?",
          "How to align temporal resolutions between OMNI data (hourly) and Swarm acceleration data?",
          "How to create lagged features for 3-hour ahead prediction?",
          "How to split data into training and test sets?",
          "What is the RMSE for Model 1 (Kp as predictor) on the test set?",
          "What is the RMSE for Model 2 (Pdyn as predictor) on the test set?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the RMSE for Model 1 (Kp input)?",
            "What is the RMSE for Model 2 (Pdyn input)?",
            "Which model has better predictive accuracy?"
          ],
          [
            "What is the structure and temporal resolution of OMNI Kp index data for May 11, 2024?",
            "What is the structure and temporal resolution of OMNI solar wind dynamic pressure (Pdyn) data for May 11, 2024?",
            "What is the along-track acceleration data for Swarm Alpha on May 11, 2024?",
            "How to align temporal resolutions between OMNI data (hourly) and Swarm acceleration data?",
            "How to create lagged features for 3-hour ahead prediction?",
            "How to split data into training and test sets?",
            "What is the RMSE for Model 1 (Kp as predictor) on the test set?",
            "What is the RMSE for Model 2 (Pdyn as predictor) on the test set?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "omni2_Kp_Index.lst",
          "omni2_Flow_Pressure.lst",
          "SW_OPER_ACCACAL_2__20240511T000000_20240511T235959_0304.cdf"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "omni2_Kp_Index.lst",
            "omni2_Flow_Pressure.lst",
            "SW_OPER_ACCACAL_2__20240511T000000_20240511T235959_0304.cdf"
          ],
          [
            "omni2_Kp_Index.lst",
            "omni2_Flow_Pressure.lst",
            "SW_OPER_ACCACAL_2__20240511T000000_20240511T235959_0304.cdf"
          ],
          [
            "omni2_Kp_Index.lst",
            "omni2_Flow_Pressure.lst",
            "SW_OPER_ACCACAL_2__20240511T000000_20240511T235959_0304.cdf"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column names are numeric strings (e.g., '2024', '132', '0', '90') which are not descriptive",
          "Kp file has integer values for the 4th column while Pdyn file has float values for the 4th column",
          "CDF file appears to have parsing error with only 'error' column",
          "omni2_Kp_Index.lst and omni2_Flow_Pressure.lst have unnamed columns requiring interpretation",
          "SW_OPER_ACCACAL_2__20240511T000000_20240511T235959_0304.cdf failed to parse - only contains error message",
          "Column headers not present in OMNI files - must infer from context (year, day of year, hour, value)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column names are numeric strings (e.g., '2024', '132', '0', '90') which are not descriptive",
            "Kp file has integer values for the 4th column while Pdyn file has float values for the 4th column",
            "CDF file appears to have parsing error with only 'error' column"
          ],
          [
            "omni2_Kp_Index.lst and omni2_Flow_Pressure.lst have unnamed columns requiring interpretation",
            "SW_OPER_ACCACAL_2__20240511T000000_20240511T235959_0304.cdf failed to parse - only contains error message",
            "Column headers not present in OMNI files - must infer from context (year, day of year, hour, value)"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "kp_index": "Kp index units (0-9 scale)",
          "pdyn": "nPa (nanopascals)",
          "kp_index_value": "Kp index (unitless, scaled by 10, e.g., 90 = 9.0, 83 = 8.3, 77 = 7.7)",
          "pdyn_value": "nPa (nanoPascals) - solar wind dynamic pressure",
          "hour": "hours (0-23)",
          "day_of_year": "day number (132 = May 11 in leap year 2024)",
          "swarm_acceleration": "m/s\u00b2 (expected unit for along-track acceleration)",
          "omni_kp": "unitless Kp index",
          "omni_pdyn": "nPa",
          "omni_year": "year",
          "omni_day_of_year": "day of year",
          "omni_hour": "hour of day"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {
            "Kp_index": "Kp index units (0-9 scale)",
            "Pdyn": "nPa (nanopascals)"
          },
          {
            "kp_index_value": "Kp index (unitless, scaled by 10, e.g., 90 = 9.0, 83 = 8.3, 77 = 7.7)",
            "pdyn_value": "nPa (nanoPascals) - solar wind dynamic pressure",
            "hour": "hours (0-23)",
            "day_of_year": "day number (132 = May 11 in leap year 2024)",
            "swarm_acceleration": "m/s\u00b2 (expected unit for along-track acceleration)"
          },
          {
            "omni_Kp": "unitless Kp index",
            "omni_Pdyn": "nPa",
            "omni_year": "year",
            "omni_day_of_year": "day of year",
            "omni_hour": "hour of day"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Kp index is on 0-9 scale while Pdyn values range ~7-49 nPa",
          "Need to align time scales: Kp and Pdyn data appear to be hourly while acceleration data may have different temporal resolution",
          "Kp index values appear to be scaled by 10 (90 = Kp 9.0, 83 = Kp 8.3)",
          "Time resolution mismatch: OMNI data is hourly, Swarm data temporal resolution unknown due to CDF parse error",
          "3-hour prediction lag requires temporal alignment and indexing"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Kp index is on 0-9 scale while Pdyn values range ~7-49 nPa",
            "Need to align time scales: Kp and Pdyn data appear to be hourly while acceleration data may have different temporal resolution"
          ],
          [
            "Kp index values appear to be scaled by 10 (90 = Kp 9.0, 83 = Kp 8.3)",
            "Time resolution mismatch: OMNI data is hourly, Swarm data temporal resolution unknown due to CDF parse error",
            "3-hour prediction lag requires temporal alignment and indexing"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Missing acceleration data from CDF file due to parsing error",
          "Unclear how to map hourly Kp/Pdyn data to 3-hour ahead acceleration forecast",
          "Swarm CDF file failed to parse - no acceleration data available",
          "Cannot determine temporal resolution or data availability for Swarm Alpha acceleration",
          "Unable to verify date/time alignment between OMNI and Swarm data sources"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Missing acceleration data from CDF file due to parsing error",
            "Unclear how to map hourly Kp/Pdyn data to 3-hour ahead acceleration forecast"
          ],
          [
            "Swarm CDF file failed to parse - no acceleration data available",
            "Cannot determine temporal resolution or data availability for Swarm Alpha acceleration",
            "Unable to verify date/time alignment between OMNI and Swarm data sources"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": false,
        "confidence": 1.0,
        "votes": [
          false,
          false,
          false
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "999.9",
          "9999.99",
          "99999",
          "-1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "999.9",
            "9999.99",
            "99999",
            "-1"
          ],
          []
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Data limited to May 11, 2024 (day 132 of 2024)",
          "Only 23 hourly data points available for Kp and Pdyn",
          "Acceleration data unavailable due to CDF parsing error",
          "Need test set definition for RMSE calculation",
          "Date must be May 11, 2024 (day 132 of year 2024)",
          "Prediction horizon fixed at 3 hours ahead",
          "Two separate models: Model 1 uses only Kp, Model 2 uses only Pdyn",
          "Both models are single-variable linear regression",
          "Output must be a list with exactly 2 numbers: [RMSE_Kp, RMSE_Pdyn]",
          "RMSE calculated on test set only",
          "The SWARM data file is either empty or cannot be parsed, so the along-track acceleration cannot be extracted.",
          "The OMNI data needs to be converted to a datetime format to be used for time series analysis."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data limited to May 11, 2024 (day 132 of 2024)",
            "Only 23 hourly data points available for Kp and Pdyn",
            "Acceleration data unavailable due to CDF parsing error",
            "Need test set definition for RMSE calculation"
          ],
          [
            "Date must be May 11, 2024 (day 132 of year 2024)",
            "Prediction horizon fixed at 3 hours ahead",
            "Two separate models: Model 1 uses only Kp, Model 2 uses only Pdyn",
            "Both models are single-variable linear regression",
            "Output must be a list with exactly 2 numbers: [RMSE_Kp, RMSE_Pdyn]",
            "RMSE calculated on test set only"
          ],
          [
            "The SWARM data file is either empty or cannot be parsed, so the along-track acceleration cannot be extracted.",
            "The OMNI data needs to be converted to a datetime format to be used for time series analysis."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to day_of_year = 132",
          "Align timestamps for 3-hour forecast lag",
          "Create lagged predictors: use Kp(t) to predict acceleration(t+3h) for Model 1",
          "Create lagged predictors: use Pdyn(t) to predict acceleration(t+3h) for Model 2",
          "Filter out rows where 3-hour ahead target is not available",
          "Split data chronologically into training and test sets"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to day_of_year = 132",
            "Align timestamps for 3-hour forecast lag"
          ],
          [
            "Create lagged predictors: use Kp(t) to predict acceleration(t+3h) for Model 1",
            "Create lagged predictors: use Pdyn(t) to predict acceleration(t+3h) for Model 2",
            "Filter out rows where 3-hour ahead target is not available",
            "Split data chronologically into training and test sets"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Linear regression coefficient significance",
          "RMSE comparison between models",
          "Time series stationarity check",
          "Linear regression fitting for Model 1: acceleration(t+3) ~ Kp(t)",
          "Linear regression fitting for Model 2: acceleration(t+3) ~ Pdyn(t)",
          "RMSE calculation: sqrt(mean((y_predicted - y_actual)^2))"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Linear regression coefficient significance",
            "RMSE comparison between models",
            "Time series stationarity check"
          ],
          [
            "Linear regression fitting for Model 1: acceleration(t+3) ~ Kp(t)",
            "Linear regression fitting for Model 2: acceleration(t+3) ~ Pdyn(t)",
            "RMSE calculation: sqrt(mean((y_predicted - y_actual)^2))"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List format: [RMSE_Kp, RMSE_Pdyn]",
          "Both RMSE values as numbers",
          "Output must be a list with exactly 2 numeric values",
          "Format: [RMSE_for_Kp_model, RMSE_for_Pdyn_model]",
          "RMSE values should be positive real numbers",
          "Values represent test set performance only",
          "Output must be a list of two numbers: [RMSE_Kp, RMSE_Pdyn]"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List format: [RMSE_Kp, RMSE_Pdyn]",
            "Both RMSE values as numbers"
          ],
          [
            "Output must be a list with exactly 2 numeric values",
            "Format: [RMSE_for_Kp_model, RMSE_for_Pdyn_model]",
            "RMSE values should be positive real numbers",
            "Values represent test set performance only"
          ],
          [
            "Output must be a list of two numbers: [RMSE_Kp, RMSE_Pdyn]"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5666666666666668
  },
  "astronomy-hard-9": {
    "m_q": {
      "target_metric": {
        "value": "Lag (0-48 hours) that maximizes r^2 correlation between semi-major axis change (km) from TLE data and OMNI AP index during May 1-30, 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "Lag (0-48 hours) that maximizes r^2 correlation between semi-major axis change (km) from TLE data and OMNI AP index during May 1-30, 2024",
          "Best lag (0-48 hours) that maximizes r^2 correlation between atmospheric drag (semi-major axis change in km from TLE data) and OMNI AP index",
          "Best lag (0-48 hours) between semi-major axis change of SATCAT 43180 and OMNI AP index that maximizes r^2 correlation during May 1-30, 2024"
        ]
      },
      "filters": {
        "value": [
          "epoch_year = 24",
          "epoch_day between 122 and 151 (May 1-30, 2024)",
          "AP index not missing",
          "semi-major axis change calculable from consecutive TLEs",
          "May 1-30, 2024",
          "SATCAT 43180",
          "TLE epoch times rounded to nearest hour",
          "Lag range 0-48 hours",
          "TLE data within May 1-30, 2024",
          "OMNI2 data within May 1-30, 2024"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "epoch_year = 24",
            "epoch_day between 122 and 151 (May 1-30, 2024)",
            "AP index not missing",
            "semi-major axis change calculable from consecutive TLEs"
          ],
          [
            "May 1-30, 2024",
            "SATCAT 43180",
            "TLE epoch times rounded to nearest hour",
            "Lag range 0-48 hours"
          ],
          [
            "TLE data within May 1-30, 2024",
            "OMNI2 data within May 1-30, 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "hourly_timestamp",
          "lag_hours"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "hourly_timestamp"
          ],
          [
            "lag_hours"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar (best lag value)",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate semi-major axis from mean_motion using mu=398600.4418",
          "Round TLE epoch times to nearest hour",
          "Compute hourly change in semi-major axis",
          "Extract AP index from OMNI2 data",
          "Calculate correlations for lags 0-48 hours",
          "Find lag with maximum r^2",
          "How to parse TLE epoch (epoch_year, epoch_day) into datetime?",
          "How to compute semi-major axis from mean_motion using mu=398600.4418 km^3/s^2?",
          "How to calculate semi-major axis change (atmospheric drag) between consecutive TLE entries?",
          "How to round TLE epoch times to nearest hour?",
          "How to identify and extract AP index from OMNI2 hourly data?",
          "How to align TLE and OMNI2 data on hourly timestamps?",
          "How to apply variable lags (0-48 hours) between AP index and drag measurements?",
          "How to compute r^2 correlation for each lag value?",
          "Which lag value produces maximum r^2?",
          "Calculate semi-major axis from TLE data.",
          "Calculate semi-major axis change (delta) between consecutive TLE data points.",
          "Extract AP index from OMNI2 data.",
          "Round TLE epoch times to the nearest hour.",
          "Merge TLE data and OMNI2 data based on rounded epoch time and lag.",
          "Calculate r^2 correlation between semi-major axis change and AP index for each lag (0-48 hours).",
          "Identify the lag with the maximum r^2 correlation."
        ],
        "confidence": 0.3333333333333332,
        "votes": [
          [
            "Calculate semi-major axis from mean_motion using mu=398600.4418",
            "Round TLE epoch times to nearest hour",
            "Compute hourly change in semi-major axis",
            "Extract AP index from OMNI2 data",
            "Calculate correlations for lags 0-48 hours",
            "Find lag with maximum r^2"
          ],
          [
            "How to parse TLE epoch (epoch_year, epoch_day) into datetime?",
            "How to compute semi-major axis from mean_motion using mu=398600.4418 km^3/s^2?",
            "How to calculate semi-major axis change (atmospheric drag) between consecutive TLE entries?",
            "How to round TLE epoch times to nearest hour?",
            "How to identify and extract AP index from OMNI2 hourly data?",
            "How to align TLE and OMNI2 data on hourly timestamps?",
            "How to apply variable lags (0-48 hours) between AP index and drag measurements?",
            "How to compute r^2 correlation for each lag value?",
            "Which lag value produces maximum r^2?"
          ],
          [
            "Calculate semi-major axis from TLE data.",
            "Calculate semi-major axis change (delta) between consecutive TLE data points.",
            "Extract AP index from OMNI2 data.",
            "Round TLE epoch times to the nearest hour.",
            "Merge TLE data and OMNI2 data based on rounded epoch time and lag.",
            "Calculate r^2 correlation between semi-major axis change and AP index for each lag (0-48 hours).",
            "Identify the lag with the maximum r^2 correlation."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "43180.tle",
          "omni2_2024.dat",
          "omni2.text"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "43180.tle",
            "omni2_2024.dat",
            "omni2.text"
          ],
          [
            "43180.tle",
            "omni2_2024.dat",
            "omni2.text"
          ],
          [
            "43180.tle",
            "omni2_2024.dat",
            "omni2.text"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "TLE epoch_year uses 2-digit year (24)",
          "OMNI2 uses 4-digit year (2024)",
          "TLE epoch_day is fractional day of year",
          "OMNI2 has separate year, day, hour columns",
          "omni2_2024.dat has numeric column names instead of meaningful headers",
          "omni2.text describes format but is not machine-readable data",
          "TLE epoch is split into epoch_year and epoch_day (fractional day of year)",
          "OMNI2 datetime is split into year, day-of-year, hour columns",
          "Column names in omni2_2024.dat are not descriptive.",
          "omni2.text provides format specification for omni2_2024.dat"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "TLE epoch_year uses 2-digit year (24)",
            "OMNI2 uses 4-digit year (2024)",
            "TLE epoch_day is fractional day of year",
            "OMNI2 has separate year, day, hour columns"
          ],
          [
            "omni2_2024.dat has numeric column names instead of meaningful headers",
            "omni2.text describes format but is not machine-readable data",
            "TLE epoch is split into epoch_year and epoch_day (fractional day of year)",
            "OMNI2 datetime is split into year, day-of-year, hour columns"
          ],
          [
            "Column names in omni2_2024.dat are not descriptive.",
            "omni2.text provides format specification for omni2_2024.dat"
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "mean_motion": "revolutions per day",
          "semi_major_axis": "km",
          "semi_major_axis_change": "km",
          "ap_index": "nT (from OMNI2 specification)",
          "inclination": "degrees",
          "raan": "degrees",
          "eccentricity": "unitless",
          "arg_perigee": "degrees",
          "mean_anomaly": "degrees",
          "mu": "km^3/s^2",
          "epoch_day": "fractional day of year",
          "lag": "hours"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "mean_motion": "revolutions/day",
            "semi_major_axis": "km",
            "semi_major_axis_change": "km",
            "AP_index": "nT (from OMNI2 specification)",
            "inclination": "degrees",
            "raan": "degrees",
            "eccentricity": "unitless",
            "arg_perigee": "degrees",
            "mean_anomaly": "degrees"
          },
          {
            "mean_motion": "revolutions per day",
            "semi_major_axis": "km",
            "semi_major_axis_change": "km",
            "mu": "km^3/s^2",
            "epoch_day": "fractional day of year",
            "inclination": "degrees",
            "raan": "degrees",
            "eccentricity": "dimensionless",
            "arg_perigee": "degrees",
            "mean_anomaly": "degrees",
            "lag": "hours",
            "AP_index": "dimensionless"
          },
          {
            "inclination": "degrees",
            "raan": "degrees",
            "eccentricity": "unitless",
            "arg_perigee": "degrees",
            "mean_anomaly": "degrees",
            "mean_motion": "revolutions per day",
            "ap_index": "unitless"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "TLE epoch_day has precision to 8 decimal places (seconds)",
          "OMNI2 hour is integer 0-23",
          "Mean motion needs conversion to semi-major axis using mu",
          "mean_motion is in revolutions per day, needs conversion for semi-major axis calculation",
          "epoch_day is fractional (e.g., 122.17811289 = day 122 + time fraction)",
          "semi-major axis calculation: a = (mu / (2*pi*n/86400)^2)^(1/3) where n is mean_motion",
          "TLE eccentricity stored without leading decimal point (e.g., 0.0001844 represents 0.0001844)",
          "Mean motion needs to be converted to radians per second for semi-major axis calculation.",
          "Semi-major axis is in km."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "TLE epoch_day has precision to 8 decimal places (seconds)",
            "OMNI2 hour is integer 0-23",
            "Mean motion needs conversion to semi-major axis using mu"
          ],
          [
            "mean_motion is in revolutions per day, needs conversion for semi-major axis calculation",
            "epoch_day is fractional (e.g., 122.17811289 = day 122 + time fraction)",
            "semi-major axis calculation: a = (mu / (2*pi*n/86400)^2)^(1/3) where n is mean_motion",
            "TLE eccentricity stored without leading decimal point (e.g., 0.0001844 represents 0.0001844)"
          ],
          [
            "Mean motion needs to be converted to radians per second for semi-major axis calculation.",
            "Semi-major axis is in km."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "TLE times need rounding to nearest hour to match OMNI2 hourly resolution",
          "TLE data points are irregular (not hourly), need interpolation or nearest matching",
          "Time representation: TLE uses year+fractional_day, OMNI2 uses year+day_of_year+hour",
          "Need to determine which column in omni2_2024.dat corresponds to AP index from omni2.text specification"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "TLE times need rounding to nearest hour to match OMNI2 hourly resolution",
            "TLE data points are irregular (not hourly), need interpolation or nearest matching"
          ],
          [
            "Time representation: TLE uses year+fractional_day, OMNI2 uses year+day_of_year+hour",
            "Need to determine which column in omni2_2024.dat corresponds to AP index from omni2.text specification"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "999999.99",
          "99999.99",
          "99999.99.1",
          "99999.99.2",
          "99999.99.3",
          "99999.99.4",
          "999.9",
          "-9",
          "",
          "9999.99",
          "9999.9",
          "NA",
          "N/A"
        ],
        "confidence": 0.6153846153846152,
        "votes": [
          [
            "999999.99",
            "99999.99",
            "99999.99.1",
            "99999.99.2",
            "99999.99.3",
            "99999.99.4",
            "999.9",
            "-9",
            ""
          ],
          [
            "999999.99",
            "99999.99",
            "999.9",
            "9999.99",
            "9999.9",
            "999.9"
          ],
          [
            "NA",
            "N/A",
            "",
            "999999.99",
            "99999.99.1",
            "99999.99.2",
            "99999.99.3",
            "99999.99.4",
            "999.9"
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 0.33,
        "votes": [
          12.0,
          55.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "mu = 398600.4418 km^3/s^2",
          "Lag range: 0-48 hours",
          "Time period: May 1-30, 2024",
          "TLE epoch times must be rounded to nearest hour",
          "Correlation must use r^2 metric",
          "Date range: May 1-30, 2024 (epoch_day between 122-151 for year 2024)",
          "SATCAT/NORAD ID: 43180",
          "Lag range: 0 to 48 hours inclusive",
          "Gravitational parameter mu = 398600.4418 km^3/s^2",
          "Use hourly OMNI2 data",
          "Round TLE epoch times to nearest hour for alignment",
          "Lag must be an integer between 0 and 48 (inclusive).",
          "TLE data must be converted to semi-major axis (a) using mu and mean_motion: a = (mu / mean_motion^2)^(1/3), where mu = 398600.4418 km^3/s^2"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "mu = 398600.4418 km^3/s^2",
            "Lag range: 0-48 hours",
            "Time period: May 1-30, 2024",
            "TLE epoch times must be rounded to nearest hour",
            "Correlation must use r^2 metric"
          ],
          [
            "Date range: May 1-30, 2024 (epoch_day between 122-151 for year 2024)",
            "SATCAT/NORAD ID: 43180",
            "Lag range: 0 to 48 hours inclusive",
            "Gravitational parameter mu = 398600.4418 km^3/s^2",
            "Use hourly OMNI2 data",
            "Round TLE epoch times to nearest hour for alignment"
          ],
          [
            "Lag must be an integer between 0 and 48 (inclusive).",
            "TLE data must be converted to semi-major axis (a) using mu and mean_motion: a = (mu / mean_motion^2)^(1/3), where mu = 398600.4418 km^3/s^2"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove TLE rows outside May 1-30, 2024",
          "Filter OMNI2 to May 1-30, 2024",
          "Exclude rows where AP index is sentinel value",
          "Ensure consecutive TLEs for change calculation",
          "Filter 43180.tle for epoch_year==24 AND epoch_day>=122 AND epoch_day<152",
          "Filter omni2_2024.dat for May 2024 (day_of_year 122-151)",
          "Exclude records with sentinel values in AP index column",
          "Require at least 2 consecutive TLE entries to compute semi-major axis change",
          "Epoch time rounded to the nearest hour.",
          "Semi-major axis change (delta) between consecutive TLE data points."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Remove TLE rows outside May 1-30, 2024",
            "Filter OMNI2 to May 1-30, 2024",
            "Exclude rows where AP index is sentinel value",
            "Ensure consecutive TLEs for change calculation"
          ],
          [
            "Filter 43180.tle for epoch_year==24 AND epoch_day>=122 AND epoch_day<152",
            "Filter omni2_2024.dat for May 2024 (day_of_year 122-151)",
            "Exclude records with sentinel values in AP index column",
            "Require at least 2 consecutive TLE entries to compute semi-major axis change"
          ],
          [
            "Epoch time rounded to the nearest hour.",
            "Semi-major axis change (delta) between consecutive TLE data points."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Pearson correlation for each lag",
          "Maximum r^2 identification",
          "Significance testing of correlations",
          "Compute Pearson r^2 correlation coefficient for each lag (0-48 hours)",
          "Identify lag with maximum r^2 value",
          "Pearson correlation coefficient (r) between semi-major axis change and AP index.",
          "R-squared (r^2) calculation."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pearson correlation for each lag",
            "Maximum r^2 identification",
            "Significance testing of correlations"
          ],
          [
            "Compute Pearson r^2 correlation coefficient for each lag (0-48 hours)",
            "Identify lag with maximum r^2 value"
          ],
          [
            "Pearson correlation coefficient (r) between semi-major axis change and AP index.",
            "R-squared (r^2) calculation."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Best lag as integer 0-48",
          "Corresponding maximum r^2 value",
          "Time-aligned dataset for verification",
          "Return the optimal lag value in hours (integer 0-48)",
          "Include the maximum r^2 value achieved",
          "Result should be a single scalar lag value",
          "Report the lag (in hours) with the highest r^2 correlation.",
          "Report the maximum r^2 value."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Best lag as integer 0-48",
            "Corresponding maximum r^2 value",
            "Time-aligned dataset for verification"
          ],
          [
            "Return the optimal lag value in hours (integer 0-48)",
            "Include the maximum r^2 value achieved",
            "Result should be a single scalar lag value"
          ],
          [
            "Report the lag (in hours) with the highest r^2 correlation.",
            "Report the maximum r^2 value."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5528247863247865
  },
  "biomedical-easy-2": {
    "m_q": {
      "target_metric": {
        "value": "average (mean) of Age values",
        "confidence": 0.3333333333333333,
        "votes": [
          "average (mean) of Age values",
          "average age of patients",
          "average age"
        ]
      },
      "filters": {
        "value": [
          "Histologic_type contains 'serous' (case-insensitive)",
          "Proteomics_Tumor_Normal == 'Tumor'",
          "Case_excluded == 'No'",
          "Histologic_type contains 'serous'",
          "Histologic_type is 'Serous'"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [
            "Histologic_type contains 'serous' (case-insensitive)",
            "Proteomics_Tumor_Normal == 'Tumor'",
            "Case_excluded == 'No'"
          ],
          [
            "Histologic_type contains 'serous'",
            "Proteomics_Tumor_Normal == 'Tumor'",
            "Case_excluded == 'No'"
          ],
          [
            "Histologic_type is 'Serous'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the distribution of Age for serous tumor patients?",
          "How many serous tumor samples are in the dataset?",
          "Are there missing Age values for serous tumor samples?",
          "How to identify serous tumor samples from Histologic_type column?",
          "Should excluded cases be filtered out?",
          "Should analysis include only tumor samples or also normal samples?",
          "How to handle missing Age values?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the distribution of Age for serous tumor patients?",
            "How many serous tumor samples are in the dataset?",
            "Are there missing Age values for serous tumor samples?"
          ],
          [
            "How to identify serous tumor samples from Histologic_type column?",
            "Should excluded cases be filtered out?",
            "Should analysis include only tumor samples or also normal samples?",
            "How to handle missing Age values?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc1.xlsx"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "age": "years",
          "bmi": "kg/m\u00b2",
          "tumor_size_cm": "centimeters",
          "estrogen_receptor_%": "percentage",
          "progesterone_receptor_%": "percentage"
        },
        "confidence": 0.6666666666666667,
        "votes": [
          {
            "Age": "years",
            "BMI": "kg/m\u00b2",
            "Tumor_Size_cm": "centimeters"
          },
          {
            "Age": "years",
            "BMI": "kg/m^2",
            "Tumor_Size_cm": "centimeters",
            "Estrogen_Receptor_%": "percentage",
            "Progesterone_Receptor_%": "percentage"
          },
          {
            "Age": "years",
            "Tumor_Size_cm": "cm"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Age is float64 but represents whole years",
          "Some percentage columns (Estrogen_Receptor_%, Progesterone_Receptor_%) are float64 but represent percentages"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age is float64 but represents whole years",
            "Some percentage columns (Estrogen_Receptor_%, Progesterone_Receptor_%) are float64 but represent percentages"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "Cannot be determined",
          "Staging Incomplete",
          "NA",
          "N/A"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "",
            "Cannot be determined",
            "Staging Incomplete"
          ],
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 179.0,
        "confidence": 0.33,
        "votes": [
          179.0,
          179.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "xlsx",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Age should be positive (typically 18+)",
          "Histologic_type should be categorical with valid tumor types",
          "Proteomics_Tumor_Normal should be either 'Tumor' or 'Normal'",
          "Age must be non-null for average calculation",
          "Histologic_type must contain 'serous' substring (case-insensitive)",
          "Only include rows where Proteomics_Tumor_Normal == 'Tumor'",
          "Only include rows where Case_excluded == 'No'",
          "Age must be a valid number",
          "Histologic_type must be a valid string"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age should be positive (typically 18+)",
            "Histologic_type should be categorical with valid tumor types",
            "Proteomics_Tumor_Normal should be either 'Tumor' or 'Normal'"
          ],
          [
            "Age must be non-null for average calculation",
            "Histologic_type must contain 'serous' substring (case-insensitive)",
            "Only include rows where Proteomics_Tumor_Normal == 'Tumor'",
            "Only include rows where Case_excluded == 'No'"
          ],
          [
            "Age must be a valid number",
            "Histologic_type must be a valid string"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where Age is missing or NaN",
          "Exclude rows where Histologic_type is missing or invalid",
          "Filter for serous histologic type: check if 'serous' appears in Histologic_type value",
          "Exclude cases marked as excluded in Case_excluded column",
          "Include only tumor samples, not normal tissue samples",
          "Patients with Histologic_type equal to 'Serous'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Age is missing or NaN",
            "Exclude rows where Histologic_type is missing or invalid"
          ],
          [
            "Filter for serous histologic type: check if 'serous' appears in Histologic_type value",
            "Exclude cases marked as excluded in Case_excluded column",
            "Include only tumor samples, not normal tissue samples"
          ],
          [
            "Patients with Histologic_type equal to 'Serous'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for normality of Age distribution in serous tumor subset",
          "Compare Age distribution between serous and other histologic types"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for normality of Age distribution in serous tumor subset",
            "Compare Age distribution between serous and other histologic types"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Report average age with appropriate precision (e.g., 64.2 years)",
          "Include sample size (n) for serous tumor patients",
          "Consider reporting confidence interval for the mean",
          "Single numeric value representing mean age",
          "Round to appropriate decimal places (1-2 decimal places)",
          "Report sample size (number of serous tumor samples included)",
          "Average age should be a number"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Report average age with appropriate precision (e.g., 64.2 years)",
            "Include sample size (n) for serous tumor patients",
            "Consider reporting confidence interval for the mean"
          ],
          [
            "Single numeric value representing mean age",
            "Round to appropriate decimal places (1-2 decimal places)",
            "Report sample size (number of serous tumor samples included)"
          ],
          [
            "Average age should be a number"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5065000000000001
  },
  "biomedical-easy-6": {
    "m_q": {
      "target_metric": {
        "value": "Most common FIGO_stage value (mode) among samples from patients aged >70",
        "confidence": 0.3333333333333333,
        "votes": [
          "Most common FIGO_stage value (mode) among samples from patients aged >70",
          "Most common FIGO_stage value among patients aged above 70",
          "most frequent FIGO_stage among patients older than 70"
        ]
      },
      "filters": {
        "value": [
          "Age > 70",
          "Case_excluded == 'No' (if applicable for analysis)"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Age > 70",
            "Case_excluded == 'No' (if applicable for analysis)"
          ],
          [
            "Age > 70"
          ],
          [
            "Age > 70"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "FIGO_stage"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "FIGO_stage"
          ],
          [
            "FIGO_stage"
          ],
          [
            "FIGO_stage"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the distribution of FIGO_stage values?",
          "How many patients are above age 70?",
          "Are there missing values in Age or FIGO_stage columns?",
          "Which patients have Age > 70?",
          "What are the FIGO_stage values for patients above 70?",
          "What is the frequency distribution of FIGO_stage among filtered patients?",
          "Which FIGO_stage has the highest count?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the distribution of FIGO_stage values?",
            "How many patients are above age 70?",
            "Are there missing values in Age or FIGO_stage columns?"
          ],
          [
            "Which patients have Age > 70?",
            "What are the FIGO_stage values for patients above 70?",
            "What is the frequency distribution of FIGO_stage among filtered patients?",
            "Which FIGO_stage has the highest count?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc1.xlsx"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "age": "years",
          "bmi": "kg/m\u00b2",
          "tumor_size_cm": "centimeters",
          "estrogen_receptor_%": "percent",
          "progesterone_receptor_%": "percent"
        },
        "confidence": 0.6666666666666667,
        "votes": [
          {
            "Age": "years",
            "BMI": "kg/m\u00b2",
            "Tumor_Size_cm": "centimeters"
          },
          {
            "Age": "years",
            "BMI": "kg/m^2",
            "Tumor_Size_cm": "centimeters",
            "Estrogen_Receptor_%": "percent",
            "Progesterone_Receptor_%": "percent"
          },
          {
            "Age": "years",
            "Tumor_Size_cm": "cm"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Age is float64 but represents whole years",
          "Some percentage columns may be represented as decimals or percentages (e.g., Estrogen_Receptor_%)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age is float64 but represents whole years",
            "Some percentage columns may be represented as decimals or percentages (e.g., Estrogen_Receptor_%)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "Cannot be determined",
          "Staging Incomplete"
        ],
        "confidence": 0.8,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined",
            "Staging Incomplete"
          ],
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 179.0,
        "confidence": 0.33,
        "votes": [
          179.0,
          179.0,
          10.0
        ]
      },
      "file_format": {
        "value": "xlsx",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "xlsx",
          "xlsx"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Age should be positive (0-120 reasonable range)",
          "FIGO_stage should be categorical (IA, IB, II, III, IV, etc.)",
          "Age > 70 filter must handle missing values appropriately",
          "Age must be > 70",
          "FIGO_stage must not be null or empty for counting",
          "Age must be a valid numeric value",
          "Age must be a numerical value",
          "FIGO_stage must be a categorical value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age should be positive (0-120 reasonable range)",
            "FIGO_stage should be categorical (IA, IB, II, III, IV, etc.)",
            "Age > 70 filter must handle missing values appropriately"
          ],
          [
            "Age must be > 70",
            "FIGO_stage must not be null or empty for counting",
            "Age must be a valid numeric value"
          ],
          [
            "Age must be a numerical value",
            "FIGO_stage must be a categorical value"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows with missing Age",
          "Exclude rows with missing FIGO_stage",
          "Consider filtering Case_excluded == 'No' if excluding poor quality samples",
          "Exclude rows where Age is null",
          "Exclude rows where FIGO_stage is null or empty",
          "Filter to Age > 70 before grouping",
          "Patients older than 70: Age > 70"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows with missing Age",
            "Exclude rows with missing FIGO_stage",
            "Consider filtering Case_excluded == 'No' if excluding poor quality samples"
          ],
          [
            "Exclude rows where Age is null",
            "Exclude rows where FIGO_stage is null or empty",
            "Filter to Age > 70 before grouping"
          ],
          [
            "Patients older than 70: Age > 70"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Frequency count of FIGO_stage values",
          "Mode calculation for FIGO_stage in age >70 subset",
          "Count frequency of each FIGO_stage value",
          "Identify mode (most frequent value) of FIGO_stage"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Frequency count of FIGO_stage values",
            "Mode calculation for FIGO_stage in age >70 subset"
          ],
          [
            "Count frequency of each FIGO_stage value",
            "Identify mode (most frequent value) of FIGO_stage"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Report the most common FIGO_stage value",
          "Include count and percentage of this value among age >70 patients",
          "Consider reporting full distribution if requested",
          "Return the single most common FIGO_stage value",
          "Handle ties by reporting all tied values or selecting one consistently",
          "Report count or frequency alongside the result for context",
          "Report the most frequent FIGO_stage"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Report the most common FIGO_stage value",
            "Include count and percentage of this value among age >70 patients",
            "Consider reporting full distribution if requested"
          ],
          [
            "Return the single most common FIGO_stage value",
            "Handle ties by reporting all tied values or selecting one consistently",
            "Report count or frequency alongside the result for context"
          ],
          [
            "Report the most frequent FIGO_stage"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5731666666666668
  },
  "biomedical-easy-9": {
    "m_q": {
      "target_metric": {
        "value": "Difference between average false discovery rate (FDR) for CBX3 genes and average FDR for all other genes",
        "confidence": 0.3333333333333333,
        "votes": [
          "Difference between average false discovery rate (FDR) for CBX3 genes and average FDR for all other genes",
          "Difference between average FDR in CBX3 genes and average FDR for the rest of the genes",
          "Difference between the average FDR.phos for CBX3 genes and the average FDR.phos for all other genes."
        ]
      },
      "filters": {
        "value": [
          "Gene = 'CBX3'",
          "Gene != 'CBX3'",
          "Gene == 'CBX3'"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "Gene = 'CBX3'",
            "Gene != 'CBX3'"
          ],
          [
            "Gene == 'CBX3'",
            "Gene != 'CBX3'"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Gene"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Gene"
          ],
          [
            "Gene"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the average FDR for CBX3 genes?",
          "What is the average FDR for all non-CBX3 genes?",
          "Which data source contains FDR values?",
          "How are CBX3 genes identified in the data?",
          "What is the difference between these two averages?",
          "What is the average FDR.phos for CBX3 genes?",
          "What is the average FDR.phos for all genes?",
          "What genes are CBX3 genes?"
        ],
        "confidence": 0.4166666666666667,
        "votes": [
          [
            "What is the average FDR for CBX3 genes?",
            "What is the average FDR for all non-CBX3 genes?",
            "Which data source contains FDR values?",
            "How are CBX3 genes identified in the data?"
          ],
          [
            "What is the average FDR for CBX3 genes?",
            "What is the average FDR for all non-CBX3 genes?",
            "What is the difference between these two averages?"
          ],
          [
            "What is the average FDR.phos for CBX3 genes?",
            "What is the average FDR.phos for all genes?",
            "What genes are CBX3 genes?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc3__F-SS-phospho",
          "1-s2.0-S0092867420301070-mmc3__C-SE-phospho"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc3__F-SS-phospho"
          ],
          [
            "1-s2.0-S0092867420301070-mmc3__F-SS-phospho"
          ],
          [
            "1-s2.0-S0092867420301070-mmc3__F-SS-phospho",
            "1-s2.0-S0092867420301070-mmc3__C-SE-phospho"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Sheet C-SE-phospho contains CBX3 gene name but lacks FDR values",
          "Sheet F-SS-phospho contains FDR values but needs to be filtered for CBX3"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sheet C-SE-phospho contains CBX3 gene name but lacks FDR values",
            "Sheet F-SS-phospho contains FDR values but needs to be filtered for CBX3"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "fdr.phos": "probability (0-1)",
          "foldchange.phos": "log2 fold change",
          "pval.phos": "probability (0-1)",
          "logp.phos": "log-transformed p-value",
          "logfdr.phos": "log-transformed FDR"
        },
        "confidence": 0.6,
        "votes": [
          {
            "FDR.phos": "probability (0-1)",
            "FoldChange.phos": "log2 fold change",
            "pval.phos": "probability (0-1)"
          },
          {
            "FDR.phos": "false discovery rate (probability)",
            "pval.phos": "p-value (probability)",
            "FoldChange.phos": "fold change (ratio)",
            "logp.phos": "log-transformed p-value",
            "logfdr.phos": "log-transformed FDR"
          },
          {
            "FDR.phos": "False Discovery Rate"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "FDR values are probabilities but also have log-transformed versions (logfdr.phos)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "FDR values are probabilities but also have log-transformed versions (logfdr.phos)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Sheet C-SE-phospho lists CBX3 as significant gene but doesn't provide FDR values",
          "Sheet F-SS-phospho provides FDR values but needs gene-level filtering"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sheet C-SE-phospho lists CBX3 as significant gene but doesn't provide FDR values",
            "Sheet F-SS-phospho provides FDR values but needs gene-level filtering"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.111111111111111,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " "
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 17.0,
        "confidence": 1.0,
        "votes": [
          17.0,
          17.0,
          17.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "FDR.phos values must be between 0 and 1",
          "CBX3 must exist in Gene column of F-SS-phospho sheet",
          "Need to filter by Gene column to separate CBX3 from other genes",
          "FDR.phos values must be numeric and non-null",
          "Gene column must be non-null",
          "CBX3 must exist as a gene value in the dataset",
          "CBX3 genes must be identified from the 'C-SE-phospho' sheet.",
          "FDR.phos values must be numeric."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "FDR.phos values must be between 0 and 1",
            "CBX3 must exist in Gene column of F-SS-phospho sheet",
            "Need to filter by Gene column to separate CBX3 from other genes"
          ],
          [
            "FDR.phos values must be numeric and non-null",
            "Gene column must be non-null",
            "CBX3 must exist as a gene value in the dataset"
          ],
          [
            "CBX3 genes must be identified from the 'C-SE-phospho' sheet.",
            "FDR.phos values must be numeric."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "WHERE Gene = 'CBX3'",
          "WHERE Gene != 'CBX3'",
          "Partition dataset into CBX3 genes (Gene == 'CBX3') and non-CBX3 genes (Gene != 'CBX3')",
          "Identify rows in 'F-SS-phospho' where Gene is 'CBX3'.",
          "Identify rows in 'F-SS-phospho' where Gene is not 'CBX3'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "WHERE Gene = 'CBX3'",
            "WHERE Gene != 'CBX3'"
          ],
          [
            "Partition dataset into CBX3 genes (Gene == 'CBX3') and non-CBX3 genes (Gene != 'CBX3')"
          ],
          [
            "Identify rows in 'F-SS-phospho' where Gene is 'CBX3'.",
            "Identify rows in 'F-SS-phospho' where Gene is not 'CBX3'."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Two-sample t-test or Mann-Whitney U test for comparing FDR distributions",
          "Check normality of FDR distributions before choosing test",
          "Calculate the average FDR.phos for CBX3 genes.",
          "Calculate the average FDR.phos for all other genes.",
          "Subtract the average FDR.phos of all other genes from the average FDR.phos of CBX3 genes."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Two-sample t-test or Mann-Whitney U test for comparing FDR distributions",
            "Check normality of FDR distributions before choosing test"
          ],
          [],
          [
            "Calculate the average FDR.phos for CBX3 genes.",
            "Calculate the average FDR.phos for all other genes.",
            "Subtract the average FDR.phos of all other genes from the average FDR.phos of CBX3 genes."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Report difference as: avg_FDR_CBX3 - avg_FDR_other",
          "Include confidence intervals if possible",
          "Report sample sizes for both groups",
          "Return single scalar value representing the difference",
          "Calculate: mean(FDR.phos where Gene=='CBX3') - mean(FDR.phos where Gene!='CBX3')",
          "Output the difference between the two averages as a single numerical value."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Report difference as: avg_FDR_CBX3 - avg_FDR_other",
            "Include confidence intervals if possible",
            "Report sample sizes for both groups"
          ],
          [
            "Return single scalar value representing the difference",
            "Calculate: mean(FDR.phos where Gene=='CBX3') - mean(FDR.phos where Gene!='CBX3')"
          ],
          [
            "Output the difference between the two averages as a single numerical value."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6286111111111111
  },
  "biomedical-hard-1": {
    "m_q": {
      "target_metric": {
        "value": "Spearman correlation coefficient between PLK1 protein abundance and CHEK2-S163 phosphoprotein abundance in tumor samples",
        "confidence": 0.3333333333333333,
        "votes": [
          "Spearman correlation coefficient between PLK1 protein abundance and CHEK2-S163 phosphoprotein abundance in tumor samples",
          "Spearman correlation coefficient between PLK1 protein abundance and CHEK2-S163 phosphoprotein abundance",
          "Spearman correlation between PLK1 abundance and CHEK2-S163 abundance"
        ]
      },
      "filters": {
        "value": [
          "Proteomics_Tumor_Normal == 'Tumor'",
          "Case_excluded == 'No'",
          "Exclude samples with missing values in PLK1 or CHEK2-S163 abundance",
          "Exclude samples where Case_excluded is not 'No'",
          "Exclude samples with missing PLK1 values",
          "Exclude samples with missing CHEK2-S163 values",
          "Include only tumor samples (Proteomics_Tumor_Normal = 'Tumor')",
          "Exclude samples where 'Case_excluded' is not 'No' in '1-s2.0-S0092867420301070-mmc1.xlsx'",
          "Exclude samples with missing values for PLK1 and CHEK2-S163"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Proteomics_Tumor_Normal == 'Tumor'",
            "Case_excluded == 'No'",
            "Exclude samples with missing values in PLK1 or CHEK2-S163 abundance"
          ],
          [
            "Exclude samples where Case_excluded is not 'No'",
            "Exclude samples with missing PLK1 values",
            "Exclude samples with missing CHEK2-S163 values",
            "Include only tumor samples (Proteomics_Tumor_Normal = 'Tumor')"
          ],
          [
            "Exclude samples where 'Case_excluded' is not 'No' in '1-s2.0-S0092867420301070-mmc1.xlsx'",
            "Exclude samples with missing values for PLK1 and CHEK2-S163"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Identify PLK1 protein abundance values from global proteomics data",
          "Identify CHEK2-S163 phosphoprotein abundance values from phosphoproteomics data",
          "Filter to tumor samples only",
          "Remove samples excluded from study",
          "Remove samples with missing values in either protein",
          "Calculate Spearman correlation",
          "Round result to 4 decimal places",
          "Which file contains PLK1 protein abundance data?",
          "Which file contains CHEK2-S163 phosphoprotein abundance data?",
          "Which file contains the Case_excluded column for filtering?",
          "How to join the proteomics data with the clinical metadata?",
          "What is the exact identifier format for CHEK2-S163 in the phosphoproteomics data?",
          "How many samples remain after applying all filters?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify PLK1 protein abundance values from global proteomics data",
            "Identify CHEK2-S163 phosphoprotein abundance values from phosphoproteomics data",
            "Filter to tumor samples only",
            "Remove samples excluded from study",
            "Remove samples with missing values in either protein",
            "Calculate Spearman correlation",
            "Round result to 4 decimal places"
          ],
          [
            "Which file contains PLK1 protein abundance data?",
            "Which file contains CHEK2-S163 phosphoprotein abundance data?",
            "Which file contains the Case_excluded column for filtering?",
            "How to join the proteomics data with the clinical metadata?",
            "What is the exact identifier format for CHEK2-S163 in the phosphoproteomics data?",
            "How many samples remain after applying all filters?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc1.xlsx",
          "1-s2.0-S0092867420301070-mmc2__A-global-proteomics",
          "1-s2.0-S0092867420301070-mmc2__B-phospho-proteomics"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2__A-global-proteomics",
            "1-s2.0-S0092867420301070-mmc2__B-phospho-proteomics"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2__A-global-proteomics",
            "1-s2.0-S0092867420301070-mmc2__B-phospho-proteomics"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2__A-global-proteomics",
            "1-s2.0-S0092867420301070-mmc2__B-phospho-proteomics"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Global proteomics data has 10999 rows \u00d7 154 columns",
          "Phosphoproteomics data has 73212 rows \u00d7 154 columns",
          "Sample metadata has 153 rows \u00d7 179 columns",
          "Different number of rows across datasets",
          "Clinical metadata has samples as rows (idx column), while proteomics data has samples as columns (S001-S153)",
          "Need to transpose or pivot proteomics data to align with clinical metadata structure"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Global proteomics data has 10999 rows \u00d7 154 columns",
            "Phosphoproteomics data has 73212 rows \u00d7 154 columns",
            "Sample metadata has 153 rows \u00d7 179 columns",
            "Different number of rows across datasets"
          ],
          [
            "Clinical metadata has samples as rows (idx column), while proteomics data has samples as columns (S001-S153)",
            "Need to transpose or pivot proteomics data to align with clinical metadata structure"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "plk1 abundance": "log2 transformed intensity",
          "chek2-s163 abundance": "log2 transformed intensity",
          "tp53_chek2": "pathway activity score (unitless)",
          "plk1": "log2 transformed abundance (median polished)",
          "chek2-s163": "log2 transformed abundance (median polished)",
          "spearman_correlation": "unitless coefficient (-1 to 1)"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {
            "PLK1 abundance": "log2 transformed intensity",
            "CHEK2-S163 abundance": "log2 transformed intensity",
            "TP53_CHEK2": "pathway activity score (unitless)"
          },
          {
            "PLK1": "log2 transformed abundance (median polished)",
            "CHEK2-S163": "log2 transformed abundance (median polished)",
            "Spearman_correlation": "unitless coefficient (-1 to 1)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Protein abundances are log2 transformed",
          "Missing values represented as empty strings in some datasets",
          "Some columns have mixed data types (object)",
          "Both proteomics datasets are log2 transformed",
          "Data is median polished, meaning batch effects have been corrected"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Protein abundances are log2 transformed",
            "Missing values represented as empty strings in some datasets",
            "Some columns have mixed data types (object)"
          ],
          [
            "Both proteomics datasets are log2 transformed",
            "Data is median polished, meaning batch effects have been corrected"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "CHEK2 appears in multiple contexts: TP53_CHEK2 pathway activity in metadata vs CHEK2-S163 phosphosite in phosphoproteomics",
          "Need to distinguish between CHEK2 protein and CHEK2-S163 phosphosite",
          "Sample identifiers are row names in clinical data but column names in proteomics data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CHEK2 appears in multiple contexts: TP53_CHEK2 pathway activity in metadata vs CHEK2-S163 phosphosite in phosphoproteomics",
            "Need to distinguish between CHEK2 protein and CHEK2-S163 phosphosite"
          ],
          [
            "Sample identifiers are row names in clinical data but column names in proteomics data"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 0.6666666666666666,
        "votes": [
          ",",
          "xlsx format (Excel)",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "Cannot be determined",
          "Staging Incomplete",
          "NaN"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "Cannot be determined",
            "Staging Incomplete"
          ],
          [
            "NA",
            "NaN",
            "",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 154.0,
        "confidence": 0.8387096774193549,
        "votes": [
          154.0,
          154.0,
          179.0
        ]
      },
      "file_format": {
        "value": "xlsx",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "xlsx",
          "xlsx"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only tumor samples (Proteomics_Tumor_Normal == 'Tumor')",
          "Only samples not excluded (Case_excluded == 'No')",
          "Complete cases only for PLK1 and CHEK2-S163",
          "Sample count \u2264 153 (total samples in metadata)",
          "Case_excluded must equal 'No'",
          "Both PLK1 and CHEK2-S163 values must be non-missing for a sample to be included",
          "Only tumor samples should be included (Proteomics_Tumor_Normal = 'Tumor')",
          "Result must be rounded to 4 decimal places",
          "Ensure 'Case_excluded' column in '1-s2.0-S0092867420301070-mmc1.xlsx' only contains 'No' after filtering."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only tumor samples (Proteomics_Tumor_Normal == 'Tumor')",
            "Only samples not excluded (Case_excluded == 'No')",
            "Complete cases only for PLK1 and CHEK2-S163",
            "Sample count \u2264 153 (total samples in metadata)"
          ],
          [
            "Case_excluded must equal 'No'",
            "Both PLK1 and CHEK2-S163 values must be non-missing for a sample to be included",
            "Only tumor samples should be included (Proteomics_Tumor_Normal = 'Tumor')",
            "Result must be rounded to 4 decimal places"
          ],
          [
            "Ensure 'Case_excluded' column in '1-s2.0-S0092867420301070-mmc1.xlsx' only contains 'No' after filtering."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove samples where PLK1 is missing",
          "Remove samples where CHEK2-S163 is missing",
          "Remove non-tumor samples",
          "Remove excluded samples",
          "Create a list of valid sample IDs where Case_excluded = 'No'",
          "Extract PLK1 row from global proteomics data",
          "Extract CHEK2-S163 row from phosphoproteomics data",
          "Filter to samples that have non-missing values in both PLK1 and CHEK2-S163",
          "Intersect valid samples across all filter conditions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove samples where PLK1 is missing",
            "Remove samples where CHEK2-S163 is missing",
            "Remove non-tumor samples",
            "Remove excluded samples"
          ],
          [
            "Create a list of valid sample IDs where Case_excluded = 'No'",
            "Extract PLK1 row from global proteomics data",
            "Extract CHEK2-S163 row from phosphoproteomics data",
            "Filter to samples that have non-missing values in both PLK1 and CHEK2-S163",
            "Intersect valid samples across all filter conditions"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Spearman rank correlation test",
          "Two-tailed significance test",
          "Spearman rank correlation coefficient (non-parametric correlation)",
          "Spearman correlation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Spearman rank correlation test",
            "Two-tailed significance test"
          ],
          [
            "Spearman rank correlation coefficient (non-parametric correlation)"
          ],
          [
            "Spearman correlation"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Correlation coefficient rounded to 4 decimal places",
          "No additional formatting or text",
          "Single numeric value",
          "Rounded to exactly 4 decimal places",
          "Result should be between -1 and 1",
          "Round the Spearman correlation to 4 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Correlation coefficient rounded to 4 decimal places",
            "No additional formatting or text"
          ],
          [
            "Single numeric value",
            "Rounded to exactly 4 decimal places",
            "Result should be between -1 and 1"
          ],
          [
            "Round the Spearman correlation to 4 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5252688172043012
  },
  "biomedical-hard-3": {
    "m_q": {
      "target_metric": {
        "value": "Age of the patient with the lowest APP_Z_score",
        "confidence": 0.6666666666666666,
        "votes": [
          "Age of the patient with the lowest APP_Z_score",
          "Age of the patient with the lowest APP-Z score",
          "Age of the patient with the lowest APP_Z_score"
        ]
      },
      "filters": {
        "value": [
          "Case_excluded == 'No'",
          "Proteomics_Tumor_Normal == 'Tumor'",
          "Exclude patients where APP_Z_score is null or missing",
          "Exclude patients where Age is null or missing"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Case_excluded == 'No'",
            "Proteomics_Tumor_Normal == 'Tumor'"
          ],
          [
            "Exclude patients where APP_Z_score is null or missing",
            "Exclude patients where Age is null or missing"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the minimum APP_Z_score value?",
          "Which patient (idx) has the minimum APP_Z_score?",
          "What is the Age value for that patient?",
          "What is the minimum APP_Z_score across all patients?",
          "Which patient has the minimum APP_Z_score?",
          "What is the Age of that patient?",
          "Find the minimum APP_Z_score.",
          "Identify the patient(s) with the minimum APP_Z_score.",
          "Retrieve the age of the identified patient(s)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the minimum APP_Z_score value?",
            "Which patient (idx) has the minimum APP_Z_score?",
            "What is the Age value for that patient?"
          ],
          [
            "What is the minimum APP_Z_score across all patients?",
            "Which patient has the minimum APP_Z_score?",
            "What is the Age of that patient?"
          ],
          [
            "Find the minimum APP_Z_score.",
            "Identify the patient(s) with the minimum APP_Z_score.",
            "Retrieve the age of the identified patient(s)."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc1.xlsx",
          "1-s2.0-S0092867420301070-mmc7__B-APM subtypes"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7__B-APM subtypes"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7__B-APM subtypes"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7__B-APM subtypes"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Both files have 'Log2_variant_per_Mbp' column - need to verify consistency",
          "mmc1 has 'Age' column while APM subtypes doesn't",
          "Both files contain 'idx' and 'Log2_variant_per_Mbp' columns",
          "mutation_classification in mmc7 may relate to MSI_status or Genomics_subtype in mmc1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Both files have 'Log2_variant_per_Mbp' column - need to verify consistency",
            "mmc1 has 'Age' column while APM subtypes doesn't"
          ],
          [
            "Both files contain 'idx' and 'Log2_variant_per_Mbp' columns",
            "mutation_classification in mmc7 may relate to MSI_status or Genomics_subtype in mmc1"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "age": "years",
          "app_z_score": "standard deviations",
          "log2_variant_per_mbp": "log2(variants per megabase)",
          "bmi": "kg/m\u00b2",
          "tumor_size_cm": "centimeters"
        },
        "confidence": 0.6666666666666667,
        "votes": [
          {
            "Age": "years",
            "APP_Z_score": "standard deviations",
            "Log2_variant_per_Mbp": "log2(variants per megabase)",
            "BMI": "kg/m\u00b2",
            "Tumor_Size_cm": "centimeters"
          },
          {
            "Age": "years",
            "APP_Z_score": "standardized z-score (dimensionless)",
            "Log2_variant_per_Mbp": "log2 scale"
          },
          {
            "Age": "years",
            "APP_Z_score": "z-score"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "APP_Z_score appears to be standardized (mean=0, SD=1)",
          "Some immune cell columns have negative values suggesting z-scores",
          "APP_Z_score is a standardized score with mean 0 and standard deviation 1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "APP_Z_score appears to be standardized (mean=0, SD=1)",
            "Some immune cell columns have negative values suggesting z-scores"
          ],
          [
            "APP_Z_score is a standardized score with mean 0 and standard deviation 1"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Sample count mismatch: mmc1 has 153 rows, APM subtypes has 95 rows",
          "Log2_variant_per_Mbp appears in both files - need to verify consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sample count mismatch: mmc1 has 153 rows, APM subtypes has 95 rows"
          ],
          [
            "Log2_variant_per_Mbp appears in both files - need to verify consistency"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "Cannot be determined",
          "Staging Incomplete",
          "Normal",
          "NA",
          "N/A"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "",
            "Cannot be determined",
            "Staging Incomplete",
            "Normal"
          ],
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined",
            "Staging Incomplete"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 179.0,
        "confidence": 0.33,
        "votes": [
          179.0,
          179.0,
          10.0
        ]
      },
      "file_format": {
        "value": "xlsx",
        "confidence": 0.6666666666666666,
        "votes": [
          "Excel (converted to CSV for display)",
          "xlsx",
          "xlsx"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Age should be positive (0-120)",
          "APP_Z_score should be continuous numeric",
          "idx values should match between files",
          "Proteomics_Participant_ID should be unique per patient",
          "APP_Z_score must be numeric and not null",
          "Age must be numeric and not null",
          "idx must match between both data sources",
          "APP_Z_score must be a numerical value.",
          "Age must be a numerical value."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age should be positive (0-120)",
            "APP_Z_score should be continuous numeric",
            "idx values should match between files",
            "Proteomics_Participant_ID should be unique per patient"
          ],
          [
            "APP_Z_score must be numeric and not null",
            "Age must be numeric and not null",
            "idx must match between both data sources"
          ],
          [
            "APP_Z_score must be a numerical value.",
            "Age must be a numerical value."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where Case_excluded != 'No'",
          "Consider only Tumor samples (Proteomics_Tumor_Normal == 'Tumor')",
          "Filter for complete APP_Z_score values",
          "Filter to patients with valid APP_Z_score in mmc7 sheet",
          "Filter to patients with valid Age in mmc1 file",
          "Only consider patients present in both datasets after join"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Case_excluded != 'No'",
            "Consider only Tumor samples (Proteomics_Tumor_Normal == 'Tumor')",
            "Filter for complete APP_Z_score values"
          ],
          [
            "Filter to patients with valid APP_Z_score in mmc7 sheet",
            "Filter to patients with valid Age in mmc1 file",
            "Only consider patients present in both datasets after join"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for normal distribution of APP_Z_score",
          "Verify correlation between Age and APP_Z_score",
          "Test for outliers in APP_Z_score"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for normal distribution of APP_Z_score",
            "Verify correlation between Age and APP_Z_score",
            "Test for outliers in APP_Z_score"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Age should be reported as numeric value",
          "Include patient identifier (idx or Proteomics_Participant_ID)",
          "Report APP_Z_score value for verification",
          "Return a single numeric value representing age in years",
          "If multiple patients have the same minimum APP_Z_score, return the age of one patient",
          "Output should be a single numerical value representing the age."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age should be reported as numeric value",
            "Include patient identifier (idx or Proteomics_Participant_ID)",
            "Report APP_Z_score value for verification"
          ],
          [
            "Return a single numeric value representing age in years",
            "If multiple patients have the same minimum APP_Z_score, return the age of one patient"
          ],
          [
            "Output should be a single numerical value representing the age."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5498333333333334
  },
  "biomedical-hard-4": {
    "m_q": {
      "target_metric": {
        "value": "Histologic_Grade_FIGO values for tumors where peptide HPKPEVLGSSADGALLVSLDGLR was detected",
        "confidence": 0.3333333333333333,
        "votes": [
          "Histologic_Grade_FIGO values for tumors where peptide HPKPEVLGSSADGALLVSLDGLR was detected",
          "Histological grades (FIGO) of tumors for peptide HPKPEVLGSSADGALLVSLDGLR",
          "Histological grades of tumors for samples where the peptide HPKPEVLGSSADGALLVSLDGLR was found."
        ]
      },
      "filters": {
        "value": [
          "Peptide = 'HPKPEVLGSSADGALLVSLDGLR'",
          "Proteomics_Tumor_Normal = 'Tumor'",
          "Peptide == 'HPKPEVLGSSADGALLVSLDGLR'",
          "The peptide is HPKPEVLGSSADGALLVSLDGLR"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Peptide = 'HPKPEVLGSSADGALLVSLDGLR'",
            "Proteomics_Tumor_Normal = 'Tumor'"
          ],
          [
            "Peptide == 'HPKPEVLGSSADGALLVSLDGLR'"
          ],
          [
            "The peptide is HPKPEVLGSSADGALLVSLDGLR"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Histologic_Grade_FIGO"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Histologic_Grade_FIGO"
          ],
          [
            "Histologic_Grade_FIGO"
          ],
          [
            "Histologic_Grade_FIGO"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which samples contain the peptide HPKPEVLGSSADGALLVSLDGLR?",
          "What are the histological grades for those tumor samples?",
          "How many samples of each histological grade contain this peptide?",
          "What are the Participant IDs for samples containing this peptide?",
          "What are the histological grades for those participants?"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Which samples contain the peptide HPKPEVLGSSADGALLVSLDGLR?",
            "What are the histological grades for those tumor samples?",
            "How many samples of each histological grade contain this peptide?"
          ],
          [
            "Which samples contain the peptide HPKPEVLGSSADGALLVSLDGLR?",
            "What are the Participant IDs for samples containing this peptide?",
            "What are the histological grades for those participants?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc1.xlsx",
          "1-s2.0-S0092867420301070-mmc4__C-Alternate Splice Junctions"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc4__C-Alternate Splice Junctions"
          ],
          [
            "1-s2.0-S0092867420301070-mmc4__C-Alternate Splice Junctions",
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          [
            "1-s2.0-S0092867420301070-mmc4__C-Alternate Splice Junctions",
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Samples_With_Peptide contains comma-separated values (e.g., 'S028,S034,S040') while idx contains single sample identifiers",
          "Need to split Samples_With_Peptide into individual sample IDs before joining",
          "Samples_With_Peptide column contains comma-separated sample IDs that need to be split",
          "idx column uses 'S' prefix format matching Samples_With_Peptide values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Samples_With_Peptide contains comma-separated values (e.g., 'S028,S034,S040') while idx contains single sample identifiers",
            "Need to split Samples_With_Peptide into individual sample IDs before joining"
          ],
          [
            "Samples_With_Peptide column contains comma-separated sample IDs that need to be split",
            "idx column uses 'S' prefix format matching Samples_With_Peptide values"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "massdiff": "Da",
          "hyperscore": "unitless",
          "expectation": "E-score",
          "bmi": "kg/m\u00b2",
          "age": "years",
          "tumor_size_cm": "cm",
          "histologic_grade_figo": "categorical (FIGO grade 1, 2, 3)",
          "peptide": "amino acid sequence string",
          "num_samples_with_peptide": "count"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {
            "MassDiff": "Da",
            "Hyperscore": "unitless",
            "Expectation": "E-score",
            "BMI": "kg/m\u00b2",
            "Age": "years",
            "Tumor_Size_cm": "cm"
          },
          {
            "Histologic_Grade_FIGO": "categorical (FIGO grade 1, 2, 3)",
            "Peptide": "amino acid sequence string",
            "Num_Samples_With_Peptide": "count"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Histologic_Grade_FIGO contains categorical values like 'FIGO grade 1' rather than numerical grades",
          "Some percentage columns (Estrogen_Receptor_%, Progesterone_Receptor_%) have float values but represent percentages"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Histologic_Grade_FIGO contains categorical values like 'FIGO grade 1' rather than numerical grades",
            "Some percentage columns (Estrogen_Receptor_%, Progesterone_Receptor_%) have float values but represent percentages"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Sample identifiers in mmc4 (S028, S034, S040) need to be mapped to idx values in mmc1 (S001, S002, etc.)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sample identifiers in mmc4 (S028, S034, S040) need to be mapped to idx values in mmc1 (S001, S002, etc.)"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "Cannot be determined",
          "Staging Incomplete",
          "Normal",
          "NA",
          "N/A"
        ],
        "confidence": 0.611111111111111,
        "votes": [
          [
            "",
            "Cannot be determined",
            "Staging Incomplete",
            "Normal"
          ],
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 13.0,
        "confidence": 0.33,
        "votes": [
          179.0,
          13.0,
          13.0
        ]
      },
      "file_format": {
        "value": "Excel",
        "confidence": 0.3333333333333333,
        "votes": [
          "Excel",
          "xlsx",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Peptide sequences must match exactly 'HPKPEVLGSSADGALLVSLDGLR'",
          "Only tumor samples should be considered (Proteomics_Tumor_Normal = 'Tumor')",
          "Histologic_Grade_FIGO should not be null for relevant samples",
          "Peptide must exactly match 'HPKPEVLGSSADGALLVSLDGLR'",
          "Only tumor samples should be considered (Proteomics_Tumor_Normal == 'Tumor')",
          "Exclude cases where Case_excluded == 'Yes'",
          "Samples_With_Peptide in 1-s2.0-S0092867420301070-mmc4__C-Alternate Splice Junctions contains comma separated values, so it needs to be split into individual sample IDs before joining."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Peptide sequences must match exactly 'HPKPEVLGSSADGALLVSLDGLR'",
            "Only tumor samples should be considered (Proteomics_Tumor_Normal = 'Tumor')",
            "Histologic_Grade_FIGO should not be null for relevant samples"
          ],
          [
            "Peptide must exactly match 'HPKPEVLGSSADGALLVSLDGLR'",
            "Only tumor samples should be considered (Proteomics_Tumor_Normal == 'Tumor')",
            "Exclude cases where Case_excluded == 'Yes'"
          ],
          [
            "Samples_With_Peptide in 1-s2.0-S0092867420301070-mmc4__C-Alternate Splice Junctions contains comma separated values, so it needs to be split into individual sample IDs before joining."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Split Samples_With_Peptide into individual sample IDs",
          "Filter mmc1 to only include rows where idx matches one of the peptide-containing samples",
          "Parse Samples_With_Peptide to extract individual sample IDs",
          "Filter clinical data to samples found in Samples_With_Peptide",
          "Extract non-null Histologic_Grade_FIGO values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Split Samples_With_Peptide into individual sample IDs",
            "Filter mmc1 to only include rows where idx matches one of the peptide-containing samples"
          ],
          [
            "Parse Samples_With_Peptide to extract individual sample IDs",
            "Filter clinical data to samples found in Samples_With_Peptide",
            "Extract non-null Histologic_Grade_FIGO values"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Frequency distribution of Histologic_Grade_FIGO values among peptide-positive samples",
          "Comparison of grade distribution between peptide-positive and peptide-negative tumors"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Frequency distribution of Histologic_Grade_FIGO values among peptide-positive samples",
            "Comparison of grade distribution between peptide-positive and peptide-negative tumors"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of distinct Histologic_Grade_FIGO values",
          "Count of samples for each grade containing the peptide",
          "Sample IDs for each grade category",
          "Return list of unique histological grades",
          "Include sample count per grade",
          "Show actual sample IDs for traceability"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of distinct Histologic_Grade_FIGO values",
            "Count of samples for each grade containing the peptide",
            "Sample IDs for each grade category"
          ],
          [
            "Return list of unique histological grades",
            "Include sample count per grade",
            "Show actual sample IDs for traceability"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5503888888888889
  },
  "biomedical-hard-5": {
    "m_q": {
      "target_metric": {
        "value": "median of Log2_variant_per_Mbp for serous tumor samples",
        "confidence": 0.3333333333333333,
        "votes": [
          "median of Log2_variant_per_Mbp for serous tumor samples",
          "median number of variants per Mbp",
          "median of Log2_variant_per_Mbp"
        ]
      },
      "filters": {
        "value": [
          "Histologic_type == 'Serous'",
          "Proteomics_Tumor_Normal == 'Tumor'",
          "Histologic_type is Serous"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "Histologic_type == 'Serous'",
            "Proteomics_Tumor_Normal == 'Tumor'"
          ],
          [
            "Histologic_type == 'Serous'"
          ],
          [
            "Histologic_type is Serous"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Identify serous tumor samples in 1-s2.0-S0092867420301070-mmc1.xlsx",
          "Extract Log2_variant_per_Mbp values for those samples",
          "Calculate median of those values",
          "Round result to 4 decimal places",
          "Which samples have Histologic_type equal to 'Serous'?",
          "What is the variants per Mbp value for each serous sample?",
          "What is the median of these values?",
          "How should the result be rounded to 4 decimal places?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify serous tumor samples in 1-s2.0-S0092867420301070-mmc1.xlsx",
            "Extract Log2_variant_per_Mbp values for those samples",
            "Calculate median of those values",
            "Round result to 4 decimal places"
          ],
          [
            "Which samples have Histologic_type equal to 'Serous'?",
            "What is the variants per Mbp value for each serous sample?",
            "What is the median of these values?",
            "How should the result be rounded to 4 decimal places?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc1.xlsx"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "log2_variant_per_mbp": "log2(variants per megabase pair)",
          "variants_per_mbp": "variants per megabase pair (derived from Log2_variant_per_Mbp)",
          "age": "years",
          "tumor_size_cm": "cm",
          "estrogen_receptor_%": "%",
          "progesterone_receptor_%": "%",
          "bmi": "kg/m^2"
        },
        "confidence": 0.42857142857142855,
        "votes": [
          {
            "Log2_variant_per_Mbp": "log2(variants per megabase pair)"
          },
          {
            "Log2_variant_per_Mbp": "log2 transformed variants per megabase pair",
            "variants_per_Mbp": "variants per megabase pair (derived from Log2_variant_per_Mbp)"
          },
          {
            "Log2_variant_per_Mbp": "log2(variants/Mbp)",
            "Age": "years",
            "Tumor_Size_cm": "cm",
            "Estrogen_Receptor_%": "%",
            "Progesterone_Receptor_%": "%",
            "BMI": "kg/m^2"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Log2_variant_per_Mbp is log2-transformed, need to convert back to linear scale for interpretation",
          "Log2_variant_per_Mbp is in log2 scale and needs to be converted to linear scale: variants_per_Mbp = 2^Log2_variant_per_Mbp",
          "Log2_variant_per_Mbp is log2 transformed, need to exponentiate to get variants per Mbp"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Log2_variant_per_Mbp is log2-transformed, need to convert back to linear scale for interpretation"
          ],
          [
            "Log2_variant_per_Mbp is in log2 scale and needs to be converted to linear scale: variants_per_Mbp = 2^Log2_variant_per_Mbp"
          ],
          [
            "Log2_variant_per_Mbp is log2 transformed, need to exponentiate to get variants per Mbp"
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "Cannot be determined",
          "Staging Incomplete"
        ],
        "confidence": 0.8,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined"
          ],
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined",
            "Staging Incomplete"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 179.0,
        "confidence": 1.0,
        "votes": [
          179.0,
          179.0,
          179.0
        ]
      },
      "file_format": {
        "value": "xlsx",
        "confidence": 0.6666666666666666,
        "votes": [
          "excel",
          "xlsx",
          "xlsx"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Histologic_type column must contain 'Serous' values",
          "Log2_variant_per_Mbp must be numeric",
          "Proteomics_Tumor_Normal must distinguish tumor vs normal",
          "Only include samples where Histologic_type is 'Serous'",
          "Exclude samples where Log2_variant_per_Mbp is null or missing",
          "Convert Log2_variant_per_Mbp to linear scale before calculating median",
          "Histologic_type must be 'Serous'",
          "Round the median to 4 decimal places"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Histologic_type column must contain 'Serous' values",
            "Log2_variant_per_Mbp must be numeric",
            "Proteomics_Tumor_Normal must distinguish tumor vs normal"
          ],
          [
            "Only include samples where Histologic_type is 'Serous'",
            "Exclude samples where Log2_variant_per_Mbp is null or missing",
            "Convert Log2_variant_per_Mbp to linear scale before calculating median"
          ],
          [
            "Histologic_type must be 'Serous'",
            "Round the median to 4 decimal places"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude Case_excluded == 'Yes' if present",
          "Filter for tumor samples only",
          "Filter to serous tumor samples using Histologic_type column",
          "Remove any samples with missing variant data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude Case_excluded == 'Yes' if present",
            "Filter for tumor samples only"
          ],
          [
            "Filter to serous tumor samples using Histologic_type column",
            "Remove any samples with missing variant data"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Median calculation",
          "Rounding to 4 decimal places",
          "Calculate median (50th percentile) of variants per Mbp for filtered samples"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Median calculation",
            "Rounding to 4 decimal places"
          ],
          [
            "Calculate median (50th percentile) of variants per Mbp for filtered samples"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numeric value with 4 decimal places",
          "No additional formatting",
          "Round final result to exactly 4 decimal places",
          "Return single scalar value representing median variants per Mbp",
          "The output must be a single numerical value rounded to 4 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric value with 4 decimal places",
            "No additional formatting"
          ],
          [
            "Round final result to exactly 4 decimal places",
            "Return single scalar value representing median variants per Mbp"
          ],
          [
            "The output must be a single numerical value rounded to 4 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5336507936507938
  },
  "biomedical-hard-7": {
    "m_q": {
      "target_metric": {
        "value": "count of significant genes identified through acetylproteomics analysis",
        "confidence": 0.3333333333333333,
        "votes": [
          "count of significant genes identified through acetylproteomics analysis",
          "Count of significant genes identified by acetylproteomics",
          "Count of significant genes identified by acetylproteomics."
        ]
      },
      "filters": {
        "value": [
          "Sheet D-SE-acetyl contains the relevant data",
          "Sheet = 'D-SE-acetyl'",
          "Genes listed in the D-SE-acetyl sheet"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sheet D-SE-acetyl contains the relevant data"
          ],
          [
            "Sheet = 'D-SE-acetyl'",
            "Genes listed in the D-SE-acetyl sheet"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What constitutes a 'significant gene' in this context?",
          "Are there any thresholds or criteria for significance?",
          "Is this count of unique genes or includes duplicates?",
          "Which sheet contains the significant genes by acetylproteomics?",
          "How many rows/genes are in the D-SE-acetyl sheet?",
          "Are there any duplicate genes that need to be deduplicated?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What constitutes a 'significant gene' in this context?",
            "Are there any thresholds or criteria for significance?",
            "Is this count of unique genes or includes duplicates?"
          ],
          [
            "Which sheet contains the significant genes by acetylproteomics?",
            "How many rows/genes are in the D-SE-acetyl sheet?",
            "Are there any duplicate genes that need to be deduplicated?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc3.xlsx",
          "1-s2.0-S0092867420301070-mmc3__D-SE-acetyl"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc3__D-SE-acetyl"
          ],
          [
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc3__D-SE-acetyl"
          ],
          [
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc3__D-SE-acetyl"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "The main Excel file has sheet descriptions but actual data is in separate files with sheet names appended",
          "D-SE-acetyl sheet has single column with gene names starting with 'BRD8' as column header"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "The main Excel file has sheet descriptions but actual data is in separate files with sheet names appended"
          ],
          [
            "D-SE-acetyl sheet has single column with gene names starting with 'BRD8' as column header"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "brd8": "gene_symbol"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {
            "BRD8": "gene_symbol"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "The acetylproteomics sheet has only 15 rows, suggesting either a small number of significant genes or a filtered subset"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "The acetylproteomics sheet has only 15 rows, suggesting either a small number of significant genes or a filtered subset"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Sheet D-SE-acetyl description says 'Significant genes by acetylproteomics in Figure 1' but doesn't specify significance criteria"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sheet D-SE-acetyl description says 'Significant genes by acetylproteomics in Figure 1' but doesn't specify significance criteria"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.111111111111111,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " "
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 1.0,
        "confidence": 1.0,
        "votes": [
          1.0,
          1.0,
          1.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "xlsx",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "The answer should be derived from file 1-s2.0-S0092867420301070-mmc3__D-SE-acetyl",
          "Each row represents one significant gene",
          "No duplicate genes appear in the sample data",
          "Only count genes from sheet D-SE-acetyl",
          "Count should include all non-empty gene entries",
          "First row appears to be a gene name (BRD8) not a header",
          "The '1-s2.0-S0092867420301070-mmc3.xlsx' file needs to be read to identify the sheet corresponding to 'Significant genes by acetylproteomics'.",
          "The number of rows in the identified sheet ('1-s2.0-S0092867420301070-mmc3__D-SE-acetyl') represents the count of significant genes."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "The answer should be derived from file 1-s2.0-S0092867420301070-mmc3__D-SE-acetyl",
            "Each row represents one significant gene",
            "No duplicate genes appear in the sample data"
          ],
          [
            "Only count genes from sheet D-SE-acetyl",
            "Count should include all non-empty gene entries",
            "First row appears to be a gene name (BRD8) not a header"
          ],
          [
            "The '1-s2.0-S0092867420301070-mmc3.xlsx' file needs to be read to identify the sheet corresponding to 'Significant genes by acetylproteomics'.",
            "The number of rows in the identified sheet ('1-s2.0-S0092867420301070-mmc3__D-SE-acetyl') represents the count of significant genes."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Count all rows in the acetylproteomics file (excluding header)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count all rows in the acetylproteomics file (excluding header)"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Simple count of rows"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Simple count of rows"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Integer count",
          "Should match the shape row count (15)",
          "Return single integer count of significant genes",
          "Count should be 15 based on the shape and sample data showing all entries are gene symbols"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Integer count",
            "Should match the shape row count (15)"
          ],
          [
            "Return single integer count of significant genes",
            "Count should be 15 based on the shape and sample data showing all entries are gene symbols"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5722222222222223
  },
  "biomedical-hard-8": {
    "m_q": {
      "target_metric": {
        "value": "List of protein sites (phosphorylation or acetylation sites) that are hyperactivated in CNV-high endometrioid samples and are targeted by FDA-approved drugs",
        "confidence": 0.3333333333333333,
        "votes": [
          "List of protein sites (phosphorylation or acetylation sites) that are hyperactivated in CNV-high endometrioid samples and are targeted by FDA-approved drugs",
          "Protein sites that are hyperactivated in CNV-high endometrioid samples and targeted by FDA-approved drugs",
          "List of protein sites hyperactivated in CNV-high endometroid samples and targeted by FDA-approved drugs"
        ]
      },
      "filters": {
        "value": [
          "Histologic_type = 'Endometrioid'",
          "CNV_class = 'CNV_HIGH' (or similar high CNV classification)",
          "Hyperactivation defined by significant positive FoldChange in phospho/acetyl sites with appropriate statistical thresholds (e.g., FDR < 0.05, FoldChange > threshold)",
          "Histologic_type == 'Endometrioid'",
          "CNV_class == 'CNV_HIGH'",
          "Hyperactivated proteins from hyperactivated.csv",
          "FDA-approved drug targets from 1-s2.0-S0092867420301070-mmc6__G-FDA approved drugs",
          "Samples with CNV_class = 'CNV_HIGH' and Histologic_type = 'Endometrioid'",
          "Protein sites that are hyperactivated",
          "Proteins targeted by FDA-approved drugs"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Histologic_type = 'Endometrioid'",
            "CNV_class = 'CNV_HIGH' (or similar high CNV classification)",
            "Hyperactivation defined by significant positive FoldChange in phospho/acetyl sites with appropriate statistical thresholds (e.g., FDR < 0.05, FoldChange > threshold)"
          ],
          [
            "Histologic_type == 'Endometrioid'",
            "CNV_class == 'CNV_HIGH'",
            "Hyperactivated proteins from hyperactivated.csv",
            "FDA-approved drug targets from 1-s2.0-S0092867420301070-mmc6__G-FDA approved drugs"
          ],
          [
            "Samples with CNV_class = 'CNV_HIGH' and Histologic_type = 'Endometrioid'",
            "Protein sites that are hyperactivated",
            "Proteins targeted by FDA-approved drugs"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "ID (protein site identifier)",
          "Gene",
          "sample_id",
          "CNV_class",
          "Histologic_type"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "ID (protein site identifier)",
            "Gene"
          ],
          [
            "sample_id",
            "CNV_class",
            "Histologic_type"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which samples are CNV-high endometrioid?",
          "Which protein sites are hyperactivated in these samples?",
          "Which of those sites correspond to genes targeted by FDA-approved drugs?",
          "What drugs target those genes?",
          "Which samples are CNV-high endometrioid samples?",
          "Which proteins are hyperactivated in these samples?",
          "Which of these hyperactivated proteins are targeted by FDA-approved drugs?",
          "What are the specific phosphorylation or acetylation sites for these proteins?",
          "Identify CNV-high endometroid samples from the clinical data.",
          "Determine which protein sites are hyperactivated in these samples.",
          "Identify FDA-approved drugs and their target proteins.",
          "Find the intersection of hyperactivated protein sites and FDA-approved drug targets."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which samples are CNV-high endometrioid?",
            "Which protein sites are hyperactivated in these samples?",
            "Which of those sites correspond to genes targeted by FDA-approved drugs?",
            "What drugs target those genes?"
          ],
          [
            "Which samples are CNV-high endometrioid samples?",
            "Which proteins are hyperactivated in these samples?",
            "Which of these hyperactivated proteins are targeted by FDA-approved drugs?",
            "What are the specific phosphorylation or acetylation sites for these proteins?"
          ],
          [
            "Identify CNV-high endometroid samples from the clinical data.",
            "Determine which protein sites are hyperactivated in these samples.",
            "Identify FDA-approved drugs and their target proteins.",
            "Find the intersection of hyperactivated protein sites and FDA-approved drug targets."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "1-s2.0-S0092867420301070-mmc1.xlsx",
          "1-s2.0-S0092867420301070-mmc6__B-SE phospho site between Serou",
          "1-s2.0-S0092867420301070-mmc6__C-SE scetyl site between Serous",
          "1-s2.0-S0092867420301070-mmc6__G-FDA approved drugs",
          "hyperactivated.csv",
          "1-s2.0-S0092867420301070-mmc6__E-SE phospho site between MSI-H",
          "1-s2.0-S0092867420301070-mmc6.xlsx"
        ],
        "confidence": 0.7619047619047618,
        "votes": [
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6__B-SE phospho site between Serou",
            "1-s2.0-S0092867420301070-mmc6__C-SE scetyl site between Serous",
            "1-s2.0-S0092867420301070-mmc6__G-FDA approved drugs",
            "hyperactivated.csv"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc6__G-FDA approved drugs",
            "1-s2.0-S0092867420301070-mmc6__B-SE phospho site between Serou",
            "1-s2.0-S0092867420301070-mmc6__E-SE phospho site between MSI-H"
          ],
          [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc6__B-SE phospho site between Serou",
            "1-s2.0-S0092867420301070-mmc6__C-SE scetyl site between Serous",
            "1-s2.0-S0092867420301070-mmc6__G-FDA approved drugs",
            "hyperactivated.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different naming conventions for CNV classification (CNV_class vs CNV_idx)",
          "Protein site IDs in phospho/acetyl sheets contain gene and site info (e.g., 'AAAS-S495') while hyperactivated.csv has only protein names",
          "Multiple drug entries per gene in FDA sheet",
          "Gene name column varies: 'protein' in hyperactivated.csv, 'gene_name' in FDA drugs, 'Gene' in phospho site sheets",
          "Sample ID column varies: 'idx' in mmc1, 'sample_id' in hyperactivated.csv",
          "The 'ID' column in '1-s2.0-S0092867420301070-mmc6__B-SE phospho site between Serou' and '1-s2.0-S0092867420301070-mmc6__C-SE scetyl site between Serous' has different meanings. One represents phospho/acetyl site ID, the other represents protein ID.",
          "The 'gene_name' column in '1-s2.0-S0092867420301070-mmc6__G-FDA approved drugs' and '1-s2.0-S0092867420301070-mmc6__B-SE phospho site between Serou'/'1-s2.0-S0092867420301070-mmc6__C-SE scetyl site between Serous' refers to the same concept but might have different naming conventions."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different naming conventions for CNV classification (CNV_class vs CNV_idx)",
            "Protein site IDs in phospho/acetyl sheets contain gene and site info (e.g., 'AAAS-S495') while hyperactivated.csv has only protein names",
            "Multiple drug entries per gene in FDA sheet"
          ],
          [
            "Gene name column varies: 'protein' in hyperactivated.csv, 'gene_name' in FDA drugs, 'Gene' in phospho site sheets",
            "Sample ID column varies: 'idx' in mmc1, 'sample_id' in hyperactivated.csv"
          ],
          [
            "The 'ID' column in '1-s2.0-S0092867420301070-mmc6__B-SE phospho site between Serou' and '1-s2.0-S0092867420301070-mmc6__C-SE scetyl site between Serous' has different meanings. One represents phospho/acetyl site ID, the other represents protein ID.",
            "The 'gene_name' column in '1-s2.0-S0092867420301070-mmc6__G-FDA approved drugs' and '1-s2.0-S0092867420301070-mmc6__B-SE phospho site between Serou'/'1-s2.0-S0092867420301070-mmc6__C-SE scetyl site between Serous' refers to the same concept but might have different naming conventions."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "foldchange.protein": "log2 fold change",
          "foldchange.phos": "log2 fold change",
          "foldchange.acetyl": "log2 fold change",
          "foldchange.rna": "log2 fold change",
          "pval.*": "p-value",
          "fdr.*": "false discovery rate",
          "cnv_del": "copy number deletion score",
          "cnv_amp": "copy number amplification score",
          "pval.phos": "p-value",
          "fdr.phos": "false discovery rate",
          "pval.protein": "p-value",
          "logp.protein": "-log10(p-value)",
          "fdr.protein": "False Discovery Rate"
        },
        "confidence": 0.4358974358974358,
        "votes": [
          {
            "FoldChange.protein": "log2 fold change",
            "FoldChange.phos": "log2 fold change",
            "FoldChange.acetyl": "log2 fold change",
            "FoldChange.rna": "log2 fold change",
            "pval.*": "p-value",
            "FDR.*": "false discovery rate",
            "CNV_DEL": "copy number deletion score",
            "CNV_AMP": "copy number amplification score"
          },
          {
            "FoldChange.phos": "log2 fold change",
            "pval.phos": "p-value",
            "FDR.phos": "false discovery rate",
            "CNV_DEL": "copy number variation deletion score",
            "CNV_AMP": "copy number variation amplification score"
          },
          {
            "FoldChange.protein": "log2 fold change",
            "pval.protein": "p-value",
            "logp.protein": "-log10(p-value)",
            "FDR.protein": "False Discovery Rate"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "FoldChange values appear to be on different scales across datasets",
          "CNV_class is categorical while CNV_idx is numeric",
          "Some p-values are extremely small (e.g., 1.9698205776291e-05)",
          "CNV_class is categorical (CNV_HIGH, CNV_LOW) but derived from numeric CNV_DEL and CNV_AMP scores",
          "Fold change values may be in log2 scale requiring interpretation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "FoldChange values appear to be on different scales across datasets",
            "CNV_class is categorical while CNV_idx is numeric",
            "Some p-values are extremely small (e.g., 1.9698205776291e-05)"
          ],
          [
            "CNV_class is categorical (CNV_HIGH, CNV_LOW) but derived from numeric CNV_DEL and CNV_AMP scores",
            "Fold change values may be in log2 scale requiring interpretation"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "hyperactivated.csv lists protein names but phospho/acetyl sheets use site-specific IDs",
          "CNV classification criteria not explicitly defined across sources",
          "Protein/gene naming conventions may differ between hyperactivated list and drug target databases",
          "Site nomenclature (e.g., S495, T389) needs to be parsed from ID column in phospho site sheets"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "hyperactivated.csv lists protein names but phospho/acetyl sheets use site-specific IDs",
            "CNV classification criteria not explicitly defined across sources"
          ],
          [
            "Protein/gene naming conventions may differ between hyperactivated list and drug target databases",
            "Site nomenclature (e.g., S495, T389) needs to be parsed from ID column in phospho site sheets"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "Cannot be determined"
        ],
        "confidence": 0.9166666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined"
          ],
          [
            "NA",
            "N/A",
            "",
            "Cannot be determined"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.33,
        "votes": [
          10.0,
          179.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "xlsx/csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only endometrioid histology samples",
          "Only CNV-high samples (need to define threshold from CNV_class or CNV_idx)",
          "Statistical significance: FDR < 0.05 for phospho/acetyl sites",
          "Hyperactivation: positive FoldChange in phospho/acetyl sites",
          "Must have corresponding FDA-approved drug targeting the gene",
          "Histologic_type must equal 'Endometrioid'",
          "CNV_class must equal 'CNV_HIGH'",
          "Protein must appear in hyperactivated.csv for corresponding sample",
          "Protein must have matching gene_name in FDA approved drugs list",
          "Case_excluded should be 'No' to include valid samples",
          "CNV_class must be 'CNV_HIGH' to be considered CNV-high.",
          "Histologic_type must be 'Endometrioid' to be considered endometroid.",
          "FoldChange.phos/FoldChange.acetyl should be positive and statistically significant (e.g., pval < 0.05) to be considered hyperactivated."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only endometrioid histology samples",
            "Only CNV-high samples (need to define threshold from CNV_class or CNV_idx)",
            "Statistical significance: FDR < 0.05 for phospho/acetyl sites",
            "Hyperactivation: positive FoldChange in phospho/acetyl sites",
            "Must have corresponding FDA-approved drug targeting the gene"
          ],
          [
            "Histologic_type must equal 'Endometrioid'",
            "CNV_class must equal 'CNV_HIGH'",
            "Protein must appear in hyperactivated.csv for corresponding sample",
            "Protein must have matching gene_name in FDA approved drugs list",
            "Case_excluded should be 'No' to include valid samples"
          ],
          [
            "CNV_class must be 'CNV_HIGH' to be considered CNV-high.",
            "Histologic_type must be 'Endometrioid' to be considered endometroid.",
            "FoldChange.phos/FoldChange.acetyl should be positive and statistically significant (e.g., pval < 0.05) to be considered hyperactivated."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter mmc1 for Histologic_type = 'Endometrioid'",
          "Filter mmc1 for CNV_class indicating high CNV (or CNV_idx > threshold)",
          "Filter phospho/acetyl sheets for FDR.phos/FDR.acetyl < 0.05 and FoldChange.phos/FoldChange.acetyl > 0",
          "Filter FDA sheet for unique gene-drug pairs",
          "Filter samples where Histologic_type contains 'Endometrioid'",
          "Identify CNV-high samples using CNV_class column",
          "Cross-reference hyperactivated proteins with FDA drug targets",
          "Extract protein sites from ID column in phospho site data",
          "CNV-high endometroid samples: CNV_class = 'CNV_HIGH' and Histologic_type = 'Endometrioid'",
          "Hyperactivated protein sites: FoldChange.phos/FoldChange.acetyl > 0 and pval.phos/pval.acetyl < 0.05"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Filter mmc1 for Histologic_type = 'Endometrioid'",
            "Filter mmc1 for CNV_class indicating high CNV (or CNV_idx > threshold)",
            "Filter phospho/acetyl sheets for FDR.phos/FDR.acetyl < 0.05 and FoldChange.phos/FoldChange.acetyl > 0",
            "Filter FDA sheet for unique gene-drug pairs"
          ],
          [
            "Filter samples where Histologic_type contains 'Endometrioid'",
            "Identify CNV-high samples using CNV_class column",
            "Cross-reference hyperactivated proteins with FDA drug targets",
            "Extract protein sites from ID column in phospho site data"
          ],
          [
            "CNV-high endometroid samples: CNV_class = 'CNV_HIGH' and Histologic_type = 'Endometrioid'",
            "Hyperactivated protein sites: FoldChange.phos/FoldChange.acetyl > 0 and pval.phos/pval.acetyl < 0.05"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Differential expression/phosphorylation/acetylation tests already performed in provided sheets",
          "Need to apply multiple testing correction (FDR already provided)",
          "No statistical tests required - this is a lookup/intersection analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Differential expression/phosphorylation/acetylation tests already performed in provided sheets",
            "Need to apply multiple testing correction (FDR already provided)"
          ],
          [
            "No statistical tests required - this is a lookup/intersection analysis"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of protein sites with: ID, Gene, Modification type, FoldChange, FDR, Associated drug names",
          "Should include both phospho and acetyl sites meeting criteria",
          "List of protein sites (gene + site notation)",
          "Associated FDA-approved drugs for each site/protein",
          "Sample IDs where hyperactivation occurs",
          "Include drug names and interaction types",
          "The output should be a list of unique protein sites.",
          "The list should be sorted alphabetically."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of protein sites with: ID, Gene, Modification type, FoldChange, FDR, Associated drug names",
            "Should include both phospho and acetyl sites meeting criteria"
          ],
          [
            "List of protein sites (gene + site notation)",
            "Associated FDA-approved drugs for each site/protein",
            "Sample IDs where hyperactivation occurs",
            "Include drug names and interaction types"
          ],
          [
            "The output should be a list of unique protein sites.",
            "The list should be sorted alphabetically."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.53889010989011
  },
  "environment-easy-1": {
    "m_q": {
      "target_metric": {
        "value": "percentage of water samples that exceeded bacterial standards (Violation = 'yes') during the 2013 bathing season in Massachusetts beaches",
        "confidence": 0.3333333333333333,
        "votes": [
          "percentage of water samples that exceeded bacterial standards (Violation = 'yes') during the 2013 bathing season in Massachusetts beaches",
          "Percentage of water samples that exceeded bacterial standards (Violation='yes') to 3 decimal places",
          "Percentage of water samples from Massachusetts beaches in 2013 that exceeded bacterial standards, leading to temporary closures."
        ]
      },
      "filters": {
        "value": [
          "Year = 2013",
          "Violation = 'yes'",
          "State = Massachusetts (implied from data context)",
          "Year == 2013",
          "Community is in Massachusetts (all records appear to be from MA)",
          "Bathing season samples only (likely June-August based on sample dates)",
          "Location is in Massachusetts",
          "Violation is 'yes'"
        ],
        "confidence": 0.375,
        "votes": [
          [
            "Year = 2013",
            "Violation = 'yes'",
            "State = Massachusetts (implied from data context)"
          ],
          [
            "Year == 2013",
            "Community is in Massachusetts (all records appear to be from MA)",
            "Bathing season samples only (likely June-August based on sample dates)"
          ],
          [
            "Year = 2013",
            "Location is in Massachusetts",
            "Violation is 'yes'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of water samples collected in 2013?",
          "How many samples had Violation = 'yes'?",
          "What dates constitute the 'bathing season' in Massachusetts?",
          "What constitutes the bathing season timeframe?",
          "How many total water samples were collected?",
          "How many samples had Violation='yes'?",
          "What is the percentage calculation: (violations/total)*100?",
          "How many water samples were collected from Massachusetts beaches in 2013?",
          "How many water samples from Massachusetts beaches in 2013 exceeded bacterial standards (Violation = 'yes')?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total number of water samples collected in 2013?",
            "How many samples had Violation = 'yes'?",
            "What dates constitute the 'bathing season' in Massachusetts?"
          ],
          [
            "What constitutes the bathing season timeframe?",
            "How many total water samples were collected?",
            "How many samples had Violation='yes'?",
            "What is the percentage calculation: (violations/total)*100?"
          ],
          [
            "How many water samples were collected from Massachusetts beaches in 2013?",
            "How many water samples from Massachusetts beaches in 2013 exceeded bacterial standards (Violation = 'yes')?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2013.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2013.csv"
          ],
          [
            "water-body-testing-2013.csv"
          ],
          [
            "water-body-testing-2013.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "bacterial concentration units (varies by Organism)",
          "sample date": "datetime (YYYY-MM-DD HH:MM:SS)",
          "year": "year",
          "violation": "categorical (yes/no)"
        },
        "confidence": 0.49999999999999994,
        "votes": [
          {
            "Indicator Level": "bacterial concentration units (varies by Organism)"
          },
          {
            "Indicator Level": "CFU/100mL or MPN/100mL (bacterial count)",
            "Sample Date": "datetime (YYYY-MM-DD HH:MM:SS)",
            "Year": "year",
            "Violation": "categorical (yes/no)"
          },
          {
            "Indicator Level": "CFU/100mL"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Sample Date stored as object/string instead of datetime",
          "Violation column uses 'yes'/'no' strings instead of boolean"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sample Date stored as object/string instead of datetime",
            "Violation column uses 'yes'/'no' strings instead of boolean"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year column should only contain 2013",
          "Violation column should only contain 'yes' or 'no'",
          "Indicator Level should be non-negative",
          "Violation column must have values 'yes' or 'no'",
          "Year must equal 2013",
          "Sample Date must be valid datetime",
          "Exclude null/missing Violation values from calculation",
          "The 'Sample Date' column should be checked for valid date formats.",
          "The 'Violation' column should be checked for consistent values (e.g., 'yes', 'no')."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column should only contain 2013",
            "Violation column should only contain 'yes' or 'no'",
            "Indicator Level should be non-negative"
          ],
          [
            "Violation column must have values 'yes' or 'no'",
            "Year must equal 2013",
            "Sample Date must be valid datetime",
            "Exclude null/missing Violation values from calculation"
          ],
          [
            "The 'Sample Date' column should be checked for valid date formats.",
            "The 'Violation' column should be checked for consistent values (e.g., 'yes', 'no')."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to Massachusetts beaches (all data appears to be from Massachusetts based on county names)",
          "Determine bathing season date range (typically summer months)",
          "Filter to bathing season dates (need to confirm: typically Memorial Day to Labor Day or June-August)",
          "Include only valid samples with non-null Violation values",
          "Isolate samples from Massachusetts beaches (implicit based on the data source).",
          "Define 'exceeded bacterial standards' as 'Violation' == 'yes'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to Massachusetts beaches (all data appears to be from Massachusetts based on county names)",
            "Determine bathing season date range (typically summer months)"
          ],
          [
            "Filter to bathing season dates (need to confirm: typically Memorial Day to Labor Day or June-August)",
            "Include only valid samples with non-null Violation values"
          ],
          [
            "Isolate samples from Massachusetts beaches (implicit based on the data source).",
            "Define 'exceeded bacterial standards' as 'Violation' == 'yes'."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate samples",
          "Validate that Violation='yes' corresponds to high Indicator Level values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate samples",
            "Validate that Violation='yes' corresponds to high Indicator Level values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Percentage to 3 decimal places",
          "Include only 2013 data",
          "Consider only samples during bathing season",
          "Output as percentage with exactly 3 decimal places",
          "Format: XX.XXX%",
          "Round to 3 decimal places, not truncate",
          "Percentage should be formatted to 3 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage to 3 decimal places",
            "Include only 2013 data",
            "Consider only samples during bathing season"
          ],
          [
            "Output as percentage with exactly 3 decimal places",
            "Format: XX.XXX%",
            "Round to 3 decimal places, not truncate"
          ],
          [
            "Percentage should be formatted to 3 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5604166666666668
  },
  "environment-easy-2": {
    "m_q": {
      "target_metric": {
        "value": "bacterial exceedance rate (percentage of water samples with violations) for freshwater beaches by year, compared to the overall average freshwater beach exceedance rate across all years",
        "confidence": 0.3333333333333333,
        "votes": [
          "bacterial exceedance rate (percentage of water samples with violations) for freshwater beaches by year, compared to the overall average freshwater beach exceedance rate across all years",
          "Years with freshwater beach bacterial exceedance rate higher than the average freshwater beach exceedance rate (2002-2023)",
          "Years between 2002 and 2023 (inclusive) with a freshwater beach bacterial exceedance rate higher than the average freshwater beach exceedance rate (to 2 decimal places)"
        ]
      },
      "filters": {
        "value": [
          "Year between 2002 and 2023 inclusive",
          "Beach Type Description = 'Fresh'",
          "Only consider samples from freshwater beaches",
          "Beach Type Description == 'Fresh' OR 'Freshwater'",
          "Year >= 2002 AND Year <= 2023",
          "Year between 2002 and 2023 (inclusive)",
          "Beach Type Description is 'Fresh'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year between 2002 and 2023 inclusive",
            "Beach Type Description = 'Fresh'",
            "Only consider samples from freshwater beaches"
          ],
          [
            "Beach Type Description == 'Fresh' OR 'Freshwater'",
            "Year >= 2002 AND Year <= 2023"
          ],
          [
            "Year between 2002 and 2023 (inclusive)",
            "Beach Type Description is 'Fresh'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Year"
          ],
          [
            "Year"
          ],
          [
            "Year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of water samples collected from freshwater beaches each year?",
          "How many of those samples had violations (exceedances) each year?",
          "What is the exceedance rate (violations/total samples) for each year to 2 decimal places?",
          "What is the average exceedance rate across all years (2002-2023) for freshwater beaches?",
          "Which years have exceedance rates higher than the overall average?",
          "What constitutes a bacterial exceedance (Violation field)?",
          "How to identify freshwater beaches (Beach Type Description)?",
          "Calculate exceedance rate per year for freshwater beaches",
          "Calculate overall average exceedance rate across all years for freshwater beaches",
          "Which years exceed the average rate",
          "Round rates to 2 decimal places for comparison",
          "Calculate the average freshwater beach exceedance rate across all years between 2002 and 2023.",
          "For each year between 2002 and 2023, calculate the freshwater beach exceedance rate.",
          "Compare each year's freshwater beach exceedance rate to the overall average freshwater beach exceedance rate.",
          "Identify the years where the freshwater beach exceedance rate is higher than the overall average."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "What is the total number of water samples collected from freshwater beaches each year?",
            "How many of those samples had violations (exceedances) each year?",
            "What is the exceedance rate (violations/total samples) for each year to 2 decimal places?",
            "What is the average exceedance rate across all years (2002-2023) for freshwater beaches?",
            "Which years have exceedance rates higher than the overall average?"
          ],
          [
            "What constitutes a bacterial exceedance (Violation field)?",
            "How to identify freshwater beaches (Beach Type Description)?",
            "Calculate exceedance rate per year for freshwater beaches",
            "Calculate overall average exceedance rate across all years for freshwater beaches",
            "Which years exceed the average rate",
            "Round rates to 2 decimal places for comparison"
          ],
          [
            "Calculate the average freshwater beach exceedance rate across all years between 2002 and 2023.",
            "For each year between 2002 and 2023, calculate the freshwater beach exceedance rate.",
            "Compare each year's freshwater beach exceedance rate to the overall average freshwater beach exceedance rate.",
            "Identify the years where the freshwater beach exceedance rate is higher than the overall average."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2002.csv",
          "water-body-testing-2003.csv",
          "water-body-testing-2004.csv",
          "water-body-testing-2005.csv",
          "water-body-testing-2006.csv",
          "water-body-testing-2007.csv",
          "water-body-testing-2008.csv",
          "water-body-testing-2009.csv",
          "water-body-testing-2010.csv",
          "water-body-testing-2011.csv",
          "water-body-testing-2012.csv",
          "water-body-testing-2013.csv",
          "water-body-testing-2014.csv",
          "water-body-testing-2015.csv",
          "water-body-testing-2016.csv",
          "water-body-testing-2017.csv",
          "water-body-testing-2018.csv",
          "water-body-testing-2019.csv",
          "water-body-testing-2020.csv",
          "water-body-testing-2021.csv",
          "water-body-testing-2022.csv",
          "water-body-testing-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Indicator Level column has different dtypes: int64 in 2002, float64 in all other years",
          "Violation column values have inconsistent casing: 'no'/'yes' vs 'NO'/'YES' vs 'No'/'Yes'",
          "Violation field has inconsistent capitalization: 'yes'/'no' vs 'YES'/'NO' vs 'Yes'/'No'",
          "Indicator Level dtype varies: int64 vs float64",
          "Violation column has inconsistent values (yes/no vs YES/NO).",
          "Violation column has inconsistent capitalization (Violation vs violation)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level column has different dtypes: int64 in 2002, float64 in all other years",
            "Violation column values have inconsistent casing: 'no'/'yes' vs 'NO'/'YES' vs 'No'/'Yes'"
          ],
          [
            "Violation field has inconsistent capitalization: 'yes'/'no' vs 'YES'/'NO' vs 'Yes'/'No'",
            "Indicator Level dtype varies: int64 vs float64"
          ],
          [
            "Violation column has inconsistent values (yes/no vs YES/NO).",
            "Violation column has inconsistent capitalization (Violation vs violation)."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "bacterial count (units not specified, likely CFU/100mL)",
          "year": "year",
          "sample date": "date of sample collection",
          "violation": "boolean (yes/no)"
        },
        "confidence": 0.6666666666666667,
        "votes": [
          {
            "Indicator Level": "bacterial count (units not specified, likely CFU/100mL)",
            "Year": "calendar year",
            "Sample Date": "date of sample collection"
          },
          {
            "Year": "year",
            "Indicator Level": "MPN/100mL or CFU/100mL depending on organism",
            "Violation": "boolean (yes/no)"
          },
          {
            "Indicator Level": "bacterial level",
            "Year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Indicator Level values vary widely (from 1 to 800 in samples shown)",
          "2002 dataset is much smaller (650 rows) than other years (~14k-16k rows)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level values vary widely (from 1 to 800 in samples shown)",
            "2002 dataset is much smaller (650 rows) than other years (~14k-16k rows)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Sample sizes vary significantly by year, which may affect rate calculations",
          "Different organisms tested (E. Coli vs Enterococci) may have different exceedance thresholds",
          "Violation field capitalization varies across years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sample sizes vary significantly by year, which may affect rate calculations",
            "Different organisms tested (E. Coli vs Enterococci) may have different exceedance thresholds"
          ],
          [
            "Violation field capitalization varies across years"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year range: 2002-2023 inclusive",
          "Only freshwater beaches (Beach Type Description = 'Fresh')",
          "Violation column indicates exceedance (yes/YES/Yes = violation)",
          "Year must be between 2002 and 2023 inclusive",
          "Beach Type Description must indicate freshwater beach",
          "Violation field indicates exceedance when value is 'yes' (case-insensitive)",
          "Each sample row represents one water test",
          "Ensure 'Beach Type Description' is accurately filtered for 'Fresh' water beaches.",
          "Handle potential missing values in 'Indicator Level' appropriately (e.g., imputation or exclusion).",
          "Ensure consistent interpretation of 'Violation' column (e.g., 'yes'/'no' or 'YES'/'NO') across all files."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Year range: 2002-2023 inclusive",
            "Only freshwater beaches (Beach Type Description = 'Fresh')",
            "Violation column indicates exceedance (yes/YES/Yes = violation)"
          ],
          [
            "Year must be between 2002 and 2023 inclusive",
            "Beach Type Description must indicate freshwater beach",
            "Violation field indicates exceedance when value is 'yes' (case-insensitive)",
            "Each sample row represents one water test"
          ],
          [
            "Ensure 'Beach Type Description' is accurately filtered for 'Fresh' water beaches.",
            "Handle potential missing values in 'Indicator Level' appropriately (e.g., imputation or exclusion).",
            "Ensure consistent interpretation of 'Violation' column (e.g., 'yes'/'no' or 'YES'/'NO') across all files."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out marine beaches (Beach Type Description != 'Fresh')",
          "Filter years outside 2002-2023 range",
          "Standardize Violation values to consistent boolean representation",
          "Filter for Beach Type Description containing 'Fresh'",
          "Normalize Violation field to lowercase for consistent comparison",
          "Group samples by Year to calculate yearly exceedance rates",
          "Calculate exceedance rate as: (count of violations) / (total count of samples) per year",
          "Freshwater beach exceedance rate for each year: (Number of 'Violation' = 'yes' where 'Beach Type Description' = 'Fresh') / (Total number of samples where 'Beach Type Description' = 'Fresh') for each year.",
          "Overall average freshwater beach exceedance rate: Average of the freshwater beach exceedance rates across all years from 2002 to 2023."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out marine beaches (Beach Type Description != 'Fresh')",
            "Filter years outside 2002-2023 range",
            "Standardize Violation values to consistent boolean representation"
          ],
          [
            "Filter for Beach Type Description containing 'Fresh'",
            "Normalize Violation field to lowercase for consistent comparison",
            "Group samples by Year to calculate yearly exceedance rates",
            "Calculate exceedance rate as: (count of violations) / (total count of samples) per year"
          ],
          [
            "Freshwater beach exceedance rate for each year: (Number of 'Violation' = 'yes' where 'Beach Type Description' = 'Fresh') / (Total number of samples where 'Beach Type Description' = 'Fresh') for each year.",
            "Overall average freshwater beach exceedance rate: Average of the freshwater beach exceedance rates across all years from 2002 to 2023."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Calculate proportion test for each year's exceedance rate",
          "Compare annual rates to overall average using rate comparison",
          "Calculate average exceedance rate across all years for freshwater beaches",
          "Compare each year's exceedance rate to the overall average",
          "Round all rates to 2 decimal places before comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate proportion test for each year's exceedance rate",
            "Compare annual rates to overall average using rate comparison"
          ],
          [
            "Calculate average exceedance rate across all years for freshwater beaches",
            "Compare each year's exceedance rate to the overall average",
            "Round all rates to 2 decimal places before comparison"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Exceedance rates must be calculated to 2 decimal places",
          "Output should list years where exceedance rate > average",
          "Results should be sorted chronologically",
          "Return list of years where freshwater exceedance rate > average rate",
          "Rates should be calculated to 2 decimal places",
          "Only include years from 2002-2023 with freshwater beach data",
          "Output should be a list of years.",
          "The average exceedance rate should be rounded to two decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exceedance rates must be calculated to 2 decimal places",
            "Output should list years where exceedance rate > average",
            "Results should be sorted chronologically"
          ],
          [
            "Return list of years where freshwater exceedance rate > average rate",
            "Rates should be calculated to 2 decimal places",
            "Only include years from 2002-2023 with freshwater beach data"
          ],
          [
            "Output should be a list of years.",
            "The average exceedance rate should be rounded to two decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.638888888888889
  },
  "environment-easy-3": {
    "m_q": {
      "target_metric": {
        "value": "Count of beaches where the bacterial exceedance rate (percentage of samples with Violation='yes') in 2013 is greater than in 2012",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of beaches where the bacterial exceedance rate (percentage of samples with Violation='yes') in 2013 is greater than in 2012",
          "Count of beaches with higher bacterial exceedance rate in 2013 compared to 2012",
          "Count of beaches with a higher bacterial exceedance rate in 2013 compared to 2012, excluding beaches with no samples in 2012."
        ]
      },
      "filters": {
        "value": [
          "Year = 2012 or 2013",
          "Exclude beaches with no samples in 2012",
          "Consider only samples where Violation is recorded as 'yes' or 'no'",
          "Year = 2012 OR Year = 2013"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "Year = 2012 or 2013",
            "Exclude beaches with no samples in 2012",
            "Consider only samples where Violation is recorded as 'yes' or 'no'"
          ],
          [
            "Exclude beaches with no samples in 2012",
            "Year = 2012 OR Year = 2013"
          ],
          [
            "Exclude beaches with no samples in 2012"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Beach Name",
          "Year"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Beach Name",
            "Year"
          ],
          [
            "Beach Name",
            "Year"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the bacterial exceedance rate per beach per year?",
          "Which beaches have samples in both 2012 and 2013?",
          "For each beach with data in both years, compare 2013 exceedance rate to 2012 exceedance rate",
          "What is the bacterial exceedance rate for each beach in 2012?",
          "What is the bacterial exceedance rate for each beach in 2013?",
          "Which beaches had samples in 2012?",
          "For beaches with samples in both years, which had higher exceedance rate in 2013?",
          "What constitutes a bacterial exceedance (Violation = 'yes')?",
          "How to calculate exceedance rate (violations / total samples)?",
          "Calculate the bacterial exceedance rate for each beach in 2012.",
          "Calculate the bacterial exceedance rate for each beach in 2013.",
          "Identify beaches present in both 2012 and 2013.",
          "Compare the exceedance rates for each beach between 2012 and 2013.",
          "Count the number of beaches where the 2013 exceedance rate is higher than the 2012 exceedance rate."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the bacterial exceedance rate per beach per year?",
            "Which beaches have samples in both 2012 and 2013?",
            "For each beach with data in both years, compare 2013 exceedance rate to 2012 exceedance rate"
          ],
          [
            "What is the bacterial exceedance rate for each beach in 2012?",
            "What is the bacterial exceedance rate for each beach in 2013?",
            "Which beaches had samples in 2012?",
            "For beaches with samples in both years, which had higher exceedance rate in 2013?",
            "What constitutes a bacterial exceedance (Violation = 'yes')?",
            "How to calculate exceedance rate (violations / total samples)?"
          ],
          [
            "Calculate the bacterial exceedance rate for each beach in 2012.",
            "Calculate the bacterial exceedance rate for each beach in 2013.",
            "Identify beaches present in both 2012 and 2013.",
            "Compare the exceedance rates for each beach between 2012 and 2013.",
            "Count the number of beaches where the 2013 exceedance rate is higher than the 2012 exceedance rate."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2012.csv",
          "water-body-testing-2013.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv"
          ],
          [
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv"
          ],
          [
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "No apparent schema conflicts - both files have identical column names and data types"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No apparent schema conflicts - both files have identical column names and data types"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "bacterial concentration units (not specified in data)",
          "year": "calendar year",
          "sample date": "datetime",
          "violation": "categorical (yes/no)"
        },
        "confidence": 0.5833333333333333,
        "votes": [
          {
            "Indicator Level": "bacterial concentration units (not specified in data)"
          },
          {
            "Indicator Level": "count per 100mL or MPN per 100mL",
            "Year": "calendar year",
            "Sample Date": "datetime",
            "Violation": "categorical (yes/no)"
          },
          {
            "Indicator Level": "bacterial level",
            "Year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Indicator Level values vary widely (e.g., 2.0 to 564.0 in sample data)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level values vary widely (e.g., 2.0 to 564.0 in sample data)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Need to verify Beach Name consistency across years - same beach might have different naming conventions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need to verify Beach Name consistency across years - same beach might have different naming conventions"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Violation must be either 'yes' or 'no'",
          "Year must be 2012 or 2013",
          "Beach Name must not be null",
          "Each beach must have at least one sample in 2012",
          "Only include beaches that have at least one sample in 2012",
          "Beach Name must be present in both years for comparison",
          "Violation field must be 'yes' or 'no' to calculate exceedance rate",
          "Need to define what constitutes an 'exceedance'.  Is it based on the 'Violation' column being 'yes'?",
          "Need to handle cases where a beach has multiple samples on the same day. How should these be aggregated?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation must be either 'yes' or 'no'",
            "Year must be 2012 or 2013",
            "Beach Name must not be null",
            "Each beach must have at least one sample in 2012"
          ],
          [
            "Only include beaches that have at least one sample in 2012",
            "Beach Name must be present in both years for comparison",
            "Violation field must be 'yes' or 'no' to calculate exceedance rate"
          ],
          [
            "Need to define what constitutes an 'exceedance'.  Is it based on the 'Violation' column being 'yes'?",
            "Need to handle cases where a beach has multiple samples on the same day. How should these be aggregated?"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to beaches present in both 2012 and 2013 data",
          "Calculate exceedance rate = (count of Violation='yes') / (total samples) per beach per year",
          "Beach must have count(samples where Year=2012) > 0",
          "Beach must have count(samples where Year=2013) > 0 for comparison",
          "Beach present in both 2012 and 2013"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to beaches present in both 2012 and 2013 data",
            "Calculate exceedance rate = (count of Violation='yes') / (total samples) per beach per year"
          ],
          [
            "Beach must have count(samples where Year=2012) > 0",
            "Beach must have count(samples where Year=2013) > 0 for comparison"
          ],
          [
            "Beach present in both 2012 and 2013"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Compare proportions (exceedance rates) between 2012 and 2013 for each beach",
          "Calculate exceedance_rate_2012 = count(Violation='yes' AND Year=2012) / count(samples in 2012) per beach",
          "Calculate exceedance_rate_2013 = count(Violation='yes' AND Year=2013) / count(samples in 2013) per beach",
          "Compare exceedance_rate_2013 > exceedance_rate_2012 for each beach"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Compare proportions (exceedance rates) between 2012 and 2013 for each beach"
          ],
          [
            "Calculate exceedance_rate_2012 = count(Violation='yes' AND Year=2012) / count(samples in 2012) per beach",
            "Calculate exceedance_rate_2013 = count(Violation='yes' AND Year=2013) / count(samples in 2013) per beach",
            "Compare exceedance_rate_2013 > exceedance_rate_2012 for each beach"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count of beaches",
          "Must exclude beaches with zero samples in 2012",
          "Output is a single integer count",
          "Count beaches where exceedance_rate_2013 > exceedance_rate_2012",
          "Exclude beaches with zero samples in 2012"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count of beaches",
            "Must exclude beaches with zero samples in 2012"
          ],
          [
            "Output is a single integer count",
            "Count beaches where exceedance_rate_2013 > exceedance_rate_2012",
            "Exclude beaches with zero samples in 2012"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6375000000000001
  },
  "environment-easy-4": {
    "m_q": {
      "target_metric": {
        "value": "percentage of time (as integer) that water quality at Quincy's Wollaston Beach met swimming standards from 2019 to 2023 inclusive",
        "confidence": 0.3333333333333333,
        "votes": [
          "percentage of time (as integer) that water quality at Quincy's Wollaston Beach met swimming standards from 2019 to 2023 inclusive",
          "Percentage (to integer) of water quality samples that met swimming standards (Violation = 'NO' or 'No')",
          "Percentage of time (days) between 2019 and 2023 (inclusive) that water quality at Quincy's Wollaston Beach met swimming standards."
        ]
      },
      "filters": {
        "value": [
          "Community = 'Quincy'",
          "Beach Name = 'Wollaston Beach'",
          "Year between 2019 and 2023 inclusive",
          "Year IN (2019, 2020, 2021, 2022, 2023)",
          "Year between 2019 and 2023 (inclusive)",
          "Beach Name is 'Wollaston Beach'",
          "Community is 'Quincy'"
        ],
        "confidence": 0.42857142857142855,
        "votes": [
          [
            "Community = 'Quincy'",
            "Beach Name = 'Wollaston Beach'",
            "Year between 2019 and 2023 inclusive"
          ],
          [
            "Community = 'Quincy'",
            "Beach Name = 'Wollaston Beach'",
            "Year IN (2019, 2020, 2021, 2022, 2023)"
          ],
          [
            "Year between 2019 and 2023 (inclusive)",
            "Beach Name is 'Wollaston Beach'",
            "Community is 'Quincy'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year",
          "Sample Date"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year",
            "Sample Date"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What constitutes 'meeting swimming standards'? (Violation = 'NO' or 'No')",
          "How to handle multiple samples on same date?",
          "How to calculate percentage of time from discrete samples?",
          "Should we consider all organisms or just Enterococci?",
          "How many total samples were taken at Wollaston Beach in Quincy from 2019-2023?",
          "How many samples had Violation = 'NO' or 'No' (case-insensitive)?",
          "What is the percentage and how to round to integer?",
          "For each year, how many days were samples taken at Wollaston Beach in Quincy?",
          "For each year, how many days had a 'Violation' value of 'NO' at Wollaston Beach in Quincy?",
          "What is the total number of days samples were taken at Wollaston Beach in Quincy between 2019 and 2023?",
          "What is the total number of days between 2019 and 2023 that Wollaston Beach in Quincy met swimming standards (Violation = 'NO')?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What constitutes 'meeting swimming standards'? (Violation = 'NO' or 'No')",
            "How to handle multiple samples on same date?",
            "How to calculate percentage of time from discrete samples?",
            "Should we consider all organisms or just Enterococci?"
          ],
          [
            "How many total samples were taken at Wollaston Beach in Quincy from 2019-2023?",
            "How many samples had Violation = 'NO' or 'No' (case-insensitive)?",
            "What is the percentage and how to round to integer?"
          ],
          [
            "For each year, how many days were samples taken at Wollaston Beach in Quincy?",
            "For each year, how many days had a 'Violation' value of 'NO' at Wollaston Beach in Quincy?",
            "What is the total number of days samples were taken at Wollaston Beach in Quincy between 2019 and 2023?",
            "What is the total number of days between 2019 and 2023 that Wollaston Beach in Quincy met swimming standards (Violation = 'NO')?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2019.csv",
          "water-body-testing-2020.csv",
          "water-body-testing-2021.csv",
          "water-body-testing-2022.csv",
          "water-body-testing-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Violation column has inconsistent capitalization: 'NO'/'YES' vs 'No'/'Yes'",
          "Sample Date format appears consistent but should verify parsing",
          "Violation column has inconsistent casing: 'YES'/'NO' in 2019 and 2021, 'Yes'/'No' in 2020, 2022, and 2023",
          "In water-body-testing-2020.csv, the 'Violation' column has values 'No' instead of 'NO'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation column has inconsistent capitalization: 'NO'/'YES' vs 'No'/'Yes'",
            "Sample Date format appears consistent but should verify parsing"
          ],
          [
            "Violation column has inconsistent casing: 'YES'/'NO' in 2019 and 2021, 'Yes'/'No' in 2020, 2022, and 2023"
          ],
          [
            "In water-body-testing-2020.csv, the 'Violation' column has values 'No' instead of 'NO'."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "CFU/100mL (colony forming units per 100 milliliters)",
          "year": "calendar year",
          "sample date": "timestamp",
          "community code": "integer code",
          "county code": "integer code"
        },
        "confidence": 0.5333333333333333,
        "votes": [
          {
            "Indicator Level": "CFU/100mL (colony forming units per 100 milliliters)"
          },
          {
            "Indicator Level": "colony forming units or MPN per 100mL",
            "Year": "calendar year",
            "Sample Date": "timestamp"
          },
          {
            "Indicator Level": "level of organism",
            "Year": "year",
            "Community Code": "integer code",
            "County Code": "integer code"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Indicator Level values range from 1.0 to 400.0 in samples, but swimming standards threshold not specified"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level values range from 1.0 to 400.0 in samples, but swimming standards threshold not specified"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Violation column inconsistency across years: 2019 uses 'NO'/'YES', 2020-2023 use 'No'/'Yes'",
          "Violation values use different casing across years (uppercase vs title case)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation column inconsistency across years: 2019 uses 'NO'/'YES', 2020-2023 use 'No'/'Yes'"
          ],
          [
            "Violation values use different casing across years (uppercase vs title case)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Violation must be 'NO', 'YES', 'No', or 'Yes'",
          "Year must be between 2019 and 2023",
          "Indicator Level should be positive numeric",
          "Sample Date must be valid date",
          "Community must equal 'Quincy'",
          "Beach Name must equal 'Wollaston Beach'",
          "Year must be between 2019 and 2023 inclusive",
          "Violation column must be normalized for case-insensitive comparison",
          "The 'Year' column should contain only valid years.",
          "The 'Beach Name' column should contain only valid beach names.",
          "The 'Violation' column should contain only 'YES' or 'NO' (case-insensitive)."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Violation must be 'NO', 'YES', 'No', or 'Yes'",
            "Year must be between 2019 and 2023",
            "Indicator Level should be positive numeric",
            "Sample Date must be valid date"
          ],
          [
            "Community must equal 'Quincy'",
            "Beach Name must equal 'Wollaston Beach'",
            "Year must be between 2019 and 2023 inclusive",
            "Violation column must be normalized for case-insensitive comparison"
          ],
          [
            "The 'Year' column should contain only valid years.",
            "The 'Beach Name' column should contain only valid beach names.",
            "The 'Violation' column should contain only 'YES' or 'NO' (case-insensitive)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to Quincy community",
          "Filter to Wollaston Beach",
          "Filter to years 2019-2023",
          "Count records where UPPER(Violation) = 'NO' (meeting standards)",
          "Count total records matching location and time filters",
          "A day meets swimming standards if 'Violation' is 'NO' (case-insensitive)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to Quincy community",
            "Filter to Wollaston Beach",
            "Filter to years 2019-2023"
          ],
          [
            "Count records where UPPER(Violation) = 'NO' (meeting standards)",
            "Count total records matching location and time filters"
          ],
          [
            "A day meets swimming standards if 'Violation' is 'NO' (case-insensitive)."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Count samples meeting standards vs total samples",
          "Check for duplicate samples on same date",
          "Verify temporal coverage across years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count samples meeting standards vs total samples",
            "Check for duplicate samples on same date",
            "Verify temporal coverage across years"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Percentage as integer (rounded)",
          "Time period clearly specified: 2019-2023",
          "Return percentage as integer (rounded)",
          "Formula: ROUND(100 * count_no_violation / total_count)",
          "Output should be a single integer representing the percentage.",
          "Percentage should be rounded to the nearest integer."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage as integer (rounded)",
            "Time period clearly specified: 2019-2023"
          ],
          [
            "Return percentage as integer (rounded)",
            "Formula: ROUND(100 * count_no_violation / total_count)"
          ],
          [
            "Output should be a single integer representing the percentage.",
            "Percentage should be rounded to the nearest integer."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6036507936507938
  },
  "environment-easy-5": {
    "m_q": {
      "target_metric": {
        "value": "Total rainfall in inches for June, July, and August combined",
        "confidence": 0.3333333333333333,
        "votes": [
          "Total rainfall in inches for June, July, and August combined",
          "Total rainfall (sum of precipitation) for June, July, and August 2020",
          "Total rainfall for June, July, and August in 2020 for each region and identify the region with the most rainfall."
        ]
      },
      "filters": {
        "value": [
          "Year = 2020",
          "Months in ['Jun', 'Jul', 'Aug']",
          "Year == 2020",
          "Months in [Jun, Jul, Aug]",
          "Regions in ['Boston', 'Chatham', 'Amherst', 'Ashburnham']"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [
            "Year = 2020",
            "Months in ['Jun', 'Jul', 'Aug']"
          ],
          [
            "Year == 2020",
            "Months in [Jun, Jul, Aug]"
          ],
          [
            "Year = 2020",
            "Months in ['Jun', 'Jul', 'Aug']",
            "Regions in ['Boston', 'Chatham', 'Amherst', 'Ashburnham']"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Region"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Region"
          ],
          [
            "Region"
          ],
          [
            "Region"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total rainfall for each region in June 2020?",
          "What is the total rainfall for each region in July 2020?",
          "What is the total rainfall for each region in August 2020?",
          "Which region has the highest sum of June, July, and August rainfall in 2020?",
          "What is the total rainfall in Boston for June, July, August 2020?",
          "What is the total rainfall in Chatham for June, July, August 2020?",
          "What is the total rainfall in Amherst for June, July, August 2020?",
          "What is the total rainfall in Ashburnham for June, July, August 2020?",
          "Which region has the maximum total rainfall among these four regions?",
          "Calculate the sum of 'Jun', 'Jul', and 'Aug' columns for each region in 2020.",
          "Find the maximum rainfall among all regions."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the total rainfall for each region in June 2020?",
            "What is the total rainfall for each region in July 2020?",
            "What is the total rainfall for each region in August 2020?",
            "Which region has the highest sum of June, July, and August rainfall in 2020?"
          ],
          [
            "What is the total rainfall in Boston for June, July, August 2020?",
            "What is the total rainfall in Chatham for June, July, August 2020?",
            "What is the total rainfall in Amherst for June, July, August 2020?",
            "What is the total rainfall in Ashburnham for June, July, August 2020?",
            "Which region has the maximum total rainfall among these four regions?"
          ],
          [
            "Calculate the sum of 'Jun', 'Jul', and 'Aug' columns for each region in 2020.",
            "Find the maximum rainfall among all regions."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "monthly_precipitations_boston.csv",
          "monthly_precipitations_chatham.csv",
          "monthly_precipitations_amherst.csv",
          "monthly_precipitations_ashburnham.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv"
          ],
          [
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv"
          ],
          [
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All files have identical column names and data types, but they represent different regions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All files have identical column names and data types, but they represent different regions"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "jan": "inches",
          "feb": "inches",
          "mar": "inches",
          "apr": "inches",
          "may": "inches",
          "jun": "inches",
          "jul": "inches",
          "aug": "inches",
          "sep": "inches",
          "oct": "inches",
          "nov": "inches",
          "dec": "inches",
          "annual": "inches"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches"
          },
          {
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches"
          },
          {
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Missing values represented by empty cells in sample data",
          "Annual column may have missing values when monthly data is incomplete"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Missing values represented by empty cells in sample data",
            "Annual column may have missing values when monthly data is incomplete"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Different regions may have different measurement methodologies or station locations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different regions may have different measurement methodologies or station locations"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 14.0,
        "confidence": 1.0,
        "votes": [
          14.0,
          14.0,
          14.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be between 2000 and 2026",
          "Monthly precipitation values should be non-negative",
          "Annual column should approximately equal sum of monthly columns when all months present",
          "Year 2020 must exist in all four regional datasets",
          "Jun, Jul, Aug columns for 2020 must contain non-null numeric values",
          "Precipitation values must be non-negative",
          "Year must be an integer.",
          "Rainfall values must be non-negative."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be between 2000 and 2026",
            "Monthly precipitation values should be non-negative",
            "Annual column should approximately equal sum of monthly columns when all months present"
          ],
          [
            "Year 2020 must exist in all four regional datasets",
            "Jun, Jul, Aug columns for 2020 must contain non-null numeric values",
            "Precipitation values must be non-negative"
          ],
          [
            "Year must be an integer.",
            "Rainfall values must be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to Year = 2020",
          "Select only Jun, Jul, Aug columns",
          "Filter to Year == 2020 for each dataset",
          "Extract Jun, Jul, Aug columns from 2020 row",
          "Sum the three months for each region",
          "Handle missing values if present in target months",
          "Year = 2020"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to Year = 2020",
            "Select only Jun, Jul, Aug columns"
          ],
          [
            "Filter to Year == 2020 for each dataset",
            "Extract Jun, Jul, Aug columns from 2020 row",
            "Sum the three months for each region",
            "Handle missing values if present in target months"
          ],
          [
            "Year = 2020"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in 2020 data",
          "Verify data consistency across months"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in 2020 data",
            "Verify data consistency across months"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Region name with highest rainfall",
          "Numeric value of total rainfall for that region",
          "Return the name of the region with maximum summer 2020 rainfall",
          "Optionally include the total rainfall values for all regions for comparison",
          "Report the region with the highest total rainfall for June, July, and August 2020."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Region name with highest rainfall",
            "Numeric value of total rainfall for that region"
          ],
          [
            "Return the name of the region with maximum summer 2020 rainfall",
            "Optionally include the total rainfall values for all regions for comparison"
          ],
          [
            "Report the region with the highest total rainfall for June, July, and August 2020."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6733333333333335
  },
  "environment-easy-6": {
    "m_q": {
      "target_metric": {
        "value": "average historical exceedance rate for marine beaches from 2002 to 2023 inclusive, rounded to 2 decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "average historical exceedance rate for marine beaches from 2002 to 2023 inclusive, rounded to 2 decimal places",
          "average historical exceedance rate for marine beaches from 2002 to 2023 (inclusive), rounded to 2 decimal places",
          "Average exceedance rate of marine beaches from 2002 to 2023 (inclusive), rounded to 2 decimal places"
        ]
      },
      "filters": {
        "value": [
          "Beach Type Description = 'Marine'",
          "Year between 2002 and 2023 inclusive",
          "Beach Type Description == 'Marine'",
          "Year >= 2002 AND Year <= 2023",
          "Year between 2002 and 2023 (inclusive)",
          "Beach Type Description is 'Marine'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Beach Type Description = 'Marine'",
            "Year between 2002 and 2023 inclusive"
          ],
          [
            "Beach Type Description == 'Marine'",
            "Year >= 2002 AND Year <= 2023"
          ],
          [
            "Year between 2002 and 2023 (inclusive)",
            "Beach Type Description is 'Marine'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year",
          "Beach Name"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Year"
          ],
          [
            "Beach Name",
            "Year"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of samples for marine beaches each year?",
          "What is the number of violation samples (Violation = 'Yes'/'YES'/'yes') for marine beaches each year?",
          "What is the exceedance rate (violations/total samples) for marine beaches each year?",
          "What is the average of these annual exceedance rates across all years 2002-2023?",
          "What constitutes an exceedance event (Violation == 'Yes' or 'yes' or 'YES')?",
          "How is the exceedance rate calculated per beach per year (number of violations / total samples)?",
          "How should beaches be aggregated across all years to compute historical average?",
          "Should the average be computed as mean of yearly rates or total violations / total samples?",
          "For each year from 2002 to 2023, calculate the exceedance rate for marine beaches.",
          "Calculate the total number of samples for marine beaches for each year.",
          "Calculate the number of violations for marine beaches for each year.",
          "Divide the number of violations by the total number of samples for each year to get the exceedance rate.",
          "Calculate the average of the yearly exceedance rates."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total number of samples for marine beaches each year?",
            "What is the number of violation samples (Violation = 'Yes'/'YES'/'yes') for marine beaches each year?",
            "What is the exceedance rate (violations/total samples) for marine beaches each year?",
            "What is the average of these annual exceedance rates across all years 2002-2023?"
          ],
          [
            "What constitutes an exceedance event (Violation == 'Yes' or 'yes' or 'YES')?",
            "How is the exceedance rate calculated per beach per year (number of violations / total samples)?",
            "How should beaches be aggregated across all years to compute historical average?",
            "Should the average be computed as mean of yearly rates or total violations / total samples?"
          ],
          [
            "For each year from 2002 to 2023, calculate the exceedance rate for marine beaches.",
            "Calculate the total number of samples for marine beaches for each year.",
            "Calculate the number of violations for marine beaches for each year.",
            "Divide the number of violations by the total number of samples for each year to get the exceedance rate.",
            "Calculate the average of the yearly exceedance rates."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2022.csv",
          "water-body-testing-2021.csv",
          "water-body-testing-2020.csv",
          "water-body-testing-2019.csv",
          "water-body-testing-2018.csv",
          "water-body-testing-2017.csv",
          "water-body-testing-2016.csv",
          "water-body-testing-2015.csv",
          "water-body-testing-2014.csv",
          "water-body-testing-2013.csv",
          "water-body-testing-2012.csv",
          "water-body-testing-2011.csv",
          "water-body-testing-2010.csv",
          "water-body-testing-2009.csv",
          "water-body-testing-2008.csv",
          "water-body-testing-2007.csv",
          "water-body-testing-2006.csv",
          "water-body-testing-2005.csv",
          "water-body-testing-2004.csv",
          "water-body-testing-2003.csv",
          "water-body-testing-2002.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2022.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2002.csv"
          ],
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv"
          ],
          [
            "water-body-testing-2022.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2002.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Violation column has inconsistent capitalization: 'No'/'NO'/'no' and 'Yes'/'YES'/'yes'",
          "Indicator Level column has different dtypes: float64 in most files, int64 in 2002.csv",
          "Violation column has inconsistent case: 'Yes'/'No', 'yes'/'no', 'YES'/'NO'",
          "Indicator Level dtype varies: float64 in most files, int64 in 2002",
          "Violation column has inconsistent casing (YES/NO, Yes/No, yes/no)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation column has inconsistent capitalization: 'No'/'NO'/'no' and 'Yes'/'YES'/'yes'",
            "Indicator Level column has different dtypes: float64 in most files, int64 in 2002.csv"
          ],
          [
            "Violation column has inconsistent case: 'Yes'/'No', 'yes'/'no', 'YES'/'NO'",
            "Indicator Level dtype varies: float64 in most files, int64 in 2002"
          ],
          [
            "Violation column has inconsistent casing (YES/NO, Yes/No, yes/no)"
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "CFU/100mL (colony forming units per 100 milliliters)",
          "year": "year",
          "sample date": "datetime"
        },
        "confidence": 0.7777777777777777,
        "votes": [
          {
            "Indicator Level": "CFU/100mL (colony forming units per 100 milliliters)"
          },
          {
            "Indicator Level": "CFU or MPN per 100mL",
            "Year": "year",
            "Sample Date": "datetime"
          },
          {
            "Indicator Level": "level of indicator organism",
            "Year": "year",
            "Sample Date": "date"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Indicator Level values vary widely from 1.0 to 800.0 across samples"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level values vary widely from 1.0 to 800.0 across samples"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Violation determination threshold may vary by organism and beach type, but not explicitly provided in data",
          "Violation field uses different case conventions across years (Yes/yes/YES, No/no/NO)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation determination threshold may vary by organism and beach type, but not explicitly provided in data"
          ],
          [
            "Violation field uses different case conventions across years (Yes/yes/YES, No/no/NO)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values must be between 2002 and 2023 inclusive",
          "Beach Type Description must be 'Marine'",
          "Violation values must be standardized to consistent case for analysis",
          "Only include records where Beach Type Description == 'Marine'",
          "Only include years from 2002 to 2023 inclusive",
          "Handle case-insensitive matching for Violation values ('Yes', 'yes', 'YES' all indicate violation)",
          "Result must be rounded to exactly 2 decimal places",
          "Year must be an integer.",
          "Indicator Level must be a number.",
          "Violation must be a categorical variable (Yes/No)."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Year values must be between 2002 and 2023 inclusive",
            "Beach Type Description must be 'Marine'",
            "Violation values must be standardized to consistent case for analysis"
          ],
          [
            "Only include records where Beach Type Description == 'Marine'",
            "Only include years from 2002 to 2023 inclusive",
            "Handle case-insensitive matching for Violation values ('Yes', 'yes', 'YES' all indicate violation)",
            "Result must be rounded to exactly 2 decimal places"
          ],
          [
            "Year must be an integer.",
            "Indicator Level must be a number.",
            "Violation must be a categorical variable (Yes/No)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Standardize Violation column to consistent case (e.g., lowercase)",
          "Filter out non-marine beach types",
          "Filter to years 2002-2023",
          "Filter out any records with null or missing Beach Type Description",
          "Normalize Violation values to lowercase or uppercase for consistent comparison",
          "Exceedance is defined as Violation = 'Yes' (case-insensitive)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Standardize Violation column to consistent case (e.g., lowercase)",
            "Filter out non-marine beach types",
            "Filter to years 2002-2023"
          ],
          [
            "Filter out any records with null or missing Beach Type Description",
            "Normalize Violation values to lowercase or uppercase for consistent comparison"
          ],
          [
            "Exceedance is defined as Violation = 'Yes' (case-insensitive)"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Calculate annual exceedance rates: count(Violation='yes')/count(all samples) per year",
          "Calculate average of annual rates across 22 years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate annual exceedance rates: count(Violation='yes')/count(all samples) per year",
            "Calculate average of annual rates across 22 years"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Final result must be a percentage or decimal rounded to 2 decimal places",
          "Return a single numeric value rounded to 2 decimal places",
          "Exceedance rate should be expressed as percentage (0-100) or proportion (0-1) - clarification needed",
          "Handle case where 2023 data may not exist in provided files",
          "Output must be a single number rounded to 2 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Final result must be a percentage or decimal rounded to 2 decimal places"
          ],
          [
            "Return a single numeric value rounded to 2 decimal places",
            "Exceedance rate should be expressed as percentage (0-100) or proportion (0-1) - clarification needed",
            "Handle case where 2023 data may not exist in provided files"
          ],
          [
            "Output must be a single number rounded to 2 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6305555555555556
  },
  "environment-hard-10": {
    "m_q": {
      "target_metric": {
        "value": "Pearson correlation coefficient (to 3 decimal places) between rainfall in the past 3 days and Enterococcus levels",
        "confidence": 0.3333333333333333,
        "votes": [
          "Pearson correlation coefficient (to 3 decimal places) between rainfall in the past 3 days and Enterococcus levels",
          "Pearson correlation coefficient between rainfall in the past 3 days and Enterococcus levels, rounded to 3 decimal places",
          "Pearson correlation between rainfall in the past 3 days and adjusted Enterococcus levels for Boston Harbor beaches in communities with >90% EJ populations"
        ]
      },
      "filters": {
        "value": [
          "Boston Harbor beaches only",
          "Communities with >90% environmental justice populations",
          "Organism = Enterococci",
          "Beach Type Description = Marine",
          "Year = 2023",
          "Handle Indicator Level tags: '< x' as x/2, '> x' as x",
          "Communities with more than 90% environmental justice (EJ) populations",
          "Beach Type Description = 'Marine' (Boston Harbor context)",
          "Municipality has >90% EJ population",
          "Beach is a Boston Harbor beach"
        ],
        "confidence": 0.3666666666666667,
        "votes": [
          [
            "Boston Harbor beaches only",
            "Communities with >90% environmental justice populations",
            "Organism = Enterococci",
            "Beach Type Description = Marine",
            "Year = 2023",
            "Handle Indicator Level tags: '< x' as x/2, '> x' as x"
          ],
          [
            "Boston Harbor beaches only",
            "Communities with more than 90% environmental justice (EJ) populations",
            "Beach Type Description = 'Marine' (Boston Harbor context)"
          ],
          [
            "Municipality has >90% EJ population",
            "Beach is a Boston Harbor beach"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Community",
          "Beach Name",
          "Sample Date"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Community",
            "Beach Name",
            "Sample Date"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which communities have >90% EJ populations?",
          "Which beaches are in Boston Harbor?",
          "How to match rainfall data to beach testing dates?",
          "How to handle multiple Enterococcus measurements per beach per day?",
          "Which communities have more than 90% EJ populations (Percent of population in EJ BGs > 90)?",
          "Which beaches in water-body-testing-2023.csv are located in Boston Harbor?",
          "Which beaches are in communities meeting the >90% EJ threshold?",
          "How to map beaches to rainfall data (wollaston_beach_datasheet.csv appears specific to Wollaston Beach)?",
          "How to handle Enterococcus values with '<' and '>' tags (e.g., '<10' becomes 5, '>100' becomes 100)?",
          "What is the 3-day rainfall for each beach sample date?",
          "What statistical method to compute Pearson correlation?",
          "Identify communities with >90% EJ populations.",
          "Identify Boston Harbor beaches and their corresponding communities.",
          "For each beach, adjust Enterococcus levels: if < x, set to x/2; if > x, set to x.",
          "Calculate the Pearson correlation between 3-day rainfall and adjusted Enterococcus levels."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Which communities have >90% EJ populations?",
            "Which beaches are in Boston Harbor?",
            "How to match rainfall data to beach testing dates?",
            "How to handle multiple Enterococcus measurements per beach per day?"
          ],
          [
            "Which communities have more than 90% EJ populations (Percent of population in EJ BGs > 90)?",
            "Which beaches in water-body-testing-2023.csv are located in Boston Harbor?",
            "Which beaches are in communities meeting the >90% EJ threshold?",
            "How to map beaches to rainfall data (wollaston_beach_datasheet.csv appears specific to Wollaston Beach)?",
            "How to handle Enterococcus values with '<' and '>' tags (e.g., '<10' becomes 5, '>100' becomes 100)?",
            "What is the 3-day rainfall for each beach sample date?",
            "What statistical method to compute Pearson correlation?"
          ],
          [
            "Identify communities with >90% EJ populations.",
            "Identify Boston Harbor beaches and their corresponding communities.",
            "For each beach, adjust Enterococcus levels: if < x, set to x/2; if > x, set to x.",
            "Calculate the Pearson correlation between 3-day rainfall and adjusted Enterococcus levels."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "environmental-justice-populations.csv",
          "water-body-testing-2023.csv",
          "wollaston_beach_datasheet.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "environmental-justice-populations.csv",
            "water-body-testing-2023.csv",
            "wollaston_beach_datasheet.csv"
          ],
          [
            "environmental-justice-populations.csv",
            "water-body-testing-2023.csv",
            "wollaston_beach_datasheet.csv"
          ],
          [
            "environmental-justice-populations.csv",
            "water-body-testing-2023.csv",
            "wollaston_beach_datasheet.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Date format mismatch: '2023-07-10 00:00:00' vs 'September 1, 2024'",
          "Municipality vs Community name matching may have inconsistencies",
          "Multiple Enterococcus columns in wollaston_beach_datasheet.csv (Enterococcus, Enterococcus.1, Enterococcus.2, Enterococcus.3)",
          "Date format differences: water-body-testing-2023.csv uses 'YYYY-MM-DD HH:MM:SS', wollaston_beach_datasheet.csv uses 'Month DD, YYYY'",
          "Organism vs Enterococcus naming: water-body-testing-2023.csv has 'Organism' column with value 'Enterococci', wollaston_beach_datasheet.csv has multiple 'Enterococcus' columns with location tags",
          "Indicator Level in water-body-testing-2023.csv vs Enterococcus columns in wollaston_beach_datasheet.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date format mismatch: '2023-07-10 00:00:00' vs 'September 1, 2024'",
            "Municipality vs Community name matching may have inconsistencies",
            "Multiple Enterococcus columns in wollaston_beach_datasheet.csv (Enterococcus, Enterococcus.1, Enterococcus.2, Enterococcus.3)"
          ],
          [
            "Date format differences: water-body-testing-2023.csv uses 'YYYY-MM-DD HH:MM:SS', wollaston_beach_datasheet.csv uses 'Month DD, YYYY'",
            "Organism vs Enterococcus naming: water-body-testing-2023.csv has 'Organism' column with value 'Enterococci', wollaston_beach_datasheet.csv has multiple 'Enterococcus' columns with location tags",
            "Indicator Level in water-body-testing-2023.csv vs Enterococcus columns in wollaston_beach_datasheet.csv"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "CFU/100mL",
          "enterococcus": "CFU/100mL",
          "1-day rain": "inches",
          "2-day rain": "inches",
          "3-day rain": "inches",
          "percent of population in ej bgs": "percentage",
          "enterococcus.1": "CFU/100mL",
          "enterococcus.2": "CFU/100mL",
          "enterococcus.3": "CFU/100mL"
        },
        "confidence": 0.7037037037037036,
        "votes": [
          {
            "Indicator Level": "CFU/100mL",
            "Enterococcus": "CFU/100mL",
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches",
            "Percent of population in EJ BGs": "percentage"
          },
          {
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches",
            "Indicator Level": "MPN/100mL or CFU/100mL",
            "Enterococcus": "MPN/100mL or CFU/100mL",
            "Percent of population in EJ BGs": "percentage"
          },
          {
            "Percent of population in EJ BGs": "%",
            "3-Day Rain": "inches",
            "Indicator Level": "CFU/100mL",
            "Enterococcus": "CFU/100mL",
            "Enterococcus.1": "CFU/100mL",
            "Enterococcus.2": "CFU/100mL",
            "Enterococcus.3": "CFU/100mL"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Rainfall data appears to be in inches but needs verification",
          "Enterococcus levels may have different measurement scales across datasets",
          "EJ percentages are already normalized to 0-100 scale",
          "Enterococcus levels with '<' tags need conversion: '<x' becomes x/2",
          "Enterococcus levels with '>' tags need conversion: '>x' becomes x",
          "EJ population threshold is 90% (>90 means strictly greater than 90)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rainfall data appears to be in inches but needs verification",
            "Enterococcus levels may have different measurement scales across datasets",
            "EJ percentages are already normalized to 0-100 scale"
          ],
          [
            "Enterococcus levels with '<' tags need conversion: '<x' becomes x/2",
            "Enterococcus levels with '>' tags need conversion: '>x' becomes x",
            "EJ population threshold is 90% (>90 means strictly greater than 90)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "wollaston_beach_datasheet.csv has 2024 dates while water-body-testing-2023.csv has 2023 dates",
          "Different measurement protocols for Enterococcus across datasets",
          "Potential duplicate beach names across different communities",
          "wollaston_beach_datasheet.csv contains only Wollaston Beach data (appears to be Quincy), not all Boston Harbor beaches",
          "water-body-testing-2023.csv contains statewide data, need to identify Boston Harbor beaches specifically",
          "Boston Harbor geography definition needed to identify relevant communities and beaches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "wollaston_beach_datasheet.csv has 2024 dates while water-body-testing-2023.csv has 2023 dates",
            "Different measurement protocols for Enterococcus across datasets",
            "Potential duplicate beach names across different communities"
          ],
          [
            "wollaston_beach_datasheet.csv contains only Wollaston Beach data (appears to be Quincy), not all Boston Harbor beaches",
            "water-body-testing-2023.csv contains statewide data, need to identify Boston Harbor beaches specifically",
            "Boston Harbor geography definition needed to identify relevant communities and beaches"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.8181818181818181,
        "votes": [
          10.0,
          11.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "EJ population percentage > 90",
          "Beach location must be in Boston Harbor",
          "Only Enterococci organism measurements",
          "Only marine beach types",
          "Year must be 2023",
          "Rainfall data must be available for corresponding dates",
          "Filter environmental-justice-populations.csv where 'Percent of population in EJ BGs' > 90",
          "Identify Boston Harbor beaches (County Description = 'Suffolk' or 'Norfolk' with marine/coastal beaches)",
          "Filter water-body-testing-2023.csv for Organism = 'Enterococci' or similar",
          "Match communities from filtered EJ data with beach communities",
          "Beach Type Description should indicate marine/harbor beaches",
          "Environmental Justice population percentage must be greater than 90%.",
          "Beach must be located in Boston Harbor.",
          "Enterococcus levels labeled as '< x' should be estimated as x/2.",
          "Enterococcus levels labeled as '> x' should be estimated as x.",
          "Pearson correlation should be rounded to 3 decimal places."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "EJ population percentage > 90",
            "Beach location must be in Boston Harbor",
            "Only Enterococci organism measurements",
            "Only marine beach types",
            "Year must be 2023",
            "Rainfall data must be available for corresponding dates"
          ],
          [
            "Filter environmental-justice-populations.csv where 'Percent of population in EJ BGs' > 90",
            "Identify Boston Harbor beaches (County Description = 'Suffolk' or 'Norfolk' with marine/coastal beaches)",
            "Filter water-body-testing-2023.csv for Organism = 'Enterococci' or similar",
            "Match communities from filtered EJ data with beach communities",
            "Beach Type Description should indicate marine/harbor beaches"
          ],
          [
            "Environmental Justice population percentage must be greater than 90%.",
            "Beach must be located in Boston Harbor.",
            "Enterococcus levels labeled as '< x' should be estimated as x/2.",
            "Enterococcus levels labeled as '> x' should be estimated as x.",
            "Pearson correlation should be rounded to 3 decimal places."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter environmental-justice-populations.csv where 'Percent of population in EJ BGs' > 90",
          "Filter water-body-testing-2023.csv where 'Organism' = 'Enterococci' and 'Beach Type Description' = 'Marine'",
          "Identify Boston Harbor beaches from beach names or community information",
          "Convert tagged Enterococcus values: '< x' \u2192 x/2, '> x' \u2192 x",
          "Boston Harbor communities: likely include Boston, Quincy, Winthrop, Hull, possibly Revere, Chelsea",
          "High EJ communities (>90%): from environmental-justice-populations.csv where 'Percent of population in EJ BGs' > 90.0",
          "Valid Enterococcus measurements: exclude null or invalid values after tag processing",
          "Communities with >90% EJ populations",
          "Boston Harbor beaches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter environmental-justice-populations.csv where 'Percent of population in EJ BGs' > 90",
            "Filter water-body-testing-2023.csv where 'Organism' = 'Enterococci' and 'Beach Type Description' = 'Marine'",
            "Identify Boston Harbor beaches from beach names or community information",
            "Convert tagged Enterococcus values: '< x' \u2192 x/2, '> x' \u2192 x"
          ],
          [
            "Boston Harbor communities: likely include Boston, Quincy, Winthrop, Hull, possibly Revere, Chelsea",
            "High EJ communities (>90%): from environmental-justice-populations.csv where 'Percent of population in EJ BGs' > 90.0",
            "Valid Enterococcus measurements: exclude null or invalid values after tag processing"
          ],
          [
            "Communities with >90% EJ populations",
            "Boston Harbor beaches"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Pearson correlation test between rainfall_3day and Enterococcus_level",
          "Check for normality of variables before correlation",
          "Handle censored data appropriately in correlation calculation",
          "Pearson correlation coefficient between 3-Day Rain and processed Enterococcus levels",
          "Apply transformation: if Tag='<', Enterococcus_adjusted = Enterococcus/2; if Tag='>', Enterococcus_adjusted = Enterococcus; else Enterococcus_adjusted = Enterococcus",
          "Pearson correlation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pearson correlation test between rainfall_3day and Enterococcus_level",
            "Check for normality of variables before correlation",
            "Handle censored data appropriately in correlation calculation"
          ],
          [
            "Pearson correlation coefficient between 3-Day Rain and processed Enterococcus levels",
            "Apply transformation: if Tag='<', Enterococcus_adjusted = Enterococcus/2; if Tag='>', Enterococcus_adjusted = Enterococcus; else Enterococcus_adjusted = Enterococcus"
          ],
          [
            "Pearson correlation"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Correlation coefficient to 3 decimal places",
          "Must specify sample size used in calculation",
          "Should note any data limitations or assumptions made",
          "Single numeric value: Pearson correlation coefficient",
          "Rounded to exactly 3 decimal places",
          "Format: X.XXX (e.g., 0.456 or -0.234)",
          "Output must be a single Pearson correlation value rounded to 3 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Correlation coefficient to 3 decimal places",
            "Must specify sample size used in calculation",
            "Should note any data limitations or assumptions made"
          ],
          [
            "Single numeric value: Pearson correlation coefficient",
            "Rounded to exactly 3 decimal places",
            "Format: X.XXX (e.g., 0.456 or -0.234)"
          ],
          [
            "Output must be a single Pearson correlation value rounded to 3 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5977609427609429
  },
  "environment-hard-11": {
    "m_q": {
      "target_metric": {
        "value": "Average rainfall (to 2 decimal places) in the one-day period before sampling",
        "confidence": 0.3333333333333333,
        "votes": [
          "Average rainfall (to 2 decimal places) in the one-day period before sampling",
          "Average rainfall in the one-day period (1-Day Rain) before sampling, rounded to 2 decimal places",
          "average of '1-Day Rain' for samples where at least one of 'Enterococcus', 'Enterococcus.1', or 'Enterococcus.2' is greater than 104"
        ]
      },
      "filters": {
        "value": [
          "Water samples from Pleasure Bay Beach",
          "Samples failed to meet swimming standards (Enterococcus > 104 counts per 100ml)",
          "Water samples from Pleasure Bay Beach only",
          "Samples that failed to meet swimming standards (Enterococcus > 104 counts per 100mL)",
          "at least one of 'Enterococcus', 'Enterococcus.1', or 'Enterococcus.2' > 104"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Water samples from Pleasure Bay Beach",
            "Samples failed to meet swimming standards (Enterococcus > 104 counts per 100ml)"
          ],
          [
            "Water samples from Pleasure Bay Beach only",
            "Samples that failed to meet swimming standards (Enterococcus > 104 counts per 100mL)"
          ],
          [
            "at least one of 'Enterococcus', 'Enterococcus.1', or 'Enterococcus.2' > 104"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which columns represent Pleasure Bay Beach samples?",
          "How to identify failed samples (Enterococcus > 104)?",
          "How to extract the 1-Day Rain value for failed samples?",
          "How to calculate average rainfall from those values?",
          "Which columns represent different beach locations (Pleasure Bay Beach vs Castle Island Beach)?",
          "Which Enterococcus columns correspond to Pleasure Bay Beach?",
          "How to interpret '<' symbols in Tag columns and their impact on Enterococcus values?",
          "What are all the samples where Enterococcus count exceeds 104?",
          "What is the 1-Day Rain value for each failed sample?",
          "What is the mean of these 1-Day Rain values?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Which columns represent Pleasure Bay Beach samples?",
            "How to identify failed samples (Enterococcus > 104)?",
            "How to extract the 1-Day Rain value for failed samples?",
            "How to calculate average rainfall from those values?"
          ],
          [
            "Which columns represent different beach locations (Pleasure Bay Beach vs Castle Island Beach)?",
            "Which Enterococcus columns correspond to Pleasure Bay Beach?",
            "How to interpret '<' symbols in Tag columns and their impact on Enterococcus values?",
            "What are all the samples where Enterococcus count exceeds 104?",
            "What is the 1-Day Rain value for each failed sample?",
            "What is the mean of these 1-Day Rain values?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "pleasure_bay_and_castle_island_beach_datasheet.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "pleasure_bay_and_castle_island_beach_datasheet.csv"
          ],
          [
            "pleasure_bay_and_castle_island_beach_datasheet.csv"
          ],
          [
            "pleasure_bay_and_castle_island_beach_datasheet.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple Enterococcus columns (Enterococcus, Enterococcus.1, Enterococcus.2) with different data types (int64 vs float64)",
          "Multiple Tag columns (Tag, Tag.1, Tag.2) with inconsistent naming",
          "Multiple Enterococcus columns (Enterococcus, Enterococcus.1, Enterococcus.2) likely represent different sampling locations",
          "Multiple Tag columns (Tag, Tag.1, Tag.2) appear to provide qualifiers for corresponding Enterococcus values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple Enterococcus columns (Enterococcus, Enterococcus.1, Enterococcus.2) with different data types (int64 vs float64)",
            "Multiple Tag columns (Tag, Tag.1, Tag.2) with inconsistent naming"
          ],
          [
            "Multiple Enterococcus columns (Enterococcus, Enterococcus.1, Enterococcus.2) likely represent different sampling locations",
            "Multiple Tag columns (Tag, Tag.1, Tag.2) appear to provide qualifiers for corresponding Enterococcus values"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "1-day rain": "inches",
          "2-day rain": "inches (likely)",
          "3-day rain": "inches (likely)",
          "enterococcus": "counts per 100 milliliters",
          "enterococcus.1": "counts per 100 milliliters",
          "enterococcus.2": "counts per 100 milliliters"
        },
        "confidence": 0.8888888888888888,
        "votes": [
          {
            "1-Day Rain": "inches (likely)",
            "2-Day Rain": "inches (likely)",
            "3-Day Rain": "inches (likely)",
            "Enterococcus": "counts per 100ml",
            "Enterococcus.1": "counts per 100ml",
            "Enterococcus.2": "counts per 100ml"
          },
          {
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches",
            "Enterococcus": "counts per 100 milliliters",
            "Enterococcus.1": "counts per 100 milliliters",
            "Enterococcus.2": "counts per 100 milliliters"
          },
          {
            "1-Day Rain": "inches",
            "Enterococcus": "counts per 100 milliliters",
            "Enterococcus.1": "counts per 100 milliliters",
            "Enterococcus.2": "counts per 100 milliliters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Rain columns appear to be cumulative (2-Day Rain includes 1-Day Rain, 3-Day Rain includes both)",
          "Enterococcus values have '<' symbols in some rows indicating censored data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rain columns appear to be cumulative (2-Day Rain includes 1-Day Rain, 3-Day Rain includes both)",
            "Enterococcus values have '<' symbols in some rows indicating censored data"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 1.0,
        "votes": [
          10.0,
          10.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Swimming standard threshold: Enterococcus \u2264 104 counts/100ml",
          "Failed samples: Enterococcus > 104 counts/100ml",
          "Time period: one-day period before sampling (1-Day Rain column)",
          "Precision requirement: average to 2 decimal places",
          "Swimming standard threshold: Enterococcus <= 104 counts per 100mL",
          "Failed samples: Enterococcus > 104 counts per 100mL",
          "Only include samples from Pleasure Bay Beach",
          "Exclude samples with missing Enterococcus or 1-Day Rain values",
          "Handle missing values in 'Enterococcus', 'Enterococcus.1', and 'Enterococcus.2' appropriately (e.g., exclude rows with missing values or impute).",
          "The 'Tag' columns are not used in the analysis."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Swimming standard threshold: Enterococcus \u2264 104 counts/100ml",
            "Failed samples: Enterococcus > 104 counts/100ml",
            "Time period: one-day period before sampling (1-Day Rain column)",
            "Precision requirement: average to 2 decimal places"
          ],
          [
            "Swimming standard threshold: Enterococcus <= 104 counts per 100mL",
            "Failed samples: Enterococcus > 104 counts per 100mL",
            "Only include samples from Pleasure Bay Beach",
            "Exclude samples with missing Enterococcus or 1-Day Rain values"
          ],
          [
            "Handle missing values in 'Enterococcus', 'Enterococcus.1', and 'Enterococcus.2' appropriately (e.g., exclude rows with missing values or impute).",
            "The 'Tag' columns are not used in the analysis."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Identify which Enterococcus column(s) correspond to Pleasure Bay Beach",
          "Filter rows where Pleasure Bay Beach Enterococcus > 104",
          "Exclude rows with censored data ('<' symbols) for failed samples",
          "Filter for Enterococcus values > 104 in the correct column(s) representing Pleasure Bay Beach",
          "Handle '<' tags appropriately - values marked with '<' are below detection and should use the stated value as upper bound",
          "A derived filter to identify rows where the Enterococcus count exceeds the swimming standard (<= 104) for at least one of the three Enterococcus measurements."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify which Enterococcus column(s) correspond to Pleasure Bay Beach",
            "Filter rows where Pleasure Bay Beach Enterococcus > 104",
            "Exclude rows with censored data ('<' symbols) for failed samples"
          ],
          [
            "Filter for Enterococcus values > 104 in the correct column(s) representing Pleasure Bay Beach",
            "Handle '<' tags appropriately - values marked with '<' are below detection and should use the stated value as upper bound"
          ],
          [
            "A derived filter to identify rows where the Enterococcus count exceeds the swimming standard (<= 104) for at least one of the three Enterococcus measurements."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of 1-Day Rain values for failed samples",
          "Verify no negative rainfall values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of 1-Day Rain values for failed samples",
            "Verify no negative rainfall values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numeric average with 2 decimal places",
          "Handle missing/NA rainfall values appropriately",
          "Round final average to exactly 2 decimal places",
          "Return single scalar value representing average rainfall",
          "The final average rainfall should be rounded to two decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric average with 2 decimal places",
            "Handle missing/NA rainfall values appropriately"
          ],
          [
            "Round final average to exactly 2 decimal places",
            "Return single scalar value representing average rainfall"
          ],
          [
            "The final average rainfall should be rounded to two decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5944444444444446
  },
  "environment-hard-12": {
    "m_q": {
      "target_metric": {
        "value": "Count of samples that failed swimming standards (Enterococcus >= 104 counts/100mL) when there was no rainfall in the preceding three days (1-Day Rain = 0, 2-Day Rain = 0, 3-Day Rain = 0)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of samples that failed swimming standards (Enterococcus >= 104 counts/100mL) when there was no rainfall in the preceding three days (1-Day Rain = 0, 2-Day Rain = 0, 3-Day Rain = 0)",
          "Count of samples that failed swimming standards (Enterococcus >= 104 counts per 100mL) by beach",
          "Number of samples that failed to meet swimming standards (Enterococcus count >= 104) for each beach, when there was no rainfall in the preceding three days, and identify the beach with the highest count."
        ]
      },
      "filters": {
        "value": [
          "Enterococcus >= 104",
          "1-Day Rain = 0",
          "2-Day Rain = 0",
          "3-Day Rain = 0",
          "3-Day Rain == 0.0 (no rainfall in preceding three days)",
          "Enterococcus >= 104 (failed standard)"
        ],
        "confidence": 0.5555555555555556,
        "votes": [
          [
            "Enterococcus >= 104",
            "1-Day Rain = 0",
            "2-Day Rain = 0",
            "3-Day Rain = 0"
          ],
          [
            "3-Day Rain == 0.0 (no rainfall in preceding three days)",
            "Enterococcus >= 104 (failed standard)"
          ],
          [
            "3-Day Rain = 0",
            "2-Day Rain = 0",
            "1-Day Rain = 0",
            "Enterococcus >= 104"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Beach",
          "Beach name (from file names)"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Beach"
          ],
          [
            "Beach name (from file names)"
          ],
          [
            "Beach"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar (single beach name with highest count)",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which beaches have Enterococcus measurements?",
          "How are multiple Enterococcus columns per beach handled?",
          "How to handle Tag columns indicating detection limits?",
          "How to parse Date column format?",
          "How to handle missing values in Enterococcus columns?",
          "What constitutes no rainfall in the preceding three days?",
          "Which samples failed the standard (Enterococcus >= 104)?",
          "How many failed samples does each beach have under no-rain conditions?",
          "Which beach has the maximum count of failed samples?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which beaches have Enterococcus measurements?",
            "How are multiple Enterococcus columns per beach handled?",
            "How to handle Tag columns indicating detection limits?",
            "How to parse Date column format?",
            "How to handle missing values in Enterococcus columns?"
          ],
          [
            "What constitutes no rainfall in the preceding three days?",
            "Which samples failed the standard (Enterococcus >= 104)?",
            "How many failed samples does each beach have under no-rain conditions?",
            "Which beach has the maximum count of failed samples?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "boston-harbor-beaches.txt",
          "constitution_beach_datasheet.csv",
          "carson_beach_datasheet.csv",
          "pleasure_bay_and_castle_island_beach_datasheet.csv",
          "m_street_beach_datasheet.csv",
          "city_point_beach_datasheet.csv",
          "malibu_beach_datasheet.csv",
          "tenean_beach_datasheet.csv",
          "wollaston_beach_datasheet.csv"
        ],
        "confidence": 0.9629629629629629,
        "votes": [
          [
            "boston-harbor-beaches.txt",
            "constitution_beach_datasheet.csv",
            "carson_beach_datasheet.csv",
            "pleasure_bay_and_castle_island_beach_datasheet.csv",
            "m_street_beach_datasheet.csv",
            "city_point_beach_datasheet.csv",
            "malibu_beach_datasheet.csv",
            "tenean_beach_datasheet.csv",
            "wollaston_beach_datasheet.csv"
          ],
          [
            "constitution_beach_datasheet.csv",
            "carson_beach_datasheet.csv",
            "pleasure_bay_and_castle_island_beach_datasheet.csv",
            "m_street_beach_datasheet.csv",
            "city_point_beach_datasheet.csv",
            "malibu_beach_datasheet.csv",
            "tenean_beach_datasheet.csv",
            "wollaston_beach_datasheet.csv"
          ],
          [
            "boston-harbor-beaches.txt",
            "constitution_beach_datasheet.csv",
            "carson_beach_datasheet.csv",
            "pleasure_bay_and_castle_island_beach_datasheet.csv",
            "m_street_beach_datasheet.csv",
            "city_point_beach_datasheet.csv",
            "malibu_beach_datasheet.csv",
            "tenean_beach_datasheet.csv",
            "wollaston_beach_datasheet.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different number of Enterococcus columns per beach file (1-4 columns)",
          "Different data types for Enterococcus (float64 vs int64)",
          "Different column naming patterns (Tag, Tag.1, Tag.2, Tag.3)",
          "Different number of total columns per file (6-12 columns)",
          "Date column format appears consistent but needs validation",
          "Different number of sample columns across beaches (constitution has 3 samples per date, carson has 2, m_street has 1, wollaston has 4)",
          "Enterococcus columns have different data types (float64 vs int64)",
          "Tag columns contain inequality symbols ('<') indicating values below detection limit"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different number of Enterococcus columns per beach file (1-4 columns)",
            "Different data types for Enterococcus (float64 vs int64)",
            "Different column naming patterns (Tag, Tag.1, Tag.2, Tag.3)",
            "Different number of total columns per file (6-12 columns)",
            "Date column format appears consistent but needs validation"
          ],
          [
            "Different number of sample columns across beaches (constitution has 3 samples per date, carson has 2, m_street has 1, wollaston has 4)",
            "Enterococcus columns have different data types (float64 vs int64)",
            "Tag columns contain inequality symbols ('<') indicating values below detection limit"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "1-day rain": "inches",
          "2-day rain": "inches",
          "3-day rain": "inches",
          "enterococcus": "counts per 100 milliliters",
          "enterococcus.1": "counts per 100 milliliters",
          "enterococcus.2": "counts per 100 milliliters",
          "enterococcus.3": "counts per 100 milliliters"
        },
        "confidence": 0.7142857142857142,
        "votes": [
          {
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches",
            "Enterococcus": "counts per 100 milliliters",
            "Enterococcus.1": "counts per 100 milliliters",
            "Enterococcus.2": "counts per 100 milliliters",
            "Enterococcus.3": "counts per 100 milliliters"
          },
          {
            "Enterococcus": "counts per 100 milliliters",
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches"
          },
          {
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches",
            "Enterococcus": "counts per 100 milliliters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Rain columns appear to be cumulative rainfall measurements",
          "Enterococcus values range from <10 to >2000",
          "Tag column contains '<' symbol indicating values below detection limit",
          "Tag column contains '<' symbol indicating values are below detection limit (e.g., '<10' means less than 10)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rain columns appear to be cumulative rainfall measurements",
            "Enterococcus values range from <10 to >2000",
            "Tag column contains '<' symbol indicating values below detection limit"
          ],
          [
            "Tag column contains '<' symbol indicating values are below detection limit (e.g., '<10' means less than 10)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Detection limit handling differs (some files use '<' tag, others may use different notation)",
          "Rainfall measurement consistency across beaches needs verification",
          "Date ranges may differ between beach datasets",
          "Same measurement units across all beaches",
          "Consistent rainfall measurement approach"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Detection limit handling differs (some files use '<' tag, others may use different notation)",
            "Rainfall measurement consistency across beaches needs verification",
            "Date ranges may differ between beach datasets"
          ],
          [
            "Same measurement units across all beaches",
            "Consistent rainfall measurement approach"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "NaN"
        ],
        "confidence": 0.9999999999999999,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            " ",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.6363636363636364,
        "votes": [
          10.0,
          6.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Enterococcus must be numeric (float or int)",
          "Rain columns must be >= 0",
          "Date must be parseable to datetime",
          "Only consider samples where all three rain columns = 0",
          "Only count samples where Enterococcus >= 104",
          "3-Day Rain must equal 0.0",
          "Enterococcus >= 104 indicates failure to meet swimming standard",
          "Must handle multiple samples per date for some beaches",
          "Tag values starting with '<' indicate below-detection measurements that should be treated as the numeric value shown",
          "Enterococcus values represented with '<' need to be converted to numeric values (e.g., <10 to 10 or a smaller value like 9)",
          "Need to iterate through each beach datasheet file.",
          "Need to handle different column names for Enterococcus counts across different files (e.g., Enterococcus, Enterococcus.1, Enterococcus.2, Enterococcus.3)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Enterococcus must be numeric (float or int)",
            "Rain columns must be >= 0",
            "Date must be parseable to datetime",
            "Only consider samples where all three rain columns = 0",
            "Only count samples where Enterococcus >= 104"
          ],
          [
            "3-Day Rain must equal 0.0",
            "Enterococcus >= 104 indicates failure to meet swimming standard",
            "Must handle multiple samples per date for some beaches",
            "Tag values starting with '<' indicate below-detection measurements that should be treated as the numeric value shown"
          ],
          [
            "Enterococcus values represented with '<' need to be converted to numeric values (e.g., <10 to 10 or a smaller value like 9)",
            "Need to iterate through each beach datasheet file.",
            "Need to handle different column names for Enterococcus counts across different files (e.g., Enterococcus, Enterococcus.1, Enterococcus.2, Enterococcus.3)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where 1-Day Rain = 0 AND 2-Day Rain = 0 AND 3-Day Rain = 0",
          "Filter Enterococcus values >= 104 (ignoring detection limit tags)",
          "Handle '<' tags by treating as actual measured value or exclusion based on analysis approach",
          "Filter rows where 3-Day Rain == 0.0",
          "For each Enterococcus column, count values >= 104",
          "Aggregate counts by beach"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where 1-Day Rain = 0 AND 2-Day Rain = 0 AND 3-Day Rain = 0",
            "Filter Enterococcus values >= 104 (ignoring detection limit tags)",
            "Handle '<' tags by treating as actual measured value or exclusion based on analysis approach"
          ],
          [
            "Filter rows where 3-Day Rain == 0.0",
            "For each Enterococcus column, count values >= 104",
            "Aggregate counts by beach"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for temporal autocorrelation in Enterococcus measurements",
          "Verify rainfall measurements are consistent across same dates",
          "Test for differences in sampling frequency between beaches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for temporal autocorrelation in Enterococcus measurements",
            "Verify rainfall measurements are consistent across same dates",
            "Test for differences in sampling frequency between beaches"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Beach name must match names from boston-harbor-beaches.txt",
          "Count must be integer",
          "Must specify how detection limit values (<10) were handled",
          "Must specify date range considered",
          "Return single beach name with highest failure count",
          "Beach name should match the file name prefix (e.g., 'Constitution Beach', 'Carson Beach')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Beach name must match names from boston-harbor-beaches.txt",
            "Count must be integer",
            "Must specify how detection limit values (<10) were handled",
            "Must specify date range considered"
          ],
          [
            "Return single beach name with highest failure count",
            "Beach name should match the file name prefix (e.g., 'Constitution Beach', 'Carson Beach')"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.601791726791727
  },
  "environment-hard-13": {
    "m_q": {
      "target_metric": {
        "value": "Count of days in 2024 where at least one sampling point at Constitution Beach had Enterococcus count < 104 and at least one other sampling point had Enterococcus count \u2265 104",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of days in 2024 where at least one sampling point at Constitution Beach had Enterococcus count < 104 and at least one other sampling point had Enterococcus count \u2265 104",
          "Count of days where at least one sampling point met the standard (Enterococcus < 104) and at least one sampling point did not meet the standard (Enterococcus >= 104)",
          "Number of days in 2024 at Constitution Beach where at least one sampling point had Enterococcus count < 104 and at least one sampling point had Enterococcus count >= 104."
        ]
      },
      "filters": {
        "value": [
          "Date in 2024",
          "Data from Constitution Beach only",
          "Year 2024",
          "Constitution Beach location",
          "Year is 2024",
          "Location is Constitution Beach"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date in 2024",
            "Data from Constitution Beach only"
          ],
          [
            "Year 2024",
            "Constitution Beach location"
          ],
          [
            "Year is 2024",
            "Location is Constitution Beach"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Date"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Date"
          ],
          [
            "Date"
          ],
          [
            "Date"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to interpret '<' symbol in Enterococcus columns?",
          "How to handle missing values in Enterococcus columns?",
          "Are all three sampling points (Enterococcus, Enterococcus.1, Enterococcus.2) always present for each date?",
          "How to parse 'Date' column from string format 'Month Day, Year'?",
          "How many sampling points exist per date?",
          "What are the Enterococcus values for each sampling point on each date?",
          "Which sampling points meet the standard (< 104) on each date?",
          "Which sampling points fail the standard (>= 104) on each date?",
          "On which dates do both conditions occur (at least one pass and at least one fail)?",
          "For each day, determine if any Enterococcus count is < 104.",
          "For each day, determine if any Enterococcus count is >= 104.",
          "Count the number of days where both conditions are true."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How to interpret '<' symbol in Enterococcus columns?",
            "How to handle missing values in Enterococcus columns?",
            "Are all three sampling points (Enterococcus, Enterococcus.1, Enterococcus.2) always present for each date?",
            "How to parse 'Date' column from string format 'Month Day, Year'?"
          ],
          [
            "How many sampling points exist per date?",
            "What are the Enterococcus values for each sampling point on each date?",
            "Which sampling points meet the standard (< 104) on each date?",
            "Which sampling points fail the standard (>= 104) on each date?",
            "On which dates do both conditions occur (at least one pass and at least one fail)?"
          ],
          [
            "For each day, determine if any Enterococcus count is < 104.",
            "For each day, determine if any Enterococcus count is >= 104.",
            "Count the number of days where both conditions are true."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "constitution_beach_datasheet.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "constitution_beach_datasheet.csv"
          ],
          [
            "constitution_beach_datasheet.csv"
          ],
          [
            "constitution_beach_datasheet.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Duplicate column names: 'Tag', 'Tag.1', 'Tag.2' and 'Enterococcus', 'Enterococcus.1', 'Enterococcus.2' suggest multiple sampling points",
          "Date column stored as object/string instead of datetime",
          "Multiple Enterococcus measurements per date stored in separate columns (Enterococcus, Enterococcus.1, Enterococcus.2)",
          "Tag columns appear to indicate comparison operators for corresponding Enterococcus values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Duplicate column names: 'Tag', 'Tag.1', 'Tag.2' and 'Enterococcus', 'Enterococcus.1', 'Enterococcus.2' suggest multiple sampling points",
            "Date column stored as object/string instead of datetime"
          ],
          [
            "Multiple Enterococcus measurements per date stored in separate columns (Enterococcus, Enterococcus.1, Enterococcus.2)",
            "Tag columns appear to indicate comparison operators for corresponding Enterococcus values"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "1-day rain": "inches",
          "2-day rain": "inches",
          "3-day rain": "inches",
          "enterococcus": "counts per 100 milliliters",
          "enterococcus.1": "counts per 100 milliliters",
          "enterococcus.2": "counts per 100 milliliters"
        },
        "confidence": 0.888888888888889,
        "votes": [
          {
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches",
            "Enterococcus": "counts per 100 milliliters",
            "Enterococcus.1": "counts per 100 milliliters",
            "Enterococcus.2": "counts per 100 milliliters"
          },
          {
            "Enterococcus": "counts per 100 milliliters",
            "Enterococcus.1": "counts per 100 milliliters",
            "Enterococcus.2": "counts per 100 milliliters",
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches"
          },
          {
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches",
            "Enterococcus": "counts per 100 milliliters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Enterococcus values appear as floats but may represent discrete counts",
          "Rain columns have very small values (0.0-0.89) suggesting possible unit conversion needed",
          "Tag columns contain '<' symbols indicating values below detection limit",
          "Enterococcus values with '<' tag represent upper bounds, not exact measurements"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Enterococcus values appear as floats but may represent discrete counts",
            "Rain columns have very small values (0.0-0.89) suggesting possible unit conversion needed"
          ],
          [
            "Tag columns contain '<' symbols indicating values below detection limit",
            "Enterococcus values with '<' tag represent upper bounds, not exact measurements"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "<"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "<"
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 1.0,
        "votes": [
          10.0,
          10.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Enterococcus values must be non-negative",
          "Date must be parseable to valid dates in 2024",
          "Standard threshold is <104 counts per 100ml",
          "Each row represents one day with up to 3 sampling points",
          "Standard threshold is Enterococcus < 104 counts per 100 mL",
          "Only count dates in year 2024",
          "Must have at least one sampling point meeting standard AND at least one not meeting standard on same date",
          "Handle '<' tags appropriately when comparing to threshold",
          "Date must be a valid date.",
          "Enterococcus counts must be non-negative numbers."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Enterococcus values must be non-negative",
            "Date must be parseable to valid dates in 2024",
            "Standard threshold is <104 counts per 100ml",
            "Each row represents one day with up to 3 sampling points"
          ],
          [
            "Standard threshold is Enterococcus < 104 counts per 100 mL",
            "Only count dates in year 2024",
            "Must have at least one sampling point meeting standard AND at least one not meeting standard on same date",
            "Handle '<' tags appropriately when comparing to threshold"
          ],
          [
            "Date must be a valid date.",
            "Enterococcus counts must be non-negative numbers."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to dates in 2024 only",
          "Exclude rows where all Enterococcus values are missing",
          "Interpret '<' in Tag columns as values below detection limit (use corresponding Enterococcus value)",
          "Extract year from Date field and filter for 2024",
          "For each date, evaluate each of the three Enterococcus measurements against 104 threshold",
          "Consider Tag '<' symbols: if Tag='<' and value=10, actual value is <10, which meets standard",
          "Year of Date is 2024",
          "Enterococcus < 104",
          "Enterococcus >= 104"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to dates in 2024 only",
            "Exclude rows where all Enterococcus values are missing",
            "Interpret '<' in Tag columns as values below detection limit (use corresponding Enterococcus value)"
          ],
          [
            "Extract year from Date field and filter for 2024",
            "For each date, evaluate each of the three Enterococcus measurements against 104 threshold",
            "Consider Tag '<' symbols: if Tag='<' and value=10, actual value is <10, which meets standard"
          ],
          [
            "Year of Date is 2024",
            "Enterococcus < 104",
            "Enterococcus >= 104"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate dates",
          "Validate that Enterococcus values <104 when corresponding Tag is '<'",
          "Verify temporal ordering of dates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate dates",
            "Validate that Enterococcus values <104 when corresponding Tag is '<'",
            "Verify temporal ordering of dates"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count of days",
          "Must handle partial data (some sampling points missing)",
          "Return single integer count of days meeting the mixed standard condition",
          "The output should be a single integer representing the number of days."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count of days",
            "Must handle partial data (some sampling points missing)"
          ],
          [
            "Return single integer count of days meeting the mixed standard condition"
          ],
          [
            "The output should be a single integer representing the number of days."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6277777777777779
  },
  "environment-hard-14": {
    "m_q": {
      "target_metric": {
        "value": "Compare correlation coefficients between monthly rainfall and exceedance rate for freshwater vs marine beaches during summer months (Jun, Jul, Aug) from 2007-2009",
        "confidence": 0.3333333333333333,
        "votes": [
          "Compare correlation coefficients between monthly rainfall and exceedance rate for freshwater vs marine beaches during summer months (Jun, Jul, Aug) from 2007-2009",
          "Compare correlation between monthly rainfall and exceedance rate for freshwater vs marine beaches during summer months (Jun, Jul, Aug) from 2007-2009",
          "Comparison of correlation between monthly rainfall and exceedance rate for freshwater vs. marine beaches from 2007-2009 during summer months (Jun, Jul, Aug)"
        ]
      },
      "filters": {
        "value": [
          "Year between 2007 and 2009 inclusive",
          "Month in (6, 7, 8) for summer months",
          "Beach Type Description in ('Freshwater', 'Marine')",
          "Regions: Boston, Chatham, Amherst, Ashburnham for freshwater; Boston and Chatham for marine",
          "Year >= 2007 AND Year <= 2009",
          "Sample Date month in [6, 7, 8] (June, July, August)",
          "Beach Type Description in ['Fresh water', 'Marine']",
          "Rainfall regions: Boston, Chatham for Marine; Boston, Chatham, Amherst, Ashburnham for Freshwater",
          "Year between 2007 and 2009 (inclusive)",
          "Months: June, July, August",
          "Beach Type: Fresh water or Marine",
          "Regions for Fresh water beaches: Boston, Chatham, Amherst, Ashburnham",
          "Regions for Marine beaches: Boston, Chatham"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year between 2007 and 2009 inclusive",
            "Month in (6, 7, 8) for summer months",
            "Beach Type Description in ('Freshwater', 'Marine')",
            "Regions: Boston, Chatham, Amherst, Ashburnham for freshwater; Boston and Chatham for marine"
          ],
          [
            "Year >= 2007 AND Year <= 2009",
            "Sample Date month in [6, 7, 8] (June, July, August)",
            "Beach Type Description in ['Fresh water', 'Marine']",
            "Rainfall regions: Boston, Chatham for Marine; Boston, Chatham, Amherst, Ashburnham for Freshwater"
          ],
          [
            "Year between 2007 and 2009 (inclusive)",
            "Months: June, July, August",
            "Beach Type: Fresh water or Marine",
            "Regions for Fresh water beaches: Boston, Chatham, Amherst, Ashburnham",
            "Regions for Marine beaches: Boston, Chatham"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Beach Type Description",
          "Year",
          "Month",
          "Region",
          "Month (from Sample Date)",
          "Beach Type (Freshwater, Marine)"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "Beach Type Description",
            "Year",
            "Month",
            "Region"
          ],
          [
            "Beach Type Description",
            "Month (from Sample Date)",
            "Year"
          ],
          [
            "Beach Type (Freshwater, Marine)"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate monthly exceedance rate per beach type and region",
          "Calculate monthly rainfall per region",
          "Impute missing rainfall values with median of month across non-missing years",
          "Compute correlation between rainfall and exceedance rate for each beach type",
          "Compare which beach type has higher correlation",
          "What is the exceedance rate (violations/total samples) for each beach type by month-year?",
          "What is the average monthly rainfall for regions affecting freshwater beaches (Boston, Chatham, Amherst, Ashburnham)?",
          "What is the average monthly rainfall for regions affecting marine beaches (Boston, Chatham)?",
          "How to impute missing rainfall values with median of the month across non-missing years?",
          "What is the correlation coefficient between monthly rainfall and exceedance rate for freshwater beaches?",
          "What is the correlation coefficient between monthly rainfall and exceedance rate for marine beaches?",
          "Which beach type has higher correlation: Freshwater or Marine?",
          "Calculate monthly exceedance rate for each beach type.",
          "Impute missing rainfall data with median of the month in non-missing years.",
          "Calculate the correlation between monthly rainfall and exceedance rate for each beach type.",
          "Compare the correlations and determine which beach type has a higher correlation."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Calculate monthly exceedance rate per beach type and region",
            "Calculate monthly rainfall per region",
            "Impute missing rainfall values with median of month across non-missing years",
            "Compute correlation between rainfall and exceedance rate for each beach type",
            "Compare which beach type has higher correlation"
          ],
          [
            "What is the exceedance rate (violations/total samples) for each beach type by month-year?",
            "What is the average monthly rainfall for regions affecting freshwater beaches (Boston, Chatham, Amherst, Ashburnham)?",
            "What is the average monthly rainfall for regions affecting marine beaches (Boston, Chatham)?",
            "How to impute missing rainfall values with median of the month across non-missing years?",
            "What is the correlation coefficient between monthly rainfall and exceedance rate for freshwater beaches?",
            "What is the correlation coefficient between monthly rainfall and exceedance rate for marine beaches?",
            "Which beach type has higher correlation: Freshwater or Marine?"
          ],
          [
            "Calculate monthly exceedance rate for each beach type.",
            "Impute missing rainfall data with median of the month in non-missing years.",
            "Calculate the correlation between monthly rainfall and exceedance rate for each beach type.",
            "Compare the correlations and determine which beach type has a higher correlation."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2007.csv",
          "water-body-testing-2008.csv",
          "water-body-testing-2009.csv",
          "monthly_precipitations_boston.csv",
          "monthly_precipitations_chatham.csv",
          "monthly_precipitations_amherst.csv",
          "monthly_precipitations_ashburnham.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv"
          ],
          [
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv"
          ],
          [
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Year column type mismatch: float64 in precipitation files vs int64 in testing files",
          "Community names in testing files need mapping to precipitation file regions",
          "Precipitation files have separate files per region while testing files have all regions combined",
          "Rainfall data is in wide format (months as columns) vs beach data in long format",
          "Beach data has individual samples while rainfall is monthly aggregate"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column type mismatch: float64 in precipitation files vs int64 in testing files",
            "Community names in testing files need mapping to precipitation file regions",
            "Precipitation files have separate files per region while testing files have all regions combined"
          ],
          [
            "Rainfall data is in wide format (months as columns) vs beach data in long format",
            "Beach data has individual samples while rainfall is monthly aggregate"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "MPN/100ml or CFU/100ml (bacterial count)",
          "jan-dec columns in precipitation files": "inches",
          "annual": "inches",
          "jan through dec columns": "inches of precipitation",
          "year": "year",
          "sample date": "datetime",
          "jan": "inches",
          "feb": "inches",
          "mar": "inches",
          "apr": "inches",
          "may": "inches",
          "jun": "inches",
          "jul": "inches",
          "aug": "inches",
          "sep": "inches",
          "oct": "inches",
          "nov": "inches",
          "dec": "inches"
        },
        "confidence": 0.4074074074074072,
        "votes": [
          {
            "Indicator Level": "MPN/100ml or CFU/100ml (bacterial count)",
            "Jan-Dec columns in precipitation files": "inches",
            "Annual": "inches"
          },
          {
            "Jan through Dec columns": "inches of precipitation",
            "Annual": "inches of precipitation",
            "Indicator Level": "colony forming units per 100mL",
            "Year": "year",
            "Sample Date": "datetime"
          },
          {
            "Indicator Level": "cfu/100mL",
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Indicator Level values vary widely (1-480)",
          "Rainfall values in inches with decimal precision",
          "Missing values in precipitation data (blanks in sample)",
          "Rainfall is monthly aggregate, beach violations are daily samples requiring aggregation to monthly",
          "Multiple beaches per type require aggregation to type level"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level values vary widely (1-480)",
            "Rainfall values in inches with decimal precision",
            "Missing values in precipitation data (blanks in sample)"
          ],
          [
            "Rainfall is monthly aggregate, beach violations are daily samples requiring aggregation to monthly",
            "Multiple beaches per type require aggregation to type level"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Testing data organized by Community, precipitation data by Region - need mapping: Boston->Boston, Chatham->Chatham, Amherst->Amherst, Ashburnham->Ashburnham",
          "Testing data has daily samples, precipitation data is monthly aggregates",
          "Missing rainfall values exist in precipitation data (NaN/empty) requiring imputation",
          "Beach types may be spelled differently across years (e.g., 'Fresh water' vs 'Freshwater')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Testing data organized by Community, precipitation data by Region - need mapping: Boston->Boston, Chatham->Chatham, Amherst->Amherst, Ashburnham->Ashburnham",
            "Testing data has daily samples, precipitation data is monthly aggregates"
          ],
          [
            "Missing rainfall values exist in precipitation data (NaN/empty) requiring imputation",
            "Beach types may be spelled differently across years (e.g., 'Fresh water' vs 'Freshwater')"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "null"
          ],
          [
            "",
            "NA",
            "N/A",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year range: 2007-2009 inclusive",
          "Summer months: June(6), July(7), August(8)",
          "Freshwater regions: Boston, Chatham, Amherst, Ashburnham",
          "Marine regions: Boston, Chatham",
          "Exceedance defined by Violation column = 'yes'",
          "Years must be exactly 2007, 2008, 2009",
          "Months must be exactly 6 (Jun), 7 (Jul), 8 (Aug)",
          "Beach Type Description must be either 'Fresh water' or 'Marine' (case-sensitive check needed)",
          "Exceedance rate = count(Violation='yes') / count(all samples) per month-year-beach_type",
          "Freshwater rainfall = mean of [Boston, Chatham, Amherst, Ashburnham] for each month-year",
          "Marine rainfall = mean of [Boston, Chatham] for each month-year",
          "Years should be between 2007 and 2009.",
          "Months should be June, July, or August.",
          "Beach Type Description should be either 'Fresh' or 'Marine'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year range: 2007-2009 inclusive",
            "Summer months: June(6), July(7), August(8)",
            "Freshwater regions: Boston, Chatham, Amherst, Ashburnham",
            "Marine regions: Boston, Chatham",
            "Exceedance defined by Violation column = 'yes'"
          ],
          [
            "Years must be exactly 2007, 2008, 2009",
            "Months must be exactly 6 (Jun), 7 (Jul), 8 (Aug)",
            "Beach Type Description must be either 'Fresh water' or 'Marine' (case-sensitive check needed)",
            "Exceedance rate = count(Violation='yes') / count(all samples) per month-year-beach_type",
            "Freshwater rainfall = mean of [Boston, Chatham, Amherst, Ashburnham] for each month-year",
            "Marine rainfall = mean of [Boston, Chatham] for each month-year"
          ],
          [
            "Years should be between 2007 and 2009.",
            "Months should be June, July, or August.",
            "Beach Type Description should be either 'Fresh' or 'Marine'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract month from Sample Date column",
          "Filter to summer months (6,7,8)",
          "Group by month and beach type to calculate exceedance rate",
          "Join precipitation data by region and month",
          "Extract month from Sample Date for filtering",
          "Match Beach Type Description to appropriate rainfall regions",
          "Create month-year keys for aggregation",
          "Exceedance Rate = Number of Violations / Total Number of Samples for each month and beach type."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract month from Sample Date column",
            "Filter to summer months (6,7,8)",
            "Group by month and beach type to calculate exceedance rate",
            "Join precipitation data by region and month"
          ],
          [
            "Extract month from Sample Date for filtering",
            "Match Beach Type Description to appropriate rainfall regions",
            "Create month-year keys for aggregation"
          ],
          [
            "Exceedance Rate = Number of Violations / Total Number of Samples for each month and beach type."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Pearson correlation between monthly rainfall and monthly exceedance rate",
          "Median imputation for missing rainfall values",
          "Comparison of correlation coefficients between beach types",
          "Pearson correlation coefficient between monthly rainfall and exceedance rate for freshwater beaches",
          "Pearson correlation coefficient between monthly rainfall and exceedance rate for marine beaches",
          "Missing value imputation: median of specific month across years 2007-2009 where data exists",
          "Pearson correlation coefficient between monthly rainfall and exceedance rate."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pearson correlation between monthly rainfall and monthly exceedance rate",
            "Median imputation for missing rainfall values",
            "Comparison of correlation coefficients between beach types"
          ],
          [
            "Pearson correlation coefficient between monthly rainfall and exceedance rate for freshwater beaches",
            "Pearson correlation coefficient between monthly rainfall and exceedance rate for marine beaches",
            "Missing value imputation: median of specific month across years 2007-2009 where data exists"
          ],
          [
            "Pearson correlation coefficient between monthly rainfall and exceedance rate."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Final answer must be either 'Freshwater' or 'Marine'",
          "Correlation coefficients should be calculated separately for each beach type",
          "Use median of month across non-missing years for imputation",
          "Final answer must be exactly 'Freshwater' or 'Marine'",
          "Answer based on which beach type has higher absolute correlation coefficient",
          "Output should be either 'Freshwater' or 'Marine'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Final answer must be either 'Freshwater' or 'Marine'",
            "Correlation coefficients should be calculated separately for each beach type",
            "Use median of month across non-missing years for imputation"
          ],
          [
            "Final answer must be exactly 'Freshwater' or 'Marine'",
            "Answer based on which beach type has higher absolute correlation coefficient"
          ],
          [
            "Output should be either 'Freshwater' or 'Marine'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5959259259259261
  },
  "environment-hard-15": {
    "m_q": {
      "target_metric": {
        "value": "highest average exceedance rate since 2020 (inclusive)",
        "confidence": 0.3333333333333333,
        "votes": [
          "highest average exceedance rate since 2020 (inclusive)",
          "Beach with highest average exceedance rate (violation rate) since 2020",
          "average exceedance rate of fresh water beaches since 2020"
        ]
      },
      "filters": {
        "value": [
          "Beach Type Description = 'Fresh Water'",
          "Year >= 2020 AND Year <= 2023",
          "Beach must have measurements in all years 2020-2023",
          "Beach Type Description == 'Fresh Water'",
          "Beach must have measurements in all years: 2020, 2021, 2022, 2023",
          "Year >= 2020",
          "Beach Type Description == 'Fresh'",
          "Beaches present in all years from 2020 to 2023"
        ],
        "confidence": 0.375,
        "votes": [
          [
            "Beach Type Description = 'Fresh Water'",
            "Year >= 2020 AND Year <= 2023",
            "Beach must have measurements in all years 2020-2023"
          ],
          [
            "Beach Type Description == 'Fresh Water'",
            "Year >= 2020 and Year <= 2023",
            "Beach must have measurements in all years: 2020, 2021, 2022, 2023"
          ],
          [
            "Year >= 2020",
            "Beach Type Description == 'Fresh'",
            "Beaches present in all years from 2020 to 2023"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Beach Name",
          "Year"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Beach Name",
            "Year"
          ],
          [
            "Beach Name"
          ],
          [
            "Beach Name"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the exceedance rate for each fresh water beach per year?",
          "Which fresh water beaches have data for all years 2020-2023?",
          "How to calculate average exceedance rate across years for each beach?",
          "Which beaches are classified as fresh water (Beach Type Description)?",
          "Which beaches have data present in all four years (2020, 2021, 2022, 2023)?",
          "For each qualifying beach, what is the exceedance rate (proportion of samples with Violation == 'Yes' or 'YES')?",
          "Which beach has the highest average exceedance rate across all years 2020-2023?",
          "Identify fresh water beaches.",
          "For each fresh water beach, calculate the exceedance rate for each year from 2020 to 2023.",
          "Identify beaches present in all years from 2020 to 2023.",
          "Calculate the average exceedance rate for each beach across the years 2020-2023.",
          "Find the beach with the highest average exceedance rate."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the exceedance rate for each fresh water beach per year?",
            "Which fresh water beaches have data for all years 2020-2023?",
            "How to calculate average exceedance rate across years for each beach?"
          ],
          [
            "Which beaches are classified as fresh water (Beach Type Description)?",
            "Which beaches have data present in all four years (2020, 2021, 2022, 2023)?",
            "For each qualifying beach, what is the exceedance rate (proportion of samples with Violation == 'Yes' or 'YES')?",
            "Which beach has the highest average exceedance rate across all years 2020-2023?"
          ],
          [
            "Identify fresh water beaches.",
            "For each fresh water beach, calculate the exceedance rate for each year from 2020 to 2023.",
            "Identify beaches present in all years from 2020 to 2023.",
            "Calculate the average exceedance rate for each beach across the years 2020-2023.",
            "Find the beach with the highest average exceedance rate."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2020.csv",
          "water-body-testing-2021.csv",
          "water-body-testing-2022.csv",
          "water-body-testing-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Violation column has inconsistent capitalization (No/NO/Yes/YES)",
          "Sample Date format appears consistent but should be verified",
          "Violation column has inconsistent case: 'No'/'Yes' in 2020, 2022, 2023 vs 'NO'/'YES' in 2021",
          "Violation column has inconsistent values ('No' vs 'NO' vs 'Yes' vs 'YES')."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation column has inconsistent capitalization (No/NO/Yes/YES)",
            "Sample Date format appears consistent but should be verified"
          ],
          [
            "Violation column has inconsistent case: 'No'/'Yes' in 2020, 2022, 2023 vs 'NO'/'YES' in 2021"
          ],
          [
            "Violation column has inconsistent values ('No' vs 'NO' vs 'Yes' vs 'YES')."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "CFU/100mL (colony forming units per 100 milliliters)",
          "sample date": "YYYY-MM-DD HH:MM:SS",
          "year": "calendar year"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Indicator Level": "CFU/100mL (colony forming units per 100 milliliters)",
            "Sample Date": "YYYY-MM-DD HH:MM:SS"
          },
          {
            "Indicator Level": "MPN/100mL or CFU/100mL (bacterial concentration)",
            "Year": "calendar year",
            "Sample Date": "datetime"
          },
          {
            "Indicator Level": "cfu/100mL"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Indicator Level values range from 5.0 to 364.0 in sample data",
          "Need to check if all measurements use same units"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level values range from 5.0 to 364.0 in sample data",
            "Need to check if all measurements use same units"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Need to verify Organism column consistency across years (all Enterococci in samples)",
          "Violation values use different case conventions across years (uppercase in 2021, title case in other years)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need to verify Organism column consistency across years (all Enterococci in samples)"
          ],
          [
            "Violation values use different case conventions across years (uppercase in 2021, title case in other years)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Violation column must be standardized (Yes/No)",
          "Only include beaches with Beach Type Description = 'Fresh Water'",
          "Exclude beaches missing data for any year 2020-2023",
          "Indicator Level must be numeric",
          "Only include beaches where Beach Type Description == 'Fresh Water'",
          "Only include data from years 2020, 2021, 2022, 2023 (inclusive)",
          "Only include beaches that have at least one measurement in each of the four years 2020-2023",
          "Exceedance rate is calculated as (count of violations) / (total count of samples) per beach across all years",
          "Ensure 'Year' column contains only valid year values (2020, 2021, 2022, 2023).",
          "Ensure 'Beach Type Description' column contains valid values (e.g., 'Fresh', 'Marine').",
          "Ensure 'Violation' column contains only 'Yes' or 'No' (case-insensitive) values."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Violation column must be standardized (Yes/No)",
            "Only include beaches with Beach Type Description = 'Fresh Water'",
            "Exclude beaches missing data for any year 2020-2023",
            "Indicator Level must be numeric"
          ],
          [
            "Only include beaches where Beach Type Description == 'Fresh Water'",
            "Only include data from years 2020, 2021, 2022, 2023 (inclusive)",
            "Only include beaches that have at least one measurement in each of the four years 2020-2023",
            "Exceedance rate is calculated as (count of violations) / (total count of samples) per beach across all years"
          ],
          [
            "Ensure 'Year' column contains only valid year values (2020, 2021, 2022, 2023).",
            "Ensure 'Beach Type Description' column contains valid values (e.g., 'Fresh', 'Marine').",
            "Ensure 'Violation' column contains only 'Yes' or 'No' (case-insensitive) values."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to Beach Type Description containing 'Fresh' or equal to 'Fresh Water'",
          "Filter to Year between 2020 and 2023 inclusive",
          "Beach must appear in Year == 2020",
          "Beach must appear in Year == 2021",
          "Beach must appear in Year == 2022",
          "Beach must appear in Year == 2023",
          "All four year appearances must be for the same Beach Type Description == 'Fresh Water'",
          "Exceedance: Violation == 'Yes' (case-insensitive)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to Beach Type Description containing 'Fresh' or equal to 'Fresh Water'",
            "Filter to Year between 2020 and 2023 inclusive"
          ],
          [
            "Beach must appear in Year == 2020",
            "Beach must appear in Year == 2021",
            "Beach must appear in Year == 2022",
            "Beach must appear in Year == 2023",
            "All four year appearances must be for the same Beach Type Description == 'Fresh Water'"
          ],
          [
            "Exceedance: Violation == 'Yes' (case-insensitive)"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Calculate exceedance rate per beach per year = (count of Violation='Yes') / (total samples)",
          "Calculate average exceedance rate across 2020-2023 for each beach",
          "Identify beach with maximum average exceedance rate"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate exceedance rate per beach per year = (count of Violation='Yes') / (total samples)",
            "Calculate average exceedance rate across 2020-2023 for each beach",
            "Identify beach with maximum average exceedance rate"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single beach name with highest average exceedance rate",
          "Include supporting metrics: average exceedance rate value, yearly exceedance rates",
          "Exceedance rate should be normalized for case-insensitive matching of Violation values (Yes/YES, No/NO)",
          "Output the name of the most polluted fresh water beach."
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "Return single beach name with highest average exceedance rate",
            "Include supporting metrics: average exceedance rate value, yearly exceedance rates"
          ],
          [
            "Return single beach name with highest average exceedance rate",
            "Exceedance rate should be normalized for case-insensitive matching of Violation values (Yes/YES, No/NO)"
          ],
          [
            "Output the name of the most polluted fresh water beach."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6229166666666667
  },
  "environment-hard-16": {
    "m_q": {
      "target_metric": {
        "value": "Count of marine beaches that had zero violations from 2002 to 2023 inclusive",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of marine beaches that had zero violations from 2002 to 2023 inclusive",
          "Count of marine beaches with no violations across all years from 2002 to 2023 inclusive",
          "Count of marine beaches that had no violations from 2002 to 2023 inclusive"
        ]
      },
      "filters": {
        "value": [
          "Beach Type Description = 'Marine'",
          "Year between 2002 and 2023 inclusive",
          "Violation = 'no' or 'NO' or 'No' for all records per beach across all years",
          "Beach Type Description == 'Marine'",
          "Year >= 2002 AND Year <= 2023",
          "Violation == 'no' OR 'NO' OR 'No' (case-insensitive check)",
          "Year between 2002 and 2023",
          "Beach Type Description is 'Marine'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Beach Type Description = 'Marine'",
            "Year between 2002 and 2023 inclusive",
            "Violation = 'no' or 'NO' or 'No' for all records per beach across all years"
          ],
          [
            "Beach Type Description == 'Marine'",
            "Year >= 2002 AND Year <= 2023",
            "Violation == 'no' OR 'NO' OR 'No' (case-insensitive check)"
          ],
          [
            "Year between 2002 and 2023",
            "Beach Type Description is 'Marine'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Beach Name",
          "Community",
          "County Description"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "Beach Name",
            "Community",
            "County Description"
          ],
          [
            "Beach Name"
          ],
          [
            "Beach Name"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which marine beaches have data for all years 2002-2023?",
          "For beaches with missing years, should they be considered safe for those years?",
          "How to handle case variations in Violation column (yes/YES/Yes, no/NO/No)?",
          "Are there duplicate beach names across different communities/counties that need disambiguation?",
          "What are all unique marine beaches across all years?",
          "For each marine beach, did any violation occur in any year from 2002-2023?",
          "How to handle beaches with no data in certain years (assume safe)?",
          "What are the distinct violation value formats ('no', 'NO', 'No', 'yes', 'YES', 'Yes')?",
          "How many marine beaches never had a violation='yes' in any record?",
          "For each beach, determine if it is a marine beach",
          "For each beach and year, determine if there was any violation",
          "Count the number of beaches that are marine and had no violations in any year from 2002 to 2023"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which marine beaches have data for all years 2002-2023?",
            "For beaches with missing years, should they be considered safe for those years?",
            "How to handle case variations in Violation column (yes/YES/Yes, no/NO/No)?",
            "Are there duplicate beach names across different communities/counties that need disambiguation?"
          ],
          [
            "What are all unique marine beaches across all years?",
            "For each marine beach, did any violation occur in any year from 2002-2023?",
            "How to handle beaches with no data in certain years (assume safe)?",
            "What are the distinct violation value formats ('no', 'NO', 'No', 'yes', 'YES', 'Yes')?",
            "How many marine beaches never had a violation='yes' in any record?"
          ],
          [
            "For each beach, determine if it is a marine beach",
            "For each beach and year, determine if there was any violation",
            "Count the number of beaches that are marine and had no violations in any year from 2002 to 2023"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2002.csv",
          "water-body-testing-2003.csv",
          "water-body-testing-2004.csv",
          "water-body-testing-2005.csv",
          "water-body-testing-2006.csv",
          "water-body-testing-2007.csv",
          "water-body-testing-2008.csv",
          "water-body-testing-2009.csv",
          "water-body-testing-2010.csv",
          "water-body-testing-2011.csv",
          "water-body-testing-2012.csv",
          "water-body-testing-2013.csv",
          "water-body-testing-2014.csv",
          "water-body-testing-2015.csv",
          "water-body-testing-2016.csv",
          "water-body-testing-2017.csv",
          "water-body-testing-2018.csv",
          "water-body-testing-2019.csv",
          "water-body-testing-2020.csv",
          "water-body-testing-2021.csv",
          "water-body-testing-2022.csv",
          "water-body-testing-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Indicator Level column has different dtypes: int64 (2002) vs float64 (2003-2023)",
          "Violation column has inconsistent casing: lowercase (2002-2018 mostly) vs uppercase (2019-2023 mostly)",
          "Violation column has inconsistent capitalization: 'no'/'yes' vs 'NO'/'YES' vs 'No'/'Yes'",
          "Indicator Level data type varies: int64 in 2002, float64 in other years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level column has different dtypes: int64 (2002) vs float64 (2003-2023)",
            "Violation column has inconsistent casing: lowercase (2002-2018 mostly) vs uppercase (2019-2023 mostly)"
          ],
          [
            "Violation column has inconsistent capitalization: 'no'/'yes' vs 'NO'/'YES' vs 'No'/'Yes'",
            "Indicator Level data type varies: int64 in 2002, float64 in other years"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "bacterial count per 100ml",
          "year": "year",
          "sample date": "YYYY-MM-DD",
          "community code": "identifier",
          "county code": "identifier",
          "beach name": "name of beach"
        },
        "confidence": 0.7777777777777777,
        "votes": [
          {
            "Indicator Level": "bacterial count per 100ml",
            "Year": "calendar year",
            "Sample Date": "YYYY-MM-DD"
          },
          {
            "Year": "year",
            "Sample Date": "datetime",
            "Indicator Level": "concentration_units_unspecified",
            "Community Code": "identifier",
            "County Code": "identifier"
          },
          {
            "Indicator Level": "level of indicator organism",
            "Community Code": "integer code",
            "County Code": "integer code",
            "Year": "year",
            "Sample Date": "date",
            "Beach Name": "name of beach"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Indicator Level values range from 1.0 to 800.0 in samples, but actual range unknown",
          "Year column consistent across files",
          "Sample Date format appears consistent"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Indicator Level values range from 1.0 to 800.0 in samples, but actual range unknown",
            "Year column consistent across files",
            "Sample Date format appears consistent"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "2002 file has only 650 rows vs 12k-16k rows for other years, suggesting incomplete data for 2002",
          "2002 file has 'Fresh' beach types while other files show only 'Marine' in samples",
          "Violation values have different case formatting across years (lowercase, uppercase, capitalized)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "2002 file has only 650 rows vs 12k-16k rows for other years, suggesting incomplete data for 2002",
            "2002 file has 'Fresh' beach types while other files show only 'Marine' in samples"
          ],
          [
            "Violation values have different case formatting across years (lowercase, uppercase, capitalized)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.5833333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each row represents one water sample test",
          "Beach Name should uniquely identify a beach location",
          "Year values should be between 2002-2023",
          "Violation should be binary (yes/no variants)",
          "Indicator Level should be non-negative",
          "Only include beaches where Beach Type Description == 'Marine'",
          "Only include years 2002 through 2023 inclusive",
          "A beach is safe for the entire time if it has NO records with Violation == 'yes'/'YES'/'Yes'",
          "If a beach has no data in a particular year, assume it is safe for that year",
          "Need to union all 22 yearly files",
          "Year must be between 2002 and 2023 inclusive",
          "Beach Type Description must be 'Marine'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each row represents one water sample test",
            "Beach Name should uniquely identify a beach location",
            "Year values should be between 2002-2023",
            "Violation should be binary (yes/no variants)",
            "Indicator Level should be non-negative"
          ],
          [
            "Only include beaches where Beach Type Description == 'Marine'",
            "Only include years 2002 through 2023 inclusive",
            "A beach is safe for the entire time if it has NO records with Violation == 'yes'/'YES'/'Yes'",
            "If a beach has no data in a particular year, assume it is safe for that year",
            "Need to union all 22 yearly files"
          ],
          [
            "Year must be between 2002 and 2023 inclusive",
            "Beach Type Description must be 'Marine'"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to Beach Type Description = 'Marine'",
          "Standardize Violation values to consistent case",
          "Group by Beach Name and check for any 'yes'/'YES'/'Yes' in Violation column across all years",
          "Count beaches with zero violations across all years",
          "Normalize Violation column to lowercase for consistent comparison",
          "Group by Beach Name and check if ANY(Violation == 'yes') exists",
          "Filter to only marine beaches before aggregation",
          "A beach is considered safe in a year if either there is no data for that beach in that year, or if all entries for that beach in that year have Violation = 'no' (case-insensitive)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to Beach Type Description = 'Marine'",
            "Standardize Violation values to consistent case",
            "Group by Beach Name and check for any 'yes'/'YES'/'Yes' in Violation column across all years",
            "Count beaches with zero violations across all years"
          ],
          [
            "Normalize Violation column to lowercase for consistent comparison",
            "Group by Beach Name and check if ANY(Violation == 'yes') exists",
            "Filter to only marine beaches before aggregation"
          ],
          [
            "A beach is considered safe in a year if either there is no data for that beach in that year, or if all entries for that beach in that year have Violation = 'no' (case-insensitive)"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Beach Names across different Community/County combinations",
          "Verify temporal coverage per beach (2002-2023)",
          "Test assumption: missing year data = safe for that year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Beach Names across different Community/County combinations",
            "Verify temporal coverage per beach (2002-2023)",
            "Test assumption: missing year data = safe for that year"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Should handle case-insensitive violation values",
          "Must account for beaches with partial data (treat missing years as safe per question)",
          "Output should be a single scalar count",
          "Count represents number of unique marine beaches with zero violations across all years 2002-2023",
          "Output must be a single integer value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count",
            "Should handle case-insensitive violation values",
            "Must account for beaches with partial data (treat missing years as safe per question)"
          ],
          [
            "Output should be a single scalar count",
            "Count represents number of unique marine beaches with zero violations across all years 2002-2023"
          ],
          [
            "Output must be a single integer value"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6125
  },
  "environment-hard-17": {
    "m_q": {
      "target_metric": {
        "value": "seasonal exceedance rate (percentage) for Chatham's Bucks Creek Beach in summer months (June, July, August) with the most rainfall in its area",
        "confidence": 0.3333333333333333,
        "votes": [
          "seasonal exceedance rate (percentage) for Chatham's Bucks Creek Beach in summer months (June, July, August) with the most rainfall in its area",
          "Seasonal exceedance rate (percentage with 2 decimal places) for Bucks Creek Beach in Chatham during the summer with the most rainfall",
          "Seasonal exceedance rate (percentage to 2 decimal places) of Chatham's Bucks Creek Beach in the summer (June, July, August) with the most rainfall in its area, imputing missing monthly rainfall values with the median of the month in non-missing years."
        ]
      },
      "filters": {
        "value": [
          "Community = 'Chatham'",
          "Beach Name = 'Bucks Creek Beach'",
          "Month in [6,7,8] (June, July, August)",
          "Year = year with highest summer rainfall in Chatham area",
          "Sample Date month in (June, July, August)",
          "Summer year with maximum total rainfall (Jun + Jul + Aug)",
          "Months in ['June', 'July', 'August']"
        ],
        "confidence": 0.4761904761904762,
        "votes": [
          [
            "Community = 'Chatham'",
            "Beach Name = 'Bucks Creek Beach'",
            "Month in [6,7,8] (June, July, August)",
            "Year = year with highest summer rainfall in Chatham area"
          ],
          [
            "Community = 'Chatham'",
            "Beach Name = 'Bucks Creek Beach'",
            "Sample Date month in (June, July, August)",
            "Summer year with maximum total rainfall (Jun + Jul + Aug)"
          ],
          [
            "Beach Name = 'Bucks Creek Beach'",
            "Months in ['June', 'July', 'August']"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year",
          "Month",
          "Summer season (June, July, August)"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "Year",
            "Month"
          ],
          [
            "Year",
            "Summer season (June, July, August)"
          ],
          [
            "Year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which year had the highest summer rainfall in Chatham area?",
          "What is the median monthly precipitation for each summer month across non-missing years?",
          "How many water quality samples were taken at Bucks Creek Beach during the target summer?",
          "What percentage of those samples had 'Violation' = 'yes'?",
          "Which years have water quality data for Bucks Creek Beach in Chatham?",
          "What are the summer (Jun, Jul, Aug) rainfall totals for each year in Chatham?",
          "Which summer has the maximum total rainfall?",
          "How to impute missing monthly precipitation values using median of the month across non-missing years?",
          "What is the exceedance rate for that specific summer (violations / total samples)?",
          "How to convert exceedance rate to percentage with 2 decimal places?",
          "Identify the year with the most rainfall in June, July, and August combined.",
          "Calculate the number of exceedances for Bucks Creek Beach in June, July, and August of the identified year.",
          "Calculate the total number of samples for Bucks Creek Beach in June, July, and August of the identified year.",
          "Divide the number of exceedances by the total number of samples and multiply by 100 to get the exceedance rate.",
          "Impute missing monthly rainfall values with the median of the month in non-missing years."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Which year had the highest summer rainfall in Chatham area?",
            "What is the median monthly precipitation for each summer month across non-missing years?",
            "How many water quality samples were taken at Bucks Creek Beach during the target summer?",
            "What percentage of those samples had 'Violation' = 'yes'?"
          ],
          [
            "Which years have water quality data for Bucks Creek Beach in Chatham?",
            "What are the summer (Jun, Jul, Aug) rainfall totals for each year in Chatham?",
            "Which summer has the maximum total rainfall?",
            "How to impute missing monthly precipitation values using median of the month across non-missing years?",
            "What is the exceedance rate for that specific summer (violations / total samples)?",
            "How to convert exceedance rate to percentage with 2 decimal places?"
          ],
          [
            "Identify the year with the most rainfall in June, July, and August combined.",
            "Calculate the number of exceedances for Bucks Creek Beach in June, July, and August of the identified year.",
            "Calculate the total number of samples for Bucks Creek Beach in June, July, and August of the identified year.",
            "Divide the number of exceedances by the total number of samples and multiply by 100 to get the exceedance rate.",
            "Impute missing monthly rainfall values with the median of the month in non-missing years."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "monthly_precipitations_chatham.csv",
          "water-body-testing-2016.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "monthly_precipitations_chatham.csv",
            "water-body-testing-2016.csv"
          ],
          [
            "monthly_precipitations_chatham.csv",
            "water-body-testing-2016.csv"
          ],
          [
            "monthly_precipitations_chatham.csv",
            "water-body-testing-2016.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Year column type mismatch (float64 vs int64)",
          "Different time granularity (monthly vs daily samples)",
          "Precipitation data only for Chatham area while water testing covers multiple communities",
          "Year column types differ (int64 in water-body-testing vs float64 in precipitation)",
          "Sample Date is string format, needs parsing to extract month",
          "Community in water-body-testing needs to match Chatham location"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column type mismatch (float64 vs int64)",
            "Different time granularity (monthly vs daily samples)",
            "Precipitation data only for Chatham area while water testing covers multiple communities"
          ],
          [
            "Year column types differ (int64 in water-body-testing vs float64 in precipitation)",
            "Sample Date is string format, needs parsing to extract month",
            "Community in water-body-testing needs to match Chatham location"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "jan-dec columns": "inches (precipitation)",
          "indicator level": "MPN/100mL (bacteria concentration)",
          "annual": "inches",
          "jan": "inches",
          "feb": "inches",
          "mar": "inches",
          "apr": "inches",
          "may": "inches",
          "jun": "inches",
          "jul": "inches",
          "aug": "inches",
          "sep": "inches",
          "oct": "inches",
          "nov": "inches",
          "dec": "inches"
        },
        "confidence": 0.6888888888888888,
        "votes": [
          {
            "Jan-Dec columns": "inches (precipitation)",
            "Indicator Level": "MPN/100mL (bacteria concentration)",
            "Annual": "inches (total annual precipitation)"
          },
          {
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches",
            "Indicator Level": "Enterococci count (CFU/100mL or MPN/100mL)"
          },
          {
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches",
            "Indicator Level": "CFU/100mL"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Precipitation data has missing values (empty cells)",
          "Water testing data only available for 2016, limiting temporal analysis",
          "Precipitation data covers 2000-2019 but water testing only for 2016",
          "Violation is categorical ('yes'/'no'), needs binary conversion for rate calculation",
          "Precipitation values may have missing data requiring imputation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Precipitation data has missing values (empty cells)",
            "Water testing data only available for 2016, limiting temporal analysis",
            "Precipitation data covers 2000-2019 but water testing only for 2016"
          ],
          [
            "Violation is categorical ('yes'/'no'), needs binary conversion for rate calculation",
            "Precipitation values may have missing data requiring imputation"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Precipitation data is area-wide (Chatham) while water testing is beach-specific",
          "Time periods don't fully overlap - precipitation data goes to 2019 but water testing only 2016",
          "Precipitation data spans 2000-2019, water quality data filename suggests 2016 only",
          "Need to verify year coverage overlap between datasets"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Precipitation data is area-wide (Chatham) while water testing is beach-specific",
            "Time periods don't fully overlap - precipitation data goes to 2019 but water testing only 2016"
          ],
          [
            "Precipitation data spans 2000-2019, water quality data filename suggests 2016 only",
            "Need to verify year coverage overlap between datasets"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "NaN"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 14.0,
        "confidence": 0.7333333333333334,
        "votes": [
          14.0,
          14.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Violation column values are 'yes'/'no'",
          "Organism column should be 'Enterococci' for beach water quality",
          "Beach Type Description should be 'Marine'",
          "Year values must be between 2000-2019 for precipitation data",
          "Month values must be between 1-12",
          "Only include Chatham community data",
          "Only include Bucks Creek Beach",
          "Only include summer months (June=6, July=7, August=8)",
          "Impute missing precipitation values with median of the specific month across non-missing years",
          "Identify the single summer with maximum total rainfall (Jun+Jul+Aug)",
          "Calculate exceedance rate only for that specific summer year",
          "Exceedance is determined by the 'Violation' column in water-body-testing-2016.csv.",
          "Summer months are defined as June, July, and August.",
          "Missing rainfall data should be imputed using the median for that month across all available years."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation column values are 'yes'/'no'",
            "Organism column should be 'Enterococci' for beach water quality",
            "Beach Type Description should be 'Marine'",
            "Year values must be between 2000-2019 for precipitation data",
            "Month values must be between 1-12"
          ],
          [
            "Only include Chatham community data",
            "Only include Bucks Creek Beach",
            "Only include summer months (June=6, July=7, August=8)",
            "Impute missing precipitation values with median of the specific month across non-missing years",
            "Identify the single summer with maximum total rainfall (Jun+Jul+Aug)",
            "Calculate exceedance rate only for that specific summer year"
          ],
          [
            "Exceedance is determined by the 'Violation' column in water-body-testing-2016.csv.",
            "Summer months are defined as June, July, and August.",
            "Missing rainfall data should be imputed using the median for that month across all available years."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter water testing to only Enterococci samples",
          "Extract month from Sample Date for summer filtering",
          "Calculate summer rainfall sum from Jun, Jul, Aug columns",
          "Identify year with maximum summer rainfall after imputation",
          "Extract month from Sample Date to filter summer months",
          "Calculate summer_total_rainfall = Jun + Jul + Aug (after imputation)",
          "Identify max_rainfall_summer = argmax(summer_total_rainfall)",
          "Filter water quality data to year of max_rainfall_summer",
          "Isolate Chatham data based on Community or County Description.",
          "Filter water quality data for 'Bucks Creek Beach'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Filter water testing to only Enterococci samples",
            "Extract month from Sample Date for summer filtering",
            "Calculate summer rainfall sum from Jun, Jul, Aug columns",
            "Identify year with maximum summer rainfall after imputation"
          ],
          [
            "Extract month from Sample Date to filter summer months",
            "Calculate summer_total_rainfall = Jun + Jul + Aug (after imputation)",
            "Identify max_rainfall_summer = argmax(summer_total_rainfall)",
            "Filter water quality data to year of max_rainfall_summer"
          ],
          [
            "Isolate Chatham data based on Community or County Description.",
            "Filter water quality data for 'Bucks Creek Beach'."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Median imputation for missing monthly precipitation values",
          "Percentage calculation: (violation samples / total samples) * 100",
          "Round final percentage to 2 decimal places",
          "Median imputation for missing monthly values within each month column",
          "Exceedance rate = (count of Violation='yes') / (total count of samples)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Median imputation for missing monthly precipitation values",
            "Percentage calculation: (violation samples / total samples) * 100",
            "Round final percentage to 2 decimal places"
          ],
          [
            "Median imputation for missing monthly values within each month column",
            "Exceedance rate = (count of Violation='yes') / (total count of samples)"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Percentage to 2 decimal places",
          "Only consider summer months (June, July, August)",
          "Use median of month across non-missing years for imputation",
          "Target year is the one with highest summer rainfall after imputation",
          "Output as percentage (multiply rate by 100)",
          "Round to exactly 2 decimal places",
          "Scalar numeric value",
          "Exceedance rate should be expressed as a percentage with 2 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage to 2 decimal places",
            "Only consider summer months (June, July, August)",
            "Use median of month across non-missing years for imputation",
            "Target year is the one with highest summer rainfall after imputation"
          ],
          [
            "Output as percentage (multiply rate by 100)",
            "Round to exactly 2 decimal places",
            "Scalar numeric value"
          ],
          [
            "Exceedance rate should be expressed as a percentage with 2 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6143650793650794
  },
  "environment-hard-18": {
    "m_q": {
      "target_metric": {
        "value": "Trend comparison between monthly rainfall and exceedance rate of fresh water beaches from 2020 to 2023 inclusive",
        "confidence": 0.3333333333333333,
        "votes": [
          "Trend comparison between monthly rainfall and exceedance rate of fresh water beaches from 2020 to 2023 inclusive",
          "Boolean (True/False) indicating whether exceedance rate of fresh water beaches and rainfall follow the same trend from 2020 to 2023",
          "Determine if the trend of fresh water beach exceedance rate from 2020 to 2023 follows the same trend as rainfall during the same period. Exceedance rate is the number of violations divided by the total number of samples."
        ]
      },
      "filters": {
        "value": [
          "Year >= 2020 AND Year <= 2023",
          "Beach Type Description = 'Fresh Water'",
          "Organism = 'Enterococci'",
          "Years 2020-2023 inclusive",
          "Filter out missing values after imputation",
          "Years between 2020 and 2023 (inclusive)",
          "Beach Type Description is 'Fresh Water'"
        ],
        "confidence": 0.38095238095238093,
        "votes": [
          [
            "Year >= 2020 AND Year <= 2023",
            "Beach Type Description = 'Fresh Water'",
            "Organism = 'Enterococci'"
          ],
          [
            "Years 2020-2023 inclusive",
            "Beach Type Description = 'Fresh Water'",
            "Filter out missing values after imputation"
          ],
          [
            "Years between 2020 and 2023 (inclusive)",
            "Beach Type Description is 'Fresh Water'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year",
          "Month",
          "Location"
        ],
        "confidence": 0.7777777777777778,
        "votes": [
          [
            "Year",
            "Month",
            "Location"
          ],
          [
            "Year",
            "Month"
          ],
          [
            "Year",
            "Month"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar (True/False)",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the monthly rainfall trend for each location from 2020-2023?",
          "What is the monthly exceedance rate for fresh water beaches from 2020-2023?",
          "Do rainfall and exceedance rate trends move in the same direction month-to-month?",
          "What is the exceedance rate (violations) for fresh water beaches per year from 2020-2023?",
          "What is the total or average rainfall per year from 2020-2023 across all precipitation locations?",
          "Do both metrics increase/decrease in the same direction year-over-year?",
          "How to impute missing precipitation values using median of the month across non-missing years?",
          "What constitutes a 'same trend' - do both need to move in same direction for all year transitions?",
          "Calculate monthly rainfall from 2020 to 2023, imputing missing values with the median of the month in non-missing years.",
          "Calculate the monthly exceedance rate for fresh water beaches from 2020 to 2023.",
          "Compare the trends of monthly rainfall and monthly exceedance rate from 2020 to 2023."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the monthly rainfall trend for each location from 2020-2023?",
            "What is the monthly exceedance rate for fresh water beaches from 2020-2023?",
            "Do rainfall and exceedance rate trends move in the same direction month-to-month?"
          ],
          [
            "What is the exceedance rate (violations) for fresh water beaches per year from 2020-2023?",
            "What is the total or average rainfall per year from 2020-2023 across all precipitation locations?",
            "Do both metrics increase/decrease in the same direction year-over-year?",
            "How to impute missing precipitation values using median of the month across non-missing years?",
            "What constitutes a 'same trend' - do both need to move in same direction for all year transitions?"
          ],
          [
            "Calculate monthly rainfall from 2020 to 2023, imputing missing values with the median of the month in non-missing years.",
            "Calculate the monthly exceedance rate for fresh water beaches from 2020 to 2023.",
            "Compare the trends of monthly rainfall and monthly exceedance rate from 2020 to 2023."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "monthly_precipitations_boston.csv",
          "monthly_precipitations_chatham.csv",
          "monthly_precipitations_amherst.csv",
          "monthly_precipitations_ashburnham.csv",
          "water-body-testing-2020.csv",
          "water-body-testing-2021.csv",
          "water-body-testing-2022.csv",
          "water-body-testing-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv",
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv"
          ],
          [
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Violation column has inconsistent capitalization ('No' vs 'NO') across years",
          "Annual column has missing values in precipitation files",
          "Month columns have missing values in precipitation files",
          "Violation column has mixed case values: 'No', 'NO', 'Yes', 'YES'",
          "Precipitation data in wide format (months as columns) vs beach data in long format",
          "Sample Date is string format, needs parsing to extract month"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation column has inconsistent capitalization ('No' vs 'NO') across years",
            "Annual column has missing values in precipitation files",
            "Month columns have missing values in precipitation files"
          ],
          [
            "Violation column has mixed case values: 'No', 'NO', 'Yes', 'YES'",
            "Precipitation data in wide format (months as columns) vs beach data in long format",
            "Sample Date is string format, needs parsing to extract month"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "jan-dec columns in precipitation files": "inches",
          "indicator level": "CFU/100ml",
          "annual": "inches",
          "jan-dec columns": "inches of precipitation",
          "year": "calendar year",
          "jan": "inches",
          "feb": "inches",
          "mar": "inches",
          "apr": "inches",
          "may": "inches",
          "jun": "inches",
          "jul": "inches",
          "aug": "inches",
          "sep": "inches",
          "oct": "inches",
          "nov": "inches",
          "dec": "inches"
        },
        "confidence": 0.4117647058823528,
        "votes": [
          {
            "Jan-Dec columns in precipitation files": "inches",
            "Indicator Level": "CFU/100ml",
            "Annual": "inches"
          },
          {
            "Jan-Dec columns": "inches of precipitation",
            "Annual": "inches of precipitation",
            "Indicator Level": "colony forming units or MPN per 100ml",
            "Year": "calendar year"
          },
          {
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches",
            "Indicator Level": "cfu/100ml"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Precipitation data has missing values that need imputation with median of month in non-missing years",
          "Indicator Level values vary widely (5-364 CFU/100ml)",
          "Precipitation measured monthly, beach testing occurs multiple times per month",
          "Need to aggregate beach violations to yearly exceedance rate"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Precipitation data has missing values that need imputation with median of month in non-missing years",
            "Indicator Level values vary widely (5-364 CFU/100ml)"
          ],
          [
            "Precipitation measured monthly, beach testing occurs multiple times per month",
            "Need to aggregate beach violations to yearly exceedance rate"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Precipitation data covers 2000-2019 but question only needs 2020-2023",
          "Precipitation files have different missing value patterns across locations",
          "Multiple precipitation measurement locations (Boston, Chatham, Amherst, Ashburnham) - unclear which to use or if should aggregate",
          "Missing precipitation values scattered across different months and years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Precipitation data covers 2000-2019 but question only needs 2020-2023",
            "Precipitation files have different missing value patterns across locations"
          ],
          [
            "Multiple precipitation measurement locations (Boston, Chatham, Amherst, Ashburnham) - unclear which to use or if should aggregate",
            "Missing precipitation values scattered across different months and years"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.111111111111111,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " "
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 14.0,
        "confidence": 1.0,
        "votes": [
          14.0,
          14.0,
          14.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year must be between 2020 and 2023 inclusive",
          "Only fresh water beaches should be considered",
          "Missing precipitation values must be imputed with median of month in non-missing years",
          "Exceedance rate should be calculated as proportion of samples with Violation = 'Yes' or 'YES'",
          "Only include Beach Type Description = 'Fresh Water'",
          "Years must be in range [2020, 2023]",
          "Impute missing precipitation values with median of that month across non-missing years",
          "Exceedance rate = (number of violations) / (total samples) per year",
          "Impute missing rainfall values using the median of the month across all available years.",
          "Calculate exceedance rate as the number of 'Violation' equal to 'Yes' divided by the total number of samples for each month and year for fresh water beaches.",
          "The 'Violation' column in water-body-testing files can have values 'Yes', 'No', 'YES', 'NO'. Need to standardize to 'Yes' and 'No'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Year must be between 2020 and 2023 inclusive",
            "Only fresh water beaches should be considered",
            "Missing precipitation values must be imputed with median of month in non-missing years",
            "Exceedance rate should be calculated as proportion of samples with Violation = 'Yes' or 'YES'"
          ],
          [
            "Only include Beach Type Description = 'Fresh Water'",
            "Years must be in range [2020, 2023]",
            "Impute missing precipitation values with median of that month across non-missing years",
            "Exceedance rate = (number of violations) / (total samples) per year"
          ],
          [
            "Impute missing rainfall values using the median of the month across all available years.",
            "Calculate exceedance rate as the number of 'Violation' equal to 'Yes' divided by the total number of samples for each month and year for fresh water beaches.",
            "The 'Violation' column in water-body-testing files can have values 'Yes', 'No', 'YES', 'NO'. Need to standardize to 'Yes' and 'No'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract month from Sample Date to match precipitation month columns",
          "Filter to only Enterococci organism measurements",
          "Group by Community/County to match precipitation locations",
          "Extract year and month from Sample Date for time-based aggregation",
          "Standardize Violation column to uppercase or lowercase for consistent filtering",
          "Calculate yearly exceedance rate for fresh water beaches only",
          "Calculate yearly total or average rainfall across all locations after imputation",
          "Beach Type Description = 'Fresh Water'",
          "Year >= 2020 and Year <= 2023"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract month from Sample Date to match precipitation month columns",
            "Filter to only Enterococci organism measurements",
            "Group by Community/County to match precipitation locations"
          ],
          [
            "Extract year and month from Sample Date for time-based aggregation",
            "Standardize Violation column to uppercase or lowercase for consistent filtering",
            "Calculate yearly exceedance rate for fresh water beaches only",
            "Calculate yearly total or average rainfall across all locations after imputation"
          ],
          [
            "Beach Type Description = 'Fresh Water'",
            "Year >= 2020 and Year <= 2023"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Correlation test between monthly rainfall and monthly exceedance rate",
          "Trend direction comparison (both increasing/decreasing)",
          "Compare year-over-year directional changes (increase/decrease) for both metrics",
          "Trend alignment check: if rainfall increases from year N to N+1, does exceedance rate also increase",
          "Compare the trends of rainfall and exceedance rate using correlation or visual inspection."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Correlation test between monthly rainfall and monthly exceedance rate",
            "Trend direction comparison (both increasing/decreasing)"
          ],
          [
            "Compare year-over-year directional changes (increase/decrease) for both metrics",
            "Trend alignment check: if rainfall increases from year N to N+1, does exceedance rate also increase"
          ],
          [
            "Compare the trends of rainfall and exceedance rate using correlation or visual inspection."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Final answer must be True or False",
          "Must consider all four precipitation locations",
          "Must aggregate across all fresh water beaches",
          "Final answer must be exactly 'True' or 'False'",
          "Trends must match for all year transitions (2020->2021, 2021->2022, 2022->2023)",
          "Output a boolean value (True or False) indicating whether the trends are the same."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Final answer must be True or False",
            "Must consider all four precipitation locations",
            "Must aggregate across all fresh water beaches"
          ],
          [
            "Final answer must be exactly 'True' or 'False'",
            "Trends must match for all year transitions (2020->2021, 2021->2022, 2022->2023)"
          ],
          [
            "Output a boolean value (True or False) indicating whether the trends are the same."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6174136321195146
  },
  "environment-hard-19": {
    "m_q": {
      "target_metric": {
        "value": "Compare the year with the highest average exceedance rate difference (with previous year) against the year with the highest total rainfall difference (with previous year) for marine beaches, and determine if they are the same (True) or different (False)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Compare the year with the highest average exceedance rate difference (with previous year) against the year with the highest total rainfall difference (with previous year) for marine beaches, and determine if they are the same (True) or different (False)",
          "Boolean comparison: whether the year with highest average exceedance rate difference (year-over-year) equals the year with highest total rainfall difference (year-over-year) for marine beaches in June-July-August",
          "Compare the year with the highest average exceedance rate difference (with the previous year) for marine beaches to the year with the highest total rainfall difference (with the previous year) in June, July, August in Boston and Chatham."
        ]
      },
      "filters": {
        "value": [
          "Beach Type Description = 'Marine'",
          "Month in [6,7,8] for rainfall data",
          "Location in ['Boston', 'Chatham'] for rainfall data",
          "Months in {June, July, August}",
          "Communities in {Boston, Chatham} for rainfall data",
          "Rainfall months are June, July, August",
          "Rainfall locations are Boston and Chatham"
        ],
        "confidence": 0.42857142857142855,
        "votes": [
          [
            "Beach Type Description = 'Marine'",
            "Month in [6,7,8] for rainfall data",
            "Location in ['Boston', 'Chatham'] for rainfall data"
          ],
          [
            "Beach Type Description = 'Marine'",
            "Months in {June, July, August}",
            "Communities in {Boston, Chatham} for rainfall data"
          ],
          [
            "Beach Type Description = 'Marine'",
            "Rainfall months are June, July, August",
            "Rainfall locations are Boston and Chatham"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year",
          "Beach Name",
          "Community",
          "Beach (for exceedance rate calculation)",
          "Community (for rainfall data)"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [
            "Year",
            "Beach Name",
            "Community"
          ],
          [
            "Year",
            "Beach (for exceedance rate calculation)",
            "Community (for rainfall data)"
          ],
          [
            "Year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the average exceedance rate per year for marine beaches?",
          "What is the year-over-year difference in average exceedance rates?",
          "What is the total rainfall for June, July, August in Boston and Chatham per year?",
          "What is the year-over-year difference in total rainfall?",
          "Which year has the maximum average exceedance rate difference?",
          "Which year has the maximum total rainfall difference?",
          "Are these years the same?",
          "What is the exceedance rate per year for marine beaches?",
          "What is the year-over-year difference in average exceedance rate?",
          "Which year has the highest exceedance rate difference?",
          "What is the total rainfall (Jun+Jul+Aug) in Boston and Chatham per year?",
          "Which year has the highest rainfall difference?",
          "Are these two years the same?",
          "Calculate the average exceedance rate for marine beaches for each year.",
          "Calculate the difference in average exceedance rate between consecutive years.",
          "Identify the year with the highest difference in average exceedance rate.",
          "Calculate the total rainfall for June, July, and August for each year in Boston and Chatham.",
          "Impute missing rainfall values with the median of the month in non-missing years.",
          "Calculate the difference in total rainfall between consecutive years.",
          "Identify the year with the highest difference in total rainfall.",
          "Compare the year with the highest exceedance rate difference to the year with the highest rainfall difference."
        ],
        "confidence": 0.3492063492063491,
        "votes": [
          [
            "What is the average exceedance rate per year for marine beaches?",
            "What is the year-over-year difference in average exceedance rates?",
            "What is the total rainfall for June, July, August in Boston and Chatham per year?",
            "What is the year-over-year difference in total rainfall?",
            "Which year has the maximum average exceedance rate difference?",
            "Which year has the maximum total rainfall difference?",
            "Are these years the same?"
          ],
          [
            "What is the exceedance rate per year for marine beaches?",
            "What is the year-over-year difference in average exceedance rate?",
            "Which year has the highest exceedance rate difference?",
            "What is the total rainfall (Jun+Jul+Aug) in Boston and Chatham per year?",
            "What is the year-over-year difference in total rainfall?",
            "Which year has the highest rainfall difference?",
            "Are these two years the same?"
          ],
          [
            "Calculate the average exceedance rate for marine beaches for each year.",
            "Calculate the difference in average exceedance rate between consecutive years.",
            "Identify the year with the highest difference in average exceedance rate.",
            "Calculate the total rainfall for June, July, and August for each year in Boston and Chatham.",
            "Impute missing rainfall values with the median of the month in non-missing years.",
            "Calculate the difference in total rainfall between consecutive years.",
            "Identify the year with the highest difference in total rainfall.",
            "Compare the year with the highest exceedance rate difference to the year with the highest rainfall difference."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "water-body-testing-2002.csv",
          "water-body-testing-2003.csv",
          "water-body-testing-2004.csv",
          "water-body-testing-2005.csv",
          "water-body-testing-2006.csv",
          "water-body-testing-2007.csv",
          "water-body-testing-2008.csv",
          "water-body-testing-2009.csv",
          "water-body-testing-2010.csv",
          "water-body-testing-2011.csv",
          "water-body-testing-2012.csv",
          "water-body-testing-2013.csv",
          "water-body-testing-2014.csv",
          "water-body-testing-2015.csv",
          "water-body-testing-2016.csv",
          "water-body-testing-2017.csv",
          "water-body-testing-2018.csv",
          "water-body-testing-2019.csv",
          "water-body-testing-2020.csv",
          "water-body-testing-2021.csv",
          "water-body-testing-2022.csv",
          "water-body-testing-2023.csv",
          "monthly_precipitations_boston.csv",
          "monthly_precipitations_chatham.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv",
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv"
          ],
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv",
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv"
          ],
          [
            "water-body-testing-2002.csv",
            "water-body-testing-2003.csv",
            "water-body-testing-2004.csv",
            "water-body-testing-2005.csv",
            "water-body-testing-2006.csv",
            "water-body-testing-2007.csv",
            "water-body-testing-2008.csv",
            "water-body-testing-2009.csv",
            "water-body-testing-2010.csv",
            "water-body-testing-2011.csv",
            "water-body-testing-2012.csv",
            "water-body-testing-2013.csv",
            "water-body-testing-2014.csv",
            "water-body-testing-2015.csv",
            "water-body-testing-2016.csv",
            "water-body-testing-2017.csv",
            "water-body-testing-2018.csv",
            "water-body-testing-2019.csv",
            "water-body-testing-2020.csv",
            "water-body-testing-2021.csv",
            "water-body-testing-2022.csv",
            "water-body-testing-2023.csv",
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Violation column has inconsistent capitalization (yes/no vs YES/NO vs Yes/No vs NO/YES)",
          "Indicator Level column has different dtypes (int64 vs float64) across files",
          "Sample Date format appears consistent but should be verified",
          "Violation column has mixed case: 'yes/no', 'YES/NO', 'Yes/No', 'NO/YES'",
          "Multiple water-body-testing files need to be unioned vertically",
          "Violation column has different values (yes/no vs YES/NO) across water-body-testing files.",
          "Column names are inconsistent across water-body-testing files (e.g., 'Community Code' vs 'CommunityCode').",
          "Violation column has different casing (e.g., 'no' vs 'No')."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Violation column has inconsistent capitalization (yes/no vs YES/NO vs Yes/No vs NO/YES)",
            "Indicator Level column has different dtypes (int64 vs float64) across files",
            "Sample Date format appears consistent but should be verified"
          ],
          [
            "Violation column has mixed case: 'yes/no', 'YES/NO', 'Yes/No', 'NO/YES'",
            "Multiple water-body-testing files need to be unioned vertically"
          ],
          [
            "Violation column has different values (yes/no vs YES/NO) across water-body-testing files.",
            "Column names are inconsistent across water-body-testing files (e.g., 'Community Code' vs 'CommunityCode').",
            "Violation column has different casing (e.g., 'no' vs 'No')."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "indicator level": "bacteria count (units unspecified)",
          "jan-dec columns in precipitation files": "inches",
          "annual columns": "inches",
          "jun": "inches of precipitation",
          "jul": "inches of precipitation",
          "aug": "inches of precipitation",
          "sample date": "datetime",
          "indicatorlevel": "Enterococci count",
          "jan": "inches",
          "feb": "inches",
          "mar": "inches",
          "apr": "inches",
          "may": "inches",
          "sep": "inches",
          "oct": "inches",
          "nov": "inches",
          "dec": "inches",
          "annual": "inches"
        },
        "confidence": 0.4074074074074072,
        "votes": [
          {
            "Indicator Level": "bacteria count (units unspecified)",
            "Jan-Dec columns in precipitation files": "inches",
            "Annual columns": "inches"
          },
          {
            "Indicator Level": "CFU or MPN per 100mL",
            "Jun": "inches of precipitation",
            "Jul": "inches of precipitation",
            "Aug": "inches of precipitation",
            "Sample Date": "datetime"
          },
          {
            "IndicatorLevel": "Enterococci count",
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Precipitation data has missing values that need imputation with median of month across non-missing years",
          "Water testing data spans multiple years with varying sample counts per year",
          "Exceedance rate is a percentage/proportion to be calculated from Violation column",
          "Rainfall differences are absolute differences in inches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Precipitation data has missing values that need imputation with median of month across non-missing years",
            "Water testing data spans multiple years with varying sample counts per year"
          ],
          [
            "Exceedance rate is a percentage/proportion to be calculated from Violation column",
            "Rainfall differences are absolute differences in inches"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Water testing data covers 2002-2023, precipitation data covers 2000-2019 (incomplete for some years)",
          "Precipitation data has missing annual totals for some years",
          "Boston and Chatham are communities in water-body-testing but separate precipitation files",
          "Missing values in precipitation data need median imputation by month across years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Water testing data covers 2002-2023, precipitation data covers 2000-2019 (incomplete for some years)",
            "Precipitation data has missing annual totals for some years"
          ],
          [
            "Boston and Chatham are communities in water-body-testing but separate precipitation files",
            "Missing values in precipitation data need median imputation by month across years"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only marine beaches (Beach Type Description = 'Marine') should be considered",
          "Only rainfall in June, July, August should be summed",
          "Only Boston and Chatham locations for rainfall",
          "Missing rainfall values must be imputed with median of that month across non-missing years",
          "Year-over-year differences require consecutive year data",
          "Only Marine beaches (Beach Type Description = 'Marine')",
          "Only June, July, August months (extract from Sample Date or use Jun/Jul/Aug columns)",
          "Only Boston and Chatham communities for rainfall calculation",
          "Exceedance rate = count(Violation='yes')/count(total samples) per year",
          "Must impute missing precipitation values with median of that month across non-missing years",
          "Years in water-body-testing files should match years in rainfall files.",
          "Ensure that the 'Sample Date' column is correctly parsed as a date.",
          "Ensure that the 'Beach Type Description' column only contains 'Marine' values after filtering."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only marine beaches (Beach Type Description = 'Marine') should be considered",
            "Only rainfall in June, July, August should be summed",
            "Only Boston and Chatham locations for rainfall",
            "Missing rainfall values must be imputed with median of that month across non-missing years",
            "Year-over-year differences require consecutive year data"
          ],
          [
            "Only Marine beaches (Beach Type Description = 'Marine')",
            "Only June, July, August months (extract from Sample Date or use Jun/Jul/Aug columns)",
            "Only Boston and Chatham communities for rainfall calculation",
            "Exceedance rate = count(Violation='yes')/count(total samples) per year",
            "Must impute missing precipitation values with median of that month across non-missing years"
          ],
          [
            "Years in water-body-testing files should match years in rainfall files.",
            "Ensure that the 'Sample Date' column is correctly parsed as a date.",
            "Ensure that the 'Beach Type Description' column only contains 'Marine' values after filtering."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter water testing data to Beach Type Description = 'Marine'",
          "Extract month from Sample Date for potential seasonal analysis",
          "Calculate exceedance rate from Violation column (yes/total samples)",
          "Extract month from Sample Date to filter June (6), July (7), August (8)",
          "Calculate year-over-year differences requires sorting by year",
          "Previous year calculations require lag operations on sorted years",
          "Create a 'Month' column from 'Sample Date' to filter for June, July, and August.",
          "Filter water-body-testing data for 'Beach Type Description' equal to 'Marine'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter water testing data to Beach Type Description = 'Marine'",
            "Extract month from Sample Date for potential seasonal analysis",
            "Calculate exceedance rate from Violation column (yes/total samples)"
          ],
          [
            "Extract month from Sample Date to filter June (6), July (7), August (8)",
            "Calculate year-over-year differences requires sorting by year",
            "Previous year calculations require lag operations on sorted years"
          ],
          [
            "Create a 'Month' column from 'Sample Date' to filter for June, July, and August.",
            "Filter water-body-testing data for 'Beach Type Description' equal to 'Marine'."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Calculate average exceedance rate per year",
          "Calculate year-over-year differences in average exceedance rates",
          "Calculate total summer rainfall per year for Boston and Chatham",
          "Calculate year-over-year differences in total summer rainfall",
          "Identify maximum differences for both metrics",
          "Calculate average exceedance rate per year across all marine beaches",
          "Calculate year-over-year difference in average exceedance rate",
          "Identify year with maximum absolute difference in exceedance rate",
          "Sum rainfall (Jun+Jul+Aug) for Boston and Chatham per year",
          "Impute missing rainfall values with median by month",
          "Calculate year-over-year difference in total rainfall",
          "Identify year with maximum absolute difference in rainfall",
          "Compare the two years for equality"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate average exceedance rate per year",
            "Calculate year-over-year differences in average exceedance rates",
            "Calculate total summer rainfall per year for Boston and Chatham",
            "Calculate year-over-year differences in total summer rainfall",
            "Identify maximum differences for both metrics"
          ],
          [
            "Calculate average exceedance rate per year across all marine beaches",
            "Calculate year-over-year difference in average exceedance rate",
            "Identify year with maximum absolute difference in exceedance rate",
            "Sum rainfall (Jun+Jul+Aug) for Boston and Chatham per year",
            "Impute missing rainfall values with median by month",
            "Calculate year-over-year difference in total rainfall",
            "Identify year with maximum absolute difference in rainfall",
            "Compare the two years for equality"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Final answer must be True or False",
          "Final answer must be 'True' or 'False'",
          "True if years match, False if years differ",
          "Output should be a boolean value (True or False)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Final answer must be True or False"
          ],
          [
            "Final answer must be 'True' or 'False'",
            "True if years match, False if years differ"
          ],
          [
            "Output should be a boolean value (True or False)."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.615925925925926
  },
  "environment-hard-20": {
    "m_q": {
      "target_metric": {
        "value": "Three beaches with highest pollution levels (Indicator Level) in 2015 for the city with lowest summer (June, July, August) rainfall in 2015",
        "confidence": 0.3333333333333333,
        "votes": [
          "Three beaches with highest pollution levels (Indicator Level) in 2015 for the city with lowest summer (June, July, August) rainfall in 2015",
          "Three most polluted beaches (highest average Indicator Level) in 2015 from the city with least summer rainfall",
          "Three most polluted beaches in the city with the least rainfall in the summer of 2015, ranked by average 'Indicator Level'"
        ]
      },
      "filters": {
        "value": [
          "Year = 2015",
          "Summer months (Jun, Jul, Aug)",
          "Focus on city with minimum summer precipitation",
          "Summer months (June, July, August)",
          "Months are June, July, and August"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [
            "Year = 2015",
            "Summer months (Jun, Jul, Aug)",
            "Focus on city with minimum summer precipitation"
          ],
          [
            "Year = 2015",
            "Summer months (June, July, August)"
          ],
          [
            "Year = 2015",
            "Months are June, July, and August"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Community",
          "Beach Name",
          "City (from precipitation files)",
          "Beach Name (from water-body-testing-2015.csv)",
          "Community (from water-body-testing-2015.csv)"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [
            "Community",
            "Beach Name"
          ],
          [
            "City (from precipitation files)",
            "Beach Name (from water-body-testing-2015.csv)",
            "Community (from water-body-testing-2015.csv)"
          ],
          [
            "Community",
            "Beach Name"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which city had the least rainfall in summer 2015?",
          "What are the pollution levels (Indicator Level) for beaches in that city in 2015?",
          "Which three beaches have the highest pollution levels?",
          "What is the total summer (Jun+Jul+Aug) rainfall for each city in 2015?",
          "Which city has the minimum summer rainfall in 2015?",
          "What beaches exist in that city in the water-body-testing-2015.csv file?",
          "What is the average Indicator Level for each beach in that city in 2015?",
          "Which three beaches have the highest average Indicator Level?",
          "Calculate total rainfall for each city in June, July, and August of 2015.",
          "Identify the city with the least total rainfall in the summer of 2015.",
          "Calculate the average 'Indicator Level' for each beach in the selected city in 2015.",
          "Rank the beaches in the selected city by their average 'Indicator Level' in descending order.",
          "Select the top three beaches from the ranked list."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which city had the least rainfall in summer 2015?",
            "What are the pollution levels (Indicator Level) for beaches in that city in 2015?",
            "Which three beaches have the highest pollution levels?"
          ],
          [
            "What is the total summer (Jun+Jul+Aug) rainfall for each city in 2015?",
            "Which city has the minimum summer rainfall in 2015?",
            "What beaches exist in that city in the water-body-testing-2015.csv file?",
            "What is the average Indicator Level for each beach in that city in 2015?",
            "Which three beaches have the highest average Indicator Level?"
          ],
          [
            "Calculate total rainfall for each city in June, July, and August of 2015.",
            "Identify the city with the least total rainfall in the summer of 2015.",
            "Calculate the average 'Indicator Level' for each beach in the selected city in 2015.",
            "Rank the beaches in the selected city by their average 'Indicator Level' in descending order.",
            "Select the top three beaches from the ranked list."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "monthly_precipitations_boston.csv",
          "monthly_precipitations_chatham.csv",
          "monthly_precipitations_amherst.csv",
          "monthly_precipitations_ashburnham.csv",
          "water-body-testing-2015.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv",
            "water-body-testing-2015.csv"
          ],
          [
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv",
            "water-body-testing-2015.csv"
          ],
          [
            "monthly_precipitations_boston.csv",
            "monthly_precipitations_chatham.csv",
            "monthly_precipitations_amherst.csv",
            "monthly_precipitations_ashburnham.csv",
            "water-body-testing-2015.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Precipitation files have city names in filenames but not in data columns",
          "Community names in water-body-testing-2015.csv need to match precipitation city names",
          "Year columns have different types: float64 in precipitation files vs int64 in water testing",
          "City name must be extracted from precipitation filename and matched to Community column",
          "Precipitation data has monthly columns while beach data has Sample Date requiring temporal alignment"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Precipitation files have city names in filenames but not in data columns",
            "Community names in water-body-testing-2015.csv need to match precipitation city names",
            "Year columns have different types: float64 in precipitation files vs int64 in water testing"
          ],
          [
            "City name must be extracted from precipitation filename and matched to Community column",
            "Precipitation data has monthly columns while beach data has Sample Date requiring temporal alignment"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "jan-dec columns in precipitation files": "inches",
          "indicator level": "MPN/100mL (Most Probable Number per 100 milliliters)",
          "annual": "inches",
          "jan": "inches",
          "feb": "inches",
          "mar": "inches",
          "apr": "inches",
          "may": "inches",
          "jun": "inches",
          "jul": "inches",
          "aug": "inches",
          "sep": "inches",
          "oct": "inches",
          "nov": "inches",
          "dec": "inches"
        },
        "confidence": 0.6888888888888888,
        "votes": [
          {
            "Jan-Dec columns in precipitation files": "inches",
            "Indicator Level": "MPN/100mL (Most Probable Number per 100 milliliters)",
            "Annual": "inches"
          },
          {
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches",
            "Indicator Level": "colony forming units per 100ml or similar microbial count"
          },
          {
            "Jan": "inches",
            "Feb": "inches",
            "Mar": "inches",
            "Apr": "inches",
            "May": "inches",
            "Jun": "inches",
            "Jul": "inches",
            "Aug": "inches",
            "Sep": "inches",
            "Oct": "inches",
            "Nov": "inches",
            "Dec": "inches",
            "Annual": "inches",
            "Indicator Level": "cfu/100mL"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Missing values in precipitation data (empty cells)",
          "Multiple samples per beach may need aggregation",
          "Different number of precipitation measurements per city",
          "Precipitation values are continuous floats while Indicator Level may have different ranges",
          "Missing values present in precipitation data for some months"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Missing values in precipitation data (empty cells)",
            "Multiple samples per beach may need aggregation",
            "Different number of precipitation measurements per city"
          ],
          [
            "Precipitation values are continuous floats while Indicator Level may have different ranges",
            "Missing values present in precipitation data for some months"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Precipitation data covers multiple years but only 2015 needed",
          "Water testing has multiple samples per beach per year - need aggregation method",
          "City name case sensitivity between filename and Community column",
          "Temporal granularity mismatch: monthly precipitation vs daily beach samples"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Precipitation data covers multiple years but only 2015 needed",
            "Water testing has multiple samples per beach per year - need aggregation method"
          ],
          [
            "City name case sensitivity between filename and Community column",
            "Temporal granularity mismatch: monthly precipitation vs daily beach samples"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "NaN"
        ],
        "confidence": 0.9166666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " "
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 14.0,
        "confidence": 1.0,
        "votes": [
          14.0,
          14.0,
          14.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only consider 2015 data",
          "Summer defined as June, July, August",
          "Pollution measured by Indicator Level",
          "Beaches must be in the identified city",
          "Year must equal 2015 in both precipitation data and beach testing data",
          "Summer months are defined as June, July, and August only",
          "Pollution level measured by Indicator Level column",
          "Must identify exactly three beaches ranked by pollution level",
          "City matching requires exact string comparison between filename-derived city and Community field",
          "Ensure that the 'Year' column is an integer in both precipitation and water quality files.",
          "Handle missing values in precipitation data appropriately (e.g., imputation or exclusion)."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Only consider 2015 data",
            "Summer defined as June, July, August",
            "Pollution measured by Indicator Level",
            "Beaches must be in the identified city"
          ],
          [
            "Year must equal 2015 in both precipitation data and beach testing data",
            "Summer months are defined as June, July, and August only",
            "Pollution level measured by Indicator Level column",
            "Must identify exactly three beaches ranked by pollution level",
            "City matching requires exact string comparison between filename-derived city and Community field"
          ],
          [
            "Ensure that the 'Year' column is an integer in both precipitation and water quality files.",
            "Handle missing values in precipitation data appropriately (e.g., imputation or exclusion)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter precipitation data to Year = 2015",
          "Sum Jun, Jul, Aug precipitation for each city",
          "Filter water testing to Year = 2015 and matching Community",
          "Aggregate beach pollution levels (max or average Indicator Level)",
          "Filter precipitation data: Year = 2015",
          "Filter beach data: Year = 2015",
          "Calculate summer_rainfall = Jun + Jul + Aug for each city",
          "Identify city_with_min_rainfall = city with minimum summer_rainfall",
          "Filter beach data: Community = city_with_min_rainfall",
          "Filter water quality data for the year 2015.",
          "Filter precipitation data for the year 2015.",
          "Filter water quality data for the city with the least rainfall."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter precipitation data to Year = 2015",
            "Sum Jun, Jul, Aug precipitation for each city",
            "Filter water testing to Year = 2015 and matching Community",
            "Aggregate beach pollution levels (max or average Indicator Level)"
          ],
          [
            "Filter precipitation data: Year = 2015",
            "Filter beach data: Year = 2015",
            "Calculate summer_rainfall = Jun + Jul + Aug for each city",
            "Identify city_with_min_rainfall = city with minimum summer_rainfall",
            "Filter beach data: Community = city_with_min_rainfall"
          ],
          [
            "Filter water quality data for the year 2015.",
            "Filter precipitation data for the year 2015.",
            "Filter water quality data for the city with the least rainfall."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Identify city with minimum summer precipitation",
          "Rank beaches by pollution level",
          "Select top 3 beaches",
          "Aggregate Indicator Level by beach using mean or median",
          "Handle missing Indicator Level values appropriately",
          "Rank beaches by aggregated pollution metric in descending order"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify city with minimum summer precipitation",
            "Rank beaches by pollution level",
            "Select top 3 beaches"
          ],
          [
            "Aggregate Indicator Level by beach using mean or median",
            "Handle missing Indicator Level values appropriately",
            "Rank beaches by aggregated pollution metric in descending order"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of 3 beach names",
          "Should include pollution levels",
          "Should identify the city with least rainfall",
          "Return list of three beach names",
          "Beaches ordered from most to least polluted",
          "Must correspond to the city with least summer rainfall in 2015",
          "List of the three most polluted beaches with their average 'Indicator Level'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of 3 beach names",
            "Should include pollution levels",
            "Should identify the city with least rainfall"
          ],
          [
            "Return list of three beach names",
            "Beaches ordered from most to least polluted",
            "Must correspond to the city with least summer rainfall in 2015"
          ],
          [
            "List of the three most polluted beaches with their average 'Indicator Level'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6269444444444445
  },
  "environment-hard-7": {
    "m_q": {
      "target_metric": {
        "value": "Difference in bacterial exceedance rates (percentage points) between communities with >50% EJ populations and communities with <25% EJ populations, calculated as: (exceedance_rate_high_EJ - exceedance_rate_low_EJ) \u00d7 100, rounded to 2 decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "Difference in bacterial exceedance rates (percentage points) between communities with >50% EJ populations and communities with <25% EJ populations, calculated as: (exceedance_rate_high_EJ - exceedance_rate_low_EJ) \u00d7 100, rounded to 2 decimal places",
          "Difference in bacterial exceedance rates (violation rates) between communities with >50% EJ populations and communities with <25% EJ populations for marine beach samples in 2023, to 2 decimal places",
          "Difference in bacterial exceedance rates (to 2 decimal places) for marine beach samples collected in 2023 between communities with >50% EJ populations and those with <25% EJ populations."
        ]
      },
      "filters": {
        "value": [
          "Year = 2023",
          "Beach Type Description = 'Marine'",
          "EJ population categories: >50% and <25%",
          "Year == 2023",
          "Beach Type Description == 'Marine'",
          "Violation field is not null",
          "Percent of population in EJ BGs > 50%",
          "Percent of population in EJ BGs < 25%"
        ],
        "confidence": 0.4166666666666667,
        "votes": [
          [
            "Year = 2023",
            "Beach Type Description = 'Marine'",
            "EJ population categories: >50% and <25%"
          ],
          [
            "Year == 2023",
            "Beach Type Description == 'Marine'",
            "Violation field is not null"
          ],
          [
            "Year = 2023",
            "Beach Type Description = 'Marine'",
            "Percent of population in EJ BGs > 50%",
            "Percent of population in EJ BGs < 25%"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Community",
          "EJ population category (>50% vs <25%)",
          "EJ population group (>50% vs <25%)"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "Community",
            "EJ population category (>50% vs <25%)"
          ],
          [
            "EJ population group (>50% vs <25%)",
            "Community"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the bacterial exceedance rate for communities with >50% EJ populations?",
          "What is the bacterial exceedance rate for communities with <25% EJ populations?",
          "How are communities categorized by EJ population percentage?",
          "How is bacterial exceedance defined (Violation = 'Yes')?",
          "Which samples are from marine beaches in 2023?",
          "What is the bacterial exceedance rate (percentage of samples with Violation='Yes') for marine beaches in communities with >50% EJ population in 2023?",
          "What is the bacterial exceedance rate (percentage of samples with Violation='Yes') for marine beaches in communities with <25% EJ population in 2023?",
          "What is the difference between these two rates to 2 decimal places?",
          "Calculate the exceedance rate for communities with >50% EJ populations.",
          "Calculate the exceedance rate for communities with <25% EJ populations.",
          "Subtract the two exceedance rates and round to 2 decimal places."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the bacterial exceedance rate for communities with >50% EJ populations?",
            "What is the bacterial exceedance rate for communities with <25% EJ populations?",
            "How are communities categorized by EJ population percentage?",
            "How is bacterial exceedance defined (Violation = 'Yes')?",
            "Which samples are from marine beaches in 2023?"
          ],
          [
            "What is the bacterial exceedance rate (percentage of samples with Violation='Yes') for marine beaches in communities with >50% EJ population in 2023?",
            "What is the bacterial exceedance rate (percentage of samples with Violation='Yes') for marine beaches in communities with <25% EJ population in 2023?",
            "What is the difference between these two rates to 2 decimal places?"
          ],
          [
            "Calculate the exceedance rate for communities with >50% EJ populations.",
            "Calculate the exceedance rate for communities with <25% EJ populations.",
            "Subtract the two exceedance rates and round to 2 decimal places."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "environmental-justice-populations.csv",
          "water-body-testing-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "environmental-justice-populations.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "environmental-justice-populations.csv",
            "water-body-testing-2023.csv"
          ],
          [
            "environmental-justice-populations.csv",
            "water-body-testing-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Municipality name format may differ between files (e.g., 'Provincetown' vs 'Provincetown')",
          "EJ data has 187 municipalities while water testing has many communities - potential mismatch",
          "Community vs Municipality column naming difference"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Municipality name format may differ between files (e.g., 'Provincetown' vs 'Provincetown')",
            "EJ data has 187 municipalities while water testing has many communities - potential mismatch"
          ],
          [
            "Community vs Municipality column naming difference"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "percent of population in ej bgs": "percentage",
          "indicator level": "bacterial count units (CFU/100mL)",
          "number of ej block groups": "count",
          "total population": "count",
          "exceedance_rate": "percentage"
        },
        "confidence": 0.6000000000000001,
        "votes": [
          {
            "Percent of population in EJ BGs": "percentage",
            "Indicator Level": "bacterial count units (CFU/100mL)",
            "Number of EJ block groups": "count",
            "Total population": "count"
          },
          {
            "Percent of population in EJ BGs": "percentage",
            "Indicator Level": "colony forming units or MPN per 100mL",
            "exceedance_rate": "percentage"
          },
          {
            "Percent of population in EJ BGs": "%",
            "Indicator Level": "CFU/100mL"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "EJ percentages range 0-100%, need to categorize into >50% and <25% groups",
          "Indicator Level values vary widely (5-178 in sample)",
          "Percent of population in EJ BGs appears to be stored as percentage (0-100) not decimal (0-1)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "EJ percentages range 0-100%, need to categorize into >50% and <25% groups",
            "Indicator Level values vary widely (5-178 in sample)"
          ],
          [
            "Percent of population in EJ BGs appears to be stored as percentage (0-100) not decimal (0-1)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "EJ data uses 'Municipality' while water testing uses 'Community' - may not match exactly",
          "Water testing includes multiple samples per beach/date while EJ data is municipality-level",
          "Community names must match exactly between datasets for successful join"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "EJ data uses 'Municipality' while water testing uses 'Community' - may not match exactly",
            "Water testing includes multiple samples per beach/date while EJ data is municipality-level"
          ],
          [
            "Community names must match exactly between datasets for successful join"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.8181818181818181,
        "votes": [
          10.0,
          11.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only marine beach samples from 2023",
          "EJ population percentage must be >50% or <25% (exclude 25-50%)",
          "Exceedance defined as Violation = 'Yes'",
          "Rate calculation: (violations/total_samples) \u00d7 100",
          "Only include marine beach samples (Beach Type Description == 'Marine')",
          "Only include 2023 data (Year == 2023)",
          "Only include communities with >50% EJ population for group 1",
          "Only include communities with <25% EJ population for group 2",
          "Exclude samples with null Violation values",
          "Ensure that the 'Violation' column in water-body-testing-2023.csv is a binary indicator (Yes/No).",
          "Ensure that 'Beach Type Description' only includes 'Marine' values.",
          "Ensure that 'Year' column in water-body-testing-2023.csv only includes 2023 values."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only marine beach samples from 2023",
            "EJ population percentage must be >50% or <25% (exclude 25-50%)",
            "Exceedance defined as Violation = 'Yes'",
            "Rate calculation: (violations/total_samples) \u00d7 100"
          ],
          [
            "Only include marine beach samples (Beach Type Description == 'Marine')",
            "Only include 2023 data (Year == 2023)",
            "Only include communities with >50% EJ population for group 1",
            "Only include communities with <25% EJ population for group 2",
            "Exclude samples with null Violation values"
          ],
          [
            "Ensure that the 'Violation' column in water-body-testing-2023.csv is a binary indicator (Yes/No).",
            "Ensure that 'Beach Type Description' only includes 'Marine' values.",
            "Ensure that 'Year' column in water-body-testing-2023.csv only includes 2023 values."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter water testing to Year=2023 and Beach Type Description='Marine'",
          "Categorize municipalities: EJ_population_percent > 50, EJ_population_percent < 25",
          "Identify exceedances: Violation = 'Yes'",
          "EJ_category: 'high' if Percent of population in EJ BGs > 50, 'low' if Percent of population in EJ BGs < 25",
          "Exceedance rate = (count of Violation='Yes') / (total count of samples) * 100",
          "Exceedance: Violation = 'Yes'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter water testing to Year=2023 and Beach Type Description='Marine'",
            "Categorize municipalities: EJ_population_percent > 50, EJ_population_percent < 25",
            "Identify exceedances: Violation = 'Yes'"
          ],
          [
            "EJ_category: 'high' if Percent of population in EJ BGs > 50, 'low' if Percent of population in EJ BGs < 25",
            "Exceedance rate = (count of Violation='Yes') / (total count of samples) * 100"
          ],
          [
            "Exceedance: Violation = 'Yes'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Proportion test for difference in exceedance rates between groups",
          "Confidence intervals for each group's exceedance rate"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Proportion test for difference in exceedance rates between groups",
            "Confidence intervals for each group's exceedance rate"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Difference to 2 decimal places",
          "Positive value means higher exceedance in high-EJ communities",
          "Result as percentage points difference",
          "Result must be a single number representing the difference",
          "Result must be rounded to 2 decimal places",
          "Difference should be calculated as (rate for >50% EJ) - (rate for <25% EJ)",
          "Output the difference in exceedance rates rounded to 2 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Difference to 2 decimal places",
            "Positive value means higher exceedance in high-EJ communities",
            "Result as percentage points difference"
          ],
          [
            "Result must be a single number representing the difference",
            "Result must be rounded to 2 decimal places",
            "Difference should be calculated as (rate for >50% EJ) - (rate for <25% EJ)"
          ],
          [
            "Output the difference in exceedance rates rounded to 2 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6056313131313134
  },
  "environment-hard-8": {
    "m_q": {
      "target_metric": {
        "value": "Percentage (to 2 decimal places) of samples that failed to meet the swimming standard (Enterococcus > 104 counts per 100mL) and had rainfall within 24 hours prior to sampling",
        "confidence": 0.3333333333333333,
        "votes": [
          "",
          "Percentage (to 2 decimal places) of samples that failed to meet the swimming standard (Enterococcus > 104 counts per 100mL) and had rainfall within 24 hours prior to sampling",
          "Percentage of samples that failed to meet the swimming standard (Enterococcus > 104) at Boston Harbor beaches that had rainfall within 24 hours prior to sampling."
        ]
      },
      "filters": {
        "value": [
          "Enterococcus > 104 counts per 100 mL (failed standard)",
          "Rainfall within 24 hours prior to sampling (1-Day Rain > 0)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Enterococcus > 104 counts per 100 mL (failed standard)",
            "Rainfall within 24 hours prior to sampling (1-Day Rain > 0)"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total count of samples that failed to meet the swimming standard (Enterococcus > 104)?",
          "Of those failed samples, how many had 1-Day Rain > 0?",
          "What is the percentage calculation: (failed samples with rain / total failed samples) * 100?",
          "Identify samples that failed to meet the swimming standard (Enterococcus > 104).",
          "Determine if there was rainfall within 24 hours prior to each failed sample.",
          "Calculate the percentage of failed samples with rainfall."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "What is the total count of samples that failed to meet the swimming standard (Enterococcus > 104)?",
            "Of those failed samples, how many had 1-Day Rain > 0?",
            "What is the percentage calculation: (failed samples with rain / total failed samples) * 100?"
          ],
          [
            "Identify samples that failed to meet the swimming standard (Enterococcus > 104).",
            "Determine if there was rainfall within 24 hours prior to each failed sample.",
            "Calculate the percentage of failed samples with rainfall."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "constitution_beach_datasheet.csv",
          "carson_beach_datasheet.csv",
          "pleasure_bay_and_castle_island_beach_datasheet.csv",
          "m_street_beach_datasheet.csv",
          "city_point_beach_datasheet.csv",
          "malibu_beach_datasheet.csv",
          "tenean_beach_datasheet.csv",
          "wollaston_beach_datasheet.csv",
          "boston-harbor-beaches.txt"
        ],
        "confidence": 0.6296296296296295,
        "votes": [
          [],
          [
            "constitution_beach_datasheet.csv",
            "carson_beach_datasheet.csv",
            "pleasure_bay_and_castle_island_beach_datasheet.csv",
            "m_street_beach_datasheet.csv",
            "city_point_beach_datasheet.csv",
            "malibu_beach_datasheet.csv",
            "tenean_beach_datasheet.csv",
            "wollaston_beach_datasheet.csv"
          ],
          [
            "boston-harbor-beaches.txt",
            "constitution_beach_datasheet.csv",
            "carson_beach_datasheet.csv",
            "pleasure_bay_and_castle_island_beach_datasheet.csv",
            "m_street_beach_datasheet.csv",
            "city_point_beach_datasheet.csv",
            "malibu_beach_datasheet.csv",
            "tenean_beach_datasheet.csv",
            "wollaston_beach_datasheet.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different number of Enterococcus measurement columns across beaches (constitution has 3, carson has 2, m_street has 1, wollaston has 4)",
          "Enterococcus columns named differently: 'Enterococcus', 'Enterococcus.1', 'Enterococcus.2', 'Enterococcus.3'",
          "Tag columns accompany Enterococcus measurements indicating '<' for values below detection limit",
          "pleasure_bay_and_castle_island_beach_datasheet.csv has Enterococcus as int64 while others have float64"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Different number of Enterococcus measurement columns across beaches (constitution has 3, carson has 2, m_street has 1, wollaston has 4)",
            "Enterococcus columns named differently: 'Enterococcus', 'Enterococcus.1', 'Enterococcus.2', 'Enterococcus.3'",
            "Tag columns accompany Enterococcus measurements indicating '<' for values below detection limit",
            "pleasure_bay_and_castle_island_beach_datasheet.csv has Enterococcus as int64 while others have float64"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "enterococcus": "counts per 100 milliliters",
          "1-day rain": "inches (implied)",
          "2-day rain": "inches (implied)",
          "3-day rain": "inches (implied)"
        },
        "confidence": 0.49999999999999994,
        "votes": [
          {},
          {
            "Enterococcus": "counts per 100 milliliters",
            "1-Day Rain": "inches (implied)",
            "2-Day Rain": "inches (implied)",
            "3-Day Rain": "inches (implied)"
          },
          {
            "1-Day Rain": "inches",
            "Enterococcus": "counts per 100 milliliters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Tag columns contain '<' symbol indicating values are below detection limit (e.g., '<10' means < 10)",
          "Rainfall measurements in decimal format suggesting inches as unit"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Tag columns contain '<' symbol indicating values are below detection limit (e.g., '<10' means < 10)",
            "Rainfall measurements in decimal format suggesting inches as unit"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Detection limit handling varies: some files show '<' in Tag column with corresponding value, others show just the value",
          "Missing values represented as NaN in Enterococcus columns across different beaches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Detection limit handling varies: some files show '<' in Tag column with corresponding value, others show just the value",
            "Missing values represented as NaN in Enterococcus columns across different beaches"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 1.0,
        "votes": [
          0.0,
          10.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Swimming standard threshold: Enterococcus <= 104 counts per 100mL (pass), > 104 (fail)",
          "Rainfall within 24 hours: 1-Day Rain > 0",
          "Only consider valid Enterococcus measurements (not NaN)",
          "Handle '<' qualifier in Tag columns - values with '<' tag should use the numeric value as upper bound",
          "Enterococcus values must be numeric and greater than 0.",
          "1-Day Rain values must be numeric and greater than or equal to 0."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Swimming standard threshold: Enterococcus <= 104 counts per 100mL (pass), > 104 (fail)",
            "Rainfall within 24 hours: 1-Day Rain > 0",
            "Only consider valid Enterococcus measurements (not NaN)",
            "Handle '<' qualifier in Tag columns - values with '<' tag should use the numeric value as upper bound"
          ],
          [
            "Enterococcus values must be numeric and greater than 0.",
            "1-Day Rain values must be numeric and greater than or equal to 0."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "failed_standard = Enterococcus > 104",
          "had_rain_24h = 1-Day Rain > 0",
          "target_samples = failed_standard AND had_rain_24h",
          "Failed Sample: Enterococcus > 104",
          "Rain within 24 hours: 1-Day Rain > 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "failed_standard = Enterococcus > 104",
            "had_rain_24h = 1-Day Rain > 0",
            "target_samples = failed_standard AND had_rain_24h"
          ],
          [
            "Failed Sample: Enterococcus > 104",
            "Rain within 24 hours: 1-Day Rain > 0"
          ]
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Percentage rounded to 2 decimal places",
          "Format: XX.XX%",
          "Count all individual samples across all beaches and all measurement columns",
          "Percentage to 2 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Percentage rounded to 2 decimal places",
            "Format: XX.XX%",
            "Count all individual samples across all beaches and all measurement columns"
          ],
          [
            "Percentage to 2 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5231481481481481
  },
  "environment-hard-9": {
    "m_q": {
      "target_metric": {
        "value": "Beaches that had 100% of samples meeting the swimming standard (Enterococcus < 104 counts per 100mL) between 2020-2024 inclusive",
        "confidence": 0.3333333333333333,
        "votes": [
          "",
          "Beaches that had 100% of samples meeting the swimming standard (Enterococcus < 104 counts per 100mL) between 2020-2024 inclusive",
          "List of Boston Harbor beaches that met swimming standards 100% of the time between 2020 and 2024 (inclusive). A sample meets the standard if it contains fewer than 104 counts of Enterococcus per 100 milliliters of water."
        ]
      },
      "filters": {
        "value": [
          "Date >= 2020-01-01 AND Date <= 2024-12-31",
          "Enterococcus < 104 counts per 100mL for swimming standard compliance",
          "Years between 2020 and 2024 (inclusive)",
          "Enterococcus count < 104"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Date >= 2020-01-01 AND Date <= 2024-12-31",
            "Enterococcus < 104 counts per 100mL for swimming standard compliance"
          ],
          [
            "Years between 2020 and 2024 (inclusive)",
            "Enterococcus count < 104"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Beach name",
          "Beach"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Beach name"
          ],
          [
            "Beach"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the date range covered in each beach datasheet?",
          "How many samples were taken at each beach between 2020-2024?",
          "How many samples at each beach exceeded 104 counts per 100mL?",
          "What is the compliance rate (% of samples meeting standard) for each beach?",
          "Which beaches have 100% compliance rate?",
          "What are all the unique beach names?",
          "For each beach, what are the Enterococcus counts for each date between 2020 and 2024?",
          "For each beach, what percentage of samples meet the Enterococcus standard?",
          "Which beaches have a 100% compliance rate?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "What is the date range covered in each beach datasheet?",
            "How many samples were taken at each beach between 2020-2024?",
            "How many samples at each beach exceeded 104 counts per 100mL?",
            "What is the compliance rate (% of samples meeting standard) for each beach?",
            "Which beaches have 100% compliance rate?"
          ],
          [
            "What are all the unique beach names?",
            "For each beach, what are the Enterococcus counts for each date between 2020 and 2024?",
            "For each beach, what percentage of samples meet the Enterococcus standard?",
            "Which beaches have a 100% compliance rate?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "constitution_beach_datasheet.csv",
          "carson_beach_datasheet.csv",
          "pleasure_bay_and_castle_island_beach_datasheet.csv",
          "m_street_beach_datasheet.csv",
          "city_point_beach_datasheet.csv",
          "malibu_beach_datasheet.csv",
          "tenean_beach_datasheet.csv",
          "wollaston_beach_datasheet.csv",
          "boston-harbor-beaches.txt"
        ],
        "confidence": 0.6296296296296295,
        "votes": [
          [],
          [
            "constitution_beach_datasheet.csv",
            "carson_beach_datasheet.csv",
            "pleasure_bay_and_castle_island_beach_datasheet.csv",
            "m_street_beach_datasheet.csv",
            "city_point_beach_datasheet.csv",
            "malibu_beach_datasheet.csv",
            "tenean_beach_datasheet.csv",
            "wollaston_beach_datasheet.csv"
          ],
          [
            "boston-harbor-beaches.txt",
            "constitution_beach_datasheet.csv",
            "carson_beach_datasheet.csv",
            "pleasure_bay_and_castle_island_beach_datasheet.csv",
            "m_street_beach_datasheet.csv",
            "city_point_beach_datasheet.csv",
            "malibu_beach_datasheet.csv",
            "tenean_beach_datasheet.csv",
            "wollaston_beach_datasheet.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different number of Enterococcus measurement columns across beaches (constitution has 3, carson has 2, m_street has 1, etc.)",
          "Inconsistent column naming: some files use 'Tag' and 'Enterococcus', others use 'Tag.1', 'Enterococcus.1', 'Tag.2', 'Enterococcus.2', etc.",
          "Enterococcus dtype varies: float64 in some files, int64 in others",
          "Different datasheet files have different numbers of Enterococcus columns (e.g., 'Enterococcus', 'Enterococcus.1', 'Enterococcus.2', 'Enterococcus.3').",
          "Some datasheet files have 'Tag' columns associated with each 'Enterococcus' column, while others do not.",
          "The 'Enterococcus' column has different dtypes across files (int64, float64)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Different number of Enterococcus measurement columns across beaches (constitution has 3, carson has 2, m_street has 1, etc.)",
            "Inconsistent column naming: some files use 'Tag' and 'Enterococcus', others use 'Tag.1', 'Enterococcus.1', 'Tag.2', 'Enterococcus.2', etc.",
            "Enterococcus dtype varies: float64 in some files, int64 in others"
          ],
          [
            "Different datasheet files have different numbers of Enterococcus columns (e.g., 'Enterococcus', 'Enterococcus.1', 'Enterococcus.2', 'Enterococcus.3').",
            "Some datasheet files have 'Tag' columns associated with each 'Enterococcus' column, while others do not.",
            "The 'Enterococcus' column has different dtypes across files (int64, float64)."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "enterococcus": "counts per 100 milliliters",
          "1-day rain": "inches",
          "2-day rain": "inches",
          "3-day rain": "inches"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {},
          {
            "Enterococcus": "counts per 100 milliliters",
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches"
          },
          {
            "1-Day Rain": "inches",
            "2-Day Rain": "inches",
            "3-Day Rain": "inches",
            "Enterococcus": "counts per 100 milliliters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Enterococcus values sometimes have '<' tags indicating values below detection limit",
          "Need to handle inequality tags (Tag, Tag.1, Tag.2, Tag.3) which contain '<' symbols"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Enterococcus values sometimes have '<' tags indicating values below detection limit",
            "Need to handle inequality tags (Tag, Tag.1, Tag.2, Tag.3) which contain '<' symbols"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Some beaches have multiple measurement locations per date (up to 4 for Wollaston), others have single measurements",
          "Missing values represented as NaN/blank in some Enterococcus columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Some beaches have multiple measurement locations per date (up to 4 for Wollaston), others have single measurements",
            "Missing values represented as NaN/blank in some Enterococcus columns"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "NaN",
          "N/A",
          "<"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [],
          [
            "",
            "NA",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            "",
            "<"
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          0.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Date must be parsed from string format to datetime for filtering 2020-2024 range",
          "Swimming standard: Enterococcus count < 104 per 100mL",
          "All samples from a beach must meet the standard for 100% compliance",
          "Handle '<' tag values: if Tag contains '<', the actual value is less than the reported Enterococcus value",
          "Each row may contain multiple samples (different Enterococcus columns), all must be evaluated",
          "The 'Date' column must be parsed to extract the year for filtering.",
          "The 'Enterococcus' columns may contain '<' characters, indicating values below a detection limit. These need to be handled appropriately (e.g., replaced with the detection limit value).",
          "Need to handle multiple Enterococcus columns in some files by combining them into a single Enterococcus reading for a given date and beach."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Date must be parsed from string format to datetime for filtering 2020-2024 range",
            "Swimming standard: Enterococcus count < 104 per 100mL",
            "All samples from a beach must meet the standard for 100% compliance",
            "Handle '<' tag values: if Tag contains '<', the actual value is less than the reported Enterococcus value",
            "Each row may contain multiple samples (different Enterococcus columns), all must be evaluated"
          ],
          [
            "The 'Date' column must be parsed to extract the year for filtering.",
            "The 'Enterococcus' columns may contain '<' characters, indicating values below a detection limit. These need to be handled appropriately (e.g., replaced with the detection limit value).",
            "Need to handle multiple Enterococcus columns in some files by combining them into a single Enterococcus reading for a given date and beach."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract year from Date string and filter for year >= 2020 AND year <= 2024",
          "For each Enterococcus measurement: if Tag is '<', value is below detection limit and meets standard; otherwise compare value < 104",
          "Exclude NaN/missing Enterococcus values from compliance calculation",
          "Year >= 2020",
          "Year <= 2024",
          "Enterococcus < 104"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Extract year from Date string and filter for year >= 2020 AND year <= 2024",
            "For each Enterococcus measurement: if Tag is '<', value is below detection limit and meets standard; otherwise compare value < 104",
            "Exclude NaN/missing Enterococcus values from compliance calculation"
          ],
          [
            "Year >= 2020",
            "Year <= 2024",
            "Enterococcus < 104"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Calculate total valid samples per beach in date range",
          "Calculate number of compliant samples per beach (Enterococcus < 104)",
          "Calculate compliance percentage: (compliant samples / total samples) * 100"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Calculate total valid samples per beach in date range",
            "Calculate number of compliant samples per beach (Enterococcus < 104)",
            "Calculate compliance percentage: (compliant samples / total samples) * 100"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return list of beach names that achieved 100% compliance",
          "Beach names should match the source file naming (e.g., 'Constitution Beach', 'Carson Beach', etc.)",
          "The output should be a list of beach names."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Return list of beach names that achieved 100% compliance",
            "Beach names should match the source file naming (e.g., 'Constitution Beach', 'Carson Beach', etc.)"
          ],
          [
            "The output should be a list of beach names."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5548148148148149
  },
  "legal-easy-10": {
    "m_q": {
      "target_metric": {
        "value": "Years where Fraud reports count is at least 50% of total reports (Fraud + Identity Theft + Other)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Years where Fraud reports count is at least 50% of total reports (Fraud + Identity Theft + Other)",
          "Years where Fraud reports are at least 50% of total reports",
          "Years where the number of Fraud reports is at least 50% of the total number of reports (Fraud + Identity Theft + Other)"
        ]
      },
      "filters": {
        "value": [
          "Fraud >= 0.5 * (Fraud + Identity Theft + Other)",
          "Fraud / (Fraud + Identity Theft + Other) >= 0.5"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Fraud >= 0.5 * (Fraud + Identity Theft + Other)"
          ],
          [
            "Fraud / (Fraud + Identity Theft + Other) >= 0.5"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Year"
          ],
          [
            "Year"
          ],
          [
            "Year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate total reports per year",
          "Calculate Fraud percentage of total reports per year",
          "Filter years where Fraud percentage >= 50%",
          "What is the total number of reports for each year?",
          "What is the proportion of Fraud reports to total reports for each year?",
          "Which years meet the threshold of Fraud being at least 50% of total?",
          "Calculate the total number of reports for each year (Fraud + Identity Theft + Other).",
          "Calculate the percentage of Fraud reports for each year (Fraud / Total Reports).",
          "Filter the years where the percentage of Fraud reports is greater than or equal to 50%."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate total reports per year",
            "Calculate Fraud percentage of total reports per year",
            "Filter years where Fraud percentage >= 50%"
          ],
          [
            "What is the total number of reports for each year?",
            "What is the proportion of Fraud reports to total reports for each year?",
            "Which years meet the threshold of Fraud being at least 50% of total?"
          ],
          [
            "Calculate the total number of reports for each year (Fraud + Identity Theft + Other).",
            "Calculate the percentage of Fraud reports for each year (Fraud / Total Reports).",
            "Filter the years where the percentage of Fraud reports is greater than or equal to 50%."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Number_of_Reports_by_Type.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "calendar year",
          "fraud": "count of reports",
          "identity theft": "count of reports",
          "other": "count of reports"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Year": "calendar year",
            "Fraud": "count of reports",
            "Identity Theft": "count of reports",
            "Other": "count of reports"
          },
          {
            "Year": "calendar year",
            "Fraud": "count of reports",
            "Identity Theft": "count of reports",
            "Other": "count of reports"
          },
          {
            "Year": "year",
            "Fraud": "number of reports",
            "Identity Theft": "number of reports",
            "Other": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "All count columns appear to be integers, no scaling issues evident"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All count columns appear to be integers, no scaling issues evident"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be sequential from 2001-2024 based on shape [24,4]",
          "All count columns should be non-negative integers",
          "Total reports = Fraud + Identity Theft + Other should be positive",
          "Fraud >= 0",
          "Identity Theft >= 0",
          "Other >= 0",
          "Total reports (Fraud + Identity Theft + Other) > 0 to avoid division by zero",
          "Year must be a valid year.",
          "Fraud, Identity Theft, and Other must be non-negative integers."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be sequential from 2001-2024 based on shape [24,4]",
            "All count columns should be non-negative integers",
            "Total reports = Fraud + Identity Theft + Other should be positive"
          ],
          [
            "Fraud >= 0",
            "Identity Theft >= 0",
            "Other >= 0",
            "Total reports (Fraud + Identity Theft + Other) > 0 to avoid division by zero"
          ],
          [
            "Year must be a valid year.",
            "Fraud, Identity Theft, and Other must be non-negative integers."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Fraud_percentage = Fraud / (Fraud + Identity Theft + Other) * 100",
          "Filter: Fraud_percentage >= 50",
          "Calculate total_reports = Fraud + Identity Theft + Other",
          "Calculate fraud_percentage = Fraud / total_reports",
          "Filter where fraud_percentage >= 0.5"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Fraud_percentage = Fraud / (Fraud + Identity Theft + Other) * 100",
            "Filter: Fraud_percentage >= 50"
          ],
          [
            "Calculate total_reports = Fraud + Identity Theft + Other",
            "Calculate fraud_percentage = Fraud / total_reports",
            "Filter where fraud_percentage >= 0.5"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify that sum of three report types equals total reports for each year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify that sum of three report types equals total reports for each year"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of years where condition is met",
          "Could optionally include Fraud percentage for each year",
          "Return list of years as integers",
          "Years should be in chronological order",
          "Only include years meeting the 50% threshold",
          "Output should be a list of years."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of years where condition is met",
            "Could optionally include Fraud percentage for each year"
          ],
          [
            "Return list of years as integers",
            "Years should be in chronological order",
            "Only include years meeting the 50% threshold"
          ],
          [
            "Output should be a list of years."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6333333333333334
  },
  "legal-easy-11": {
    "m_q": {
      "target_metric": {
        "value": "Whether 'Other' reports were more than half of total reports in any year between 2001-2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "Whether 'Other' reports were more than half of total reports in any year between 2001-2024",
          "Determine if 'Other' reports exceeded 50% of total reports in any year from 2001-2024",
          "Determine if there was any year between 2001 and 2024 (inclusive) where the 'Other' reports were more than half of the total reports (Fraud + Identity Theft + Other)."
        ]
      },
      "filters": {
        "value": [
          "Year >= 2001",
          "Year <= 2024"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Year >= 2001",
            "Year <= 2024"
          ],
          [
            "Year >= 2001",
            "Year <= 2024"
          ],
          [
            "Year >= 2001",
            "Year <= 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Year"
          ],
          [
            "Year"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate total reports per year (sum of Fraud, Identity Theft, Other)",
          "Calculate proportion of Other reports per year",
          "Check if proportion > 0.5 for any year in range",
          "Calculate total reports per year (Fraud + Identity Theft + Other)",
          "Calculate the proportion of Other reports to total reports for each year",
          "Check if any year has Other proportion > 0.5"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate total reports per year (sum of Fraud, Identity Theft, Other)",
            "Calculate proportion of Other reports per year",
            "Check if proportion > 0.5 for any year in range"
          ],
          [
            "Calculate total reports per year (Fraud + Identity Theft + Other)",
            "Calculate the proportion of Other reports to total reports for each year",
            "Check if any year has Other proportion > 0.5"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Number_of_Reports_by_Type.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "year",
          "fraud": "count of reports",
          "identity theft": "count of reports",
          "other": "count of reports"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Year": "calendar year",
            "Fraud": "count of reports",
            "Identity Theft": "count of reports",
            "Other": "count of reports"
          },
          {
            "Year": "year",
            "Fraud": "count",
            "Identity Theft": "count",
            "Other": "count"
          },
          {
            "Year": "year",
            "Fraud": "number of reports",
            "Identity Theft": "number of reports",
            "Other": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be integers 2001-2024",
          "All count columns should be non-negative integers",
          "Total reports = Fraud + Identity Theft + Other > 0",
          "Year must be between 2001 and 2024 inclusive",
          "All report counts (Fraud, Identity Theft, Other) must be non-negative",
          "Year must be an integer.",
          "Fraud, Identity Theft, and Other must be non-negative integers."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be integers 2001-2024",
            "All count columns should be non-negative integers",
            "Total reports = Fraud + Identity Theft + Other > 0"
          ],
          [
            "Year must be between 2001 and 2024 inclusive",
            "All report counts (Fraud, Identity Theft, Other) must be non-negative"
          ],
          [
            "Year must be an integer.",
            "Fraud, Identity Theft, and Other must be non-negative integers."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Calculate Total = Fraud + Identity Theft + Other",
          "Calculate Other_Proportion = Other / Total",
          "Filter where Other_Proportion > 0.5"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Calculate Total = Fraud + Identity Theft + Other",
            "Calculate Other_Proportion = Other / Total",
            "Filter where Other_Proportion > 0.5"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Binary Yes/No answer only",
          "No explanation text",
          "Output must be either 'Yes' or 'No'",
          "No explanation or additional text required",
          "Output must be 'Yes' or 'No'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Binary Yes/No answer only",
            "No explanation text"
          ],
          [
            "Output must be either 'Yes' or 'No'",
            "No explanation or additional text required"
          ],
          [
            "Output must be 'Yes' or 'No'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6000000000000001
  },
  "legal-easy-12": {
    "m_q": {
      "target_metric": {
        "value": "minimum number of report categories needed to reach cumulative percentage >= 50%",
        "confidence": 0.3333333333333333,
        "votes": [
          "minimum number of report categories needed to reach cumulative percentage >= 50%",
          "minimum number of report categories that collectively account for at least 50% of total reports",
          "Minimum number of report categories needed to account for at least 50% of reports in 2024"
        ]
      },
      "filters": {
        "value": [
          "Year = 2024",
          "year == 2024"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Year = 2024"
          ],
          [
            "year == 2024"
          ],
          [
            "Year = 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Category"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Category"
          ],
          [
            "Category"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the cumulative percentage of reports by category?",
          "At which rank does cumulative percentage first reach or exceed 50%?",
          "What is the total number of reports across all categories in 2024?",
          "What percentage threshold equals 50% of total reports?",
          "When categories are sorted by number of reports (descending), what is the cumulative sum at each rank?",
          "At what rank does the cumulative percentage first reach or exceed 50%?",
          "Calculate the cumulative percentage of reports for each category, sorted by '# of Reports' in descending order.",
          "Determine the minimum number of categories required to reach a cumulative percentage of at least 50%."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the cumulative percentage of reports by category?",
            "At which rank does cumulative percentage first reach or exceed 50%?"
          ],
          [
            "What is the total number of reports across all categories in 2024?",
            "What percentage threshold equals 50% of total reports?",
            "When categories are sorted by number of reports (descending), what is the cumulative sum at each rank?",
            "At what rank does the cumulative percentage first reach or exceed 50%?"
          ],
          [
            "Calculate the cumulative percentage of reports for each category, sorted by '# of Reports' in descending order.",
            "Determine the minimum number of categories required to reach a cumulative percentage of at least 50%."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Report_Categories.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Report_Categories.csv"
          ],
          [
            "2024_CSN_Report_Categories.csv"
          ],
          [
            "2024_CSN_Report_Categories.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count",
          "percentage": "percentage",
          "rank": "ordinal_position"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "# of Reports": "count",
            "Percentage": "percentage"
          },
          {
            "# of Reports": "count",
            "Percentage": "percentage_string",
            "Rank": "ordinal_position"
          },
          {
            "# of Reports": "number of reports",
            "Percentage": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Percentage column contains '%' symbol and needs conversion to numeric",
          "Percentage column is stored as string with '%' symbol and needs parsing",
          "Percentage values may need conversion to decimal for calculations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage column contains '%' symbol and needs conversion to numeric"
          ],
          [
            "Percentage column is stored as string with '%' symbol and needs parsing",
            "Percentage values may need conversion to decimal for calculations"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Rank values should be sequential 1-29",
          "Sum of percentages should equal 100%",
          "Categories are mutually exclusive",
          "Rank must be sequential integers starting from 1",
          "# of Reports must be non-negative integers",
          "Percentage values when summed should equal approximately 100%",
          "Categories must be unique",
          "The sum of percentages for all categories should be close to 100%."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rank values should be sequential 1-29",
            "Sum of percentages should equal 100%",
            "Categories are mutually exclusive"
          ],
          [
            "Rank must be sequential integers starting from 1",
            "# of Reports must be non-negative integers",
            "Percentage values when summed should equal approximately 100%",
            "Categories must be unique"
          ],
          [
            "The sum of percentages for all categories should be close to 100%."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Sort by Rank ascending for cumulative calculation",
          "cumulative_percentage >= 50%"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sort by Rank ascending for cumulative calculation"
          ],
          [
            "cumulative_percentage >= 50%"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify percentage column sums to 100% after conversion",
          "Verify sum of '# of Reports' matches expected total",
          "Verify percentage calculations are consistent with report counts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify percentage column sums to 100% after conversion"
          ],
          [
            "Verify sum of '# of Reports' matches expected total",
            "Verify percentage calculations are consistent with report counts"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Integer count of categories",
          "Output must be a single integer representing the minimum number of categories",
          "The result should be the rank at which cumulative percentage first reaches 50%",
          "The output should be an integer representing the minimum number of categories."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Integer count of categories"
          ],
          [
            "Output must be a single integer representing the minimum number of categories",
            "The result should be the rank at which cumulative percentage first reaches 50%"
          ],
          [
            "The output should be an integer representing the minimum number of categories."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.613888888888889
  },
  "legal-easy-13": {
    "m_q": {
      "target_metric": {
        "value": "ratio of reports between the most frequent and least frequent category, rounded to two decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "ratio of reports between the most frequent and least frequent category, rounded to two decimal places",
          "Ratio of the number of reports from the most frequent category to the least frequent category in 2024, rounded to two decimal places",
          "Ratio of the number of reports between the most frequent and least frequent category in 2024"
        ]
      },
      "filters": {
        "value": [
          "Year = 2024"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Year = 2024"
          ],
          [
            "Year = 2024"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Category"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Category"
          ],
          [
            "Category"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Identify the category with maximum # of Reports",
          "Identify the category with minimum # of Reports",
          "Calculate ratio = max_reports / min_reports",
          "Round result to two decimal places",
          "What is the maximum value in the '# of Reports' column?",
          "What is the minimum value in the '# of Reports' column?",
          "What is the ratio of maximum to minimum number of reports?",
          "How should the ratio be rounded to two decimal places?",
          "What is the category with the maximum '# of Reports' in 2024?",
          "What is the category with the minimum '# of Reports' in 2024?",
          "What is the '# of Reports' for the most frequent category?",
          "What is the '# of Reports' for the least frequent category?",
          "What is the ratio of the maximum '# of Reports' to the minimum '# of Reports'?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify the category with maximum # of Reports",
            "Identify the category with minimum # of Reports",
            "Calculate ratio = max_reports / min_reports",
            "Round result to two decimal places"
          ],
          [
            "What is the maximum value in the '# of Reports' column?",
            "What is the minimum value in the '# of Reports' column?",
            "What is the ratio of maximum to minimum number of reports?",
            "How should the ratio be rounded to two decimal places?"
          ],
          [
            "What is the category with the maximum '# of Reports' in 2024?",
            "What is the category with the minimum '# of Reports' in 2024?",
            "What is the '# of Reports' for the most frequent category?",
            "What is the '# of Reports' for the least frequent category?",
            "What is the ratio of the maximum '# of Reports' to the minimum '# of Reports'?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Report_Categories.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Report_Categories.csv"
          ],
          [
            "2024_CSN_Report_Categories.csv"
          ],
          [
            "2024_CSN_Report_Categories.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count",
          "percentage": "percentage",
          "rank": "ordinal ranking"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "# of Reports": "count",
            "Percentage": "percentage"
          },
          {
            "# of Reports": "count",
            "Percentage": "percentage",
            "Rank": "ordinal ranking"
          },
          {
            "# of Reports": "number of reports",
            "Percentage": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Percentage column contains '%' symbol and needs cleaning for numerical operations",
          "Percentage column is stored as text with '%' symbol, not as numeric value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage column contains '%' symbol and needs cleaning for numerical operations"
          ],
          [
            "Percentage column is stored as text with '%' symbol, not as numeric value"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Rank values should be unique and sequential",
          "Sum of # of Reports should be consistent",
          "Percentage values should sum to approximately 100%",
          "The dataset contains exactly 29 categories",
          "Sample shows only first 20 rows, need all 29 rows to determine true minimum",
          "Ratio must be calculated as max / min",
          "Result must be rounded to exactly two decimal places",
          "The ratio must be rounded to two decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rank values should be unique and sequential",
            "Sum of # of Reports should be consistent",
            "Percentage values should sum to approximately 100%"
          ],
          [
            "The dataset contains exactly 29 categories",
            "Sample shows only first 20 rows, need all 29 rows to determine true minimum",
            "Ratio must be calculated as max / min",
            "Result must be rounded to exactly two decimal places"
          ],
          [
            "The ratio must be rounded to two decimal places."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude header row",
          "Use all 28 category rows for analysis",
          "Identify category with maximum '# of Reports'",
          "Identify category with minimum '# of Reports'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude header row",
            "Use all 28 category rows for analysis"
          ],
          [
            "Identify category with maximum '# of Reports'",
            "Identify category with minimum '# of Reports'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in # of Reports column",
          "Verify Rank ordering matches # of Reports ordering"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in # of Reports column",
            "Verify Rank ordering matches # of Reports ordering"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Ratio must be rounded to two decimal places",
          "Result should be a single numerical value",
          "Output should be a single numeric value",
          "Value must be rounded to 2 decimal places",
          "Format: X.XX",
          "The output should be a single numerical value representing the ratio."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Ratio must be rounded to two decimal places",
            "Result should be a single numerical value"
          ],
          [
            "Output should be a single numeric value",
            "Value must be rounded to 2 decimal places",
            "Format: X.XX"
          ],
          [
            "The output should be a single numerical value representing the ratio."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6222222222222223
  },
  "legal-easy-19": {
    "m_q": {
      "target_metric": {
        "value": "proportion of fraud reporters who lost between $1-$500 in 2024",
        "confidence": 0.6666666666666666,
        "votes": [
          "proportion of fraud reporters who lost between $1-$500 in 2024",
          "Proportion of fraud reporters who lost between $1-$500 in 2024, rounded to 3 decimal places",
          "Proportion of fraud reporters who lost between $1-$500 in 2024"
        ]
      },
      "filters": {
        "value": [
          "Year = 2024",
          "Amount Lost between $1 and $500 inclusive",
          "Amount Lost between $1 and $500"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Year = 2024",
            "Amount Lost between $1 and $500 inclusive"
          ],
          [
            "Year = 2024",
            "Amount Lost between $1 and $500"
          ],
          [
            "Year = 2024",
            "Amount Lost between $1 and $500"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Amount Lost"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Amount Lost"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of fraud reports with $1-$500 loss?",
          "What is the total number of fraud reports with any dollar loss?",
          "How to handle the '$1 - $1,000' aggregated category in the first file?",
          "What is the total number of fraud reporters in 2024?",
          "How many fraud reporters lost between $1-$500 in 2024?",
          "What is the sum of reports for ranges $1-$100, $101-$200, $201-$300, $301-$400, and $401-$500?",
          "What is the total number of fraud reports in 2024?",
          "What is the number of fraud reports where the amount lost is between $1 and $500 in 2024?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total number of fraud reports with $1-$500 loss?",
            "What is the total number of fraud reports with any dollar loss?",
            "How to handle the '$1 - $1,000' aggregated category in the first file?"
          ],
          [
            "What is the total number of fraud reporters in 2024?",
            "How many fraud reporters lost between $1-$500 in 2024?",
            "What is the sum of reports for ranges $1-$100, $101-$200, $201-$300, $301-$400, and $401-$500?"
          ],
          [
            "What is the total number of fraud reports in 2024?",
            "What is the number of fraud reports where the amount lost is between $1 and $500 in 2024?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Fraud_Reports_by_Amount_Lost.csv",
          "2024_CSN_Fraud_Reports_by_Amount_Lost__Fraud Reports by Amount Lost",
          "2024_CSN_Fraud_Reports_by_Amount_Lost__Reported Fraud Losses in $1 - $1,000 Range",
          "2024_CSN_Fraud_Reports_by_Amount_Lost__Reported Fraud Losses in $1 - $1,000 Range.csv"
        ],
        "confidence": 0.5833333333333334,
        "votes": [
          [
            "2024_CSN_Fraud_Reports_by_Amount_Lost.csv",
            "2024_CSN_Fraud_Reports_by_Amount_Lost__Fraud Reports by Amount Lost",
            "2024_CSN_Fraud_Reports_by_Amount_Lost__Reported Fraud Losses in $1 - $1,000 Range"
          ],
          [
            "2024_CSN_Fraud_Reports_by_Amount_Lost__Fraud Reports by Amount Lost",
            "2024_CSN_Fraud_Reports_by_Amount_Lost__Reported Fraud Losses in $1 - $1,000 Range"
          ],
          [
            "2024_CSN_Fraud_Reports_by_Amount_Lost.csv",
            "2024_CSN_Fraud_Reports_by_Amount_Lost__Reported Fraud Losses in $1 - $1,000 Range.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "First file has aggregated '$1 - $1,000' category while third file has detailed breakdown of same range",
          "Second file has different column naming with numeric column name '2,600,678' and 'Unnamed: 2'",
          "Different granularity levels for amount lost ranges across files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "First file has aggregated '$1 - $1,000' category while third file has detailed breakdown of same range",
            "Second file has different column naming with numeric column name '2,600,678' and 'Unnamed: 2'"
          ],
          [
            "Different granularity levels for amount lost ranges across files"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "amount lost": "USD dollar ranges",
          "# of reports": "count",
          "number of fraud reports": "description",
          "2,600,678": "total count",
          "total $ loss": "USD",
          "median $ loss": "USD"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Amount Lost": "USD dollar ranges",
            "# of Reports": "count of reports",
            "Number of Fraud Reports": "description",
            "2,600,678": "total count",
            "Total $ Loss": "USD",
            "Median $ Loss": "USD"
          },
          {
            "# of Reports": "count",
            "Amount Lost": "US dollars range",
            "Total $ Loss": "US dollars",
            "Median $ Loss": "US dollars"
          },
          {
            "Amount Lost": "USD",
            "# of Reports": "count"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "First file's '$1 - $1,000' category overlaps with third file's detailed $1-$1,000 breakdown",
          "Need to ensure no double-counting between aggregated and detailed data",
          "Amount Lost ranges are string ranges, not numeric values",
          "Dollar amounts include currency symbols and comma separators"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "First file's '$1 - $1,000' category overlaps with third file's detailed $1-$1,000 breakdown",
            "Need to ensure no double-counting between aggregated and detailed data"
          ],
          [
            "Amount Lost ranges are string ranges, not numeric values",
            "Dollar amounts include currency symbols and comma separators"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "First file shows 624,110 reports in '$1 - $1,000' range, third file shows sum of detailed $1-$1,000 categories = 243174+114336+67064+44982+46752+26702+19271+19664+14019+28146 = 624,110 (matches)",
          "Total reports across sources need verification for consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "First file shows 624,110 reports in '$1 - $1,000' range, third file shows sum of detailed $1-$1,000 categories = 243174+114336+67064+44982+46752+26702+19271+19664+14019+28146 = 624,110 (matches)"
          ],
          [
            "Total reports across sources need verification for consistency"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Proportion must be between 0 and 1",
          "Result must be rounded to 3 decimal places",
          "Amount ranges are inclusive (e.g., $1-$500 includes both endpoints)",
          "Only include reports from 2024",
          "Only include amount lost ranges from $1 to $500 inclusive",
          "Proportion must be rounded to exactly 3 decimal places",
          "The sum of '# of Reports' for all 'Amount Lost' ranges in '2024_CSN_Fraud_Reports_by_Amount_Lost__Reported Fraud Losses in $1 - $1,000 Range.csv' should be less than or equal to the '# of Reports' for '$1 - $1,000' in '2024_CSN_Fraud_Reports_by_Amount_Lost.csv'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Proportion must be between 0 and 1",
            "Result must be rounded to 3 decimal places",
            "Amount ranges are inclusive (e.g., $1-$500 includes both endpoints)"
          ],
          [
            "Only include reports from 2024",
            "Only include amount lost ranges from $1 to $500 inclusive",
            "Proportion must be rounded to exactly 3 decimal places"
          ],
          [
            "The sum of '# of Reports' for all 'Amount Lost' ranges in '2024_CSN_Fraud_Reports_by_Amount_Lost__Reported Fraud Losses in $1 - $1,000 Range.csv' should be less than or equal to the '# of Reports' for '$1 - $1,000' in '2024_CSN_Fraud_Reports_by_Amount_Lost.csv'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Use third file for detailed $1-$500 breakdown: $1-$100, $101-$200, $201-$300, $301-$400, $401-$500",
          "Exclude '$501 - $1,000' categories from third file",
          "Sum reports where Amount Lost in ['$1 - $100', '$101 - $200', '$201 - $300', '$301 - $400', '$401 - $500']",
          "Use 'Number of Fraud Reports' value of 2,600,678 as denominator",
          "Amount Lost between $1 and $500"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Use third file for detailed $1-$500 breakdown: $1-$100, $101-$200, $201-$300, $301-$400, $401-$500",
            "Exclude '$501 - $1,000' categories from third file"
          ],
          [
            "Sum reports where Amount Lost in ['$1 - $100', '$101 - $200', '$201 - $300', '$301 - $400', '$401 - $500']",
            "Use 'Number of Fraud Reports' value of 2,600,678 as denominator"
          ],
          [
            "Amount Lost between $1 and $500"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify sum of detailed $1-$1,000 categories equals aggregated '$1 - $1,000' value from first file"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify sum of detailed $1-$1,000 categories equals aggregated '$1 - $1,000' value from first file"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Proportion as decimal rounded to 3 decimal places",
          "Need denominator: total fraud reports with dollar loss (987,520 from second file)",
          "Output must be a single decimal number rounded to 3 decimal places",
          "Format: 0.XXX",
          "Round the proportion to 3 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Proportion as decimal rounded to 3 decimal places",
            "Need denominator: total fraud reports with dollar loss (987,520 from second file)"
          ],
          [
            "Output must be a single decimal number rounded to 3 decimal places",
            "Format: 0.XXX"
          ],
          [
            "Round the proportion to 3 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6291666666666668
  },
  "legal-easy-20": {
    "m_q": {
      "target_metric": {
        "value": "percentage of fraud reporters in 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "percentage of fraud reporters in 2024",
          "Percentage of fraud reporters out of total reporters in 2024",
          "Percentage of fraud reports out of total reports in 2024, expressed as an integer"
        ]
      },
      "filters": {
        "value": [
          "Year = 2024",
          "Year == 2024"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Year = 2024"
          ],
          [
            "Year == 2024"
          ],
          [
            "Year = 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of fraud reports in 2024?",
          "What is the total number of all reports (Fraud + Identity Theft + Other) in 2024?",
          "How to calculate percentage from these two numbers?",
          "What is the number of fraud reports in 2024?",
          "What is the percentage calculated as (Fraud / Total) * 100?",
          "How should the percentage be rounded to an integer?",
          "What is the total number of reports (Fraud + Identity Theft + Other) in 2024?"
        ],
        "confidence": 0.42857142857142866,
        "votes": [
          [
            "What is the total number of fraud reports in 2024?",
            "What is the total number of all reports (Fraud + Identity Theft + Other) in 2024?",
            "How to calculate percentage from these two numbers?"
          ],
          [
            "What is the number of fraud reports in 2024?",
            "What is the total number of all reports (Fraud + Identity Theft + Other) in 2024?",
            "What is the percentage calculated as (Fraud / Total) * 100?",
            "How should the percentage be rounded to an integer?"
          ],
          [
            "What is the number of fraud reports in 2024?",
            "What is the total number of reports (Fraud + Identity Theft + Other) in 2024?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Number_of_Reports_by_Type.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "year",
          "fraud": "count of reports",
          "identity theft": "count of reports",
          "other": "count of reports"
        },
        "confidence": 0.9166666666666666,
        "votes": [
          {
            "Year": "year",
            "Fraud": "count of reports",
            "Identity Theft": "count of reports",
            "Other": "count of reports"
          },
          {
            "Year": "year",
            "Fraud": "count of fraud reports",
            "Identity Theft": "count of identity theft reports",
            "Other": "count of other reports"
          },
          {
            "Fraud": "number of reports",
            "Identity Theft": "number of reports",
            "Other": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Data shows increasing trend over years, need to verify if 2024 data is available in the file"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data shows increasing trend over years, need to verify if 2024 data is available in the file"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be integers between 2001-2024",
          "Fraud, Identity Theft, Other counts should be non-negative integers",
          "Total reports = Fraud + Identity Theft + Other",
          "Year must equal 2024",
          "All report counts (Fraud, Identity Theft, Other) must be non-negative",
          "Total reports must be greater than 0 to avoid division by zero",
          "Year must be an integer",
          "Fraud, Identity Theft, and Other must be non-negative integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be integers between 2001-2024",
            "Fraud, Identity Theft, Other counts should be non-negative integers",
            "Total reports = Fraud + Identity Theft + Other"
          ],
          [
            "Year must equal 2024",
            "All report counts (Fraud, Identity Theft, Other) must be non-negative",
            "Total reports must be greater than 0 to avoid division by zero"
          ],
          [
            "Year must be an integer",
            "Fraud, Identity Theft, and Other must be non-negative integers"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to Year = 2024",
          "Filter dataset to Year == 2024",
          "Calculate Total = Fraud + Identity Theft + Other",
          "Calculate Percentage = (Fraud / Total) * 100"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to Year = 2024"
          ],
          [
            "Filter dataset to Year == 2024",
            "Calculate Total = Fraud + Identity Theft + Other",
            "Calculate Percentage = (Fraud / Total) * 100"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if 2024 data exists in the file",
          "Verify data completeness for 2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if 2024 data exists in the file",
            "Verify data completeness for 2024"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Percentage must be expressed as integer",
          "Calculation: (Fraud_2024 / (Fraud_2024 + Identity_Theft_2024 + Other_2024)) * 100, rounded to nearest integer",
          "Output must be an integer",
          "Percentage should be rounded (not truncated) to nearest integer",
          "No decimal places in final answer",
          "No percentage symbol in numeric output",
          "The final answer must be an integer representing the percentage."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage must be expressed as integer",
            "Calculation: (Fraud_2024 / (Fraud_2024 + Identity_Theft_2024 + Other_2024)) * 100, rounded to nearest integer"
          ],
          [
            "Output must be an integer",
            "Percentage should be rounded (not truncated) to nearest integer",
            "No decimal places in final answer",
            "No percentage symbol in numeric output"
          ],
          [
            "The final answer must be an integer representing the percentage."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5922619047619049
  },
  "legal-easy-21": {
    "m_q": {
      "target_metric": {
        "value": "total number of identity theft reporters",
        "confidence": 0.3333333333333333,
        "votes": [
          "total number of identity theft reporters",
          "total number of identity theft reporters in Alabama in 2024",
          "Total number of identity theft reports"
        ]
      },
      "filters": {
        "value": [
          "State = 'Alabama'",
          "Year = 2024",
          "State == 'Alabama'",
          "Year == 2024"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "State = 'Alabama'",
            "Year = 2024"
          ],
          [
            "State == 'Alabama'",
            "Year == 2024"
          ],
          [
            "State = 'Alabama'",
            "Year = 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "State"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "State"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the sum of '# of Reports' for Alabama across all theft types?",
          "Does the data represent 2024 specifically or is that inferred from the filename?",
          "Are there any duplicate entries for Alabama that need to be checked?",
          "What are all the theft types reported in Alabama?",
          "What is the number of reports for each theft type in Alabama?",
          "What is the sum of all reports across all theft types for Alabama?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the sum of '# of Reports' for Alabama across all theft types?",
            "Does the data represent 2024 specifically or is that inferred from the filename?",
            "Are there any duplicate entries for Alabama that need to be checked?"
          ],
          [
            "What are all the theft types reported in Alabama?",
            "What is the number of reports for each theft type in Alabama?",
            "What is the sum of all reports across all theft types for Alabama?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_State_Identity_Theft_Reports.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_State_Identity_Theft_Reports.csv"
          ],
          [
            "2024_CSN_State_Identity_Theft_Reports.csv"
          ],
          [
            "2024_CSN_State_Identity_Theft_Reports.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of reports",
          "percentage": "percentage of total reports for the state"
        },
        "confidence": 1.0,
        "votes": [
          {
            "# of Reports": "count of reports",
            "Percentage": "percentage of total reports for the state"
          },
          {
            "# of Reports": "count of individual reports",
            "Percentage": "percentage (stored as string with % symbol)"
          },
          {
            "# of Reports": "number of reports",
            "Percentage": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Percentage column is stored as string with '%' symbol, needs conversion to numeric for calculations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage column is stored as string with '%' symbol, needs conversion to numeric for calculations"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Sum of percentages for each state should equal 100% (or close due to rounding)",
          "# of Reports should be non-negative integers",
          "Each state-theft type combination should be unique",
          "State must equal 'Alabama'",
          "All Theft Type categories for Alabama must be included in the sum",
          "# of Reports must be non-negative integers",
          "The '# of Reports' column must be a non-negative integer."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sum of percentages for each state should equal 100% (or close due to rounding)",
            "# of Reports should be non-negative integers",
            "Each state-theft type combination should be unique"
          ],
          [
            "State must equal 'Alabama'",
            "All Theft Type categories for Alabama must be included in the sum",
            "# of Reports must be non-negative integers"
          ],
          [
            "The '# of Reports' column must be a non-negative integer."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where State = 'Alabama'",
          "Group by State and sum '# of Reports'",
          "Filter rows where State column equals 'Alabama'",
          "Sum all '# of Reports' values for filtered rows"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where State = 'Alabama'",
            "Group by State and sum '# of Reports'"
          ],
          [
            "Filter rows where State column equals 'Alabama'",
            "Sum all '# of Reports' values for filtered rows"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if sum of '# of Reports' for Alabama matches expected total based on percentages"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if sum of '# of Reports' for Alabama matches expected total based on percentages"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric value representing total reporters",
          "Should be integer format",
          "Return a single scalar integer value representing the total count",
          "The result should be the sum of all theft type reports for Alabama",
          "The output should be a single integer representing the total number of identity theft reports in Alabama in 2024."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric value representing total reporters",
            "Should be integer format"
          ],
          [
            "Return a single scalar integer value representing the total count",
            "The result should be the sum of all theft type reports for Alabama"
          ],
          [
            "The output should be a single integer representing the total number of identity theft reports in Alabama in 2024."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6083333333333334
  },
  "legal-easy-25": {
    "m_q": {
      "target_metric": {
        "value": "Median Fraud Loss",
        "confidence": 0.3333333333333333,
        "votes": [
          "Median Fraud Loss",
          "Maximum Median Fraud Loss among U.S. Military branches",
          "Maximum 'Median Fraud Loss'"
        ]
      },
      "filters": {
        "value": [
          "Year = 2024",
          "Data from military consumers who provided branch information"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Year = 2024",
            "Data from military consumers who provided branch information"
          ],
          [
            "Year = 2024"
          ],
          [
            "Year = 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Military Branch"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Military Branch"
          ],
          [
            "Military Branch"
          ],
          [
            "Military Branch"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which military branch has the maximum Median Fraud Loss value?",
          "Are there any data quality issues with the Median Fraud Loss column?",
          "Does the sample include all U.S. Military branches?",
          "What are all the U.S. Military branches in the dataset?",
          "What is the Median Fraud Loss for each Military Branch?",
          "Which Military Branch has the highest Median Fraud Loss value?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which military branch has the maximum Median Fraud Loss value?",
            "Are there any data quality issues with the Median Fraud Loss column?",
            "Does the sample include all U.S. Military branches?"
          ],
          [
            "What are all the U.S. Military branches in the dataset?",
            "What is the Median Fraud Loss for each Military Branch?",
            "Which Military Branch has the highest Median Fraud Loss value?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers.csv",
          "2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers__Fraud, Identity Theft, and Other Reports by Military Consumers",
          "2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers__Of the 212,158 total reports from military consumers in 2024, 92% provided military branch information."
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers.csv",
            "2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers__Fraud, Identity Theft, and Other Reports by Military Consumers",
            "2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers__Of the 212,158 total reports from military consumers in 2024, 92% provided military branch information."
          ],
          [
            "2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers.csv"
          ],
          [
            "2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different primary grouping columns: 'Military Branch' vs 'Military Status' vs 'Military Rank'",
          "Same metric columns but different grouping contexts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different primary grouping columns: 'Military Branch' vs 'Military Status' vs 'Military Rank'",
            "Same metric columns but different grouping contexts"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "median fraud loss": "dollars",
          "total fraud loss": "millions of dollars with 'M' suffix",
          "% reporting fraud loss": "percentage with '%' symbol"
        },
        "confidence": 0.7777777777777777,
        "votes": [
          {
            "Median Fraud Loss": "dollars",
            "Total Fraud Loss": "millions of dollars with 'M' suffix",
            "% Reporting Fraud Loss": "percentage with '%' symbol"
          },
          {
            "Median Fraud Loss": "USD (dollars)",
            "Total Fraud Loss": "USD (abbreviated with M for millions)",
            "% Reporting Fraud Loss": "percentage"
          },
          {
            "Median Fraud Loss": "USD"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Total Fraud Loss column contains string values with 'M' suffix (e.g., '$96M') that need conversion for numerical operations",
          "% Reporting Fraud Loss contains '%' symbol that needs removal for numerical operations",
          "Total Fraud Loss is abbreviated with 'M' suffix representing millions (e.g., '$96M' = $96,000,000)",
          "% Reporting Fraud Loss includes '%' symbol in string format",
          "Median Fraud Loss is in raw dollar amounts as integer"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total Fraud Loss column contains string values with 'M' suffix (e.g., '$96M') that need conversion for numerical operations",
            "% Reporting Fraud Loss contains '%' symbol that needs removal for numerical operations"
          ],
          [
            "Total Fraud Loss is abbreviated with 'M' suffix representing millions (e.g., '$96M' = $96,000,000)",
            "% Reporting Fraud Loss includes '%' symbol in string format",
            "Median Fraud Loss is in raw dollar amounts as integer"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "First file groups by Military Branch, second by Military Status, third by Military Rank - cannot be directly joined"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "First file groups by Military Branch, second by Military Status, third by Military Rank - cannot be directly joined"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Military Branch column contains exactly 6 unique values: U.S. Air Force, U.S. Army, U.S. Coast Guard, U.S. Marines, U.S. Navy, U.S. Space Force",
          "Median Fraud Loss values are integers",
          "Total rows in first file = 6 (matches number of military branches)",
          "Military Branch must be one of: U.S. Air Force, U.S. Army, U.S. Coast Guard, U.S. Marines, U.S. Navy, U.S. Space Force",
          "Median Fraud Loss must be non-negative integer",
          "Only data from 2024 should be considered",
          "'Military Branch' must be one of the valid U.S. Military Branches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Military Branch column contains exactly 6 unique values: U.S. Air Force, U.S. Army, U.S. Coast Guard, U.S. Marines, U.S. Navy, U.S. Space Force",
            "Median Fraud Loss values are integers",
            "Total rows in first file = 6 (matches number of military branches)"
          ],
          [
            "Military Branch must be one of: U.S. Air Force, U.S. Army, U.S. Coast Guard, U.S. Marines, U.S. Navy, U.S. Space Force",
            "Median Fraud Loss must be non-negative integer",
            "Only data from 2024 should be considered"
          ],
          [
            "'Military Branch' must be one of the valid U.S. Military Branches"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Use only first file (2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers.csv) as it contains Military Branch grouping",
          "Filter to year 2024 (implied by filename)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Use only first file (2024_CSN_Fraud, Identity Theft, and Other Reports by Military Consumers.csv) as it contains Military Branch grouping",
            "Filter to year 2024 (implied by filename)"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Find maximum value in Median Fraud Loss column",
          "Verify data completeness for all military branches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Find maximum value in Median Fraud Loss column",
            "Verify data completeness for all military branches"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single military branch name with highest Median Fraud Loss",
          "Include the actual Median Fraud Loss value for verification",
          "Output should identify the single Military Branch with the largest Median Fraud Loss",
          "The answer should include the branch name and the corresponding Median Fraud Loss value",
          "Report the 'Military Branch' with the highest 'Median Fraud Loss'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single military branch name with highest Median Fraud Loss",
            "Include the actual Median Fraud Loss value for verification"
          ],
          [
            "Output should identify the single Military Branch with the largest Median Fraud Loss",
            "The answer should include the branch name and the corresponding Median Fraud Loss value"
          ],
          [
            "Report the 'Military Branch' with the highest 'Median Fraud Loss'"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6500000000000001
  },
  "legal-easy-26": {
    "m_q": {
      "target_metric": {
        "value": "States where 'Identity Theft' is the top report category",
        "confidence": 0.6666666666666666,
        "votes": [
          "States where 'Identity Theft' is the top report category",
          "List of states where 'Identity Theft' is the top report category",
          "States where 'Identity Theft' is the top report category"
        ]
      },
      "filters": {
        "value": [
          "Category = 'Identity Theft'",
          "Top rank per state",
          "Category equals 'Identity Theft'",
          "Category rank is 1 (top category) for each state"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Category = 'Identity Theft'",
            "Top rank per state"
          ],
          [
            "Category equals 'Identity Theft'",
            "Category rank is 1 (top category) for each state"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "State"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "State"
          ],
          [
            "State"
          ],
          [
            "State"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the ranking of categories within each state?",
          "How many states have Identity Theft as their top category?",
          "What are the report counts and percentages for Identity Theft in those states?",
          "What is the top category for each state?",
          "Which states have 'Identity Theft' as that top category?",
          "How is 'top' determined - by '# of Reports' or by ranking order in the dataset?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the ranking of categories within each state?",
            "How many states have Identity Theft as their top category?",
            "What are the report counts and percentages for Identity Theft in those states?"
          ],
          [
            "What is the top category for each state?",
            "Which states have 'Identity Theft' as that top category?",
            "How is 'top' determined - by '# of Reports' or by ranking order in the dataset?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_State_Top_Ten_Report_Categories.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_State_Top_Ten_Report_Categories.csv"
          ],
          [
            "2024_CSN_State_Top_Ten_Report_Categories.csv"
          ],
          [
            "2024_CSN_State_Top_Ten_Report_Categories.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count",
          "percentage": "percentage"
        },
        "confidence": 1.0,
        "votes": [
          {
            "# of Reports": "count",
            "Percentage": "percentage"
          },
          {
            "# of Reports": "count",
            "Percentage": "percentage (as string with % symbol)"
          },
          {
            "# of Reports": "number of reports",
            "Percentage": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Percentage column is stored as string with '%' symbol, needs conversion to numeric for ranking",
          "Percentage column is stored as string (e.g., '32%') rather than numeric decimal",
          "Need to determine if top category is first row per state or highest '# of Reports'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage column is stored as string with '%' symbol, needs conversion to numeric for ranking"
          ],
          [
            "Percentage column is stored as string (e.g., '32%') rather than numeric decimal",
            "Need to determine if top category is first row per state or highest '# of Reports'"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each state should have exactly one top category (highest # of Reports or Percentage)",
          "Categories are ranked within each state",
          "Total percentage per state should sum to 100%",
          "Each state should have multiple category entries (up to 10)",
          "'Identity Theft' must exist as a category value",
          "The top category is assumed to be the first occurrence for each state or the one with highest '# of Reports'",
          "For each state, determine the category with the highest '# of Reports'.",
          "Identify states where the top category is 'Identity Theft'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each state should have exactly one top category (highest # of Reports or Percentage)",
            "Categories are ranked within each state",
            "Total percentage per state should sum to 100%"
          ],
          [
            "Each state should have multiple category entries (up to 10)",
            "'Identity Theft' must exist as a category value",
            "The top category is assumed to be the first occurrence for each state or the one with highest '# of Reports'"
          ],
          [
            "For each state, determine the category with the highest '# of Reports'.",
            "Identify states where the top category is 'Identity Theft'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to rows where Category = 'Identity Theft'",
          "Then filter to rows where Identity Theft has the highest # of Reports within its state",
          "For each state, identify the category with maximum '# of Reports'",
          "Filter to only states where that maximum category is 'Identity Theft'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to rows where Category = 'Identity Theft'",
            "Then filter to rows where Identity Theft has the highest # of Reports within its state"
          ],
          [
            "For each state, identify the category with maximum '# of Reports'",
            "Filter to only states where that maximum category is 'Identity Theft'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify ranking consistency between # of Reports and Percentage columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify ranking consistency between # of Reports and Percentage columns"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of state names",
          "Include supporting metrics (# of Reports, Percentage) for verification",
          "Return list of state names",
          "States should be those where 'Identity Theft' has the highest '# of Reports' compared to other categories in that state",
          "List of states."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of state names",
            "Include supporting metrics (# of Reports, Percentage) for verification"
          ],
          [
            "Return list of state names",
            "States should be those where 'Identity Theft' has the highest '# of Reports' compared to other categories in that state"
          ],
          [
            "List of states."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6333333333333335
  },
  "legal-easy-27": {
    "m_q": {
      "target_metric": {
        "value": "Count of distinct states where 'Prizes, Sweepstakes and Lotteries' appears in the top-10 categories",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of distinct states where 'Prizes, Sweepstakes and Lotteries' appears in the top-10 categories",
          "Count of distinct states that have 'Prizes, Sweepstakes and Lotteries' in their top-10 report categories",
          "Count of states where 'Prizes, Sweepstakes and Lotteries' is in the top 10 report categories"
        ]
      },
      "filters": {
        "value": [
          "Year = 2024",
          "Category = 'Prizes, Sweepstakes and Lotteries'",
          "Category equals 'Prizes, Sweepstakes and Lotteries'",
          "Year is 2024",
          "Category is in top 10 for each state"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Year = 2024",
            "Category = 'Prizes, Sweepstakes and Lotteries'"
          ],
          [
            "Category equals 'Prizes, Sweepstakes and Lotteries'",
            "Year is 2024"
          ],
          [
            "Year = 2024",
            "Category is in top 10 for each state"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "State"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "State"
          ],
          [
            "State"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which states have 'Prizes, Sweepstakes and Lotteries' in their top-10 categories?",
          "What is the count of such states?",
          "Which states have 'Prizes, Sweepstakes and Lotteries' as one of their categories?",
          "How many unique states are represented in the filtered dataset?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which states have 'Prizes, Sweepstakes and Lotteries' in their top-10 categories?",
            "What is the count of such states?"
          ],
          [
            "Which states have 'Prizes, Sweepstakes and Lotteries' as one of their categories?",
            "How many unique states are represented in the filtered dataset?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_State_Top_Ten_Report_Categories.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_State_Top_Ten_Report_Categories.csv"
          ],
          [
            "2024_CSN_State_Top_Ten_Report_Categories.csv"
          ],
          [
            "2024_CSN_State_Top_Ten_Report_Categories.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count",
          "percentage": "percentage"
        },
        "confidence": 1.0,
        "votes": [
          {
            "# of Reports": "count",
            "Percentage": "percentage"
          },
          {
            "# of Reports": "count",
            "Percentage": "percentage (with % symbol)"
          },
          {
            "# of Reports": "number of reports",
            "Percentage": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Percentage column is stored as string with '%' symbol, needs conversion to numeric for calculations",
          "Percentage column contains string values with '%' symbol rather than numeric decimals or floats"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage column is stored as string with '%' symbol, needs conversion to numeric for calculations"
          ],
          [
            "Percentage column contains string values with '%' symbol rather than numeric decimals or floats"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each state should have exactly 10 rows (top-10 categories)",
          "Sum of percentages for each state should be 100%",
          "Category names are consistent across states",
          "Each state should have exactly 10 category entries (top-10)",
          "Category name must match exactly: 'Prizes, Sweepstakes and Lotteries'",
          "Data is already filtered to 2024 based on filename",
          "The data represents the top 10 categories for each state, so no explicit filtering for top 10 is needed."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each state should have exactly 10 rows (top-10 categories)",
            "Sum of percentages for each state should be 100%",
            "Category names are consistent across states"
          ],
          [
            "Each state should have exactly 10 category entries (top-10)",
            "Category name must match exactly: 'Prizes, Sweepstakes and Lotteries'",
            "Data is already filtered to 2024 based on filename"
          ],
          [
            "The data represents the top 10 categories for each state, so no explicit filtering for top 10 is needed."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Category contains 'Prizes, Sweepstakes and Lotteries'",
          "Filter rows where Category == 'Prizes, Sweepstakes and Lotteries'",
          "Count unique values in State column after filtering"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Category contains 'Prizes, Sweepstakes and Lotteries'"
          ],
          [
            "Filter rows where Category == 'Prizes, Sweepstakes and Lotteries'",
            "Count unique values in State column after filtering"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify each state has exactly 10 rows",
          "Check for duplicate state-category combinations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify each state has exactly 10 rows",
            "Check for duplicate state-category combinations"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "No decimal places",
          "Return a single integer representing the count of states",
          "The answer should be between 0 and 52 (50 states + DC + territories)",
          "The output should be a single integer representing the number of states."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count",
            "No decimal places"
          ],
          [
            "Return a single integer representing the count of states",
            "The answer should be between 0 and 52 (50 states + DC + territories)"
          ],
          [
            "The output should be a single integer representing the number of states."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6033333333333334
  },
  "legal-easy-3": {
    "m_q": {
      "target_metric": {
        "value": "ratio of identity theft reports in 2024 vs 2001",
        "confidence": 0.6666666666666666,
        "votes": [
          "ratio of identity theft reports in 2024 vs 2001",
          "Ratio of identity theft reports in 2024 vs 2001",
          "Ratio of identity theft reports in 2024 to identity theft reports in 2001, rounded to 4 decimal places"
        ]
      },
      "filters": {
        "value": [
          "Year = 2001",
          "Year = 2024",
          "Year == 2024",
          "Year == 2001"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year = 2001",
            "Year = 2024"
          ],
          [
            "Year == 2024",
            "Year == 2001"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the count of identity theft reports in 2001?",
          "What is the count of identity theft reports in 2024?",
          "What is the ratio of 2024 count to 2001 count?",
          "What is the number of identity theft reports in 2024?",
          "What is the number of identity theft reports in 2001?",
          "What is the ratio of 2024 identity theft reports divided by 2001 identity theft reports?"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "What is the count of identity theft reports in 2001?",
            "What is the count of identity theft reports in 2024?",
            "What is the ratio of 2024 count to 2001 count?"
          ],
          [
            "What is the number of identity theft reports in 2024?",
            "What is the number of identity theft reports in 2001?",
            "What is the ratio of 2024 identity theft reports divided by 2001 identity theft reports?"
          ],
          [
            "What is the number of identity theft reports in 2024?",
            "What is the number of identity theft reports in 2001?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Number_of_Reports_by_Type.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Number_of_Reports_by_Type.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "year",
          "fraud": "count",
          "identity theft": "count",
          "other": "count"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Year": "year",
            "Fraud": "count",
            "Identity Theft": "count",
            "Other": "count"
          },
          {
            "Year": "calendar year",
            "Fraud": "count of fraud reports",
            "Identity Theft": "count of identity theft reports",
            "Other": "count of other reports"
          },
          {
            "Year": "year",
            "Fraud": "number of reports",
            "Identity Theft": "number of reports",
            "Other": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Data shows significant growth in all report types over time, suggesting scale differences between early and late years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data shows significant growth in all report types over time, suggesting scale differences between early and late years"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be between 2001 and 2024",
          "Count values should be non-negative integers",
          "2024 data must be present in the full dataset (not shown in sample)",
          "Year 2001 must exist in dataset",
          "Year 2024 must exist in dataset",
          "Identity Theft column must have non-zero value for 2001 to avoid division by zero",
          "Year must be an integer",
          "Fraud, Identity Theft, and Other must be non-negative integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be between 2001 and 2024",
            "Count values should be non-negative integers",
            "2024 data must be present in the full dataset (not shown in sample)"
          ],
          [
            "Year 2001 must exist in dataset",
            "Year 2024 must exist in dataset",
            "Identity Theft column must have non-zero value for 2001 to avoid division by zero"
          ],
          [
            "Year must be an integer",
            "Fraud, Identity Theft, and Other must be non-negative integers"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to only Identity Theft column",
          "Filter to only Year 2001 and 2024",
          "Extract Identity Theft value where Year == 2024",
          "Extract Identity Theft value where Year == 2001",
          "Year = 2001",
          "Year = 2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to only Identity Theft column",
            "Filter to only Year 2001 and 2024"
          ],
          [
            "Extract Identity Theft value where Year == 2024",
            "Extract Identity Theft value where Year == 2001"
          ],
          [
            "Year = 2001",
            "Year = 2024"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if 2024 data exists in full dataset",
          "Verify no missing values for required years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if 2024 data exists in full dataset",
            "Verify no missing values for required years"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Ratio must be rounded to 4 decimal places",
          "Round result to 4 decimal places",
          "Output should be a single numeric value"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "Ratio must be rounded to 4 decimal places"
          ],
          [
            "Round result to 4 decimal places",
            "Output should be a single numeric value"
          ],
          [
            "Ratio must be rounded to 4 decimal places"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6277777777777779
  },
  "legal-easy-4": {
    "m_q": {
      "target_metric": {
        "value": "total number of fraud reports submitted to FTC via web between 2022 and 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "total number of fraud reports submitted to FTC via web between 2022 and 2024",
          "Total number of fraud reports submitted via FTC web interface between 2022 and 2024",
          "Total number of fraud reports reported by FTC over the web between 2022 and 2024"
        ]
      },
      "filters": {
        "value": [
          "Year between 2022 and 2024 inclusive",
          "Data Contributor contains 'FTC - Web Reports (Fraud & Other)'",
          "Year in [2022, 2023, 2024]",
          "Data Contributor contains 'FTC - Web Reports' and 'Fraud'",
          "Exclude IDT (Identity Theft) reports, include only Fraud & Other",
          "Year between 2022 and 2024",
          "Data Contributor contains 'FTC - Web Reports'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year between 2022 and 2024 inclusive",
            "Data Contributor contains 'FTC - Web Reports (Fraud & Other)'"
          ],
          [
            "Year in [2022, 2023, 2024]",
            "Data Contributor contains 'FTC - Web Reports' and 'Fraud'",
            "Exclude IDT (Identity Theft) reports, include only Fraud & Other"
          ],
          [
            "Year between 2022 and 2024",
            "Data Contributor contains 'FTC - Web Reports'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the breakdown by year?",
          "Are there other FTC web report categories that should be included?",
          "Should IDT (Identity Theft) web reports be considered fraud?",
          "Which file contains FTC web fraud reports?",
          "How to identify web-based fraud reports vs other report types?",
          "What is the distinction between 'Fraud & Other' and 'IDT' in the data?",
          "Should IDT be included in fraud count or excluded?",
          "What are the report counts for each year (2022, 2023, 2024)?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the breakdown by year?",
            "Are there other FTC web report categories that should be included?",
            "Should IDT (Identity Theft) web reports be considered fraud?"
          ],
          [
            "Which file contains FTC web fraud reports?",
            "How to identify web-based fraud reports vs other report types?",
            "What is the distinction between 'Fraud & Other' and 'IDT' in the data?",
            "Should IDT be included in fraud count or excluded?",
            "What are the report counts for each year (2022, 2023, 2024)?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Data_Contributors__FTC"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Data_Contributors__FTC"
          ],
          [
            "2024_CSN_Data_Contributors__FTC"
          ],
          [
            "2024_CSN_Data_Contributors__FTC"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "First file has 'State Law Enforcement Agencies' column instead of 'Year' column",
          "Different files have same column names but different data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "First file has 'State Law Enforcement Agencies' column instead of 'Year' column",
            "Different files have same column names but different data"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of reports",
          "%": "percentage of total reports",
          "year": "calendar year"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "# of Reports": "count of reports",
            "%": "percentage of total reports"
          },
          {
            "# of Reports": "count",
            "%": "percentage",
            "Year": "calendar year"
          },
          {
            "# of Reports": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Percentages are stored as strings with '%' symbol",
          "Need to convert percentage strings to numeric values if needed"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentages are stored as strings with '%' symbol",
            "Need to convert percentage strings to numeric values if needed"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Multiple files contain 'Data Contributor' column with different values",
          "Year ranges may differ across files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple files contain 'Data Contributor' column with different values",
            "Year ranges may differ across files"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year must be 2022, 2023, or 2024",
          "Data Contributor must exactly match 'FTC - Web Reports (Fraud & Other)'",
          "Sum of # of Reports should be calculated",
          "Year must be between 2022 and 2024 inclusive",
          "Data Contributor must match pattern 'FTC - Web Reports (Fraud & Other)'",
          "# of Reports must be non-negative integer",
          "Year must be between 2022 and 2024",
          "Data Contributor must contain 'FTC - Web Reports'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year must be 2022, 2023, or 2024",
            "Data Contributor must exactly match 'FTC - Web Reports (Fraud & Other)'",
            "Sum of # of Reports should be calculated"
          ],
          [
            "Year must be between 2022 and 2024 inclusive",
            "Data Contributor must match pattern 'FTC - Web Reports (Fraud & Other)'",
            "# of Reports must be non-negative integer"
          ],
          [
            "Year must be between 2022 and 2024",
            "Data Contributor must contain 'FTC - Web Reports'"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter 2024_CSN_Data_Contributors__FTC where Year in [2022, 2023, 2024]",
          "Filter where Data Contributor = 'FTC - Web Reports (Fraud & Other)'",
          "Filter rows where Data Contributor == 'FTC - Web Reports (Fraud & Other)'",
          "Filter rows where Year in [2022, 2023, 2024]"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter 2024_CSN_Data_Contributors__FTC where Year in [2022, 2023, 2024]",
            "Filter where Data Contributor = 'FTC - Web Reports (Fraud & Other)'"
          ],
          [
            "Filter rows where Data Contributor == 'FTC - Web Reports (Fraud & Other)'",
            "Filter rows where Year in [2022, 2023, 2024]"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing years in the range",
          "Verify no duplicate rows for same Year and Data Contributor combination"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing years in the range",
            "Verify no duplicate rows for same Year and Data Contributor combination"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numeric total without formatting",
          "Should be integer value",
          "Return single integer value representing sum of fraud reports",
          "Sum # of Reports column across filtered rows",
          "Output must be a single number representing the total number of reports"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric total without formatting",
            "Should be integer value"
          ],
          [
            "Return single integer value representing sum of fraud reports",
            "Sum # of Reports column across filtered rows"
          ],
          [
            "Output must be a single number representing the total number of reports"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6000000000000001
  },
  "legal-easy-5": {
    "m_q": {
      "target_metric": {
        "value": "total number of money befrauded summed over all payment methods in millions of dollars",
        "confidence": 0.3333333333333333,
        "votes": [
          "total number of money befrauded summed over all payment methods in millions of dollars",
          "Total amount of money defrauded across all payment methods in millions of dollars",
          "Total money defrauded in millions of dollars, summed over all payment methods."
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Extract numeric values from 'Total $ Loss' column",
          "Convert all values to consistent millions unit",
          "Sum all values",
          "Format as integer",
          "What is the Total $ Loss for each payment method?",
          "How to parse the dollar amounts from string format with 'M' suffix and commas?",
          "What is the sum of all parsed dollar amounts?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract numeric values from 'Total $ Loss' column",
            "Convert all values to consistent millions unit",
            "Sum all values",
            "Format as integer"
          ],
          [
            "What is the Total $ Loss for each payment method?",
            "How to parse the dollar amounts from string format with 'M' suffix and commas?",
            "What is the sum of all parsed dollar amounts?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Fraud_Reports_by_Payment_Method.csv",
          "2024_CSN_Fraud_Reports_by_Payment_Method__section_2"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "2024_CSN_Fraud_Reports_by_Payment_Method.csv",
            "2024_CSN_Fraud_Reports_by_Payment_Method__section_2"
          ],
          [
            "2024_CSN_Fraud_Reports_by_Payment_Method.csv"
          ],
          [
            "2024_CSN_Fraud_Reports_by_Payment_Method.csv",
            "2024_CSN_Fraud_Reports_by_Payment_Method__section_2"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Second file has different structure with only 1 row and 3 columns",
          "Column names in second file don't match first file"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Second file has different structure with only 1 row and 3 columns",
            "Column names in second file don't match first file"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "total $ loss": "millions of dollars with 'M' suffix",
          "# of reports": "count",
          "2,600,678": "total fraud reports count"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "Total $ Loss": "millions of dollars with 'M' suffix",
            "# of Reports": "count",
            "2,600,678": "total fraud reports count"
          },
          {
            "Total $ Loss": "millions of US dollars",
            "# of Reports": "count"
          },
          {
            "Total $ Loss": "US Dollars",
            "# of Reports": "Number of Reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "'Total $ Loss' values contain 'M' suffix and commas in some values (e.g., '$2,089M')",
          "Need to convert all monetary values to numeric millions",
          "Total $ Loss column contains string values with dollar signs, 'M' suffix indicating millions, and comma separators",
          "Values need to be parsed to extract numeric amounts (e.g., '$275M' -> 275, '$2,089M' -> 2089)",
          "The 'Total $ Loss' column is in millions of dollars, indicated by the 'M' suffix. Need to remove '$' and 'M' and convert to numeric."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Total $ Loss' values contain 'M' suffix and commas in some values (e.g., '$2,089M')",
            "Need to convert all monetary values to numeric millions"
          ],
          [
            "Total $ Loss column contains string values with dollar signs, 'M' suffix indicating millions, and comma separators",
            "Values need to be parsed to extract numeric amounts (e.g., '$275M' -> 275, '$2,089M' -> 2089)"
          ],
          [
            "The 'Total $ Loss' column is in millions of dollars, indicated by the 'M' suffix. Need to remove '$' and 'M' and convert to numeric."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Second file appears to contain summary statistics but doesn't directly contribute to the analytical question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Second file appears to contain summary statistics but doesn't directly contribute to the analytical question"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 3.0,
        "confidence": 1.0,
        "votes": [
          3.0,
          3.0,
          3.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "All 'Total $ Loss' values must be converted to numeric millions",
          "Sum should include all 10 payment methods from first file",
          "Final output must be integer in millions of dollars",
          "All Total $ Loss values should be positive numbers",
          "All Payment Method entries should be non-null",
          "Sum should include all 10 payment methods listed",
          "The final result should be an integer representing millions of dollars.",
          "The 'Total $ Loss' column needs to be cleaned by removing '$' and 'M' before summing."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All 'Total $ Loss' values must be converted to numeric millions",
            "Sum should include all 10 payment methods from first file",
            "Final output must be integer in millions of dollars"
          ],
          [
            "All Total $ Loss values should be positive numbers",
            "All Payment Method entries should be non-null",
            "Sum should include all 10 payment methods listed"
          ],
          [
            "The final result should be an integer representing millions of dollars.",
            "The 'Total $ Loss' column needs to be cleaned by removing '$' and 'M' before summing."
          ]
        ]
      },
      "derived_filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify sum of '# of Reports' matches patterns in second file if needed"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify sum of '# of Reports' matches patterns in second file if needed"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Integer number",
          "In millions of dollars",
          "No currency symbols or commas",
          "Output must be an integer number",
          "Output must be in millions of dollars",
          "No decimal places required",
          "The output should be an integer."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Integer number",
            "In millions of dollars",
            "No currency symbols or commas"
          ],
          [
            "Output must be an integer number",
            "Output must be in millions of dollars",
            "No decimal places required"
          ],
          [
            "The output should be an integer."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.563888888888889
  },
  "legal-easy-9": {
    "m_q": {
      "target_metric": {
        "value": "year with greatest relative increase in total reports (Fraud, Identity Theft and Other) compared to previous year",
        "confidence": 0.3333333333333333,
        "votes": [
          "year with greatest relative increase in total reports (Fraud, Identity Theft and Other) compared to previous year",
          "Year with greatest relative increase (percentage change) in total reports compared to previous year",
          "Year with the greatest relative increase in total reports compared to the previous year, between 2002 and 2024 inclusive."
        ]
      },
      "filters": {
        "value": [
          "Year >= 2002",
          "Year <= 2024"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Year >= 2002",
            "Year <= 2024"
          ],
          [
            "Year >= 2002",
            "Year <= 2024"
          ],
          [
            "Year >= 2002",
            "Year <= 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Year"
          ],
          [
            "Year"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate total reports for each year",
          "Calculate year-over-year relative increase for each year",
          "Identify maximum relative increase",
          "What is the total number of reports for each year from 2002 to 2024?",
          "What is the year-over-year absolute change in reports for each consecutive year pair?",
          "What is the year-over-year relative (percentage) change in reports for each consecutive year pair?",
          "Which year has the maximum relative increase compared to its previous year?",
          "Calculate the total number of reports for each year.",
          "Calculate the relative increase in total reports for each year compared to the previous year.",
          "Identify the year with the maximum relative increase."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Calculate total reports for each year",
            "Calculate year-over-year relative increase for each year",
            "Identify maximum relative increase"
          ],
          [
            "What is the total number of reports for each year from 2002 to 2024?",
            "What is the year-over-year absolute change in reports for each consecutive year pair?",
            "What is the year-over-year relative (percentage) change in reports for each consecutive year pair?",
            "Which year has the maximum relative increase compared to its previous year?"
          ],
          [
            "Calculate the total number of reports for each year.",
            "Calculate the relative increase in total reports for each year compared to the previous year.",
            "Identify the year with the maximum relative increase."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Report_Count.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Report_Count.csv"
          ],
          [
            "2024_CSN_Report_Count.csv"
          ],
          [
            "2024_CSN_Report_Count.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "calendar year",
          "# of reports": "count of reports",
          "number of reports": "number of reports"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Year": "calendar year",
            "# of Reports": "count of reports"
          },
          {
            "Year": "calendar year (integer)",
            "# of Reports": "count (integer)"
          },
          {
            "Year": "year",
            "Number of Reports": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Year column appears as float64 but contains integer years",
          "# of Reports column is float64 but contains integer counts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column appears as float64 but contains integer years",
            "# of Reports column is float64 but contains integer counts"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be integers between 2001-2024",
          "# of Reports should be non-negative integers",
          "Data should be sorted by Year ascending",
          "Year must be between 2002 and 2024 inclusive",
          "Previous year data must exist to calculate relative increase (2002 requires 2001 data)",
          "# of Reports must be positive and non-zero for valid percentage calculation",
          "Year must be an integer.",
          "Number of Reports must be a non-negative number."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be integers between 2001-2024",
            "# of Reports should be non-negative integers",
            "Data should be sorted by Year ascending"
          ],
          [
            "Year must be between 2002 and 2024 inclusive",
            "Previous year data must exist to calculate relative increase (2002 requires 2001 data)",
            "# of Reports must be positive and non-zero for valid percentage calculation"
          ],
          [
            "Year must be an integer.",
            "Number of Reports must be a non-negative number."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude 2001 as baseline year for comparison",
          "Require consecutive years for year-over-year calculation",
          "Filter out Year = 2001 from final analysis (but retain for calculating 2002's increase)",
          "Only consider years from 2002 to 2024 as candidates for greatest increase"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude 2001 as baseline year for comparison",
            "Require consecutive years for year-over-year calculation"
          ],
          [
            "Filter out Year = 2001 from final analysis (but retain for calculating 2002's increase)",
            "Only consider years from 2002 to 2024 as candidates for greatest increase"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing years in sequence",
          "Verify monotonic increase in reports over time"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing years in sequence",
            "Verify monotonic increase in reports over time"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Year value as integer",
          "Relative increase as percentage",
          "Return the specific year (between 2002-2024) with the greatest relative increase",
          "Relative increase calculated as: ((current_year - previous_year) / previous_year) * 100",
          "Output the year with the greatest relative increase."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year value as integer",
            "Relative increase as percentage"
          ],
          [
            "Return the specific year (between 2002-2024) with the greatest relative increase",
            "Relative increase calculated as: ((current_year - previous_year) / previous_year) * 100"
          ],
          [
            "Output the year with the greatest relative increase."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6333333333333335
  },
  "legal-hard-1": {
    "m_q": {
      "target_metric": {
        "value": "average number of reported identity thefts",
        "confidence": 0.3333333333333333,
        "votes": [
          "average number of reported identity thefts",
          "Average number of reported identity thefts for metropolitan areas with population > 1,000,000 in 2023",
          "average number of reported identity thefts for metropolitan areas with population greater than one million in 2023"
        ]
      },
      "filters": {
        "value": [
          "metropolitan areas with population > 1,000,000 in 2023",
          "drop entries with no match in fraud reports",
          "Metropolitan areas with 2023 population > 1,000,000",
          "Exclude entries with no match between fraud reports and population data",
          "metropolitan area population > 1,000,000 in 2023",
          "valid metropolitan area name match between fraud reports and population data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "metropolitan areas with population > 1,000,000 in 2023",
            "drop entries with no match in fraud reports"
          ],
          [
            "Metropolitan areas with 2023 population > 1,000,000",
            "Exclude entries with no match between fraud reports and population data"
          ],
          [
            "metropolitan area population > 1,000,000 in 2023",
            "valid metropolitan area name match between fraud reports and population data"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Metropolitan Area"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan Area"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "extract population data from HTML table",
          "normalize metropolitan area names by removing suffixes like 'Metropolitan Statistical Area' and normalizing punctuation",
          "linearly interpolate population for 2023 if needed",
          "match normalized names between population and fraud data",
          "calculate average of # of Reports for qualifying areas",
          "round result to 4 decimal places",
          "What are the population sizes of metropolitan areas in 2023 (with linear interpolation if needed)?",
          "Which metropolitan areas have population > 1,000,000 in 2023?",
          "What are the reported identity thefts for each metropolitan area?",
          "How to normalize and match metropolitan area names between sources?",
          "What is the average of identity theft reports across filtered areas?",
          "extract metropolitan area names and number of reports from state-level CSV files",
          "extract metropolitan area names and populations from the HTML file",
          "normalize metropolitan area names for matching",
          "estimate 2023 population by linear interpolation if necessary",
          "filter metropolitan areas with population > 1,000,000 in 2023",
          "calculate the average number of reported identity thefts for the filtered metropolitan areas"
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "extract population data from HTML table",
            "normalize metropolitan area names by removing suffixes like 'Metropolitan Statistical Area' and normalizing punctuation",
            "linearly interpolate population for 2023 if needed",
            "match normalized names between population and fraud data",
            "calculate average of # of Reports for qualifying areas",
            "round result to 4 decimal places"
          ],
          [
            "What are the population sizes of metropolitan areas in 2023 (with linear interpolation if needed)?",
            "Which metropolitan areas have population > 1,000,000 in 2023?",
            "What are the reported identity thefts for each metropolitan area?",
            "How to normalize and match metropolitan area names between sources?",
            "What is the average of identity theft reports across filtered areas?"
          ],
          [
            "extract metropolitan area names and number of reports from state-level CSV files",
            "extract metropolitan area names and populations from the HTML file",
            "normalize metropolitan area names for matching",
            "estimate 2023 population by linear interpolation if necessary",
            "filter metropolitan areas with population > 1,000,000 in 2023",
            "calculate the average number of reported identity thefts for the filtered metropolitan areas"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "metropolitan_statistics.html",
          "Alabama.csv",
          "Alaska.csv",
          "Arizona.csv",
          "Arkansas.csv",
          "California.csv",
          "Colorado.csv",
          "Connecticut.csv",
          "Delaware.csv",
          "DistrictofColumbia.csv",
          "Florida.csv",
          "Georgia.csv",
          "Hawaii.csv",
          "Idaho.csv",
          "Illinois.csv",
          "Indiana.csv",
          "Iowa.csv",
          "Kansas.csv",
          "Kentucky.csv",
          "Louisiana.csv",
          "Maine.csv",
          "Maryland.csv",
          "Massachusetts.csv",
          "Michigan.csv",
          "Minnesota.csv",
          "Mississippi.csv",
          "Missouri.csv",
          "Montana.csv",
          "Nebraska.csv",
          "Nevada.csv",
          "NewHampshire.csv",
          "NewJersey.csv",
          "NewMexico.csv",
          "NewYork.csv",
          "NorthCarolina.csv",
          "NorthDakota.csv",
          "Ohio.csv",
          "Oklahoma.csv",
          "Oregon.csv",
          "Pennsylvania.csv",
          "PuertoRico.csv",
          "RhodeIsland.csv",
          "SouthCarolina.csv",
          "SouthDakota.csv",
          "Tennessee.csv",
          "Texas.csv",
          "Utah.csv",
          "Vermont.csv",
          "Virginia.csv",
          "Washington.csv",
          "WestVirginia.csv",
          "Wisconsin.csv",
          "Wyoming.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "metropolitan_statistics.html",
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "metropolitan_statistics.html",
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "metropolitan_statistics.html",
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "HTML contains population data in unstructured format while CSVs contain structured fraud reports",
          "state CSV files have identical schema but separate files",
          "HTML may contain different naming conventions for metropolitan areas",
          "Metropolitan area names include suffixes like 'Metropolitan Statistical Area' or 'MSA' in state CSVs but may differ in HTML",
          "State CSVs contain footer rows with metadata that need filtering",
          "Multi-state metropolitan areas appear in multiple state files",
          "Column names are inconsistent across state CSV files (e.g., '# of Reports' vs '# of reports')",
          "The metropolitan_statistics.html file contains population data in HTML format, requiring parsing",
          "The metropolitan_statistics.html file contains multiple tables, requiring identification of the correct table",
          "The metropolitan_statistics.html file contains population data for multiple years, requiring filtering and interpolation"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "HTML contains population data in unstructured format while CSVs contain structured fraud reports",
            "state CSV files have identical schema but separate files",
            "HTML may contain different naming conventions for metropolitan areas"
          ],
          [
            "Metropolitan area names include suffixes like 'Metropolitan Statistical Area' or 'MSA' in state CSVs but may differ in HTML",
            "State CSVs contain footer rows with metadata that need filtering",
            "Multi-state metropolitan areas appear in multiple state files"
          ],
          [
            "Column names are inconsistent across state CSV files (e.g., '# of Reports' vs '# of reports')",
            "The metropolitan_statistics.html file contains population data in HTML format, requiring parsing",
            "The metropolitan_statistics.html file contains multiple tables, requiring identification of the correct table",
            "The metropolitan_statistics.html file contains population data for multiple years, requiring filtering and interpolation"
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of identity theft reports",
          "population": "number of people",
          "year": "year",
          "numberofreports": "number of identity theft reports"
        },
        "confidence": 0.5833333333333333,
        "votes": [
          {
            "# of Reports": "count of identity theft reports",
            "population": "number of people"
          },
          {
            "# of Reports": "count",
            "population": "persons",
            "year": "year"
          },
          {
            "NumberOfReports": "number of identity theft reports",
            "population": "number of people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "population data may be in thousands or raw counts - need to verify from HTML",
          "fraud reports are counts, not rates",
          "Population data may be in different census years requiring interpolation to 2023"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "population data may be in thousands or raw counts - need to verify from HTML",
            "fraud reports are counts, not rates"
          ],
          [
            "Population data may be in different census years requiring interpolation to 2023"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "metropolitan area naming conventions differ between HTML population data and CSV fraud data",
          "some metropolitan areas span multiple states (e.g., 'Chicago-Naperville-Elgin, IL-IN') and may appear in multiple CSV files",
          "Metropolitan area naming conventions differ between HTML and CSV sources",
          "Multi-state metropolitan areas may have duplicate or partial entries across state files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "metropolitan area naming conventions differ between HTML population data and CSV fraud data",
            "some metropolitan areas span multiple states (e.g., 'Chicago-Naperville-Elgin, IL-IN') and may appear in multiple CSV files"
          ],
          [
            "Metropolitan area naming conventions differ between HTML and CSV sources",
            "Multi-state metropolitan areas may have duplicate or partial entries across state files"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "Metropolitan Areas are defined by the Office of Management and Budget..."
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "Metropolitan Areas are defined by the Office of Management and Budget..."
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "csv and html",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "population > 1,000,000",
          "must have matching fraud report data",
          "use only city and state portion of name (e.g., 'Chicago, IL' from 'Chicago-Naperville-Elgin, IL-IN Metropolitan Statistical Area')",
          "Only include metropolitan areas with 2023 population > 1,000,000",
          "Drop entries where metropolitan area name cannot be matched between HTML and CSV sources",
          "Use linear interpolation between two known census years to estimate 2023 population if direct data unavailable",
          "Metropolitan area names must be normalized to ensure accurate matching across data sources",
          "Population data must be available for at least two census years to allow for linear interpolation",
          "Metropolitan areas with missing population data should be excluded from the analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "population > 1,000,000",
            "must have matching fraud report data",
            "use only city and state portion of name (e.g., 'Chicago, IL' from 'Chicago-Naperville-Elgin, IL-IN Metropolitan Statistical Area')"
          ],
          [
            "Only include metropolitan areas with 2023 population > 1,000,000",
            "Drop entries where metropolitan area name cannot be matched between HTML and CSV sources",
            "Use linear interpolation between two known census years to estimate 2023 population if direct data unavailable"
          ],
          [
            "Metropolitan area names must be normalized to ensure accurate matching across data sources",
            "Population data must be available for at least two census years to allow for linear interpolation",
            "Metropolitan areas with missing population data should be excluded from the analysis"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "remove rows containing 'Metropolitan Areas are defined by the Office of Management and Budget...'",
          "normalize names by removing 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area' suffixes",
          "normalize punctuation and spacing",
          "Normalize metropolitan area names by extracting city and state, removing suffixes like 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area', normalizing punctuation",
          "Exclude footer/metadata rows from state CSV files",
          "Handle multi-state metropolitan areas that may appear in multiple CSV files",
          "Population in 2023 > 1,000,000",
          "Valid metropolitan area name match"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "remove rows containing 'Metropolitan Areas are defined by the Office of Management and Budget...'",
            "normalize names by removing 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area' suffixes",
            "normalize punctuation and spacing"
          ],
          [
            "Normalize metropolitan area names by extracting city and state, removing suffixes like 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area', normalizing punctuation",
            "Exclude footer/metadata rows from state CSV files",
            "Handle multi-state metropolitan areas that may appear in multiple CSV files"
          ],
          [
            "Population in 2023 > 1,000,000",
            "Valid metropolitan area name match"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "check for duplicate metropolitan area entries across state files",
          "validate linear interpolation assumptions for population estimation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "check for duplicate metropolitan area entries across state files",
            "validate linear interpolation assumptions for population estimation"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "round final average to 4 decimal places",
          "exclude areas without population > 1M",
          "exclude areas without matching fraud data",
          "Return single scalar value representing average identity theft reports",
          "The average number of reported identity thefts should be rounded to 4 decimal places"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "round final average to 4 decimal places",
            "exclude areas without population > 1M",
            "exclude areas without matching fraud data"
          ],
          [
            "Round final average to 4 decimal places",
            "Return single scalar value representing average identity theft reports"
          ],
          [
            "The average number of reported identity thefts should be rounded to 4 decimal places"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5908333333333334
  },
  "legal-hard-14": {
    "m_q": {
      "target_metric": {
        "value": "Top 5 distinct Metropolitan areas in New England with highest '# of Reports' values",
        "confidence": 0.3333333333333333,
        "votes": [
          "Top 5 distinct Metropolitan areas in New England with highest '# of Reports' values",
          "Top 5 distinct Metropolitan areas with highest number of Identity Theft reports",
          "Total number of Identity Theft reports for each distinct Metropolitan area in New England in 2024, then rank these areas by the number of reports and select the top 5."
        ]
      },
      "filters": {
        "value": [
          "Year = 2024 (implied by question)",
          "Metropolitan areas located in New England states: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont",
          "New England states only (Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont)",
          "Year 2024",
          "Metropolitan areas located in New England",
          "Data from the year 2024 (implicitly assumed from the question context)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year = 2024 (implied by question)",
            "Metropolitan areas located in New England states: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont"
          ],
          [
            "New England states only (Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont)",
            "Year 2024"
          ],
          [
            "Metropolitan areas located in New England",
            "Data from the year 2024 (implicitly assumed from the question context)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Metropolitan Area"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Metropolitan Area"
          ],
          [
            "Metropolitan Area"
          ],
          [
            "Metropolitan Area"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which states are considered New England?",
          "How to identify metropolitan areas belonging to New England states?",
          "How to handle metropolitan areas that span multiple states (including non-New England states)?",
          "How to sort and limit to top 5 by '# of Reports'?",
          "Which files contain data for New England states?",
          "What are the New England states?",
          "How to identify distinct Metropolitan areas across multiple states?",
          "How to handle Metropolitan areas that span multiple states?",
          "How to rank Metropolitan areas by number of reports?",
          "How to select top 5 after ranking?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Which states are considered New England?",
            "How to identify metropolitan areas belonging to New England states?",
            "How to handle metropolitan areas that span multiple states (including non-New England states)?",
            "How to sort and limit to top 5 by '# of Reports'?"
          ],
          [
            "Which files contain data for New England states?",
            "What are the New England states?",
            "How to identify distinct Metropolitan areas across multiple states?",
            "How to handle Metropolitan areas that span multiple states?",
            "How to rank Metropolitan areas by number of reports?",
            "How to select top 5 after ranking?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Connecticut.csv",
          "Maine.csv",
          "Massachusetts.csv",
          "NewHampshire.csv",
          "RhodeIsland.csv",
          "Vermont.csv",
          "new_england_states.csv",
          "Alabama.csv",
          "Alaska.csv",
          "Arizona.csv",
          "Arkansas.csv",
          "California.csv",
          "Colorado.csv",
          "Delaware.csv",
          "DistrictofColumbia.csv",
          "Florida.csv",
          "Georgia.csv",
          "Hawaii.csv",
          "Idaho.csv",
          "Illinois.csv",
          "Indiana.csv",
          "Iowa.csv",
          "Kansas.csv",
          "Kentucky.csv",
          "Louisiana.csv",
          "Maryland.csv",
          "Michigan.csv",
          "Minnesota.csv",
          "Mississippi.csv",
          "Missouri.csv",
          "Montana.csv",
          "Nebraska.csv",
          "Nevada.csv",
          "NewJersey.csv",
          "NewMexico.csv",
          "NewYork.csv",
          "NorthCarolina.csv",
          "NorthDakota.csv",
          "Ohio.csv",
          "Oklahoma.csv",
          "Oregon.csv",
          "Pennsylvania.csv",
          "PuertoRico.csv",
          "SouthCarolina.csv",
          "SouthDakota.csv",
          "Tennessee.csv",
          "Texas.csv",
          "Utah.csv",
          "Virginia.csv",
          "Washington.csv",
          "WestVirginia.csv",
          "Wisconsin.csv",
          "Wyoming.csv"
        ],
        "confidence": 0.42138364779874193,
        "votes": [
          [
            "Connecticut.csv",
            "Maine.csv",
            "Massachusetts.csv",
            "NewHampshire.csv",
            "RhodeIsland.csv",
            "Vermont.csv",
            "new_england_states.csv"
          ],
          [
            "Connecticut.csv",
            "Maine.csv",
            "Massachusetts.csv",
            "NewHampshire.csv",
            "RhodeIsland.csv",
            "Vermont.csv",
            "new_england_states.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv",
            "new_england_states.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All state CSV files have identical schema: ['Metropolitan Area', '# of Reports']",
          "new_england_states.csv has different schema: ['Name'] only",
          "No explicit state column in state CSV files - state information is in filenames",
          "All state files have identical schema with columns: Metropolitan Area, # of Reports",
          "Last row in each state file contains metadata footnote, not data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All state CSV files have identical schema: ['Metropolitan Area', '# of Reports']",
            "new_england_states.csv has different schema: ['Name'] only",
            "No explicit state column in state CSV files - state information is in filenames"
          ],
          [
            "All state files have identical schema with columns: Metropolitan Area, # of Reports",
            "Last row in each state file contains metadata footnote, not data"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of identity theft reports (implied to be for 2024)",
          "metropolitan area": "categorical text",
          "number of reports": "Count of identity theft reports"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "# of Reports": "count of identity theft reports (implied to be for 2024)"
          },
          {
            "# of Reports": "count",
            "Metropolitan Area": "categorical text"
          },
          {
            "Number of Reports": "Count of identity theft reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Values are floats but represent counts (should be integers)",
          "Some rows contain descriptive text instead of data (e.g., 'Metropolitan Areas are defined by...')",
          "Reports are absolute counts, not rates per population"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Values are floats but represent counts (should be integers)",
            "Some rows contain descriptive text instead of data (e.g., 'Metropolitan Areas are defined by...')"
          ],
          [
            "Reports are absolute counts, not rates per population"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Metropolitan areas may appear in multiple state files if they span state boundaries (e.g., 'Boston-Cambridge-Newton, MA-NH' appears in both Massachusetts.csv and NewHampshire.csv)",
          "Need to decide whether to sum reports for cross-state areas or treat as separate entries",
          "Metropolitan areas may appear in multiple state files if they span state boundaries",
          "Same Metropolitan area should not be double-counted when aggregating"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan areas may appear in multiple state files if they span state boundaries (e.g., 'Boston-Cambridge-Newton, MA-NH' appears in both Massachusetts.csv and NewHampshire.csv)",
            "Need to decide whether to sum reports for cross-state areas or treat as separate entries"
          ],
          [
            "Metropolitan areas may appear in multiple state files if they span state boundaries",
            "Same Metropolitan area should not be double-counted when aggregating"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "Metropolitan Areas are defined by..."
        ],
        "confidence": 0.75,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "Metropolitan Areas are defined by..."
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "# of Reports must be non-negative",
          "Metropolitan Area names should be unique within each state",
          "Year is implied to be 2024 from question but not explicitly in data",
          "New England states are defined by new_england_states.csv",
          "Must filter to only New England states: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont",
          "Must exclude footer/metadata rows from each file",
          "Must deduplicate Metropolitan areas that appear in multiple state files",
          "Must return exactly 5 Metropolitan areas",
          "Must rank by number of Identity Theft reports in descending order",
          "Result must be distinct Metropolitan areas",
          "Need to filter the data to include only states within New England using the 'new_england_states.csv' file.",
          "Need to aggregate the number of reports by Metropolitan Area across all relevant states.",
          "Need to handle cases where a Metropolitan Area spans multiple states (e.g., 'Boston-Cambridge-Newton, MA-NH Metropolitan Statistical Area')."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "# of Reports must be non-negative",
            "Metropolitan Area names should be unique within each state",
            "Year is implied to be 2024 from question but not explicitly in data",
            "New England states are defined by new_england_states.csv"
          ],
          [
            "Must filter to only New England states: Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont",
            "Must exclude footer/metadata rows from each file",
            "Must deduplicate Metropolitan areas that appear in multiple state files",
            "Must return exactly 5 Metropolitan areas",
            "Must rank by number of Identity Theft reports in descending order",
            "Result must be distinct Metropolitan areas"
          ],
          [
            "Need to filter the data to include only states within New England using the 'new_england_states.csv' file.",
            "Need to aggregate the number of reports by Metropolitan Area across all relevant states.",
            "Need to handle cases where a Metropolitan Area spans multiple states (e.g., 'Boston-Cambridge-Newton, MA-NH Metropolitan Statistical Area')."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows where '# of Reports' is NaN or contains text",
          "Filter to only metropolitan areas from the 6 New England states",
          "Handle duplicate metropolitan areas that span multiple New England states",
          "Exclude rows where '# of Reports' is NaN or null",
          "Exclude rows where 'Metropolitan Area' contains 'Metropolitan Areas are defined'",
          "For multi-state Metropolitan areas, use the maximum or aggregate count appropriately",
          "States in New England"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows where '# of Reports' is NaN or contains text",
            "Filter to only metropolitan areas from the 6 New England states",
            "Handle duplicate metropolitan areas that span multiple New England states"
          ],
          [
            "Exclude rows where '# of Reports' is NaN or null",
            "Exclude rows where 'Metropolitan Area' contains 'Metropolitan Areas are defined'",
            "For multi-state Metropolitan areas, use the maximum or aggregate count appropriately"
          ],
          [
            "States in New England"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in '# of Reports' values",
          "Verify distribution of reports across metropolitan areas",
          "Validate that top 5 areas have significantly higher reports than others"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in '# of Reports' values",
            "Verify distribution of reports across metropolitan areas",
            "Validate that top 5 areas have significantly higher reports than others"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of exactly 5 metropolitan areas",
          "Should include both area name and report count",
          "Areas should be distinct (no duplicates)",
          "Sorted descending by '# of Reports'",
          "Return list of 5 Metropolitan area names",
          "Ordered by number of reports descending",
          "Include only distinct Metropolitan areas",
          "Include both Metropolitan area name and report count for clarity",
          "The output should be a list of the top 5 Metropolitan Areas in New England with the highest number of identity theft reports.",
          "The list should be sorted in descending order of the number of reports."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "List of exactly 5 metropolitan areas",
            "Should include both area name and report count",
            "Areas should be distinct (no duplicates)",
            "Sorted descending by '# of Reports'"
          ],
          [
            "Return list of 5 Metropolitan area names",
            "Ordered by number of reports descending",
            "Include only distinct Metropolitan areas",
            "Include both Metropolitan area name and report count for clarity"
          ],
          [
            "The output should be a list of the top 5 Metropolitan Areas in New England with the highest number of identity theft reports.",
            "The list should be sorted in descending order of the number of reports."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5974580712788261
  },
  "legal-hard-15": {
    "m_q": {
      "target_metric": {
        "value": "Total number of Identity Theft reports from cross-state Metropolitan Statistical Areas in 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "Total number of Identity Theft reports from cross-state Metropolitan Statistical Areas in 2024",
          "Total count of Identity Theft reports",
          "Sum of '# of Reports' for all cross-state Metropolitan Statistical Areas in 2024"
        ]
      },
      "filters": {
        "value": [
          "Metropolitan Area contains hyphenated state abbreviations (e.g., 'TN-MS-AR')",
          "Exclude single-state Metropolitan Areas",
          "Exclude footnote rows containing 'Metropolitan Areas are defined'",
          "Year = 2024",
          "Metropolitan Statistical Areas that span multiple states (cross-state MSAs)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan Area contains hyphenated state abbreviations (e.g., 'TN-MS-AR')",
            "Exclude single-state Metropolitan Areas",
            "Exclude footnote rows containing 'Metropolitan Areas are defined'"
          ],
          [
            "Year = 2024",
            "Metropolitan Statistical Areas that span multiple states (cross-state MSAs)"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Metropolitan Area"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan Area"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which Metropolitan Areas span multiple states?",
          "How to avoid double-counting reports from cross-state areas?",
          "How to identify and exclude footnote rows?",
          "Which Metropolitan Statistical Areas span multiple states?",
          "What is the value in '# of Reports' column for each cross-state MSA?",
          "What is the sum of all reports from cross-state MSAs?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which Metropolitan Areas span multiple states?",
            "How to avoid double-counting reports from cross-state areas?",
            "How to identify and exclude footnote rows?"
          ],
          [
            "Which Metropolitan Statistical Areas span multiple states?",
            "What is the value in '# of Reports' column for each cross-state MSA?",
            "What is the sum of all reports from cross-state MSAs?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Alabama.csv",
          "Alaska.csv",
          "Arizona.csv",
          "Arkansas.csv",
          "California.csv",
          "Colorado.csv",
          "Connecticut.csv",
          "Delaware.csv",
          "DistrictofColumbia.csv",
          "Florida.csv",
          "Georgia.csv",
          "Hawaii.csv",
          "Idaho.csv",
          "Illinois.csv",
          "Indiana.csv",
          "Iowa.csv",
          "Kansas.csv",
          "Kentucky.csv",
          "Louisiana.csv",
          "Maine.csv",
          "Maryland.csv",
          "Massachusetts.csv",
          "Michigan.csv",
          "Minnesota.csv",
          "Mississippi.csv",
          "Missouri.csv",
          "Montana.csv",
          "Nebraska.csv",
          "Nevada.csv",
          "NewHampshire.csv",
          "NewJersey.csv",
          "NewMexico.csv",
          "NewYork.csv",
          "NorthCarolina.csv",
          "NorthDakota.csv",
          "Ohio.csv",
          "Oklahoma.csv",
          "Oregon.csv",
          "Pennsylvania.csv",
          "PuertoRico.csv",
          "RhodeIsland.csv",
          "SouthCarolina.csv",
          "SouthDakota.csv",
          "Tennessee.csv",
          "Texas.csv",
          "Utah.csv",
          "Vermont.csv",
          "Virginia.csv",
          "Washington.csv",
          "WestVirginia.csv",
          "Wisconsin.csv",
          "Wyoming.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All files have identical schema: ['Metropolitan Area', '# of Reports']",
          "All files have same dtypes: {'Metropolitan Area': 'object', '# of Reports': 'float64'}",
          "Last row in each file contains metadata description instead of data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All files have identical schema: ['Metropolitan Area', '# of Reports']",
            "All files have same dtypes: {'Metropolitan Area': 'object', '# of Reports': 'float64'}"
          ],
          [
            "Last row in each file contains metadata description instead of data"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of identity theft reports"
        },
        "confidence": 1.0,
        "votes": [
          {
            "# of Reports": "count of identity theft reports"
          },
          {
            "# of Reports": "count of identity theft reports"
          },
          {
            "# of Reports": "number of identity theft reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Reports are aggregated at Metropolitan Statistical Area level",
          "Cross-state areas appear in multiple state files (potential double-counting)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Reports are aggregated at Metropolitan Statistical Area level",
            "Cross-state areas appear in multiple state files (potential double-counting)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Same cross-state Metropolitan Area appears in multiple state files with identical report counts (e.g., 'Columbus, GA-AL' appears in both Alabama.csv and Georgia.csv)",
          "Same cross-state MSA appears in multiple state files with identical report counts (e.g., 'Washington-Arlington-Alexandria, DC-VA-MD-WV' appears in DC, MD, VA, WV files)",
          "Duplicate counting risk: cross-state MSAs replicated across state files must be counted only once"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Same cross-state Metropolitan Area appears in multiple state files with identical report counts (e.g., 'Columbus, GA-AL' appears in both Alabama.csv and Georgia.csv)"
          ],
          [
            "Same cross-state MSA appears in multiple state files with identical report counts (e.g., 'Washington-Arlington-Alexandria, DC-VA-MD-WV' appears in DC, MD, VA, WV files)",
            "Duplicate counting risk: cross-state MSAs replicated across state files must be counted only once"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Report counts must be non-negative",
          "Each cross-state Metropolitan Area should be counted only once",
          "Footnote rows must be excluded from calculations",
          "Data is for year 2024 only",
          "Only count Metropolitan Statistical Areas that contain multiple state abbreviations in their name",
          "Each unique cross-state MSA should be counted only once despite appearing in multiple state files",
          "Exclude the metadata row at the end of each file",
          "Exclude Micropolitan Statistical Areas unless they are cross-state",
          "Need to filter for Metropolitan Statistical Areas that span multiple states based on the 'Metropolitan Area' column.",
          "Year is assumed to be 2024 based on the question."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Report counts must be non-negative",
            "Each cross-state Metropolitan Area should be counted only once",
            "Footnote rows must be excluded from calculations"
          ],
          [
            "Data is for year 2024 only",
            "Only count Metropolitan Statistical Areas that contain multiple state abbreviations in their name",
            "Each unique cross-state MSA should be counted only once despite appearing in multiple state files",
            "Exclude the metadata row at the end of each file",
            "Exclude Micropolitan Statistical Areas unless they are cross-state"
          ],
          [
            "Need to filter for Metropolitan Statistical Areas that span multiple states based on the 'Metropolitan Area' column.",
            "Year is assumed to be 2024 based on the question."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Metropolitan Area NOT LIKE '%Metropolitan Areas are defined%'",
          "Metropolitan Area CONTAINS '-' between state abbreviations",
          "Filter Metropolitan Area names containing multiple state codes (e.g., patterns like 'XX-YY' where XX and YY are different state abbreviations)",
          "Identify cross-state by presence of comma followed by multiple state abbreviations before 'Metropolitan Statistical Area' or 'Micropolitan Statistical Area'",
          "Metropolitan areas that contain a hyphenated list of state abbreviations (e.g., GA-SC) in the 'Metropolitan Area' column."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan Area NOT LIKE '%Metropolitan Areas are defined%'",
            "Metropolitan Area CONTAINS '-' between state abbreviations"
          ],
          [
            "Filter Metropolitan Area names containing multiple state codes (e.g., patterns like 'XX-YY' where XX and YY are different state abbreviations)",
            "Identify cross-state by presence of comma followed by multiple state abbreviations before 'Metropolitan Statistical Area' or 'Micropolitan Statistical Area'"
          ],
          [
            "Metropolitan areas that contain a hyphenated list of state abbreviations (e.g., GA-SC) in the 'Metropolitan Area' column."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate cross-state areas across files",
          "Verify sum consistency for cross-state areas appearing in multiple files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate cross-state areas across files",
            "Verify sum consistency for cross-state areas appearing in multiple files"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric total",
          "Should specify that cross-state areas were deduplicated",
          "Return single integer value representing total count",
          "Sum should not double-count MSAs that appear in multiple state files",
          "The output should be a single number representing the total number of identity theft reports."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric total",
            "Should specify that cross-state areas were deduplicated"
          ],
          [
            "Return single integer value representing total count",
            "Sum should not double-count MSAs that appear in multiple state files"
          ],
          [
            "The output should be a single number representing the total number of identity theft reports."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6333333333333334
  },
  "legal-hard-16": {
    "m_q": {
      "target_metric": {
        "value": "Highest concentration of identity theft reports in a single metropolitan area within states that have at least two metropolitan areas, with multi-state metropolitan areas counted separately for each state they belong to",
        "confidence": 0.3333333333333333,
        "votes": [
          "Highest concentration of identity theft reports in a single metropolitan area within states that have at least two metropolitan areas, with multi-state metropolitan areas counted separately for each state they belong to",
          "State with highest concentration (proportion) of identity theft reports in a single metropolitan area",
          "highest concentration of identity theft reports in a single metropolitan area among states with at least two metropolitan areas, considering each multi-state metropolitan area separately in each state it belongs to"
        ]
      },
      "filters": {
        "value": [
          "States with at least two metropolitan areas",
          "Multi-state metropolitan areas considered separately per state",
          "States with at least 2 metropolitan areas",
          "Exclude footer rows with metadata text",
          "Multi-state metropolitan areas counted separately per state"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "States with at least two metropolitan areas",
            "Multi-state metropolitan areas considered separately per state"
          ],
          [
            "States with at least 2 metropolitan areas",
            "Exclude footer rows with metadata text",
            "Multi-state metropolitan areas counted separately per state"
          ],
          [
            "states with at least two metropolitan areas"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "State",
          "Metropolitan Area"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "State",
            "Metropolitan Area"
          ],
          [
            "State",
            "Metropolitan Area"
          ],
          [
            "state"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which states have at least two metropolitan areas?",
          "How should multi-state metropolitan areas be handled for each state?",
          "What is the maximum # of Reports value for each qualifying state?",
          "Which state has the highest maximum value?",
          "How many valid metropolitan areas does each state have?",
          "What is the total number of reports per state?",
          "What is the maximum number of reports in a single metro area per state?",
          "What is the concentration ratio (max metro reports / total state reports) for each state?",
          "Which states have at least 2 metropolitan areas?",
          "Among qualifying states, which has the highest concentration ratio?",
          "For each state, identify all metropolitan areas.",
          "For each state, count the number of metropolitan areas.",
          "Filter states with at least two metropolitan areas.",
          "For each state and metropolitan area, find the number of identity theft reports.",
          "For each state, find the maximum number of identity theft reports across all its metropolitan areas.",
          "Find the state with the highest maximum number of identity theft reports."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Which states have at least two metropolitan areas?",
            "How should multi-state metropolitan areas be handled for each state?",
            "What is the maximum # of Reports value for each qualifying state?",
            "Which state has the highest maximum value?"
          ],
          [
            "How many valid metropolitan areas does each state have?",
            "What is the total number of reports per state?",
            "What is the maximum number of reports in a single metro area per state?",
            "What is the concentration ratio (max metro reports / total state reports) for each state?",
            "Which states have at least 2 metropolitan areas?",
            "Among qualifying states, which has the highest concentration ratio?"
          ],
          [
            "For each state, identify all metropolitan areas.",
            "For each state, count the number of metropolitan areas.",
            "Filter states with at least two metropolitan areas.",
            "For each state and metropolitan area, find the number of identity theft reports.",
            "For each state, find the maximum number of identity theft reports across all its metropolitan areas.",
            "Find the state with the highest maximum number of identity theft reports."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Alabama.csv",
          "Alaska.csv",
          "Arizona.csv",
          "Arkansas.csv",
          "California.csv",
          "Colorado.csv",
          "Connecticut.csv",
          "Delaware.csv",
          "DistrictofColumbia.csv",
          "Florida.csv",
          "Georgia.csv",
          "Hawaii.csv",
          "Idaho.csv",
          "Illinois.csv",
          "Indiana.csv",
          "Iowa.csv",
          "Kansas.csv",
          "Kentucky.csv",
          "Louisiana.csv",
          "Maine.csv",
          "Maryland.csv",
          "Massachusetts.csv",
          "Michigan.csv",
          "Minnesota.csv",
          "Mississippi.csv",
          "Missouri.csv",
          "Montana.csv",
          "Nebraska.csv",
          "Nevada.csv",
          "NewHampshire.csv",
          "NewJersey.csv",
          "NewMexico.csv",
          "NewYork.csv",
          "NorthCarolina.csv",
          "NorthDakota.csv",
          "Ohio.csv",
          "Oklahoma.csv",
          "Oregon.csv",
          "Pennsylvania.csv",
          "PuertoRico.csv",
          "RhodeIsland.csv",
          "SouthCarolina.csv",
          "SouthDakota.csv",
          "Tennessee.csv",
          "Texas.csv",
          "Utah.csv",
          "Vermont.csv",
          "Virginia.csv",
          "Washington.csv",
          "WestVirginia.csv",
          "Wisconsin.csv",
          "Wyoming.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All files have identical schema: ['Metropolitan Area', '# of Reports']",
          "All files have same dtypes: {'Metropolitan Area': 'object', '# of Reports': 'float64'}",
          "Multi-state metropolitan areas appear in multiple state files with same name but potentially different report counts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All files have identical schema: ['Metropolitan Area', '# of Reports']",
            "All files have same dtypes: {'Metropolitan Area': 'object', '# of Reports': 'float64'}"
          ],
          [
            "Multi-state metropolitan areas appear in multiple state files with same name but potentially different report counts"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "Number of identity theft reports (absolute count)",
          "concentration": "ratio (0-1)",
          "number of reports": "number of identity theft reports"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "# of Reports": "Number of identity theft reports (absolute count)"
          },
          {
            "# of Reports": "count (integer)",
            "concentration": "ratio (0-1)"
          },
          {
            "Number of Reports": "number of identity theft reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Data appears to be absolute counts, not rates per 100,000 population as mentioned in footnote",
          "Large variation in report counts across metropolitan areas (from 76 to 71,624)",
          "Report counts are absolute numbers, not rates per capita"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data appears to be absolute counts, not rates per 100,000 population as mentioned in footnote",
            "Large variation in report counts across metropolitan areas (from 76 to 71,624)"
          ],
          [
            "Report counts are absolute numbers, not rates per capita"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Consistent column naming and data types across all files",
          "All files include identical footnote row at the end",
          "Multi-state metropolitan areas like 'Washington-Arlington-Alexandria, DC-VA-MD-WV' appear in multiple files",
          "Multi-state metro areas like 'Memphis, TN-MS-AR' should be counted separately for each state it appears in",
          "Same metropolitan area name may have different report values when split across states"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Consistent column naming and data types across all files",
            "All files include identical footnote row at the end"
          ],
          [
            "Multi-state metropolitan areas like 'Washington-Arlington-Alexandria, DC-VA-MD-WV' appear in multiple files",
            "Multi-state metro areas like 'Memphis, TN-MS-AR' should be counted separately for each state it appears in",
            "Same metropolitan area name may have different report values when split across states"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "# of Reports must be non-negative",
          "Each state file contains data for that state's metropolitan areas",
          "Multi-state metropolitan areas appear in multiple state files",
          "Only include states with at least 2 metropolitan areas",
          "Exclude rows where 'Metropolitan Area' contains 'Metropolitan Areas are defined by'",
          "Multi-state metropolitan areas must be treated as separate entries for each state",
          "Only count valid metropolitan/micropolitan statistical areas",
          "Need to consider multi-state metropolitan areas separately for each state.",
          "Need to filter states with at least two metropolitan areas."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "# of Reports must be non-negative",
            "Each state file contains data for that state's metropolitan areas",
            "Multi-state metropolitan areas appear in multiple state files"
          ],
          [
            "Only include states with at least 2 metropolitan areas",
            "Exclude rows where 'Metropolitan Area' contains 'Metropolitan Areas are defined by'",
            "Multi-state metropolitan areas must be treated as separate entries for each state",
            "Only count valid metropolitan/micropolitan statistical areas"
          ],
          [
            "Need to consider multi-state metropolitan areas separately for each state.",
            "Need to filter states with at least two metropolitan areas."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove footnote rows from each file",
          "Filter states with shape[0] >= 3 (at least 2 metropolitan areas plus footnote)",
          "Parse state abbreviations from Metropolitan Area names to identify multi-state areas",
          "num_reports IS NOT NULL",
          "metro_area NOT LIKE '%Metropolitan Areas are defined%'",
          "COUNT(metro_area) >= 2 per state",
          "States with at least two metropolitan areas"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove footnote rows from each file",
            "Filter states with shape[0] >= 3 (at least 2 metropolitan areas plus footnote)",
            "Parse state abbreviations from Metropolitan Area names to identify multi-state areas"
          ],
          [
            "num_reports IS NOT NULL",
            "metro_area NOT LIKE '%Metropolitan Areas are defined%'",
            "COUNT(metro_area) >= 2 per state"
          ],
          [
            "States with at least two metropolitan areas"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in # of Reports",
          "Verify consistency of multi-state area counts across different state files",
          "Calculate concentration = MAX(num_reports) / SUM(num_reports) per state",
          "Verify all states in analysis have at least 2 metro areas"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in # of Reports",
            "Verify consistency of multi-state area counts across different state files"
          ],
          [
            "Calculate concentration = MAX(num_reports) / SUM(num_reports) per state",
            "Verify all states in analysis have at least 2 metro areas"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "State name",
          "Metropolitan Area with highest reports",
          "Number of reports for that area",
          "Return single state name with highest concentration",
          "Show the metropolitan area name with highest concentration",
          "Show the concentration value (ratio)",
          "State with the highest concentration of identity theft reports in a single metropolitan area."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "State name",
            "Metropolitan Area with highest reports",
            "Number of reports for that area"
          ],
          [
            "Return single state name with highest concentration",
            "Show the metropolitan area name with highest concentration",
            "Show the concentration value (ratio)"
          ],
          [
            "State with the highest concentration of identity theft reports in a single metropolitan area."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.633888888888889
  },
  "legal-hard-17": {
    "m_q": {
      "target_metric": {
        "value": "Number of Auto Related reports in 2007 if category distribution matched 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "Number of Auto Related reports in 2007 if category distribution matched 2024",
          "Number of Auto Related reports in 2007 if the category distribution matched 2024 proportions",
          "Number of reports in 2007 that would be 'Auto Related' if the category distribution matched 2024, rounded to the nearest integer."
        ]
      },
      "filters": {
        "value": [
          "Year = 2007",
          "Category = 'Auto Related'",
          "Year == 2007"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year = 2007",
            "Category = 'Auto Related'"
          ],
          [
            "Year == 2007"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Year",
          "Category"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Year",
            "Category"
          ],
          [
            "Year",
            "Category"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of reports in 2007?",
          "What percentage of reports in 2024 are Auto Related?",
          "How to apply 2024 Auto Related percentage to 2007 total reports?",
          "What was the total number of reports in 2007?",
          "What is the percentage of Auto Related reports in 2024?",
          "What is the calculated number of Auto Related reports for 2007 using 2024 percentage?",
          "What is the percentage of 'Auto Related' reports in 2024?",
          "Multiply the total number of reports in 2007 by the percentage of 'Auto Related' reports in 2024.",
          "Round the result to the nearest integer."
        ],
        "confidence": 0.3703703703703704,
        "votes": [
          [
            "What is the total number of reports in 2007?",
            "What percentage of reports in 2024 are Auto Related?",
            "How to apply 2024 Auto Related percentage to 2007 total reports?"
          ],
          [
            "What was the total number of reports in 2007?",
            "What is the percentage of Auto Related reports in 2024?",
            "What is the calculated number of Auto Related reports for 2007 using 2024 percentage?"
          ],
          [
            "What is the total number of reports in 2007?",
            "What is the percentage of 'Auto Related' reports in 2024?",
            "Multiply the total number of reports in 2007 by the percentage of 'Auto Related' reports in 2024.",
            "Round the result to the nearest integer."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Report_Count.csv",
          "2024_CSN_Report_Categories.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Report_Count.csv",
            "2024_CSN_Report_Categories.csv"
          ],
          [
            "2024_CSN_Report_Count.csv",
            "2024_CSN_Report_Categories.csv"
          ],
          [
            "2024_CSN_Report_Count.csv",
            "2024_CSN_Report_Categories.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Year column in 2024_CSN_Report_Count.csv is float64 but represents integer years",
          "Percentage column in 2024_CSN_Report_Categories.csv is object/string with '%' symbol",
          "Both files have '# of Reports' column but represent different concepts (total vs category-specific)",
          "No natural join key between files - relationship is implicit through 2024 data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column in 2024_CSN_Report_Count.csv is float64 but represents integer years",
            "Percentage column in 2024_CSN_Report_Categories.csv is object/string with '%' symbol"
          ],
          [
            "Both files have '# of Reports' column but represent different concepts (total vs category-specific)",
            "No natural join key between files - relationship is implicit through 2024 data"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count",
          "percentage": "percentage",
          "year": "year"
        },
        "confidence": 0.8888888888888888,
        "votes": [
          {
            "# of Reports": "count",
            "Percentage": "percentage",
            "Year": "year"
          },
          {
            "# of Reports": "count",
            "Percentage": "percentage_string",
            "Year": "year"
          },
          {
            "# of Reports": "number of reports",
            "Percentage": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "2024_CSN_Report_Categories.csv shows 2024 data but filename suggests it might contain historical data",
          "2024_CSN_Report_Count.csv contains data from 2001-2020 but filename suggests only 2024",
          "Percentage column in 2024_CSN_Report_Categories.csv is stored as string with '%' symbol, needs conversion to numeric",
          "Percentage column in 2024_CSN_Report_Categories.csv needs to be converted to a decimal by removing the '%' sign and dividing by 100."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "2024_CSN_Report_Categories.csv shows 2024 data but filename suggests it might contain historical data",
            "2024_CSN_Report_Count.csv contains data from 2001-2020 but filename suggests only 2024"
          ],
          [
            "Percentage column in 2024_CSN_Report_Categories.csv is stored as string with '%' symbol, needs conversion to numeric"
          ],
          [
            "Percentage column in 2024_CSN_Report_Categories.csv needs to be converted to a decimal by removing the '%' sign and dividing by 100."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "2024_CSN_Report_Categories.csv doesn't have Year column, so we assume it represents 2024 distribution only",
          "2024_CSN_Report_Categories.csv contains 2024 category distribution data",
          "2024_CSN_Report_Count.csv contains historical yearly totals including 2007",
          "No direct year linkage in category file"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "2024_CSN_Report_Categories.csv doesn't have Year column, so we assume it represents 2024 distribution only"
          ],
          [
            "2024_CSN_Report_Categories.csv contains 2024 category distribution data",
            "2024_CSN_Report_Count.csv contains historical yearly totals including 2007",
            "No direct year linkage in category file"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 0.33,
        "votes": [
          4.0,
          2.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total percentage in 2024_CSN_Report_Categories.csv should sum to 100%",
          "2007 must exist in 2024_CSN_Report_Count.csv",
          "'Auto Related' category must exist in 2024_CSN_Report_Categories.csv",
          "Year must equal 2007 when filtering 2024_CSN_Report_Count.csv",
          "Category must equal 'Auto Related' when filtering 2024_CSN_Report_Categories.csv",
          "Percentage values must be converted from string format (e.g., '3.04%') to numeric decimal",
          "Final result must be rounded to nearest integer",
          "The 'Year' column in 2024_CSN_Report_Count.csv should be filtered to the year 2007.",
          "The 'Category' column in 2024_CSN_Report_Categories.csv should be filtered to the category 'Auto Related'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total percentage in 2024_CSN_Report_Categories.csv should sum to 100%",
            "2007 must exist in 2024_CSN_Report_Count.csv",
            "'Auto Related' category must exist in 2024_CSN_Report_Categories.csv"
          ],
          [
            "Year must equal 2007 when filtering 2024_CSN_Report_Count.csv",
            "Category must equal 'Auto Related' when filtering 2024_CSN_Report_Categories.csv",
            "Percentage values must be converted from string format (e.g., '3.04%') to numeric decimal",
            "Final result must be rounded to nearest integer"
          ],
          [
            "The 'Year' column in 2024_CSN_Report_Count.csv should be filtered to the year 2007.",
            "The 'Category' column in 2024_CSN_Report_Categories.csv should be filtered to the category 'Auto Related'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract Auto Related percentage from 2024_CSN_Report_Categories.csv",
          "Extract 2007 total reports from 2024_CSN_Report_Count.csv",
          "Extract 2007 total from 2024_CSN_Report_Count.csv where Year == 2007.0",
          "Extract Auto Related percentage from 2024_CSN_Report_Categories.csv where Category == 'Auto Related'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract Auto Related percentage from 2024_CSN_Report_Categories.csv",
            "Extract 2007 total reports from 2024_CSN_Report_Count.csv"
          ],
          [
            "Extract 2007 total from 2024_CSN_Report_Count.csv where Year == 2007.0",
            "Extract Auto Related percentage from 2024_CSN_Report_Categories.csv where Category == 'Auto Related'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if 2024 distribution percentages sum to 100%",
          "Verify 2007 exists in the time series"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if 2024 distribution percentages sum to 100%",
            "Verify 2007 exists in the time series"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Rounded to nearest integer",
          "Single numeric value",
          "Single integer value",
          "Result represents hypothetical Auto Related reports in 2007 using 2024 distribution",
          "The final answer should be rounded to the nearest integer."
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Rounded to nearest integer",
            "Single numeric value"
          ],
          [
            "Single integer value",
            "Rounded to nearest integer",
            "Result represents hypothetical Auto Related reports in 2007 using 2024 distribution"
          ],
          [
            "The final answer should be rounded to the nearest integer."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6161296296296297
  },
  "legal-hard-18": {
    "m_q": {
      "target_metric": {
        "value": "Number of identity theft reports in 2007 concerning people ages 40 or older (rounded to nearest thousand)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Number of identity theft reports in 2007 concerning people ages 40 or older (rounded to nearest thousand)",
          "Number of identity theft reports in 2007 for people ages 40 or older (rounded to nearest thousand), assuming 2007 had the same age distribution as 2024",
          "Number of identity theft reports in 2007 concerning people ages 40 or older, rounded to the nearest thousand, assuming the age distribution of identity theft reports in 2007 was the same as in 2024."
        ]
      },
      "filters": {
        "value": [
          "Year = 2007",
          "Category = Identity Theft",
          "Age Range in ['40 - 49', '50 - 59', '60 - 69', '70 - 79', '80 and Over']"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Year = 2007",
            "Category = Identity Theft",
            "Age Range in ['40 - 49', '50 - 59', '60 - 69', '70 - 79', '80 and Over']"
          ],
          [
            "Year = 2007",
            "Category = Identity Theft",
            "Age Range in ['40 - 49', '50 - 59', '60 - 69', '70 - 79', '80 and Over']"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Year",
          "Category",
          "Age Range"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "Year",
            "Category",
            "Age Range"
          ],
          [
            "Age Range"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of reports in 2007?",
          "What percentage of 2024 reports are identity theft?",
          "What percentage of 2024 identity theft reports are from ages 40+?",
          "How to apply 2024 distribution patterns to 2007 data?",
          "What was the total number of identity theft reports in 2007?",
          "What is the age distribution of identity theft reports in 2024?",
          "What proportion of 2024 identity theft reports concern people ages 40 or older?",
          "Apply the 2024 age distribution proportion to the 2007 total identity theft count",
          "Round the result to the nearest thousand",
          "What is the proportion of identity theft reports in 2024 that concern people ages 40 or older?",
          "Apply the 2024 age distribution to the 2007 total identity theft reports.",
          "Round the result to the nearest thousand."
        ],
        "confidence": 0.36111111111111116,
        "votes": [
          [
            "What is the total number of reports in 2007?",
            "What percentage of 2024 reports are identity theft?",
            "What percentage of 2024 identity theft reports are from ages 40+?",
            "How to apply 2024 distribution patterns to 2007 data?"
          ],
          [
            "What was the total number of identity theft reports in 2007?",
            "What is the age distribution of identity theft reports in 2024?",
            "What proportion of 2024 identity theft reports concern people ages 40 or older?",
            "Apply the 2024 age distribution proportion to the 2007 total identity theft count",
            "Round the result to the nearest thousand"
          ],
          [
            "What was the total number of identity theft reports in 2007?",
            "What is the proportion of identity theft reports in 2024 that concern people ages 40 or older?",
            "Apply the 2024 age distribution to the 2007 total identity theft reports.",
            "Round the result to the nearest thousand."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Report_Count.csv",
          "2024_CSN_Report_Categories.csv",
          "2024_CSN_Identity_Theft_Reports_by_Age.csv"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "2024_CSN_Report_Count.csv",
            "2024_CSN_Report_Categories.csv",
            "2024_CSN_Identity_Theft_Reports_by_Age.csv"
          ],
          [
            "2024_CSN_Report_Count.csv",
            "2024_CSN_Report_Categories.csv",
            "2024_CSN_Identity_Theft_Reports_by_Age.csv"
          ],
          [
            "2024_CSN_Report_Count.csv",
            "2024_CSN_Identity_Theft_Reports_by_Age.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Year column in 2024_CSN_Report_Count.csv is float64 but represents integer years",
          "Percentage column in 2024_CSN_Report_Categories.csv is object/string with % symbol",
          "Last row in 2024_CSN_Identity_Theft_Reports_by_Age.csv contains metadata text instead of data",
          "2024_CSN_Identity_Theft_Reports_by_Age.csv contains a footer row with text description instead of data",
          "# of Reports column name appears in multiple files with different contexts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column in 2024_CSN_Report_Count.csv is float64 but represents integer years",
            "Percentage column in 2024_CSN_Report_Categories.csv is object/string with % symbol",
            "Last row in 2024_CSN_Identity_Theft_Reports_by_Age.csv contains metadata text instead of data"
          ],
          [
            "2024_CSN_Identity_Theft_Reports_by_Age.csv contains a footer row with text description instead of data",
            "# of Reports column name appears in multiple files with different contexts"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count",
          "percentage": "percentage",
          "year": "year",
          "age range": "categorical_age_bracket"
        },
        "confidence": 0.75,
        "votes": [
          {
            "# of Reports": "count",
            "Percentage": "percentage",
            "Year": "year"
          },
          {
            "# of Reports": "count",
            "Year": "year",
            "Percentage": "percentage_string",
            "Age Range": "categorical_age_bracket"
          },
          {
            "# of Reports": "number of reports",
            "Percentage": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "2024_CSN_Report_Count.csv shows increasing report counts over time (2001-2020)",
          "Need to normalize 2007 data using 2024 proportions",
          "Age distribution data only available for 2024",
          "All report counts are absolute numbers, not rates or percentages",
          "Age distribution percentages need to be calculated from counts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "2024_CSN_Report_Count.csv shows increasing report counts over time (2001-2020)",
            "Need to normalize 2007 data using 2024 proportions",
            "Age distribution data only available for 2024"
          ],
          [
            "All report counts are absolute numbers, not rates or percentages",
            "Age distribution percentages need to be calculated from counts"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "2024_CSN_Report_Categories.csv shows 1,135,291 identity theft reports in 2024",
          "2024_CSN_Identity_Theft_Reports_by_Age.csv references same 1,135,291 total",
          "But age data sum doesn't match total (21420+187195+291807+207658+135758+83485+36790+9605 = 983,718)",
          "2024_CSN_Report_Categories.csv shows 1,135,291 identity theft reports for 2024",
          "2024_CSN_Identity_Theft_Reports_by_Age.csv age groups sum to approximately 973,718, with footer note indicating 86% of reports included age information",
          "Need to use age distribution proportions from available age data, not absolute counts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "2024_CSN_Report_Categories.csv shows 1,135,291 identity theft reports in 2024",
            "2024_CSN_Identity_Theft_Reports_by_Age.csv references same 1,135,291 total",
            "But age data sum doesn't match total (21420+187195+291807+207658+135758+83485+36790+9605 = 983,718)"
          ],
          [
            "2024_CSN_Report_Categories.csv shows 1,135,291 identity theft reports for 2024",
            "2024_CSN_Identity_Theft_Reports_by_Age.csv age groups sum to approximately 973,718, with footer note indicating 86% of reports included age information",
            "Need to use age distribution proportions from available age data, not absolute counts"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.33,
        "votes": [
          10.0,
          2.0,
          0.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total 2024 reports = 6,470,000 (sum of # of Reports in 2024_CSN_Report_Categories.csv)",
          "Identity theft percentage in 2024 = 17.54%",
          "2007 total reports = 1,070,447 from 2024_CSN_Report_Count.csv",
          "Age data only available for 2024, must be applied proportionally to 2007",
          "Only use data from rows with valid Age Range values (exclude footer text row)",
          "Sum of age groups 40-49, 50-59, 60-69, 70-79, and 80 and Over from 2024 data",
          "Must identify 2007 total reports but NOT identity theft specific count for 2007",
          "2007 identity theft count must be inferred by applying 2024 proportions",
          "The age distribution in 2024 is assumed to be representative of the age distribution in 2007 for identity theft reports.",
          "The percentage column in '2024_CSN_Report_Categories.csv' is not used."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Total 2024 reports = 6,470,000 (sum of # of Reports in 2024_CSN_Report_Categories.csv)",
            "Identity theft percentage in 2024 = 17.54%",
            "2007 total reports = 1,070,447 from 2024_CSN_Report_Count.csv",
            "Age data only available for 2024, must be applied proportionally to 2007"
          ],
          [
            "Only use data from rows with valid Age Range values (exclude footer text row)",
            "Sum of age groups 40-49, 50-59, 60-69, 70-79, and 80 and Over from 2024 data",
            "Must identify 2007 total reports but NOT identity theft specific count for 2007",
            "2007 identity theft count must be inferred by applying 2024 proportions"
          ],
          [
            "The age distribution in 2024 is assumed to be representative of the age distribution in 2007 for identity theft reports.",
            "The percentage column in '2024_CSN_Report_Categories.csv' is not used."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter age ranges to >=40: '40 - 49', '50 - 59', '60 - 69', '70 - 79', '80 and Over'",
          "Calculate proportion of 40+ reports from 2024 age distribution",
          "Apply same proportion to estimated 2007 identity theft reports",
          "Calculate proportion of 2024 identity theft reports that are age 40+: (207658 + 135758 + 83485 + 36790 + 9605) / sum of all age groups with valid data",
          "Identity theft reports in 2007 not directly available - need to estimate or use total 2007 reports as proxy",
          "Apply age 40+ proportion to 2007 identity theft volume",
          "Age 40 or older includes the age ranges '40 - 49', '50 - 59', '60 - 69', '70 - 79', and '80 and Over' from the '2024_CSN_Identity_Theft_Reports_by_Age.csv' file."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter age ranges to >=40: '40 - 49', '50 - 59', '60 - 69', '70 - 79', '80 and Over'",
            "Calculate proportion of 40+ reports from 2024 age distribution",
            "Apply same proportion to estimated 2007 identity theft reports"
          ],
          [
            "Calculate proportion of 2024 identity theft reports that are age 40+: (207658 + 135758 + 83485 + 36790 + 9605) / sum of all age groups with valid data",
            "Identity theft reports in 2007 not directly available - need to estimate or use total 2007 reports as proxy",
            "Apply age 40+ proportion to 2007 identity theft volume"
          ],
          [
            "Age 40 or older includes the age ranges '40 - 49', '50 - 59', '60 - 69', '70 - 79', and '80 and Over' from the '2024_CSN_Identity_Theft_Reports_by_Age.csv' file."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if age distribution is consistent across years (assumption)",
          "Verify that 2024 proportions can be applied to 2007",
          "Validate that rounding to nearest thousand is appropriate"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if age distribution is consistent across years (assumption)",
            "Verify that 2024 proportions can be applied to 2007",
            "Validate that rounding to nearest thousand is appropriate"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Round final result to nearest thousand",
          "Return integer value",
          "Assume 2024 age distribution applies to 2007",
          "Final answer must be rounded to the nearest thousand",
          "Output should be a single scalar value representing count of reports",
          "The final answer must be rounded to the nearest thousand."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Round final result to nearest thousand",
            "Return integer value",
            "Assume 2024 age distribution applies to 2007"
          ],
          [
            "Final answer must be rounded to the nearest thousand",
            "Output should be a single scalar value representing count of reports"
          ],
          [
            "The final answer must be rounded to the nearest thousand."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5970555555555557
  },
  "legal-hard-2": {
    "m_q": {
      "target_metric": {
        "value": "metropolitan area with highest identity theft rate per 100,000 population",
        "confidence": 0.3333333333333333,
        "votes": [
          "metropolitan area with highest identity theft rate per 100,000 population",
          "Metropolitan area with highest rate of identity thefts per 100,000 population in 2023",
          "Identity theft rate per 100,000 population for each metropolitan area, and the maximum of these rates"
        ]
      },
      "filters": {
        "value": [
          "drop entries with no population match in html",
          "ignore rows containing explanatory text like 'Metropolitan Areas are defined...'",
          "use only city and state portion of metropolitan area names",
          "Drop entries where metropolitan area name cannot be matched between fraud reports CSV files and HTML population data",
          "Ignore footer rows in CSV files that contain metadata text",
          "Drop entries where there's no match in the html for the areas fraud reports",
          "Use only the city and state portion of the name, ignoring suffixes like 'Metropolitan Statistical Area' or 'MSA' and normalizing punctuation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "drop entries with no population match in html",
            "ignore rows containing explanatory text like 'Metropolitan Areas are defined...'",
            "use only city and state portion of metropolitan area names"
          ],
          [
            "Drop entries where metropolitan area name cannot be matched between fraud reports CSV files and HTML population data",
            "Ignore footer rows in CSV files that contain metadata text"
          ],
          [
            "Drop entries where there's no match in the html for the areas fraud reports",
            "Use only the city and state portion of the name, ignoring suffixes like 'Metropolitan Statistical Area' or 'MSA' and normalizing punctuation"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "normalized_metro_area_name",
          "Metropolitan Area"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "normalized_metro_area_name"
          ],
          [
            "Metropolitan Area"
          ],
          [
            "Metropolitan Area"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "extract population data from wikipedia html",
          "normalize metropolitan area names across datasets",
          "linearly interpolate population to 2023 if needed",
          "calculate theft rate per 100,000 population",
          "find maximum rate",
          "What are the identity theft report counts for each metropolitan area from state CSV files?",
          "What are the population sizes for each metropolitan area from the HTML file?",
          "What census years are available in the HTML for population data?",
          "How to linearly interpolate population to 2023 from available census years?",
          "How to normalize metropolitan area names for matching (extract city/state, remove suffixes like 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area')?",
          "What is the identity theft rate per 100,000 population for each matched metropolitan area?",
          "Which metropolitan area has the maximum rate?",
          "Extract metropolitan area names and population data from the HTML file.",
          "Clean and normalize metropolitan area names in both the CSV files and the extracted HTML data.",
          "Estimate 2023 population for each metropolitan area using linear interpolation if necessary.",
          "Calculate the identity theft rate per 100,000 population for each metropolitan area.",
          "Find the metropolitan area with the highest identity theft rate."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "extract population data from wikipedia html",
            "normalize metropolitan area names across datasets",
            "linearly interpolate population to 2023 if needed",
            "calculate theft rate per 100,000 population",
            "find maximum rate"
          ],
          [
            "What are the identity theft report counts for each metropolitan area from state CSV files?",
            "What are the population sizes for each metropolitan area from the HTML file?",
            "What census years are available in the HTML for population data?",
            "How to linearly interpolate population to 2023 from available census years?",
            "How to normalize metropolitan area names for matching (extract city/state, remove suffixes like 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area')?",
            "What is the identity theft rate per 100,000 population for each matched metropolitan area?",
            "Which metropolitan area has the maximum rate?"
          ],
          [
            "Extract metropolitan area names and population data from the HTML file.",
            "Clean and normalize metropolitan area names in both the CSV files and the extracted HTML data.",
            "Estimate 2023 population for each metropolitan area using linear interpolation if necessary.",
            "Calculate the identity theft rate per 100,000 population for each metropolitan area.",
            "Find the metropolitan area with the highest identity theft rate."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "metropolitan_statistics.html",
          "Alabama.csv",
          "Alaska.csv",
          "Arizona.csv",
          "Arkansas.csv",
          "California.csv",
          "Colorado.csv",
          "Connecticut.csv",
          "Delaware.csv",
          "DistrictofColumbia.csv",
          "Florida.csv",
          "Georgia.csv",
          "Hawaii.csv",
          "Idaho.csv",
          "Illinois.csv",
          "Indiana.csv",
          "Iowa.csv",
          "Kansas.csv",
          "Kentucky.csv",
          "Louisiana.csv",
          "Maine.csv",
          "Maryland.csv",
          "Massachusetts.csv",
          "Michigan.csv",
          "Minnesota.csv",
          "Mississippi.csv",
          "Missouri.csv",
          "Montana.csv",
          "Nebraska.csv",
          "Nevada.csv",
          "NewHampshire.csv",
          "NewJersey.csv",
          "NewMexico.csv",
          "NewYork.csv",
          "NorthCarolina.csv",
          "NorthDakota.csv",
          "Ohio.csv",
          "Oklahoma.csv",
          "Oregon.csv",
          "Pennsylvania.csv",
          "PuertoRico.csv",
          "RhodeIsland.csv",
          "SouthCarolina.csv",
          "SouthDakota.csv",
          "Tennessee.csv",
          "Texas.csv",
          "Utah.csv",
          "Vermont.csv",
          "Virginia.csv",
          "Washington.csv",
          "WestVirginia.csv",
          "Wisconsin.csv",
          "Wyoming.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "metropolitan_statistics.html",
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "metropolitan_statistics.html",
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "metropolitan_statistics.html",
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "different state file naming conventions",
          "html contains unstructured population data",
          "csv files have same column names but different metro area formats",
          "Metropolitan area names have different suffixes across sources (e.g., 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area')",
          "HTML contains full Wikipedia page structure requiring parsing of tables",
          "State CSV files have consistent schema but need to be unioned"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "different state file naming conventions",
            "html contains unstructured population data",
            "csv files have same column names but different metro area formats"
          ],
          [
            "Metropolitan area names have different suffixes across sources (e.g., 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area')",
            "HTML contains full Wikipedia page structure requiring parsing of tables",
            "State CSV files have consistent schema but need to be unioned"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of identity theft reports",
          "population": "number of people",
          "theft_rate": "reports per 100,000 population",
          "rate": "per 100,000 population",
          "number of reports": "count"
        },
        "confidence": 0.5333333333333333,
        "votes": [
          {
            "# of Reports": "count of identity theft reports",
            "population": "number of people",
            "theft_rate": "reports per 100,000 population"
          },
          {
            "# of Reports": "count",
            "population": "persons",
            "rate": "per 100,000 population"
          },
          {
            "Number of Reports": "count",
            "Population": "people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "population data may be in thousands or raw counts",
          "need to normalize to per 100,000 scale",
          "Need to convert absolute identity theft counts to rates per 100,000 population",
          "Population estimates need to be interpolated to 2023 if not directly available"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "population data may be in thousands or raw counts",
            "need to normalize to per 100,000 scale"
          ],
          [
            "Need to convert absolute identity theft counts to rates per 100,000 population",
            "Population estimates need to be interpolated to 2023 if not directly available"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "metro area naming conventions differ between csv files and html",
          "population data may be from different years",
          "Metropolitan area naming conventions differ between CSV files and HTML (suffixes, punctuation)",
          "Population data years in HTML may not align with 2023 identity theft report year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "metro area naming conventions differ between csv files and html",
            "population data may be from different years"
          ],
          [
            "Metropolitan area naming conventions differ between CSV files and HTML (suffixes, punctuation)",
            "Population data years in HTML may not align with 2023 identity theft report year"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "Metropolitan Areas are defined by the Office of Management and Budget...",
          "NaN"
        ],
        "confidence": 0.6,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "Metropolitan Areas are defined by the Office of Management and Budget..."
          ],
          [
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv",
          "csv and html",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "theft_reports must be numeric",
          "population must be positive",
          "metro area names must be normalized before joining",
          "use linear interpolation for missing 2023 population",
          "Only include metropolitan areas that can be matched between CSV files and HTML population data",
          "Use linear interpolation between two known census years to estimate 2023 population",
          "Normalize metropolitan area names by extracting only city and state portions, removing suffixes like 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area'",
          "Normalize punctuation when matching names",
          "Exclude footer rows from state CSV files",
          "Metropolitan area names must be normalized to ensure accurate matching.",
          "Population data from the HTML file may need cleaning and parsing.",
          "Linear interpolation should be used only when necessary and between valid census years."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "theft_reports must be numeric",
            "population must be positive",
            "metro area names must be normalized before joining",
            "use linear interpolation for missing 2023 population"
          ],
          [
            "Only include metropolitan areas that can be matched between CSV files and HTML population data",
            "Use linear interpolation between two known census years to estimate 2023 population",
            "Normalize metropolitan area names by extracting only city and state portions, removing suffixes like 'Metropolitan Statistical Area', 'MSA', 'Micropolitan Statistical Area'",
            "Normalize punctuation when matching names",
            "Exclude footer rows from state CSV files"
          ],
          [
            "Metropolitan area names must be normalized to ensure accurate matching.",
            "Population data from the HTML file may need cleaning and parsing.",
            "Linear interpolation should be used only when necessary and between valid census years."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "remove non-data rows from csv files",
          "normalize metro names by removing suffixes and punctuation",
          "drop areas without population match",
          "Filter out rows where 'Metropolitan Area' field contains the text 'Metropolitan Areas are defined by'",
          "Filter out metropolitan areas with null or missing '# of Reports'",
          "Filter out metropolitan areas that cannot be matched to population data after normalization"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "remove non-data rows from csv files",
            "normalize metro names by removing suffixes and punctuation",
            "drop areas without population match"
          ],
          [
            "Filter out rows where 'Metropolitan Area' field contains the text 'Metropolitan Areas are defined by'",
            "Filter out metropolitan areas with null or missing '# of Reports'",
            "Filter out metropolitan areas that cannot be matched to population data after normalization"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "check for outliers in theft rates",
          "validate linear interpolation assumptions",
          "Linear interpolation for 2023 population estimation from available census years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "check for outliers in theft rates",
            "validate linear interpolation assumptions"
          ],
          [
            "Linear interpolation for 2023 population estimation from available census years"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "single metropolitan area name as result",
          "include theft rate calculation",
          "document population estimation method if interpolation used",
          "Return single metropolitan area name with highest identity theft rate per 100,000 population",
          "Rate calculation: (# of Reports / 2023 Population) * 100,000",
          "The output should be the name of the metropolitan area with the highest identity theft rate per 100,000 population."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "single metropolitan area name as result",
            "include theft rate calculation",
            "document population estimation method if interpolation used"
          ],
          [
            "Return single metropolitan area name with highest identity theft rate per 100,000 population",
            "Rate calculation: (# of Reports / 2023 Population) * 100,000"
          ],
          [
            "The output should be the name of the metropolitan area with the highest identity theft rate per 100,000 population."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5816666666666668
  },
  "legal-hard-22": {
    "m_q": {
      "target_metric": {
        "value": "proportion of all reports who reported identity theft with Bank Account (Theft Type) and New Accounts (Theft Subtype)",
        "confidence": 0.3333333333333333,
        "votes": [
          "proportion of all reports who reported identity theft with Bank Account (Theft Type) and New Accounts (Theft Subtype)",
          "Proportion of all reports that are identity theft reports with Bank Account (Theft Type) and New Accounts (Theft Subtype), rounded to 4 decimal places",
          "Proportion of reports with Identity Theft where Theft Type is 'Bank Account' and Theft Subtype is 'New Accounts'"
        ]
      },
      "filters": {
        "value": [
          "Theft Type = 'Bank Account'",
          "Theft Subtype = 'New Accounts'",
          "Theft Type == 'Bank Account'",
          "Theft Subtype == 'New Accounts'"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "Theft Type = 'Bank Account'",
            "Theft Subtype = 'New Accounts'"
          ],
          [
            "Theft Type == 'Bank Account'",
            "Theft Subtype == 'New Accounts'"
          ],
          [
            "Theft Type = 'Bank Account'",
            "Theft Subtype = 'New Accounts'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of reports for Bank Account/New Accounts?",
          "What is the total number of all reports across all categories?",
          "What is the number of identity theft reports with Bank Account as Theft Type and New Accounts as Theft Subtype?",
          "What is the ratio of Bank Account/New Accounts identity theft reports to total reports?",
          "What is the total number of reports with Identity Theft?",
          "What is the number of reports with Identity Theft, Theft Type 'Bank Account', and Theft Subtype 'New Accounts'?"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "What is the total number of reports for Bank Account/New Accounts?",
            "What is the total number of all reports across all categories?"
          ],
          [
            "What is the total number of all reports across all categories?",
            "What is the number of identity theft reports with Bank Account as Theft Type and New Accounts as Theft Subtype?",
            "What is the ratio of Bank Account/New Accounts identity theft reports to total reports?"
          ],
          [
            "What is the total number of reports with Identity Theft?",
            "What is the number of reports with Identity Theft, Theft Type 'Bank Account', and Theft Subtype 'New Accounts'?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Report_Type__Report Type",
          "2024_CSN_Identity_Theft_Reports_by_Type.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Report_Type__Report Type",
            "2024_CSN_Identity_Theft_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Report_Type__Report Type",
            "2024_CSN_Identity_Theft_Reports_by_Type.csv"
          ],
          [
            "2024_CSN_Report_Type__Report Type",
            "2024_CSN_Identity_Theft_Reports_by_Type.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "2024_CSN_Report_Type__Report Type has an 'Unnamed: 2' column with percentage text",
          "2024_CSN_Identity_Theft_Reports_by_Type.csv has '% Difference From Previous Year' column with percentage text",
          "Different granularity: Report Type file has aggregated report counts by type, while Identity Theft file has detailed theft subtypes"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "2024_CSN_Report_Type__Report Type has an 'Unnamed: 2' column with percentage text",
            "2024_CSN_Identity_Theft_Reports_by_Type.csv has '% Difference From Previous Year' column with percentage text"
          ],
          [
            "Different granularity: Report Type file has aggregated report counts by type, while Identity Theft file has detailed theft subtypes"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count",
          "% reporting $ loss": "percentage",
          "total $ loss": "dollars",
          "median $ loss": "dollars",
          "% difference from previous year": "percentage",
          "proportion": "decimal fraction"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "# of Reports": "count",
            "% Reporting $ Loss": "percentage",
            "Total $ Loss": "dollars",
            "Median $ Loss": "dollars",
            "% Difference From Previous Year": "percentage"
          },
          {
            "# of Reports": "count",
            "proportion": "decimal fraction"
          },
          {
            "# of Reports": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Total $ Loss column contains 'M' suffix for millions (e.g., '$2,952M')",
          "Percentage columns contain '%' symbol and need conversion to numeric",
          "Need to sum all report types (Fraud, Identity Theft, Other) to get total reports",
          "Bank Account New Accounts is a subset of Identity Theft reports"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total $ Loss column contains 'M' suffix for millions (e.g., '$2,952M')",
            "Percentage columns contain '%' symbol and need conversion to numeric"
          ],
          [
            "Need to sum all report types (Fraud, Identity Theft, Other) to get total reports",
            "Bank Account New Accounts is a subset of Identity Theft reports"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "2024_CSN_Report_Type__Report Type shows total reports = 2600678 + 1135291 + 2759963 = 6,495,932",
          "2024_CSN_Report_Type.csv shows top 10 categories sum to much smaller numbers",
          "Total Identity Theft count in Report Type file (1135291) should match sum of all Theft Type counts in Identity Theft Types file",
          "Bank Account count in Identity Theft Types (114608) should match sum of Bank Account subtypes in Identity Theft Reports by Type"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "2024_CSN_Report_Type__Report Type shows total reports = 2600678 + 1135291 + 2759963 = 6,495,932",
            "2024_CSN_Report_Type.csv shows top 10 categories sum to much smaller numbers"
          ],
          [
            "Total Identity Theft count in Report Type file (1135291) should match sum of all Theft Type counts in Identity Theft Types file",
            "Bank Account count in Identity Theft Types (114608) should match sum of Bank Account subtypes in Identity Theft Reports by Type"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 0.33,
        "votes": [
          10.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Proportion must be rounded to 4 decimal places",
          "Bank Account theft type has 3 subtypes in 2024_CSN_Identity_Theft_Reports_by_Type.csv",
          "Total reports count must come from 2024_CSN_Report_Type__Report Type",
          "Total reports = Fraud + Identity Theft + Other",
          "Bank Account New Accounts reports must be <= total Bank Account reports",
          "Bank Account New Accounts reports must be <= total Identity Theft reports",
          "Proportion must be between 0 and 1",
          "The proportion must be rounded to 4 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Proportion must be rounded to 4 decimal places",
            "Bank Account theft type has 3 subtypes in 2024_CSN_Identity_Theft_Reports_by_Type.csv",
            "Total reports count must come from 2024_CSN_Report_Type__Report Type"
          ],
          [
            "Total reports = Fraud + Identity Theft + Other",
            "Bank Account New Accounts reports must be <= total Bank Account reports",
            "Bank Account New Accounts reports must be <= total Identity Theft reports",
            "Proportion must be between 0 and 1"
          ],
          [
            "The proportion must be rounded to 4 decimal places."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Report Type = 'Identity Theft'",
          "Theft Type = 'Bank Account'",
          "Theft Subtype = 'New Accounts'",
          "Filter Identity Theft Reports by Type where Theft Type='Bank Account' AND Theft Subtype='New Accounts'",
          "Isolate Identity Theft reports from '2024_CSN_Report_Type__Report Type'",
          "Filter '2024_CSN_Identity_Theft_Reports_by_Type.csv' for 'Theft Type' = 'Bank Account' and 'Theft Subtype' = 'New Accounts'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Report Type = 'Identity Theft'",
            "Theft Type = 'Bank Account'",
            "Theft Subtype = 'New Accounts'"
          ],
          [
            "Filter Identity Theft Reports by Type where Theft Type='Bank Account' AND Theft Subtype='New Accounts'"
          ],
          [
            "Isolate Identity Theft reports from '2024_CSN_Report_Type__Report Type'",
            "Filter '2024_CSN_Identity_Theft_Reports_by_Type.csv' for 'Theft Type' = 'Bank Account' and 'Theft Subtype' = 'New Accounts'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify that sum of all # of Reports in 2024_CSN_Report_Type__Report Type equals sum of all reports across all files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify that sum of all # of Reports in 2024_CSN_Report_Type__Report Type equals sum of all reports across all files"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Proportion as decimal rounded to 4 places",
          "Calculation: (# Bank Account/New Accounts reports) / (Total all reports)",
          "Round result to exactly 4 decimal places",
          "Output should be a single numeric value representing a proportion",
          "Output must be a JSON object.",
          "The proportion must be a number."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Proportion as decimal rounded to 4 places",
            "Calculation: (# Bank Account/New Accounts reports) / (Total all reports)"
          ],
          [
            "Round result to exactly 4 decimal places",
            "Output should be a single numeric value representing a proportion"
          ],
          [
            "Output must be a JSON object.",
            "The proportion must be a number."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5665000000000001
  },
  "legal-hard-23": {
    "m_q": {
      "target_metric": {
        "value": "highest report density (reports per 100K population) considering all types of reports (identity theft, fraud and others)",
        "confidence": 0.3333333333333333,
        "votes": [
          "highest report density (reports per 100K population) considering all types of reports (identity theft, fraud and others)",
          "highest report density (reports per 100K population) combining all types of reports (identity theft, fraud and others)",
          "State with the highest total reports per 100K population across all report types (identity theft, fraud, and others)"
        ]
      },
      "filters": {
        "value": [
          "include all states (including DC and PR)",
          "include all report types",
          "include all states, DC, and PR"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "include all states (including DC and PR)",
            "include all report types"
          ],
          [
            "include all states, DC, and PR"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "State"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "State"
          ],
          [
            "State"
          ],
          [
            "State"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to combine identity theft and fraud/other reports?",
          "Should we sum reports per 100K or calculate new combined rate?",
          "Do both files cover all 52 states (including DC and PR)?",
          "What is the identity theft report density for each state?",
          "What is the fraud and other report density for each state?",
          "What is the total combined report density for each state?",
          "Which state has the maximum combined report density?",
          "Calculate the total reports per 100K population for each state by summing the 'Reports per 100K Population' from both datasets.",
          "Identify the state with the maximum 'Total Reports per 100K Population'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How to combine identity theft and fraud/other reports?",
            "Should we sum reports per 100K or calculate new combined rate?",
            "Do both files cover all 52 states (including DC and PR)?"
          ],
          [
            "What is the identity theft report density for each state?",
            "What is the fraud and other report density for each state?",
            "What is the total combined report density for each state?",
            "Which state has the maximum combined report density?"
          ],
          [
            "Calculate the total reports per 100K population for each state by summing the 'Reports per 100K Population' from both datasets.",
            "Identify the state with the maximum 'Total Reports per 100K Population'."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_State_Rankings_Identity_Theft_Reports.csv",
          "2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_State_Rankings_Identity_Theft_Reports.csv",
            "2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv"
          ],
          [
            "2024_CSN_State_Rankings_Identity_Theft_Reports.csv",
            "2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv"
          ],
          [
            "2024_CSN_State_Rankings_Identity_Theft_Reports.csv",
            "2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Reports per 100K Population has different dtypes (float64 vs int64)",
          "Both files have same column names but different data",
          "Both files have identical column names but represent different report types",
          "Reports per 100K Population has different dtype: float64 in identity theft file vs int64 in fraud file",
          "The 'Reports per 100K Population' column has different data types across the two CSV files (float64 in Identity Theft, int64 in Fraud and Other).",
          "The 'Rank' column is not needed for the final calculation and can be dropped."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Reports per 100K Population has different dtypes (float64 vs int64)",
            "Both files have same column names but different data"
          ],
          [
            "Both files have identical column names but represent different report types",
            "Reports per 100K Population has different dtype: float64 in identity theft file vs int64 in fraud file"
          ],
          [
            "The 'Reports per 100K Population' column has different data types across the two CSV files (float64 in Identity Theft, int64 in Fraud and Other).",
            "The 'Rank' column is not needed for the final calculation and can be dropped."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "reports per 100k population": "reports per 100,000 population",
          "# of reports": "count of reports",
          "identity_theft_reports_per_100k": "identity theft reports per 100,000 population",
          "fraud_other_reports_per_100k": "fraud and other reports per 100,000 population"
        },
        "confidence": 0.6666666666666667,
        "votes": [
          {
            "Reports per 100K Population": "reports per 100,000 population",
            "# of Reports": "count of reports",
            "Identity_Theft_Reports_per_100K": "identity theft reports per 100,000 population",
            "Fraud_Other_Reports_per_100K": "fraud and other reports per 100,000 population"
          },
          {
            "Reports per 100K Population": "reports per 100,000 population",
            "# of Reports": "absolute count of reports"
          },
          {
            "Reports per 100K Population": "reports per 100,000 people",
            "# of Reports": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Reports per 100K values differ significantly between files (identity theft ~200-500, fraud/other ~1200-2100)",
          "Reports per 100K Population is already normalized, can be directly summed across report types",
          "Population denominator should be consistent across both files for the same state"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Reports per 100K values differ significantly between files (identity theft ~200-500, fraud/other ~1200-2100)"
          ],
          [
            "Reports per 100K Population is already normalized, can be directly summed across report types",
            "Population denominator should be consistent across both files for the same state"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Different ranking systems in each file",
          "Cannot directly sum Reports per 100K columns as they are rates based on different denominators",
          "Need to verify that both files use the same population base for calculating per 100K rates",
          "Both datasets should cover the same states (52 entries each including DC and PR)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different ranking systems in each file",
            "Cannot directly sum Reports per 100K columns as they are rates based on different denominators"
          ],
          [
            "Need to verify that both files use the same population base for calculating per 100K rates",
            "Both datasets should cover the same states (52 entries each including DC and PR)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total reports = identity theft reports + fraud/other reports",
          "Combined reports per 100K must be calculated from total reports and population",
          "Population can be derived from (# of Reports) / (Reports per 100K Population) * 100,000",
          "Must include all 52 jurisdictions (50 states + DC + PR)",
          "Reports per 100K Population values must be non-negative",
          "Each state should appear exactly once in each file",
          "Combined density is sum of identity theft density and fraud/other density",
          "The 'State' column should contain valid U.S. state names, including DC and PR.",
          "The 'Reports per 100K Population' and '# of Reports' columns should contain non-negative numerical values."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total reports = identity theft reports + fraud/other reports",
            "Combined reports per 100K must be calculated from total reports and population",
            "Population can be derived from (# of Reports) / (Reports per 100K Population) * 100,000"
          ],
          [
            "Must include all 52 jurisdictions (50 states + DC + PR)",
            "Reports per 100K Population values must be non-negative",
            "Each state should appear exactly once in each file",
            "Combined density is sum of identity theft density and fraud/other density"
          ],
          [
            "The 'State' column should contain valid U.S. state names, including DC and PR.",
            "The 'Reports per 100K Population' and '# of Reports' columns should contain non-negative numerical values."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Include all 52 states from both files",
          "No filtering required - all states should be included in analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Include all 52 states from both files"
          ],
          [
            "No filtering required - all states should be included in analysis"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check that state lists match between files",
          "Verify population consistency between files using derived population calculations",
          "Verify all 52 jurisdictions present in both files",
          "Check for missing values in Reports per 100K Population columns",
          "Validate that join produces 52 records"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check that state lists match between files",
            "Verify population consistency between files using derived population calculations"
          ],
          [
            "Verify all 52 jurisdictions present in both files",
            "Check for missing values in Reports per 100K Population columns",
            "Validate that join produces 52 records"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "State name",
          "Combined reports per 100K population",
          "Ranking of states by combined rate",
          "Return the state name with highest combined report density",
          "Optionally include the combined density value",
          "Result should be a single state (scalar output)",
          "The output should be a single state name."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "State name",
            "Combined reports per 100K population",
            "Ranking of states by combined rate"
          ],
          [
            "Return the state name with highest combined report density",
            "Optionally include the combined density value",
            "Result should be a single state (scalar output)"
          ],
          [
            "The output should be a single state name."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6500000000000001
  },
  "legal-hard-24": {
    "m_q": {
      "target_metric": {
        "value": "Metropolitan area with highest number of identity theft reports within the state that has the highest total reports (identity theft + fraud/other)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Metropolitan area with highest number of identity theft reports within the state that has the highest total reports (identity theft + fraud/other)",
          "Metropolitan area with highest # of identity theft reports within the state that has the highest total # of reports (identity theft + fraud and other)",
          "Metropolitan area with the highest number of identity theft reports within the state having the highest total number of reports (identity theft, fraud, and other)"
        ]
      },
      "filters": {
        "value": [
          "State with highest total reports across both report types",
          "Identity theft reports only for metropolitan area analysis",
          "State with maximum total reports across both report types",
          "Metropolitan areas within that state",
          "Identity theft reports only for metropolitan areas"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "State with highest total reports across both report types",
            "Identity theft reports only for metropolitan area analysis"
          ],
          [
            "State with maximum total reports across both report types",
            "Metropolitan areas within that state",
            "Identity theft reports only for metropolitan areas"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "State",
          "Metropolitan Area"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "State",
            "Metropolitan Area"
          ],
          [
            "State",
            "Metropolitan Area"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar (single metropolitan area name)",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which state has the highest total number of reports across both identity theft and fraud/other categories?",
          "For that identified state, which metropolitan area has the highest number of identity theft reports?",
          "What is the total # of reports for each state (sum of identity theft reports and fraud/other reports)?",
          "Which state has the highest total # of reports?",
          "What are the metropolitan areas in that state?",
          "Which metropolitan area in that state has the highest # of identity theft reports?",
          "What is the total number of reports (identity theft, fraud, and other) for each state?",
          "Which state has the highest total number of reports?",
          "What are the identity theft reports for each metropolitan area in the state with the highest total reports?",
          "Which metropolitan area in that state has the highest number of identity theft reports?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Which state has the highest total number of reports across both identity theft and fraud/other categories?",
            "For that identified state, which metropolitan area has the highest number of identity theft reports?"
          ],
          [
            "What is the total # of reports for each state (sum of identity theft reports and fraud/other reports)?",
            "Which state has the highest total # of reports?",
            "What are the metropolitan areas in that state?",
            "Which metropolitan area in that state has the highest # of identity theft reports?"
          ],
          [
            "What is the total number of reports (identity theft, fraud, and other) for each state?",
            "Which state has the highest total number of reports?",
            "What are the identity theft reports for each metropolitan area in the state with the highest total reports?",
            "Which metropolitan area in that state has the highest number of identity theft reports?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_State_Rankings_Identity_Theft_Reports.csv",
          "2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv",
          "State-specific CSV files (e.g., Florida.csv, California.csv, etc.)",
          "Alabama.csv",
          "Alaska.csv",
          "Arizona.csv",
          "Arkansas.csv",
          "California.csv",
          "Colorado.csv",
          "Connecticut.csv",
          "Delaware.csv",
          "DistrictofColumbia.csv",
          "Florida.csv",
          "Georgia.csv",
          "Hawaii.csv",
          "Idaho.csv",
          "Illinois.csv",
          "Indiana.csv",
          "Iowa.csv",
          "Kansas.csv",
          "Kentucky.csv",
          "Louisiana.csv",
          "Maine.csv",
          "Maryland.csv",
          "Massachusetts.csv",
          "Michigan.csv",
          "Minnesota.csv",
          "Mississippi.csv",
          "Missouri.csv",
          "Montana.csv",
          "Nebraska.csv",
          "Nevada.csv",
          "NewHampshire.csv",
          "NewJersey.csv",
          "NewMexico.csv",
          "NewYork.csv",
          "NorthCarolina.csv",
          "NorthDakota.csv",
          "Ohio.csv",
          "Oklahoma.csv",
          "Oregon.csv",
          "Pennsylvania.csv",
          "PuertoRico.csv",
          "RhodeIsland.csv",
          "SouthCarolina.csv",
          "SouthDakota.csv",
          "Tennessee.csv",
          "Texas.csv",
          "Utah.csv",
          "Vermont.csv",
          "Virginia.csv",
          "Washington.csv",
          "WestVirginia.csv",
          "Wisconsin.csv",
          "Wyoming.csv"
        ],
        "confidence": 0.6727272727272728,
        "votes": [
          [
            "2024_CSN_State_Rankings_Identity_Theft_Reports.csv",
            "2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv",
            "State-specific CSV files (e.g., Florida.csv, California.csv, etc.)"
          ],
          [
            "2024_CSN_State_Rankings_Identity_Theft_Reports.csv",
            "2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv",
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "2024_CSN_State_Rankings_Identity_Theft_Reports.csv",
            "2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv",
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "State column names are consistent across files",
          "Rank column is float64 in identity theft file but int64 in fraud file (per dtypes)",
          "Reports per 100K Population is float64 in identity theft file but int64 in fraud file",
          "State name in ranking files vs state-specific CSV filenames require normalization (e.g., 'District of Columbia' vs 'DistrictofColumbia.csv')",
          "Metropolitan area CSV files contain footer row with metadata that needs to be filtered out"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "State column names are consistent across files",
            "Rank column is float64 in identity theft file but int64 in fraud file (per dtypes)",
            "Reports per 100K Population is float64 in identity theft file but int64 in fraud file"
          ],
          [
            "State name in ranking files vs state-specific CSV filenames require normalization (e.g., 'District of Columbia' vs 'DistrictofColumbia.csv')",
            "Metropolitan area CSV files contain footer row with metadata that needs to be filtered out"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of reports",
          "reports per 100k population": "reports per 100,000 population",
          "rank": "ordinal ranking (1 = highest)"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "# of Reports": "count of reports",
            "Reports per 100K Population": "reports per 100,000 population"
          },
          {
            "# of Reports": "count of reports",
            "Reports per 100K Population": "reports per 100,000 population",
            "Rank": "ordinal ranking (1 = highest)"
          },
          {
            "# of Reports": "number of reports",
            "Reports per 100K Population": "reports per 100,000 population"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "State-level reports are integers, metropolitan area reports are floats (likely due to decimal values or missing data representation)",
          "Need to sum state-level identity theft and fraud/other reports for total comparison",
          "State-level reports are absolute counts, not rates",
          "Metropolitan area reports are absolute counts, not normalized by population"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "State-level reports are integers, metropolitan area reports are floats (likely due to decimal values or missing data representation)",
            "Need to sum state-level identity theft and fraud/other reports for total comparison"
          ],
          [
            "State-level reports are absolute counts, not rates",
            "Metropolitan area reports are absolute counts, not normalized by population"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Metropolitan area files contain descriptive rows that are not actual data (e.g., 'Metropolitan Areas are defined by...') which must be filtered out",
          "State-level '# of Reports' represents identity theft only in one file, fraud and other in another file",
          "Metropolitan area '# of Reports' may include all types of reports (identity theft + fraud + other), not specified in column header"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan area files contain descriptive rows that are not actual data (e.g., 'Metropolitan Areas are defined by...') which must be filtered out"
          ],
          [
            "State-level '# of Reports' represents identity theft only in one file, fraud and other in another file",
            "Metropolitan area '# of Reports' may include all types of reports (identity theft + fraud + other), not specified in column header"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "Metropolitan Areas are defined by the Office of Management and Budget...",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "Metropolitan Areas are defined by the Office of Management and Budget..."
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 0.6,
        "votes": [
          4.0,
          4.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "All 52 states/territories (including DC and PR) must be considered",
          "Metropolitan areas may span multiple states (e.g., 'Chicago-Naperville-Elgin, IL-IN') - need to filter to only areas within target state",
          "Total reports = identity theft reports + fraud/other reports at state level",
          "Only consider states present in both identity theft and fraud/other ranking files",
          "Exclude footer rows from metropolitan area files (rows containing 'Metropolitan Areas are defined by')",
          "Exclude rows with NaN values in '# of Reports' column from metropolitan area files",
          "State with highest total reports must be determined before filtering metropolitan areas",
          "Puerto Rico (PR) and District of Columbia (DC) should be included as states per question specification",
          "Need to sum '# of Reports' from '2024_CSN_State_Rankings_Identity_Theft_Reports.csv' and '2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv' to get the total reports per state.",
          "Need to iterate through state files to find the metropolitan area with the highest '# of Reports' for identity theft."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "All 52 states/territories (including DC and PR) must be considered",
            "Metropolitan areas may span multiple states (e.g., 'Chicago-Naperville-Elgin, IL-IN') - need to filter to only areas within target state",
            "Total reports = identity theft reports + fraud/other reports at state level"
          ],
          [
            "Only consider states present in both identity theft and fraud/other ranking files",
            "Exclude footer rows from metropolitan area files (rows containing 'Metropolitan Areas are defined by')",
            "Exclude rows with NaN values in '# of Reports' column from metropolitan area files",
            "State with highest total reports must be determined before filtering metropolitan areas",
            "Puerto Rico (PR) and District of Columbia (DC) should be included as states per question specification"
          ],
          [
            "Need to sum '# of Reports' from '2024_CSN_State_Rankings_Identity_Theft_Reports.csv' and '2024_CSN_State_Rankings_Fraud_and_Other_Reports.csv' to get the total reports per state.",
            "Need to iterate through state files to find the metropolitan area with the highest '# of Reports' for identity theft."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out non-data rows from metropolitan area files (those containing 'Metropolitan Areas are defined by')",
          "Filter metropolitan areas to only those belonging to the identified state",
          "Calculate total_reports = identity_theft_reports + fraud_and_other_reports for each state",
          "Identify state with max(total_reports)",
          "Filter metropolitan areas to only those in the identified state",
          "Select metropolitan area with max(# of Reports) for identity theft"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out non-data rows from metropolitan area files (those containing 'Metropolitan Areas are defined by')",
            "Filter metropolitan areas to only those belonging to the identified state"
          ],
          [
            "Calculate total_reports = identity_theft_reports + fraud_and_other_reports for each state",
            "Identify state with max(total_reports)",
            "Filter metropolitan areas to only those in the identified state",
            "Select metropolitan area with max(# of Reports) for identity theft"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify state with highest total reports is consistent across both ranking files",
          "Check that sum of metropolitan area reports approximates state total for identity theft"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify state with highest total reports is consistent across both ranking files",
            "Check that sum of metropolitan area reports approximates state total for identity theft"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Metropolitan area name as string",
          "Should exclude footnotes/descriptive text from output",
          "Return the name of the metropolitan area with highest identity theft reports in the top state",
          "Output should be a single metropolitan area name",
          "Metropolitan area name should match the format in the CSV (e.g., 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan area name as string",
            "Should exclude footnotes/descriptive text from output"
          ],
          [
            "Return the name of the metropolitan area with highest identity theft reports in the top state",
            "Output should be a single metropolitan area name",
            "Metropolitan area name should match the format in the CSV (e.g., 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area')"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5725252525252527
  },
  "legal-hard-28": {
    "m_q": {
      "target_metric": {
        "value": "Percentage of Imposter Scams reports among all reports for military consumers compared to the same percentage for the general population",
        "confidence": 0.3333333333333333,
        "votes": [
          "Percentage of Imposter Scams reports among all reports for military consumers compared to the same percentage for the general population",
          "Comparison of Imposter Scams percentage for military consumers vs general population in 2024",
          "Compare the percentage of Imposter Scams reports relative to total reports for military consumers versus the general population in 2024."
        ]
      },
      "filters": {
        "value": [
          "Year = 2024",
          "Category = 'Imposter Scams'",
          "Demographic = military consumers vs general population"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "Year = 2024",
            "Category = 'Imposter Scams'",
            "Demographic = military consumers vs general population"
          ],
          [
            "Year = 2024",
            "Category = 'Imposter Scams'"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Demographic (military/general)",
          "Category",
          "demographic (military vs general population)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Demographic (military/general)",
            "Category"
          ],
          [
            "demographic (military vs general population)"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar (Yes/No)",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What percentage of military consumer reports are Imposter Scams?",
          "What percentage of general population reports are Imposter Scams?",
          "Is the military percentage higher than the general population percentage?",
          "What is the total number of reports for military consumers?",
          "What is the number of Imposter Scams reports for military consumers?",
          "What is the percentage of Imposter Scams among military consumer reports?",
          "What is the total number of reports in the general population?",
          "What is the number of Imposter Scams reports in the general population?",
          "What is the percentage of Imposter Scams among general population reports?",
          "What is the total number of reports for military consumers in 2024?",
          "What is the number of Imposter Scams reports for military consumers in 2024?",
          "What is the percentage of Imposter Scams reports for military consumers in 2024?",
          "What is the total number of reports for the general population in 2024?",
          "What is the number of Imposter Scams reports for the general population in 2024?",
          "What is the percentage of Imposter Scams reports for the general population in 2024?"
        ],
        "confidence": 0.35555555555555546,
        "votes": [
          [
            "What percentage of military consumer reports are Imposter Scams?",
            "What percentage of general population reports are Imposter Scams?",
            "Is the military percentage higher than the general population percentage?"
          ],
          [
            "What is the total number of reports for military consumers?",
            "What is the number of Imposter Scams reports for military consumers?",
            "What is the percentage of Imposter Scams among military consumer reports?",
            "What is the total number of reports in the general population?",
            "What is the number of Imposter Scams reports in the general population?",
            "What is the percentage of Imposter Scams among general population reports?",
            "Is the military percentage higher than the general population percentage?"
          ],
          [
            "What is the total number of reports for military consumers in 2024?",
            "What is the number of Imposter Scams reports for military consumers in 2024?",
            "What is the percentage of Imposter Scams reports for military consumers in 2024?",
            "What is the total number of reports for the general population in 2024?",
            "What is the number of Imposter Scams reports for the general population in 2024?",
            "What is the percentage of Imposter Scams reports for the general population in 2024?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Reports_by_Military_Consumers.csv",
          "2024_CSN_Report_Categories.csv",
          "2024_CSN_Reports_by_Military_Consumers__Reports by Military Consumers"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "2024_CSN_Reports_by_Military_Consumers.csv",
            "2024_CSN_Report_Categories.csv"
          ],
          [
            "2024_CSN_Reports_by_Military_Consumers.csv",
            "2024_CSN_Reports_by_Military_Consumers__Reports by Military Consumers",
            "2024_CSN_Report_Categories.csv"
          ],
          [
            "2024_CSN_Reports_by_Military_Consumers.csv",
            "2024_CSN_Reports_by_Military_Consumers__Reports by Military Consumers",
            "2024_CSN_Report_Categories.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Military data has separate files for different report types (Fraud, Identity Theft, Other) while general data combines them",
          "Military data has additional columns (% Reporting $ Loss, Total $ Loss, Median $ Loss) not in general data",
          "Different total report counts across files",
          "Military data has fraud-specific categories while general data includes all report types"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Military data has separate files for different report types (Fraud, Identity Theft, Other) while general data combines them",
            "Military data has additional columns (% Reporting $ Loss, Total $ Loss, Median $ Loss) not in general data"
          ],
          [
            "Different total report counts across files",
            "Military data has fraud-specific categories while general data includes all report types"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count",
          "% reporting $ loss": "percentage",
          "total $ loss": "dollars",
          "median $ loss": "dollars",
          "percentage": "percentage"
        },
        "confidence": 0.7999999999999999,
        "votes": [
          {
            "# of Reports": "count",
            "% Reporting $ Loss": "percentage",
            "Total $ Loss": "dollars",
            "Median $ Loss": "dollars",
            "Percentage": "percentage"
          },
          {
            "# of Reports": "count",
            "Percentage": "percentage",
            "% Reporting $ Loss": "percentage",
            "Total $ Loss": "US dollars (millions indicated by M suffix)",
            "Median $ Loss": "US dollars"
          },
          {
            "# of Reports": "number of reports",
            "Percentage": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Military data shows top 10 categories only, while general data shows 29 categories",
          "Military data includes only fraud categories, while general data includes all complaint categories",
          "Total $ Loss values use 'M' suffix for millions in military consumer file",
          "Percentages stored as strings with '%' suffix in multiple files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Military data shows top 10 categories only, while general data shows 29 categories",
            "Military data includes only fraud categories, while general data includes all complaint categories"
          ],
          [
            "Total $ Loss values use 'M' suffix for millions in military consumer file",
            "Percentages stored as strings with '%' suffix in multiple files"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Military data 'Imposter Scams' count (44,587) appears to be only fraud reports, while general data 'Imposter Scams' count (845,806) includes all reports",
          "Military consumer data appears to represent fraud reports only, while general population data includes all report types (fraud, identity theft, other)",
          "Need to determine if military Imposter Scams should be compared against general population fraud reports or all reports"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Military data 'Imposter Scams' count (44,587) appears to be only fraud reports, while general data 'Imposter Scams' count (845,806) includes all reports"
          ],
          [
            "Military consumer data appears to represent fraud reports only, while general population data includes all report types (fraud, identity theft, other)",
            "Need to determine if military Imposter Scams should be compared against general population fraud reports or all reports"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 0.4285714285714286,
        "votes": [
          6.0,
          6.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Military Imposter Scams reports = 44,587",
          "Total military fraud reports = 99,443",
          "General population Imposter Scams reports = 845,806",
          "Total general population reports = sum of all # of Reports in 2024_CSN_Report_Categories.csv",
          "Imposter Scams must exist in both military and general population datasets",
          "Total reports for each demographic must be positive",
          "Percentages must be calculated from the same denominator type (fraud reports vs all reports)",
          "Ensure that the percentages are calculated correctly (Imposter Scams / Total Reports * 100).",
          "The total reports for military consumers needs to be derived from '2024_CSN_Reports_by_Military_Consumers__Reports by Military Consumers' by summing '# of Reports' for all 'Report Type' categories.",
          "The number of Imposter Scams reports for military consumers is directly available in '2024_CSN_Reports_by_Military_Consumers.csv' under the 'Category' column.",
          "The total reports for the general population needs to be derived from summing '# of Reports' in '2024_CSN_Report_Categories.csv'.",
          "The number of Imposter Scams reports for the general population is directly available in '2024_CSN_Report_Categories.csv' under the 'Category' column."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Military Imposter Scams reports = 44,587",
            "Total military fraud reports = 99,443",
            "General population Imposter Scams reports = 845,806",
            "Total general population reports = sum of all # of Reports in 2024_CSN_Report_Categories.csv"
          ],
          [
            "Imposter Scams must exist in both military and general population datasets",
            "Total reports for each demographic must be positive",
            "Percentages must be calculated from the same denominator type (fraud reports vs all reports)"
          ],
          [
            "Ensure that the percentages are calculated correctly (Imposter Scams / Total Reports * 100).",
            "The total reports for military consumers needs to be derived from '2024_CSN_Reports_by_Military_Consumers__Reports by Military Consumers' by summing '# of Reports' for all 'Report Type' categories.",
            "The number of Imposter Scams reports for military consumers is directly available in '2024_CSN_Reports_by_Military_Consumers.csv' under the 'Category' column.",
            "The total reports for the general population needs to be derived from summing '# of Reports' in '2024_CSN_Report_Categories.csv'.",
            "The number of Imposter Scams reports for the general population is directly available in '2024_CSN_Report_Categories.csv' under the 'Category' column."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to only Imposter Scams category",
          "Calculate percentages relative to total reports for each demographic",
          "Calculate total fraud reports for military: sum of # of Reports from 2024_CSN_Reports_by_Military_Consumers__Reports by Military Consumers where Report Type = 'Fraud'",
          "Calculate total reports for general population: sum of all # of Reports from 2024_CSN_Report_Categories.csv",
          "Extract Imposter Scams count for military from 2024_CSN_Reports_by_Military_Consumers.csv where Category = 'Imposter Scams'",
          "Extract Imposter Scams count for general population from 2024_CSN_Report_Categories.csv where Category = 'Imposter Scams'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to only Imposter Scams category",
            "Calculate percentages relative to total reports for each demographic"
          ],
          [
            "Calculate total fraud reports for military: sum of # of Reports from 2024_CSN_Reports_by_Military_Consumers__Reports by Military Consumers where Report Type = 'Fraud'",
            "Calculate total reports for general population: sum of all # of Reports from 2024_CSN_Report_Categories.csv",
            "Extract Imposter Scams count for military from 2024_CSN_Reports_by_Military_Consumers.csv where Category = 'Imposter Scams'",
            "Extract Imposter Scams count for general population from 2024_CSN_Report_Categories.csv where Category = 'Imposter Scams'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Percentage comparison: (military_imposter/total_military) vs (general_imposter/total_general)",
          "Compare (military_imposter_scams / military_total_reports) vs (general_imposter_scams / general_total_reports)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage comparison: (military_imposter/total_military) vs (general_imposter/total_general)"
          ],
          [
            "Compare (military_imposter_scams / military_total_reports) vs (general_imposter_scams / general_total_reports)"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Binary Yes/No answer",
          "No explanation text",
          "Return only 'Yes' or 'No'",
          "No explanation or additional text required",
          "Output should be 'Yes' if the percentage of Imposter Scams reports is higher for military consumers than for the general population, and 'No' otherwise."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Binary Yes/No answer",
            "No explanation text"
          ],
          [
            "Return only 'Yes' or 'No'",
            "No explanation or additional text required"
          ],
          [
            "Output should be 'Yes' if the percentage of Imposter Scams reports is higher for military consumers than for the general population, and 'No' otherwise."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5847619047619049
  },
  "legal-hard-29": {
    "m_q": {
      "target_metric": {
        "value": "percentage of fraud reports in its own state",
        "confidence": 0.3333333333333333,
        "votes": [
          "percentage of fraud reports in its own state",
          "Metropolitan area with the largest percentage of fraud reports within its own state, among states with 5 or more metro areas",
          "The metropolitan area with the largest percentage of fraud reports in its own state, among states with 5 or more metro areas."
        ]
      },
      "filters": {
        "value": [
          "states with 5 or more metro areas",
          "exclude footnote rows containing 'Metropolitan Areas are defined'",
          "States with 5 or more metropolitan areas only"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "states with 5 or more metro areas",
            "exclude footnote rows containing 'Metropolitan Areas are defined'"
          ],
          [
            "States with 5 or more metropolitan areas only"
          ],
          [
            "States with 5 or more metro areas"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "state",
          "metropolitan area"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "state",
            "metropolitan area"
          ],
          [
            "State",
            "Metropolitan Area"
          ],
          [
            "State"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which states have 5 or more metro areas?",
          "For each qualifying state, what is the total number of reports?",
          "For each metro area in qualifying states, what percentage of state reports does it represent?",
          "Which metro area has the highest percentage among all qualifying states?",
          "How many metropolitan areas does each state have?",
          "Which states have 5 or more metropolitan areas?",
          "What is the total number of fraud reports per state (for states with 5+ metro areas)?",
          "What percentage of the state's total fraud reports does each metropolitan area represent?",
          "Which metropolitan area has the highest percentage within its state?",
          "For each state, calculate the total number of fraud reports.",
          "For each state, identify the metropolitan area with the highest percentage of fraud reports within that state.",
          "Filter states with at least 5 metropolitan areas.",
          "Among the filtered states, find the metropolitan area with the largest percentage of fraud reports in its state."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which states have 5 or more metro areas?",
            "For each qualifying state, what is the total number of reports?",
            "For each metro area in qualifying states, what percentage of state reports does it represent?",
            "Which metro area has the highest percentage among all qualifying states?"
          ],
          [
            "How many metropolitan areas does each state have?",
            "Which states have 5 or more metropolitan areas?",
            "What is the total number of fraud reports per state (for states with 5+ metro areas)?",
            "What percentage of the state's total fraud reports does each metropolitan area represent?",
            "Which metropolitan area has the highest percentage within its state?"
          ],
          [
            "For each state, calculate the total number of fraud reports.",
            "For each state, identify the metropolitan area with the highest percentage of fraud reports within that state.",
            "Filter states with at least 5 metropolitan areas.",
            "Among the filtered states, find the metropolitan area with the largest percentage of fraud reports in its state."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Alabama.csv",
          "Alaska.csv",
          "Arizona.csv",
          "Arkansas.csv",
          "California.csv",
          "Colorado.csv",
          "Connecticut.csv",
          "Delaware.csv",
          "DistrictofColumbia.csv",
          "Florida.csv",
          "Georgia.csv",
          "Hawaii.csv",
          "Idaho.csv",
          "Illinois.csv",
          "Indiana.csv",
          "Iowa.csv",
          "Kansas.csv",
          "Kentucky.csv",
          "Louisiana.csv",
          "Maine.csv",
          "Maryland.csv",
          "Massachusetts.csv",
          "Michigan.csv",
          "Minnesota.csv",
          "Mississippi.csv",
          "Missouri.csv",
          "Montana.csv",
          "Nebraska.csv",
          "Nevada.csv",
          "NewHampshire.csv",
          "NewJersey.csv",
          "NewMexico.csv",
          "NewYork.csv",
          "NorthCarolina.csv",
          "NorthDakota.csv",
          "Ohio.csv",
          "Oklahoma.csv",
          "Oregon.csv",
          "Pennsylvania.csv",
          "PuertoRico.csv",
          "RhodeIsland.csv",
          "SouthCarolina.csv",
          "SouthDakota.csv",
          "Tennessee.csv",
          "Texas.csv",
          "Utah.csv",
          "Vermont.csv",
          "Virginia.csv",
          "Washington.csv",
          "WestVirginia.csv",
          "Wisconsin.csv",
          "Wyoming.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All files have identical schema: ['Metropolitan Area', '# of Reports']",
          "Last row in each file contains metadata text instead of data",
          "Some metropolitan areas span multiple states (cross-border metros)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All files have identical schema: ['Metropolitan Area', '# of Reports']"
          ],
          [
            "Last row in each file contains metadata text instead of data",
            "Some metropolitan areas span multiple states (cross-border metros)"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of fraud reports",
          "percentage": "percentage of state total",
          "number of reports": "Number of fraud reports"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "# of Reports": "count of fraud reports"
          },
          {
            "# of Reports": "count",
            "percentage": "percentage of state total"
          },
          {
            "Number of Reports": "Number of fraud reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Reports are counts, not rates per population (despite footnote mentioning 'reports per 100,000 population')",
          "Cross-state metropolitan areas appear in multiple state files with same report count",
          "Need to assign cross-border metros to their primary state or allocate reports appropriately"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Reports are counts, not rates per population (despite footnote mentioning 'reports per 100,000 population')"
          ],
          [
            "Cross-state metropolitan areas appear in multiple state files with same report count",
            "Need to assign cross-border metros to their primary state or allocate reports appropriately"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Cross-state metro areas (e.g., 'Columbus, GA-AL') appear in multiple state files, causing double-counting if aggregating across states",
          "Metropolitan areas like 'Chicago-Naperville-Elgin, IL-IN' appear in both Illinois.csv and Indiana.csv with identical report counts",
          "Philadelphia-Camden-Wilmington appears in Pennsylvania, New Jersey, Delaware, and Maryland files",
          "Cross-state metros should only be counted once per state to avoid double-counting"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Cross-state metro areas (e.g., 'Columbus, GA-AL') appear in multiple state files, causing double-counting if aggregating across states"
          ],
          [
            "Metropolitan areas like 'Chicago-Naperville-Elgin, IL-IN' appear in both Illinois.csv and Indiana.csv with identical report counts",
            "Philadelphia-Camden-Wilmington appears in Pennsylvania, New Jersey, Delaware, and Maryland files",
            "Cross-state metros should only be counted once per state to avoid double-counting"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "# of Reports must be \u2265 0",
          "Percentage calculations must sum to 100% within each state (excluding cross-state metros)",
          "Only consider states with 5 or more metropolitan areas",
          "Exclude the metadata footer row from each file",
          "Handle cross-state metropolitan areas appropriately (count only within each state's context)",
          "# of Reports must be numeric and non-null",
          "Metropolitan Area names should be standardized for consistency.",
          "Need to extract state name from the file name."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "# of Reports must be \u2265 0",
            "Percentage calculations must sum to 100% within each state (excluding cross-state metros)"
          ],
          [
            "Only consider states with 5 or more metropolitan areas",
            "Exclude the metadata footer row from each file",
            "Handle cross-state metropolitan areas appropriately (count only within each state's context)",
            "# of Reports must be numeric and non-null"
          ],
          [
            "Metropolitan Area names should be standardized for consistency.",
            "Need to extract state name from the file name."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where Metropolitan Area contains 'Metropolitan Areas are defined'",
          "Include only states where count of unique metro areas (excluding footnote) \u2265 5",
          "Count non-footer rows per state file to determine metro area count",
          "Filter to states where metro_count >= 5",
          "States with 5 or more metropolitan areas."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Metropolitan Area contains 'Metropolitan Areas are defined'",
            "Include only states where count of unique metro areas (excluding footnote) \u2265 5"
          ],
          [
            "Count non-footer rows per state file to determine metro area count",
            "Filter to states where metro_count >= 5"
          ],
          [
            "States with 5 or more metropolitan areas."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify no negative report counts",
          "Check that within-state percentages sum to approximately 100% for non-cross-state metros"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify no negative report counts",
            "Check that within-state percentages sum to approximately 100% for non-cross-state metros"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Metro area name with state abbreviation",
          "Percentage value with appropriate precision",
          "Return the single metropolitan area name with highest percentage",
          "Include the percentage value",
          "Include the state name",
          "Result should identify one specific metro area",
          "Output the name of the metropolitan area and the state it belongs to."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metro area name with state abbreviation",
            "Percentage value with appropriate precision"
          ],
          [
            "Return the single metropolitan area name with highest percentage",
            "Include the percentage value",
            "Include the state name",
            "Result should identify one specific metro area"
          ],
          [
            "Output the name of the metropolitan area and the state it belongs to."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6361111111111113
  },
  "legal-hard-30": {
    "m_q": {
      "target_metric": {
        "value": "Compare number of identity theft reports vs fraud reports for each metropolitan area in 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "Compare number of identity theft reports vs fraud reports for each metropolitan area in 2024",
          "Existence of a metropolitan area where identity theft reports exceeded fraud reports in 2024",
          "existence of a metropolitan area where the number of identity theft reports exceeded the number of fraud reports in 2024"
        ]
      },
      "filters": {
        "value": [
          "Year = 2024",
          "Report type = identity theft OR fraud",
          "Report type = identity theft",
          "Report type = fraud"
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "Year = 2024",
            "Report type = identity theft OR fraud"
          ],
          [
            "Year = 2024",
            "Report type = identity theft",
            "Report type = fraud"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Metropolitan Area"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Metropolitan Area"
          ],
          [
            "Metropolitan Area"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar (Yes/No)",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the report counts for identity theft per metropolitan area?",
          "What are the report counts for fraud per metropolitan area?",
          "For which metropolitan areas does identity theft count exceed fraud count?",
          "What defines identity theft vs fraud reports in these datasets?",
          "Are there separate data sources for identity theft and fraud, or are they in the same files?",
          "How to identify which rows represent identity theft vs fraud?",
          "How to compare counts between the two types for each metropolitan area?",
          "Does any metro area have identity theft count > fraud count?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the report counts for identity theft per metropolitan area?",
            "What are the report counts for fraud per metropolitan area?",
            "For which metropolitan areas does identity theft count exceed fraud count?"
          ],
          [
            "What defines identity theft vs fraud reports in these datasets?",
            "Are there separate data sources for identity theft and fraud, or are they in the same files?",
            "How to identify which rows represent identity theft vs fraud?",
            "How to compare counts between the two types for each metropolitan area?",
            "Does any metro area have identity theft count > fraud count?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Alabama.csv",
          "Alaska.csv",
          "Arizona.csv",
          "Arkansas.csv",
          "California.csv",
          "Colorado.csv",
          "Connecticut.csv",
          "Delaware.csv",
          "DistrictofColumbia.csv",
          "Florida.csv",
          "Georgia.csv",
          "Hawaii.csv",
          "Idaho.csv",
          "Illinois.csv",
          "Indiana.csv",
          "Iowa.csv",
          "Kansas.csv",
          "Kentucky.csv",
          "Louisiana.csv",
          "Maine.csv",
          "Maryland.csv",
          "Massachusetts.csv",
          "Michigan.csv",
          "Minnesota.csv",
          "Mississippi.csv",
          "Missouri.csv",
          "Montana.csv",
          "Nebraska.csv",
          "Nevada.csv",
          "NewHampshire.csv",
          "NewJersey.csv",
          "NewMexico.csv",
          "NewYork.csv",
          "NorthCarolina.csv",
          "NorthDakota.csv",
          "Ohio.csv",
          "Oklahoma.csv",
          "Oregon.csv",
          "Pennsylvania.csv",
          "PuertoRico.csv",
          "RhodeIsland.csv",
          "SouthCarolina.csv",
          "SouthDakota.csv",
          "Tennessee.csv",
          "Texas.csv",
          "Utah.csv",
          "Vermont.csv",
          "Virginia.csv",
          "Washington.csv",
          "WestVirginia.csv",
          "Wisconsin.csv",
          "Wyoming.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ],
          [
            "Alabama.csv",
            "Alaska.csv",
            "Arizona.csv",
            "Arkansas.csv",
            "California.csv",
            "Colorado.csv",
            "Connecticut.csv",
            "Delaware.csv",
            "DistrictofColumbia.csv",
            "Florida.csv",
            "Georgia.csv",
            "Hawaii.csv",
            "Idaho.csv",
            "Illinois.csv",
            "Indiana.csv",
            "Iowa.csv",
            "Kansas.csv",
            "Kentucky.csv",
            "Louisiana.csv",
            "Maine.csv",
            "Maryland.csv",
            "Massachusetts.csv",
            "Michigan.csv",
            "Minnesota.csv",
            "Mississippi.csv",
            "Missouri.csv",
            "Montana.csv",
            "Nebraska.csv",
            "Nevada.csv",
            "NewHampshire.csv",
            "NewJersey.csv",
            "NewMexico.csv",
            "NewYork.csv",
            "NorthCarolina.csv",
            "NorthDakota.csv",
            "Ohio.csv",
            "Oklahoma.csv",
            "Oregon.csv",
            "Pennsylvania.csv",
            "PuertoRico.csv",
            "RhodeIsland.csv",
            "SouthCarolina.csv",
            "SouthDakota.csv",
            "Tennessee.csv",
            "Texas.csv",
            "Utah.csv",
            "Vermont.csv",
            "Virginia.csv",
            "Washington.csv",
            "WestVirginia.csv",
            "Wisconsin.csv",
            "Wyoming.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All files have identical schema: ['Metropolitan Area', '# of Reports']",
          "Missing report type column - cannot distinguish identity theft vs fraud from current data",
          "All files have identical schema (Metropolitan Area, # of Reports)",
          "Each file contains a footer row with metadata text that should be filtered out",
          "No differentiation between identity theft and fraud in current schema"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All files have identical schema: ['Metropolitan Area', '# of Reports']",
            "Missing report type column - cannot distinguish identity theft vs fraud from current data"
          ],
          [
            "All files have identical schema (Metropolitan Area, # of Reports)",
            "Each file contains a footer row with metadata text that should be filtered out",
            "No differentiation between identity theft and fraud in current schema"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of reports (type unspecified)"
        },
        "confidence": 1.0,
        "votes": [
          {
            "# of Reports": "count of reports (type unspecified)"
          },
          {
            "# of Reports": "count"
          },
          {
            "# of Reports": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Cannot determine if reports are absolute counts or per capita rates",
          "Data includes footnote rows that are not actual data records",
          "Metadata footer mentions 'reports per 100,000 population' but # of Reports column appears to be raw counts, not rates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Cannot determine if reports are absolute counts or per capita rates",
            "Data includes footnote rows that are not actual data records"
          ],
          [
            "Metadata footer mentions 'reports per 100,000 population' but # of Reports column appears to be raw counts, not rates"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Metropolitan areas span multiple states (e.g., 'Columbus, GA-AL Metropolitan Statistical Area') but appear in multiple state files",
          "No clear way to deduplicate cross-state metropolitan areas",
          "Metropolitan areas may appear in multiple state files (cross-state metros)",
          "Cannot distinguish identity theft from fraud reports - all reports appear aggregated",
          "No temporal dimension visible - cannot confirm data is from 2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan areas span multiple states (e.g., 'Columbus, GA-AL Metropolitan Statistical Area') but appear in multiple state files",
            "No clear way to deduplicate cross-state metropolitan areas"
          ],
          [
            "Metropolitan areas may appear in multiple state files (cross-state metros)",
            "Cannot distinguish identity theft from fraud reports - all reports appear aggregated",
            "No temporal dimension visible - cannot confirm data is from 2024"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "nan"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            "",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Missing critical dimension: report type (identity theft vs fraud)",
          "Year information not present in data",
          "Cannot answer analytical question with current data structure",
          "Must identify whether reports are identity theft or fraud (not possible with current schema)",
          "Must filter to 2024 data only (no year column available)",
          "Must compare identity theft count vs fraud count per metropolitan area",
          "Must return Yes or No answer only",
          "Need to iterate through each file, and for each file, iterate through each row to find the maximum '# of Reports' for each 'Metropolitan Area'.",
          "Need to determine if the '# of Reports' refers to identity theft or fraud. This information is not directly available in the provided data files and would require external data or assumptions."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Missing critical dimension: report type (identity theft vs fraud)",
            "Year information not present in data",
            "Cannot answer analytical question with current data structure"
          ],
          [
            "Must identify whether reports are identity theft or fraud (not possible with current schema)",
            "Must filter to 2024 data only (no year column available)",
            "Must compare identity theft count vs fraud count per metropolitan area",
            "Must return Yes or No answer only"
          ],
          [
            "Need to iterate through each file, and for each file, iterate through each row to find the maximum '# of Reports' for each 'Metropolitan Area'.",
            "Need to determine if the '# of Reports' refers to identity theft or fraud. This information is not directly available in the provided data files and would require external data or assumptions."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude footnote rows containing 'Metropolitan Areas are defined' text",
          "Exclude footer rows containing 'Metropolitan Areas are defined by'",
          "Exclude rows where # of Reports is NaN"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude footnote rows containing 'Metropolitan Areas are defined' text"
          ],
          [
            "Exclude footer rows containing 'Metropolitan Areas are defined by'",
            "Exclude rows where # of Reports is NaN"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Need to verify if any metropolitan area appears in multiple files with different counts",
          "For each metropolitan area: identity_theft_count > fraud_count",
          "Existence check: ANY(identity_theft_count > fraud_count)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need to verify if any metropolitan area appears in multiple files with different counts"
          ],
          [
            "For each metropolitan area: identity_theft_count > fraud_count",
            "Existence check: ANY(identity_theft_count > fraud_count)"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Final answer must be Yes or No only",
          "Answer must be exactly 'Yes' or 'No'",
          "No explanation needed per question instructions",
          "Cannot be answered with available data - missing report type categorization and year information"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Final answer must be Yes or No only"
          ],
          [
            "Answer must be exactly 'Yes' or 'No'",
            "No explanation needed per question instructions",
            "Cannot be answered with available data - missing report type categorization and year information"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6291666666666668
  },
  "legal-hard-6": {
    "m_q": {
      "target_metric": {
        "value": "ratio = (2024 credit card fraud reports) / (2020 credit card fraud reports), rounded to 4 decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "ratio = (2024 credit card fraud reports) / (2020 credit card fraud reports), rounded to 4 decimal places",
          "Ratio of reported credit card frauds between 2024 and 2020, calculated as (2024 reports) / (2020 reports), rounded to 4 decimal places",
          "Ratio of credit card fraud reports in 2024 to 2020, rounded to 4 decimal places: (2024 reports) / (2020 reports)"
        ]
      },
      "filters": {
        "value": [
          "Theft Type = 'Credit Card'",
          "Year in ['2020', '2024']",
          "Theft Type == 'Credit Card'",
          "Year = 2024",
          "Year = 2020"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [
            "Theft Type = 'Credit Card'",
            "Year in ['2020', '2024']"
          ],
          [
            "Theft Type == 'Credit Card'",
            "Year in ['2020', '2024']"
          ],
          [
            "Year = 2024",
            "Year = 2020",
            "Theft Type = 'Credit Card'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Year"
          ],
          [
            "Year"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the number of credit card fraud reports in 2020?",
          "What is the number of credit card fraud reports in 2024?",
          "How to compute the ratio between these two values?",
          "What is the ratio of 2024 reports to 2020 reports?"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "What is the number of credit card fraud reports in 2020?",
            "What is the number of credit card fraud reports in 2024?",
            "How to compute the ratio between these two values?"
          ],
          [
            "What is the number of credit card fraud reports in 2024?",
            "What is the number of credit card fraud reports in 2020?",
            "What is the ratio of 2024 reports to 2020 reports?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Top_Three_Identity_Theft_Reports_by_Year.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Top_Three_Identity_Theft_Reports_by_Year.csv"
          ],
          [
            "2024_CSN_Top_Three_Identity_Theft_Reports_by_Year.csv"
          ],
          [
            "2024_CSN_Top_Three_Identity_Theft_Reports_by_Year.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column 'Theft Type' has dtype 'int64' but contains string values like 'Bank Account', 'Credit Card', 'Loan or Lease' in sample data",
          "Column 'Theft Type' has dtype int64 but contains string values like 'Credit Card' in the data",
          "Column 'Year' appears in both column name and first data position, suggesting header misalignment"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column 'Theft Type' has dtype 'int64' but contains string values like 'Bank Account', 'Credit Card', 'Loan or Lease' in sample data"
          ],
          [
            "Column 'Theft Type' has dtype int64 but contains string values like 'Credit Card' in the data",
            "Column 'Year' appears in both column name and first data position, suggesting header misalignment"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of fraud reports",
          "year": "year",
          "theft type": "categorical"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "# of Reports": "count of fraud reports"
          },
          {
            "# of Reports": "count",
            "Year": "year",
            "Theft Type": "categorical"
          },
          {
            "# of Reports": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Year column is stored as object/string instead of integer",
          "All report counts are in absolute numbers, no scaling needed"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column is stored as object/string instead of integer"
          ],
          [
            "All report counts are in absolute numbers, no scaling needed"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 3.0,
        "confidence": 1.0,
        "votes": [
          3.0,
          3.0,
          3.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be between 2020-2024",
          "# of Reports should be non-negative integers",
          "Theft Type should be one of: 'Bank Account', 'Credit Card', 'Loan or Lease'",
          "Year must be in ['2020', '2024']",
          "Theft Type must be 'Credit Card'",
          "# of Reports must be > 0",
          "Both 2020 and 2024 must have Credit Card data",
          "The 'Year' column should contain valid year values.",
          "The '# of Reports' column should contain non-negative integer values."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be between 2020-2024",
            "# of Reports should be non-negative integers",
            "Theft Type should be one of: 'Bank Account', 'Credit Card', 'Loan or Lease'"
          ],
          [
            "Year must be in ['2020', '2024']",
            "Theft Type must be 'Credit Card'",
            "# of Reports must be > 0",
            "Both 2020 and 2024 must have Credit Card data"
          ],
          [
            "The 'Year' column should contain valid year values.",
            "The '# of Reports' column should contain non-negative integer values."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to only rows where Theft Type = 'Credit Card'",
          "Filter to only rows where Year = '2020' or Year = '2024'",
          "Filter for rows where actual year column contains '2020' or '2024'",
          "Filter for rows where actual theft type column contains 'Credit Card'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to only rows where Theft Type = 'Credit Card'",
            "Filter to only rows where Year = '2020' or Year = '2024'"
          ],
          [
            "Filter for rows where actual year column contains '2020' or '2024'",
            "Filter for rows where actual theft type column contains 'Credit Card'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in Year and Theft Type columns",
          "Verify that each (Year, Theft Type) combination appears only once"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in Year and Theft Type columns",
            "Verify that each (Year, Theft Type) combination appears only once"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Ratio must be rounded to 4 decimal places",
          "Result should be a single numeric value",
          "Output must be a single numeric value",
          "Result must be rounded to exactly 4 decimal places",
          "Format: decimal number (e.g., 1.1413)",
          "The final ratio should be rounded to 4 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Ratio must be rounded to 4 decimal places",
            "Result should be a single numeric value"
          ],
          [
            "Output must be a single numeric value",
            "Result must be rounded to exactly 4 decimal places",
            "Format: decimal number (e.g., 1.1413)"
          ],
          [
            "The final ratio should be rounded to 4 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6261111111111112
  },
  "legal-hard-7": {
    "m_q": {
      "target_metric": {
        "value": "relative growth rate (percentage change) of identity theft reports between 2020 and 2024 for each theft category",
        "confidence": 0.3333333333333333,
        "votes": [
          "relative growth rate (percentage change) of identity theft reports between 2020 and 2024 for each theft category",
          "Identity theft category with fastest relative growth rate between 2020 and 2024",
          "Relative growth rate of each identity theft category between 2020 and 2024"
        ]
      },
      "filters": {
        "value": [
          "Year between 2020 and 2024 inclusive",
          "Theft Type in top three categories: Bank Account, Credit Card, Loan or Lease",
          "Year IN (2020, 2024)",
          "Years 2020 and 2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year between 2020 and 2024 inclusive",
            "Theft Type in top three categories: Bank Account, Credit Card, Loan or Lease"
          ],
          [
            "Year IN (2020, 2024)"
          ],
          [
            "Years 2020 and 2024"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Theft Type"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Theft Type"
          ],
          [
            "Theft Type"
          ],
          [
            "Theft Type"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.3333333333333333,
        "votes": [
          "table",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the number of reports for each theft type in 2020?",
          "What is the number of reports for each theft type in 2024?",
          "How to calculate relative growth: (2024_reports - 2020_reports) / 2020_reports * 100%?",
          "Which theft type has the highest percentage growth from 2020 to 2024?",
          "What are the number of reports for each theft type in 2020?",
          "What are the number of reports for each theft type in 2024?",
          "What is the relative growth rate (percentage change) for each theft type from 2020 to 2024?",
          "Which theft type has the maximum relative growth rate?",
          "What is the relative growth rate for each theft type between 2020 and 2024?"
        ],
        "confidence": 0.40740740740740744,
        "votes": [
          [
            "What is the number of reports for each theft type in 2020?",
            "What is the number of reports for each theft type in 2024?",
            "How to calculate relative growth: (2024_reports - 2020_reports) / 2020_reports * 100%?",
            "Which theft type has the highest percentage growth from 2020 to 2024?"
          ],
          [
            "What are the number of reports for each theft type in 2020?",
            "What are the number of reports for each theft type in 2024?",
            "What is the relative growth rate (percentage change) for each theft type from 2020 to 2024?",
            "Which theft type has the maximum relative growth rate?"
          ],
          [
            "What is the number of reports for each theft type in 2020?",
            "What is the number of reports for each theft type in 2024?",
            "What is the relative growth rate for each theft type between 2020 and 2024?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "2024_CSN_Top_Three_Identity_Theft_Reports_by_Year.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "2024_CSN_Top_Three_Identity_Theft_Reports_by_Year.csv"
          ],
          [
            "2024_CSN_Top_Three_Identity_Theft_Reports_by_Year.csv"
          ],
          [
            "2024_CSN_Top_Three_Identity_Theft_Reports_by_Year.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column 'Theft Type' has dtype int64 but contains string values like 'Bank Account' in sample data - likely mislabeled dtype",
          "Column 'Theft Type' has dtype int64 but contains string values like 'Bank Account', 'Credit Card', 'Loan or Lease' in the data",
          "Column 'Year' has dtype object but contains numeric year values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column 'Theft Type' has dtype int64 but contains string values like 'Bank Account' in sample data - likely mislabeled dtype"
          ],
          [
            "Column 'Theft Type' has dtype int64 but contains string values like 'Bank Account', 'Credit Card', 'Loan or Lease' in the data",
            "Column 'Year' has dtype object but contains numeric year values"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of identity theft reports",
          "year": "year",
          "theft type": "categorical"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "# of Reports": "count of identity theft reports"
          },
          {
            "# of Reports": "count",
            "Year": "year",
            "Theft Type": "categorical"
          },
          {
            "# of Reports": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Data appears to be aggregated yearly counts - no scaling issues evident"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data appears to be aggregated yearly counts - no scaling issues evident"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Only one data source provided - no cross-source conflicts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only one data source provided - no cross-source conflicts"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 3.0,
        "confidence": 1.0,
        "votes": [
          3.0,
          3.0,
          3.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be 2020-2024 inclusive",
          "Theft Type should be one of: Bank Account, Credit Card, Loan or Lease",
          "# of Reports should be non-negative integers",
          "Only years 2020 and 2024 are needed for the analysis",
          "Relative growth calculated as ((2024 value - 2020 value) / 2020 value) * 100",
          "All three theft categories must have data for both 2020 and 2024",
          "Theft Type should be one of 'Bank Account', 'Credit Card', 'Loan or Lease'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be 2020-2024 inclusive",
            "Theft Type should be one of: Bank Account, Credit Card, Loan or Lease",
            "# of Reports should be non-negative integers"
          ],
          [
            "Only years 2020 and 2024 are needed for the analysis",
            "Relative growth calculated as ((2024 value - 2020 value) / 2020 value) * 100",
            "All three theft categories must have data for both 2020 and 2024"
          ],
          [
            "Theft Type should be one of 'Bank Account', 'Credit Card', 'Loan or Lease'"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to only include years 2020 and 2024 for growth calculation",
          "Ensure each theft type has data for both 2020 and 2024",
          "Filter to baseline year 2020 and end year 2024",
          "Year is 2020 or 2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to only include years 2020 and 2024 for growth calculation",
            "Ensure each theft type has data for both 2020 and 2024"
          ],
          [
            "Filter to baseline year 2020 and end year 2024"
          ],
          [
            "Year is 2020 or 2024"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing years for any theft category",
          "Verify monotonic growth/decline patterns are reasonable"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing years for any theft category",
            "Verify monotonic growth/decline patterns are reasonable"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Table showing theft type, 2020 reports, 2024 reports, percentage growth",
          "Sort by percentage growth descending to identify fastest growing",
          "Return the single theft category name with highest relative growth",
          "Include the growth rate percentage as supporting information",
          "List of theft types sorted by relative growth rate in descending order"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Table showing theft type, 2020 reports, 2024 reports, percentage growth",
            "Sort by percentage growth descending to identify fastest growing"
          ],
          [
            "Return the single theft category name with highest relative growth",
            "Include the growth rate percentage as supporting information"
          ],
          [
            "List of theft types sorted by relative growth rate in descending order"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6148148148148149
  },
  "legal-hard-8": {
    "m_q": {
      "target_metric": {
        "value": "Consistency of '# of Reports' for 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area' in 2024",
        "confidence": 0.3333333333333333,
        "votes": [
          "Consistency of '# of Reports' for 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area' in 2024",
          "Consistency check of report counts for 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area' between two data sources for 2024",
          "Consistency of '# of Reports' for 'frauds and other data' in 2024 for the 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'."
        ]
      },
      "filters": {
        "value": [
          "Metropolitan Area = 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'",
          "Year = 2024",
          "Metropolitan Area == 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'",
          "Year == 2024",
          "Metropolitan Area is 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan Area = 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'",
            "Year = 2024"
          ],
          [
            "Metropolitan Area == 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'",
            "Year == 2024"
          ],
          [
            "Metropolitan Area is 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Metropolitan Area"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Metropolitan Area"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the '# of Reports' value in Florida.csv?",
          "What is the '# of Reports' value in 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv?",
          "Do these two values match exactly?",
          "What is the '# of Reports' value for Miami-Fort Lauderdale-West Palm Beach in Florida.csv?",
          "What is the '# of Reports' value for Miami-Fort Lauderdale-West Palm Beach in 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv?",
          "Are these two values equal?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the '# of Reports' value in Florida.csv?",
            "What is the '# of Reports' value in 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv?",
            "Do these two values match exactly?"
          ],
          [
            "What is the '# of Reports' value for Miami-Fort Lauderdale-West Palm Beach in Florida.csv?",
            "What is the '# of Reports' value for Miami-Fort Lauderdale-West Palm Beach in 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv?",
            "Are these two values equal?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Florida.csv",
          "2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Florida.csv",
            "2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv"
          ],
          [
            "Florida.csv",
            "2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv"
          ],
          [
            "Florida.csv",
            "2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Florida.csv has '# of Reports' as float64 while 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv has '# of Reports' as int64",
          "Florida.csv has only 2 columns while 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv has 4 columns",
          "Florida.csv has '# of Reports' as float64, while 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv has '# of Reports' as int64"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Florida.csv has '# of Reports' as float64 while 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv has '# of Reports' as int64",
            "Florida.csv has only 2 columns while 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv has 4 columns"
          ],
          [
            "Florida.csv has '# of Reports' as float64, while 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv has '# of Reports' as int64"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# of reports": "count of fraud and other data reports",
          "reports per 100k population": "reports per 100,000 population"
        },
        "confidence": 0.8333333333333333,
        "votes": [
          {
            "# of Reports": "count of fraud and other data reports",
            "Reports per 100K Population": "reports per 100,000 population"
          },
          {
            "# of Reports": "count",
            "Reports per 100K Population": "rate per 100,000 population"
          },
          {
            "# of Reports": "number of reports"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Florida.csv contains only Florida metropolitan areas while 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv contains national data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Florida.csv contains only Florida metropolitan areas while 2024_CSN_Metropolitan_Areas_Fraud_and_Other_Reports.csv contains national data"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Florida.csv may represent a different time period or subset of data than the 2024 file",
          "Need to verify if '# of Reports' values for the same metropolitan area are consistent across both files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Florida.csv may represent a different time period or subset of data than the 2024 file"
          ],
          [
            "Need to verify if '# of Reports' values for the same metropolitan area are consistent across both files"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.33,
        "votes": [
          10.0,
          2.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "The same metropolitan area name must appear in both files",
          "The '# of Reports' values should be comparable numeric values",
          "Both datasets should refer to the same time period (2024)",
          "Only consider 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'",
          "Both files should represent 2024 data (2024_CSN file is explicit, Florida.csv is implied)",
          "Report counts should be non-negative integers",
          "The '# of Reports' value for 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area' must be the same in both files."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "The same metropolitan area name must appear in both files",
            "The '# of Reports' values should be comparable numeric values",
            "Both datasets should refer to the same time period (2024)"
          ],
          [
            "Only consider 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'",
            "Both files should represent 2024 data (2024_CSN file is explicit, Florida.csv is implied)",
            "Report counts should be non-negative integers"
          ],
          [
            "The '# of Reports' value for 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area' must be the same in both files."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter both datasets to only include 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'",
          "Filter both datasets to the specific metropolitan area",
          "Extract '# of Reports' from both sources"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter both datasets to only include 'Miami-Fort Lauderdale-West Palm Beach, FL Metropolitan Statistical Area'"
          ],
          [
            "Filter both datasets to the specific metropolitan area",
            "Extract '# of Reports' from both sources"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Exact equality test between the two '# of Reports' values",
          "Exact equality comparison between the two '# of Reports' values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exact equality test between the two '# of Reports' values"
          ],
          [
            "Exact equality comparison between the two '# of Reports' values"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Boolean True/False output",
          "No explanation needed",
          "Return True if values are equal, False otherwise",
          "Boolean output only"
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "Boolean True/False output",
            "No explanation needed"
          ],
          [
            "Return True if values are equal, False otherwise",
            "No explanation needed",
            "Boolean output only"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5956666666666667
  },
  "wildfire-easy-1": {
    "m_q": {
      "target_metric": {
        "value": "total acres burned in a 3-month period",
        "confidence": 0.3333333333333333,
        "votes": [
          "total acres burned in a 3-month period",
          "Maximum total acres burned in any 3-month consecutive period since January 2000",
          "Total acres burned in the 3-month period with the highest total acres burned since January 2000"
        ]
      },
      "filters": {
        "value": [
          "Date >= 200001 (Jan 2000 or later)",
          "Date >= 200001 (January 2000)",
          "Date >= 200001"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date >= 200001 (Jan 2000 or later)"
          ],
          [
            "Date >= 200001 (January 2000)"
          ],
          [
            "Date >= 200001"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "3-month period (rolling window)",
          "3-month rolling window periods"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "3-month period (rolling window)"
          ],
          [
            "3-month rolling window periods"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Identify all possible 3-month rolling windows since Jan 2000",
          "Calculate sum of 'Acres Burned' for each 3-month window",
          "Find the maximum sum value",
          "Identify the date range of that 3-month period",
          "What is the date range format in the Date column?",
          "How to parse Date column (format: YYYYMM)?",
          "How to create rolling 3-month windows from monthly data?",
          "What is the sum of Acres Burned for each 3-month window?",
          "Which 3-month period has the maximum total Acres Burned?",
          "What is the total Acres Burned value for that period?",
          "Calculate the total acres burned for each 3-month period since January 2000",
          "Identify the 3-month period with the maximum total acres burned",
          "What is the total acres burned in that period?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify all possible 3-month rolling windows since Jan 2000",
            "Calculate sum of 'Acres Burned' for each 3-month window",
            "Find the maximum sum value",
            "Identify the date range of that 3-month period"
          ],
          [
            "What is the date range format in the Date column?",
            "How to parse Date column (format: YYYYMM)?",
            "How to create rolling 3-month windows from monthly data?",
            "What is the sum of Acres Burned for each 3-month window?",
            "Which 3-month period has the maximum total Acres Burned?",
            "What is the total Acres Burned value for that period?"
          ],
          [
            "Calculate the total acres burned for each 3-month period since January 2000",
            "Identify the 3-month period with the maximum total acres burned",
            "What is the total acres burned in that period?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires_monthly_stats.csv",
          "noaa_wildfires_monthly_stats__section_1"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "noaa_wildfires_monthly_stats.csv"
          ],
          [
            "noaa_wildfires_monthly_stats.csv"
          ],
          [
            "noaa_wildfires_monthly_stats.csv",
            "noaa_wildfires_monthly_stats__section_1"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Second file 'noaa_wildfires_monthly_stats__section_1' appears to be metadata or documentation with different structure and no joinable columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Second file 'noaa_wildfires_monthly_stats__section_1' appears to be metadata or documentation with different structure and no joinable columns"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "acres burned": "acres",
          "number of fires": "count",
          "acres burned per fire": "acres per fire",
          "date": "YYYYMM integer format"
        },
        "confidence": 0.8333333333333334,
        "votes": [
          {
            "Acres Burned": "acres",
            "Number of Fires": "count",
            "Acres Burned per Fire": "acres per fire"
          },
          {
            "Acres Burned": "acres",
            "Number of Fires": "count",
            "Acres Burned per Fire": "acres per fire",
            "Date": "YYYYMM integer format"
          },
          {
            "Acres Burned": "acres",
            "Number of Fires": "count",
            "Acres Burned per Fire": "acres/fire"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Date column uses YYYYMM integer format, not datetime",
          "Acres Burned values span multiple orders of magnitude (40K to 2.5M in sample)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date column uses YYYYMM integer format, not datetime",
            "Acres Burned values span multiple orders of magnitude (40K to 2.5M in sample)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Second file contains only a title and missing value indicator, cannot be integrated with main data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Second file contains only a title and missing value indicator, cannot be integrated with main data"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "-999",
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "-999"
          ],
          [
            "-999"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Data starts at 200001 (Jan 2000)",
          "Monthly granularity requires rolling window calculation",
          "3-month periods must be consecutive months (e.g., Jan-Mar, Feb-Apr)",
          "Only include data from January 2000 onwards (Date >= 200001)",
          "3-month periods must be consecutive months",
          "Each 3-month window should roll forward by 1 month",
          "Date must be properly sorted chronologically",
          "Date column represents year and month (YYYYMM)",
          "Need to convert 'Date' column to a datetime object for time-based calculations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data starts at 200001 (Jan 2000)",
            "Monthly granularity requires rolling window calculation",
            "3-month periods must be consecutive months (e.g., Jan-Mar, Feb-Apr)"
          ],
          [
            "Only include data from January 2000 onwards (Date >= 200001)",
            "3-month periods must be consecutive months",
            "Each 3-month window should roll forward by 1 month",
            "Date must be properly sorted chronologically"
          ],
          [
            "Date column represents year and month (YYYYMM)",
            "Need to convert 'Date' column to a datetime object for time-based calculations"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude any rows with Date < 200001",
          "Handle edge cases for partial data at end of time series",
          "Filter out any records with Date < 200001",
          "Exclude any rows where Acres Burned = -999 (sentinel value for missing data)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude any rows with Date < 200001",
            "Handle edge cases for partial data at end of time series"
          ],
          [
            "Filter out any records with Date < 200001",
            "Exclude any rows where Acres Burned = -999 (sentinel value for missing data)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in 'Acres Burned' column",
          "Verify date sequence is complete and consecutive"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in 'Acres Burned' column",
            "Verify date sequence is complete and consecutive"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Report both the 3-month period (start and end dates) and the total acres burned",
          "Report the starting month-year of the 3-month period with highest acres burned",
          "Report the total acres burned as an integer or float value",
          "Clearly identify which 3 consecutive months comprise the period",
          "The output should be a single number representing the total acres burned in the 3-month period with the highest total acres burned."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Report both the 3-month period (start and end dates) and the total acres burned"
          ],
          [
            "Report the starting month-year of the 3-month period with highest acres burned",
            "Report the total acres burned as an integer or float value",
            "Clearly identify which 3 consecutive months comprise the period"
          ],
          [
            "The output should be a single number representing the total acres burned in the 3-month period with the highest total acres burned."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5791666666666668
  },
  "wildfire-easy-13": {
    "m_q": {
      "target_metric": {
        "value": "Maximum value of Total Helicopter Requests among NICC-defined geographic areas",
        "confidence": 0.3333333333333333,
        "votes": [
          "Maximum value of Total Helicopter Requests among NICC-defined geographic areas",
          "Geographic area with maximum helicopter requests among the 10 NICC-defined areas",
          "Total helicopter requests"
        ]
      },
      "filters": {
        "value": [
          "Exclude 'National Interagency Coordination Center' (administrative entity)",
          "Exclude 'Other' (non-geographic category)",
          "Exclude 'Canada' (non-US area)",
          "Exclude non-NICC areas: 'National Interagency Coordination Center', 'Other', 'Canada'",
          "Region is one of the 10 NICC-defined geographic areas"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude 'National Interagency Coordination Center' (administrative entity)",
            "Exclude 'Other' (non-geographic category)",
            "Exclude 'Canada' (non-US area)"
          ],
          [
            "Exclude non-NICC areas: 'National Interagency Coordination Center', 'Other', 'Canada'"
          ],
          [
            "Region is one of the 10 NICC-defined geographic areas"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Region"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Region"
          ],
          [
            "Region"
          ],
          [
            "Region"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Which specific region had the highest request count?",
          "What was the numerical value of the maximum requests?",
          "How many regions are considered in the analysis after filtering?",
          "What are the 10 NICC-defined geographic areas?",
          "Which region has the maximum value in 'Total Helicopter Requests'?",
          "Should 'National Interagency Coordination Center', 'Other', and 'Canada' be excluded from the comparison?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which specific region had the highest request count?",
            "What was the numerical value of the maximum requests?",
            "How many regions are considered in the analysis after filtering?"
          ],
          [
            "What are the 10 NICC-defined geographic areas?",
            "Which region has the maximum value in 'Total Helicopter Requests'?",
            "Should 'National Interagency Coordination Center', 'Other', and 'Canada' be excluded from the comparison?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "cleaned_helicopter_requests_by_region.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "cleaned_helicopter_requests_by_region.csv"
          ],
          [
            "cleaned_helicopter_requests_by_region.csv"
          ],
          [
            "cleaned_helicopter_requests_by_region.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "total helicopter requests": "count of helicopter requests"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Total Helicopter Requests": "count of helicopter requests"
          },
          {
            "Total Helicopter Requests": "count"
          },
          {
            "Total Helicopter Requests": "number of helicopters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Data appears aggregated at regional level, no temporal granularity"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data appears aggregated at regional level, no temporal granularity"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total Helicopter Requests must be non-negative integers",
          "Region names should be unique",
          "After filtering non-geographic areas, exactly 10 regions should remain",
          "Only consider the 10 NICC-defined geographic areas",
          "Exclude 'National Interagency Coordination Center', 'Other', and 'Canada' from analysis",
          "Region must be one of the 10 NICC-defined geographic areas"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Total Helicopter Requests must be non-negative integers",
            "Region names should be unique",
            "After filtering non-geographic areas, exactly 10 regions should remain"
          ],
          [
            "Only consider the 10 NICC-defined geographic areas",
            "Exclude 'National Interagency Coordination Center', 'Other', and 'Canada' from analysis",
            "Total Helicopter Requests must be non-negative integers"
          ],
          [
            "Region must be one of the 10 NICC-defined geographic areas"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Region != 'National Interagency Coordination Center'",
          "Region != 'Other'",
          "Region != 'Canada'",
          "Region NOT IN ('National Interagency Coordination Center', 'Other', 'Canada')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Region != 'National Interagency Coordination Center'",
            "Region != 'Other'",
            "Region != 'Canada'"
          ],
          [
            "Region NOT IN ('National Interagency Coordination Center', 'Other', 'Canada')"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify exactly 10 regions remain after filtering",
          "Check for duplicate region names",
          "Validate integer data type for Total Helicopter Requests"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify exactly 10 regions remain after filtering",
            "Check for duplicate region names",
            "Validate integer data type for Total Helicopter Requests"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output should specify region name and request count",
          "Clarify that analysis excludes non-geographic entries",
          "Return the single region name with the highest helicopter request count",
          "Include the corresponding request count value",
          "Sort the output table by 'Total Helicopter Requests' in descending order",
          "Display only the region with the highest 'Total Helicopter Requests'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Output should specify region name and request count",
            "Clarify that analysis excludes non-geographic entries"
          ],
          [
            "Return the single region name with the highest helicopter request count",
            "Include the corresponding request count value"
          ],
          [
            "Sort the output table by 'Total Helicopter Requests' in descending order",
            "Display only the region with the highest 'Total Helicopter Requests'"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6083333333333334
  },
  "wildfire-easy-15": {
    "m_q": {
      "target_metric": {
        "value": "correlation between fire size and median wind speed in July",
        "confidence": 0.6666666666666666,
        "votes": [
          "correlation between fire size and median wind speed in July",
          "Correlation coefficient between fire size and median wind speed for fires occurring in July",
          "Correlation between fire size and median wind speed in July"
        ]
      },
      "filters": {
        "value": [
          "start_date month = July",
          "valid wind_med values",
          "valid fire size values",
          "Filter to fires occurring in July (month extracted from start_date)",
          "Exclude records with missing or null values for fire size and wind_med",
          "start_date is in July (month = 7)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "start_date month = July",
            "valid wind_med values",
            "valid fire size values"
          ],
          [
            "Filter to fires occurring in July (month extracted from start_date)",
            "Exclude records with missing or null values for fire size and wind_med"
          ],
          [
            "start_date is in July (month = 7)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What column represents fire size?",
          "How to extract July data from start_date?",
          "What correlation measure to use (Pearson/Spearman)?",
          "What threshold defines 'strong correlation'?",
          "What column represents fire size in the dataset?",
          "How to extract month from start_date to filter for July fires?",
          "What correlation threshold constitutes 'strong correlation'?",
          "Should the answer be based on Pearson or Spearman correlation?",
          "What p-value threshold should be used to determine statistical significance?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What column represents fire size?",
            "How to extract July data from start_date?",
            "What correlation measure to use (Pearson/Spearman)?",
            "What threshold defines 'strong correlation'?"
          ],
          [
            "What column represents fire size in the dataset?",
            "How to extract month from start_date to filter for July fires?",
            "What correlation threshold constitutes 'strong correlation'?",
            "Should the answer be based on Pearson or Spearman correlation?",
            "What p-value threshold should be used to determine statistical significance?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires.csv",
          "noaa_wildfires_variabledescrip.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Fire size column not clearly identified in sample data - need to check variable descriptions for size metric",
          "Fire size column is not explicitly identified in the provided column list",
          "Need to determine which column represents fire size (likely 'hec' for hectares or related to incident scale)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Fire size column not clearly identified in sample data - need to check variable descriptions for size metric"
          ],
          [
            "Fire size column is not explicitly identified in the provided column list",
            "Need to determine which column represents fire size (likely 'hec' for hectares or related to incident scale)"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "wind_med": "miles per hour",
          "avrh_mean": "percentage",
          "rain_sum": "precipitation units",
          "duration": "days",
          "latitude": "degrees",
          "longitude": "degrees",
          "hec": "hectares (assumed fire size)",
          "start_date": "MM/DD/YYYY format",
          "erc_med": "BTUs per square foot"
        },
        "confidence": 0.5185185185185185,
        "votes": [
          {
            "wind_med": "miles per hour",
            "avrh_mean": "percentage",
            "rain_sum": "precipitation units",
            "duration": "days",
            "latitude": "degrees",
            "longitude": "degrees"
          },
          {
            "wind_med": "miles per hour",
            "hec": "hectares (assumed fire size)",
            "start_date": "MM/DD/YYYY format",
            "avrh_mean": "percentage",
            "erc_med": "BTUs per square foot",
            "rain_sum": "precipitation amount (units not specified)"
          },
          {
            "wind_med": "miles per hour",
            "duration": "days"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "wind_med appears as integer but may need decimal precision",
          "erc_med has decimal values while other weather metrics are integers",
          "Fire size unit needs confirmation (hec likely represents hectares)",
          "Wind speed in miles per hour may need standardization for correlation analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "wind_med appears as integer but may need decimal precision",
            "erc_med has decimal values while other weather metrics are integers"
          ],
          [
            "Fire size unit needs confirmation (hec likely represents hectares)",
            "Wind speed in miles per hour may need standardization for correlation analysis"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Variable descriptions file may clarify which column represents fire size - possibly prim_threatened_aggregate or other threatened columns",
          "Variable description file does not provide explicit mapping for fire size variable"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Variable descriptions file may clarify which column represents fire size - possibly prim_threatened_aggregate or other threatened columns"
          ],
          [
            "Variable description file does not provide explicit mapping for fire size variable"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "wind_med >= 0",
          "duration >= 1",
          "start_year consistent with start_date year",
          "July defined as month 7 in start_date",
          "Only include fires that started in July (month == 7)",
          "Fire size must be non-null and non-zero",
          "wind_med must be non-null",
          "Correlation result must be classified as strong or not strong",
          "start_date must be parsed to extract the month",
          "Fire size needs to be determined.  Duration could be a proxy, or total_fire_region/total_fire_west could be used if they represent fire size.  Otherwise, this question cannot be answered.",
          "wind_med represents the median wind speed."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "wind_med >= 0",
            "duration >= 1",
            "start_year consistent with start_date year",
            "July defined as month 7 in start_date"
          ],
          [
            "Only include fires that started in July (month == 7)",
            "Fire size must be non-null and non-zero",
            "wind_med must be non-null",
            "Correlation result must be classified as strong or not strong"
          ],
          [
            "start_date must be parsed to extract the month",
            "Fire size needs to be determined.  Duration could be a proxy, or total_fire_region/total_fire_west could be used if they represent fire size.  Otherwise, this question cannot be answered.",
            "wind_med represents the median wind speed."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where start_date month = 7",
          "Exclude rows with missing wind_med or fire size values",
          "Consider only rows with valid numeric values for both variables",
          "Extract month from start_date column",
          "Filter where month == 7 (July)",
          "Remove records where hec (fire size) is null or zero",
          "Remove records where wind_med is null",
          "start_month = month(start_date)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where start_date month = 7",
            "Exclude rows with missing wind_med or fire size values",
            "Consider only rows with valid numeric values for both variables"
          ],
          [
            "Extract month from start_date column",
            "Filter where month == 7 (July)",
            "Remove records where hec (fire size) is null or zero",
            "Remove records where wind_med is null"
          ],
          [
            "start_month = month(start_date)"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Pearson correlation coefficient",
          "Significance test for correlation",
          "Check for normality assumptions",
          "Calculate Pearson correlation coefficient between fire size (hec) and wind_med",
          "Calculate p-value for correlation significance",
          "Apply threshold for 'strong correlation' (typically |r| > 0.7 or |r| > 0.5 depending on domain standards)",
          "Test for statistical significance (p < 0.05)",
          "Pearson correlation between fire size and wind_med for July fires"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pearson correlation coefficient",
            "Significance test for correlation",
            "Check for normality assumptions"
          ],
          [
            "Calculate Pearson correlation coefficient between fire size (hec) and wind_med",
            "Calculate p-value for correlation significance",
            "Apply threshold for 'strong correlation' (typically |r| > 0.7 or |r| > 0.5 depending on domain standards)",
            "Test for statistical significance (p < 0.05)"
          ],
          [
            "Pearson correlation between fire size and wind_med for July fires"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Binary Yes/No answer based on correlation strength threshold",
          "May need to define 'strong correlation' threshold (e.g., |r| > 0.7)",
          "Final answer must be 'Yes' or 'No'",
          "Yes if correlation is strong (absolute value above threshold and statistically significant)",
          "No if correlation is weak or not statistically significant",
          "Output should be Yes if the absolute value of the correlation coefficient is greater than 0.7, otherwise No."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Binary Yes/No answer based on correlation strength threshold",
            "May need to define 'strong correlation' threshold (e.g., |r| > 0.7)"
          ],
          [
            "Final answer must be 'Yes' or 'No'",
            "Yes if correlation is strong (absolute value above threshold and statistically significant)",
            "No if correlation is weak or not statistically significant"
          ],
          [
            "Output should be Yes if the absolute value of the correlation coefficient is greater than 0.7, otherwise No."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6092592592592594
  },
  "wildfire-easy-2": {
    "m_q": {
      "target_metric": {
        "value": "Count of unique US states intersected by each NIFC geographic area",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of unique US states intersected by each NIFC geographic area",
          "The NIFC geographic area abbreviation that intersects with the most US states",
          "The NIFC geographic area (GACCAbbreviation) that intersects with the most US states"
        ]
      },
      "filters": {
        "value": [
          "Only consider intersections where NIFC area geometry overlaps with US state geometry",
          "Exclude any NIFC areas with null GACCAbbreviation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only consider intersections where NIFC area geometry overlaps with US state geometry",
            "Exclude any NIFC areas with null GACCAbbreviation"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "GACCAbbreviation",
          "GACCName"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "GACCAbbreviation",
            "GACCName"
          ],
          [
            "GACCAbbreviation"
          ],
          [
            "GACCAbbreviation"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How many states does each NIFC area intersect?",
          "Which NIFC area has the maximum intersection count?",
          "What is the abbreviation of that NIFC area?",
          "What are the NIFC geographic areas from nifc_geographic_areas.gpkg?",
          "What are the US states from usa.gpkg?",
          "Which NIFC geographic areas spatially intersect with which US states?",
          "Count the number of unique states each NIFC geographic area intersects with",
          "Which NIFC geographic area has the maximum count of state intersections?",
          "What is the abbreviation (GACCAbbreviation) of that geographic area?",
          "Determine the intersection between each NIFC geographic area and each US state.",
          "Count the number of US states each NIFC geographic area intersects with.",
          "Find the maximum number of intersecting US states.",
          "Identify the NIFC geographic area(s) with the maximum number of intersecting US states."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How many states does each NIFC area intersect?",
            "Which NIFC area has the maximum intersection count?",
            "What is the abbreviation of that NIFC area?"
          ],
          [
            "What are the NIFC geographic areas from nifc_geographic_areas.gpkg?",
            "What are the US states from usa.gpkg?",
            "Which NIFC geographic areas spatially intersect with which US states?",
            "Count the number of unique states each NIFC geographic area intersects with",
            "Which NIFC geographic area has the maximum count of state intersections?",
            "What is the abbreviation (GACCAbbreviation) of that geographic area?"
          ],
          [
            "Determine the intersection between each NIFC geographic area and each US state.",
            "Count the number of US states each NIFC geographic area intersects with.",
            "Find the maximum number of intersecting US states.",
            "Identify the NIFC geographic area(s) with the maximum number of intersecting US states."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "usa.gpkg",
          "nifc_geographic_areas.gpkg"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "usa.gpkg",
            "nifc_geographic_areas.gpkg"
          ],
          [
            "usa.gpkg",
            "nifc_geographic_areas.gpkg"
          ],
          [
            "usa.gpkg",
            "nifc_geographic_areas.gpkg"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different geometry column naming conventions",
          "Different coordinate reference systems may need alignment",
          "Date formats differ (datetime64[ms] vs datetime64[ms, UTC])"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different geometry column naming conventions",
            "Different coordinate reference systems may need alignment",
            "Date formats differ (datetime64[ms] vs datetime64[ms, UTC])"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "geometry": "geographic coordinates (degrees)",
          "src_date": "date",
          "datecurrent": "datetime with UTC timezone",
          "adm1_name": "US state name",
          "gaccabbreviation": "NIFC Geographic Area Coordination Center abbreviation"
        },
        "confidence": 0.39999999999999997,
        "votes": [
          {
            "geometry": "geographic coordinates (degrees)",
            "src_date": "date",
            "DateCurrent": "datetime with UTC timezone"
          },
          {
            "geometry": "geographic coordinates (likely WGS84 or similar CRS)",
            "adm1_name": "US state name",
            "GACCAbbreviation": "NIFC Geographic Area Coordination Center abbreviation"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "usa.gpkg contains county-level data (adm2), need to aggregate to state level (adm1) for analysis",
          "NIFC areas may cover multiple states, requiring spatial intersection analysis",
          "Geometry columns may be in different coordinate reference systems requiring projection alignment"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "usa.gpkg contains county-level data (adm2), need to aggregate to state level (adm1) for analysis",
            "NIFC areas may cover multiple states, requiring spatial intersection analysis"
          ],
          [
            "Geometry columns may be in different coordinate reference systems requiring projection alignment"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "usa.gpkg uses administrative boundaries while nifc_geographic_areas.gpkg uses operational boundaries",
          "Different spatial extents - NIFC areas may include non-US territories",
          "Potential CRS mismatch between usa.gpkg and nifc_geographic_areas.gpkg geometry columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "usa.gpkg uses administrative boundaries while nifc_geographic_areas.gpkg uses operational boundaries",
            "Different spatial extents - NIFC areas may include non-US territories"
          ],
          [
            "Potential CRS mismatch between usa.gpkg and nifc_geographic_areas.gpkg geometry columns"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 0.6666666666666666,
        "votes": [
          ",",
          null,
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "",
            "NA",
            "N/A",
            null
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 26.0,
        "confidence": 1.0,
        "votes": [
          26.0,
          26.0,
          26.0
        ]
      },
      "file_format": {
        "value": "GeoPackage",
        "confidence": 0.6666666666666666,
        "votes": [
          "GeoPackage",
          "geopackage",
          "gpkg"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each US state should be counted only once per NIFC area intersection",
          "Geometry must be valid for spatial operations",
          "GACCAbbreviation must not be null for result",
          "Focus on state-level (adm1) boundaries from usa.gpkg, not county-level (adm2)",
          "Each state should be counted only once per NIFC geographic area intersection",
          "Spatial intersection must be meaningful (not just touching boundaries)",
          "Result must be a single abbreviation value from GACCAbbreviation column",
          "The 'geometry' columns in both files must be valid geometries.",
          "The spatial reference systems of the 'geometry' columns must be compatible for intersection calculations."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each US state should be counted only once per NIFC area intersection",
            "Geometry must be valid for spatial operations",
            "GACCAbbreviation must not be null for result"
          ],
          [
            "Focus on state-level (adm1) boundaries from usa.gpkg, not county-level (adm2)",
            "Each state should be counted only once per NIFC geographic area intersection",
            "Spatial intersection must be meaningful (not just touching boundaries)",
            "Result must be a single abbreviation value from GACCAbbreviation column"
          ],
          [
            "The 'geometry' columns in both files must be valid geometries.",
            "The spatial reference systems of the 'geometry' columns must be compatible for intersection calculations."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter usa.gpkg to unique adm1_name values (states)",
          "Remove any NIFC areas with null or empty GACCAbbreviation",
          "Filter usa.gpkg to distinct state geometries using adm1_id or adm1_name",
          "Ensure adm0_name = 'United States' to exclude territories if needed"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter usa.gpkg to unique adm1_name values (states)",
            "Remove any NIFC areas with null or empty GACCAbbreviation"
          ],
          [
            "Filter usa.gpkg to distinct state geometries using adm1_id or adm1_name",
            "Ensure adm0_name = 'United States' to exclude territories if needed"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct states per NIFC area",
          "Verify spatial intersection results are non-empty"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct states per NIFC area",
            "Verify spatial intersection results are non-empty"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single abbreviation string as final answer",
          "Should validate that the maximum count is unique",
          "Return only the GACCAbbreviation value as the answer",
          "The answer should be a string abbreviation (e.g., 'AICC', 'NWCC', etc.)",
          "The output should be the GACCAbbreviation of the NIFC geographic area that intersects with the most US states."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single abbreviation string as final answer",
            "Should validate that the maximum count is unique"
          ],
          [
            "Return only the GACCAbbreviation value as the answer",
            "The answer should be a string abbreviation (e.g., 'AICC', 'NWCC', etc.)"
          ],
          [
            "The output should be the GACCAbbreviation of the NIFC geographic area that intersects with the most US states."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5783333333333334
  },
  "wildfire-easy-3": {
    "m_q": {
      "target_metric": {
        "value": "Count of NIFC Geographic Areas each US state intersects with",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of NIFC Geographic Areas each US state intersects with",
          "Count of NIFC Geographic Areas that each US state falls into",
          "Count of NIFC Geographic Areas for each US state"
        ]
      },
      "filters": {
        "value": [
          "adm0_name = 'United States'",
          "adm1_name is not null",
          "GACCName is not null",
          "US states only (full names)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "adm0_name = 'United States'",
            "adm1_name is not null",
            "GACCName is not null"
          ],
          [
            "US states only (full names)"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "adm1_name",
          "US state (adm1_name)"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "adm1_name"
          ],
          [
            "US state (adm1_name)"
          ],
          [
            "adm1_name"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "list",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Which US state has the highest count of intersecting NIFC Geographic Areas?",
          "What is the distribution of intersection counts across states?",
          "Are there states that don't intersect any NIFC Geographic Areas?",
          "What are the full names of US states?",
          "Which NIFC Geographic Areas does each state intersect with?",
          "What is the count of geographic areas per state?",
          "Which state(s) have the maximum count?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which US state has the highest count of intersecting NIFC Geographic Areas?",
            "What is the distribution of intersection counts across states?",
            "Are there states that don't intersect any NIFC Geographic Areas?"
          ],
          [
            "What are the full names of US states?",
            "Which NIFC Geographic Areas does each state intersect with?",
            "What is the count of geographic areas per state?",
            "Which state(s) have the maximum count?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "usa.gpkg",
          "nifc_geographic_areas.gpkg"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "usa.gpkg",
            "nifc_geographic_areas.gpkg"
          ],
          [
            "usa.gpkg",
            "nifc_geographic_areas.gpkg"
          ],
          [
            "usa.gpkg",
            "nifc_geographic_areas.gpkg"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Different geometry column names: 'geometry' in both files but representing different spatial features",
          "Different administrative level representations: usa.gpkg has hierarchical admin columns while nifc_geographic_areas.gpkg has geographic area metadata",
          "Different geometry representations between files",
          "No common non-spatial keys for joining"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different geometry column names: 'geometry' in both files but representing different spatial features",
            "Different administrative level representations: usa.gpkg has hierarchical admin columns while nifc_geographic_areas.gpkg has geographic area metadata"
          ],
          [
            "Different geometry representations between files",
            "No common non-spatial keys for joining"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "adm2_id": "string identifier",
          "adm1_id": "string identifier",
          "adm0_id": "string identifier",
          "gaccunitid": "string identifier",
          "contactphone": "phone number",
          "datecurrent": "timestamp",
          "src_date": "date",
          "src_update": "date",
          "geometry": "spatial coordinates (likely WGS84)",
          "adm1_name": "state full name text",
          "gaccname": "NIFC geographic area name text"
        },
        "confidence": 0.33333333333333337,
        "votes": [
          {
            "adm2_id": "string identifier",
            "adm1_id": "string identifier",
            "adm0_id": "string identifier",
            "GACCUnitID": "string identifier",
            "ContactPhone": "phone number",
            "DateCurrent": "timestamp",
            "src_date": "date",
            "src_update": "date"
          },
          {
            "geometry": "spatial coordinates (likely WGS84)",
            "adm1_name": "state full name text",
            "GACCName": "NIFC geographic area name text"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "usa.gpkg contains county-level geometries (adm2) while question requires state-level aggregation",
          "nifc_geographic_areas.gpkg contains 120 geographic areas that may cross state boundaries",
          "Need spatial intersection at state level rather than county level",
          "Geometry data may be in different coordinate reference systems",
          "Administrative boundaries (adm1) represent states",
          "NIFC geographic areas may overlap state boundaries"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "usa.gpkg contains county-level geometries (adm2) while question requires state-level aggregation",
            "nifc_geographic_areas.gpkg contains 120 geographic areas that may cross state boundaries",
            "Need spatial intersection at state level rather than county level"
          ],
          [
            "Geometry data may be in different coordinate reference systems",
            "Administrative boundaries (adm1) represent states",
            "NIFC geographic areas may overlap state boundaries"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Spatial reference systems may differ between the two GeoPackage files",
          "usa.gpkg uses 2018-01-01 source date while nifc_geographic_areas.gpkg uses 2022-01-03 date",
          "Different geographic coverage: usa.gpkg covers all US states while nifc areas may not cover all states",
          "Coordinate reference system alignment needed for spatial join",
          "State boundaries may not align perfectly with NIFC geographic area boundaries"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Spatial reference systems may differ between the two GeoPackage files",
            "usa.gpkg uses 2018-01-01 source date while nifc_geographic_areas.gpkg uses 2022-01-03 date",
            "Different geographic coverage: usa.gpkg covers all US states while nifc areas may not cover all states"
          ],
          [
            "Coordinate reference system alignment needed for spatial join",
            "State boundaries may not align perfectly with NIFC geographic area boundaries"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "None"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "",
            "NA",
            "N/A",
            "None"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 26.0,
        "confidence": 1.0,
        "votes": [
          26.0,
          26.0,
          26.0
        ]
      },
      "file_format": {
        "value": "gpkg",
        "confidence": 0.6666666666666666,
        "votes": [
          "geopackage",
          "gpkg",
          "gpkg"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each state must be represented by its full name from adm1_name column",
          "Spatial intersection must be calculated between state geometries and NIFC area geometries",
          "Counts must be based on distinct NIFC Geographic Areas per state",
          "Results should be sorted by count descending",
          "Only include adm1 (state) level records from usa.gpkg",
          "adm0_name should be 'United States'",
          "Count distinct NIFC Geographic Areas per state",
          "Output must use full state names from adm1_name column",
          "Need to perform a spatial join between the two datasets to determine which states fall within which NIFC Geographic Areas."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each state must be represented by its full name from adm1_name column",
            "Spatial intersection must be calculated between state geometries and NIFC area geometries",
            "Counts must be based on distinct NIFC Geographic Areas per state",
            "Results should be sorted by count descending"
          ],
          [
            "Only include adm1 (state) level records from usa.gpkg",
            "adm0_name should be 'United States'",
            "Count distinct NIFC Geographic Areas per state",
            "Output must use full state names from adm1_name column"
          ],
          [
            "Need to perform a spatial join between the two datasets to determine which states fall within which NIFC Geographic Areas."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter usa.gpkg to unique state geometries by aggregating county geometries",
          "Exclude territories if adm1_name represents non-state entities",
          "Consider only active NIFC areas (DateCurrent indicates recent data)",
          "Filter usa.gpkg to state-level administrative units (adm1)",
          "Ensure records represent US states, not territories or other subdivisions",
          "Remove duplicate state-area intersections if any"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter usa.gpkg to unique state geometries by aggregating county geometries",
            "Exclude territories if adm1_name represents non-state entities",
            "Consider only active NIFC areas (DateCurrent indicates recent data)"
          ],
          [
            "Filter usa.gpkg to state-level administrative units (adm1)",
            "Ensure records represent US states, not territories or other subdivisions",
            "Remove duplicate state-area intersections if any"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing state geometries",
          "Verify spatial coverage of NIFC areas across all states",
          "Validate that each state intersects with at least one NIFC area",
          "Maximum count of geographic areas per state",
          "Distribution of geographic area counts across states"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing state geometries",
            "Verify spatial coverage of NIFC areas across all states",
            "Validate that each state intersects with at least one NIFC area"
          ],
          [
            "Maximum count of geographic areas per state",
            "Distribution of geographic area counts across states"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output should list states with their intersection counts",
          "State names should be full names from adm1_name",
          "Results should be sorted by count descending",
          "Include tie handling for states with equal counts",
          "Return full state names (adm1_name), not abbreviations",
          "Include count of NIFC Geographic Areas per state",
          "Identify state(s) with maximum count",
          "Results should list states falling into the most geographic areas",
          "Output should be a table with US state full names and the count of NIFC Geographic Areas they fall into, sorted by the count in descending order."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Output should list states with their intersection counts",
            "State names should be full names from adm1_name",
            "Results should be sorted by count descending",
            "Include tie handling for states with equal counts"
          ],
          [
            "Return full state names (adm1_name), not abbreviations",
            "Include count of NIFC Geographic Areas per state",
            "Identify state(s) with maximum count",
            "Results should list states falling into the most geographic areas"
          ],
          [
            "Output should be a table with US state full names and the count of NIFC Geographic Areas they fall into, sorted by the count in descending order."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5616666666666668
  },
  "wildfire-easy-8": {
    "m_q": {
      "target_metric": {
        "value": "Compare total count of fires impacting above 100 acres caused by lightning versus humans",
        "confidence": 0.3333333333333333,
        "votes": [
          "Compare total count of fires impacting above 100 acres caused by lightning versus humans",
          "Count of fires by cause category for fires impacting above 100 acres",
          "Count of fires caused by 'Lightning' vs. 'Human' where 'gt_100' is 1, and return the cause with the higher count."
        ]
      },
      "filters": {
        "value": [
          "gt_100 == 1",
          "cause in ['L', 'H']",
          "cause in ['Lightning', 'Human']"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "gt_100 == 1",
            "cause in ['L', 'H']"
          ],
          [
            "gt_100 == 1"
          ],
          [
            "gt_100 == 1",
            "cause in ['Lightning', 'Human']"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "cause"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "cause"
          ],
          [
            "cause"
          ],
          [
            "cause"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the count of fires with gt_100=1 where cause='L'?",
          "What is the count of fires with gt_100=1 where cause='H'?",
          "Which count is larger?",
          "How many fires with gt_100 == 1 were caused by lightning?",
          "How many fires with gt_100 == 1 were caused by humans?",
          "Which cause category (lightning or human) has the higher count?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the count of fires with gt_100=1 where cause='L'?",
            "What is the count of fires with gt_100=1 where cause='H'?",
            "Which count is larger?"
          ],
          [
            "How many fires with gt_100 == 1 were caused by lightning?",
            "How many fires with gt_100 == 1 were caused by humans?",
            "Which cause category (lightning or human) has the higher count?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Fire_Weather_Data_2002-2014_2016.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ],
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ],
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "avrh_mean": "percentage?",
          "wind_med": "speed units?",
          "erc_med": "energy release component",
          "rain_sum": "precipitation units?",
          "duration": "days",
          "latitude": "degrees",
          "longitude": "degrees",
          "start_day_of_year": "day number",
          "control_day_of_year": "day number",
          "gt_100": "binary indicator (1=fire impacted above 100 acres, 0=otherwise)",
          "cause": "categorical fire cause code"
        },
        "confidence": 0.33333333333333337,
        "votes": [
          {
            "avrh_mean": "percentage?",
            "wind_med": "speed units?",
            "erc_med": "energy release component",
            "rain_sum": "precipitation units?",
            "duration": "days",
            "latitude": "degrees",
            "longitude": "degrees",
            "start_day_of_year": "day number",
            "control_day_of_year": "day number"
          },
          {
            "gt_100": "binary indicator (1=fire impacted above 100 acres, 0=otherwise)",
            "cause": "categorical fire cause code"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "latitude and longitude stored as integers (likely scaled)",
          "Some columns like prim_threatened_aggregate have large ranges"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "latitude and longitude stored as integers (likely scaled)",
            "Some columns like prim_threatened_aggregate have large ranges"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "gt_100 is binary (0 or 1)",
          "cause_ind likely maps to cause (0=U, 1=H, etc.)",
          "start_year should match control_year for most rows",
          "gt_100 must equal 1",
          "cause values must be mapped to 'Lightning' or 'Human' categories",
          "The 'cause' column should only contain 'Lightning' or 'Human' after filtering."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "gt_100 is binary (0 or 1)",
            "cause_ind likely maps to cause (0=U, 1=H, etc.)",
            "start_year should match control_year for most rows"
          ],
          [
            "gt_100 must equal 1",
            "cause values must be mapped to 'Lightning' or 'Human' categories"
          ],
          [
            "The 'cause' column should only contain 'Lightning' or 'Human' after filtering."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to rows where gt_100 == 1",
          "Map cause codes: 'L' = Lightning, 'H' = Human",
          "Filter rows where gt_100 == 1",
          "Map cause 'L' to Lightning category",
          "Map cause 'H' to Human category",
          "Determine mapping for other cause codes (U, N, etc.)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to rows where gt_100 == 1",
            "Map cause codes: 'L' = Lightning, 'H' = Human"
          ],
          [
            "Filter rows where gt_100 == 1",
            "Map cause 'L' to Lightning category",
            "Map cause 'H' to Human category",
            "Determine mapping for other cause codes (U, N, etc.)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count comparison between cause='L' and cause='H' for gt_100=1",
          "Compare count of Lightning-caused fires vs Human-caused fires where gt_100 == 1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count comparison between cause='L' and cause='H' for gt_100=1"
          ],
          [
            "Compare count of Lightning-caused fires vs Human-caused fires where gt_100 == 1"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Final answer must be exactly 'Lightning' or 'Human' with no additional text",
          "Return only the word 'Lightning' or 'Human'",
          "No explanation or additional text",
          "Return the cause category with higher count of fires impacting above 100 acres",
          "Output must be either 'Lightning' or 'Human'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Final answer must be exactly 'Lightning' or 'Human' with no additional text"
          ],
          [
            "Return only the word 'Lightning' or 'Human'",
            "No explanation or additional text",
            "Return the cause category with higher count of fires impacting above 100 acres"
          ],
          [
            "Output must be either 'Lightning' or 'Human'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6111111111111113
  },
  "wildfire-easy-9": {
    "m_q": {
      "target_metric": {
        "value": "Difference between average fatalities on days with humidity <30% and overall average fatalities, rounded to four decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "Difference between average fatalities on days with humidity <30% and overall average fatalities, rounded to four decimal places",
          "Difference between fatalities on days with humidity <30% and average fatalities across all days",
          "Difference between fatalities on days with humidity less than 30% and the average fatalities, rounded to four decimal places."
        ]
      },
      "filters": {
        "value": [
          "avrh_mean < 30",
          "fatalities_last is not null",
          "avrh_mean < 30 for low humidity subset"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "avrh_mean < 30",
            "fatalities_last is not null"
          ],
          [
            "avrh_mean < 30 for low humidity subset"
          ],
          [
            "avrh_mean < 30"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the average fatalities across all days?",
          "What is the average fatalities on days with humidity <30%?",
          "What is the difference between these two averages?",
          "What is the total number of fatalities on days where avrh_mean < 30?",
          "How many wildfire incidents occurred on days where avrh_mean < 30?",
          "What is the average fatalities per incident on days where avrh_mean < 30?",
          "What is the overall average fatalities per incident across all days?",
          "Calculate the average fatalities across all days.",
          "Calculate the total fatalities on days with humidity less than 30%.",
          "Calculate the difference between the total fatalities on low humidity days and the average fatalities."
        ],
        "confidence": 0.3666666666666667,
        "votes": [
          [
            "What is the average fatalities across all days?",
            "What is the average fatalities on days with humidity <30%?",
            "What is the difference between these two averages?"
          ],
          [
            "What is the total number of fatalities on days where avrh_mean < 30?",
            "How many wildfire incidents occurred on days where avrh_mean < 30?",
            "What is the average fatalities per incident on days where avrh_mean < 30?",
            "What is the overall average fatalities per incident across all days?",
            "What is the difference between these two averages?"
          ],
          [
            "Calculate the average fatalities across all days.",
            "Calculate the total fatalities on days with humidity less than 30%.",
            "Calculate the difference between the total fatalities on low humidity days and the average fatalities."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Fire_Weather_Data_2002-2014_2016.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ],
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ],
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "avrh_mean": "percentage humidity",
          "fatalities_last": "count of fatalities",
          "wind_med": "wind speed unit unspecified",
          "erc_med": "energy release component index",
          "rain_sum": "precipitation unit unspecified"
        },
        "confidence": 0.6000000000000001,
        "votes": [
          {
            "avrh_mean": "percentage humidity",
            "fatalities_last": "count of fatalities"
          },
          {
            "avrh_mean": "percentage (humidity)",
            "fatalities_last": "count of fatalities",
            "wind_med": "wind speed unit unspecified",
            "erc_med": "energy release component index",
            "rain_sum": "precipitation unit unspecified"
          },
          {
            "avrh_mean": "percent",
            "fatalities_last": "count"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "avrh_mean appears to be integer percentage values (e.g., 39, 46, 19)",
          "fatalities_last appears to be integer counts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "avrh_mean appears to be integer percentage values (e.g., 39, 46, 19)",
            "fatalities_last appears to be integer counts"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "fatalities_last >= 0",
          "avrh_mean between 0 and 100",
          "start_year between 2002 and 2016",
          "avrh_mean represents humidity and should be between 0 and 100",
          "fatalities_last should be non-negative integer",
          "Result should be rounded to 4 decimal places"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "fatalities_last >= 0",
            "avrh_mean between 0 and 100",
            "start_year between 2002 and 2016"
          ],
          [
            "avrh_mean represents humidity and should be between 0 and 100",
            "fatalities_last should be non-negative integer",
            "Result should be rounded to 4 decimal places"
          ],
          [
            "fatalities_last >= 0"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where avrh_mean is missing",
          "Exclude rows where fatalities_last is missing",
          "low_humidity_subset = avrh_mean < 30",
          "all_incidents_subset = no filter"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where avrh_mean is missing",
            "Exclude rows where fatalities_last is missing"
          ],
          [
            "low_humidity_subset = avrh_mean < 30",
            "all_incidents_subset = no filter"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of fatalities_last for skewness",
          "Compare variance of fatalities between humidity groups"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of fatalities_last for skewness",
            "Compare variance of fatalities between humidity groups"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Round final result to four decimal places",
          "Positive number indicates more fatalities on low humidity days",
          "Negative number indicates fewer fatalities on low humidity days",
          "Single scalar value rounded to 4 decimal places",
          "Positive value indicates more fatalities in low humidity conditions",
          "Negative value indicates fewer fatalities in low humidity conditions",
          "Round the final result to four decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Round final result to four decimal places",
            "Positive number indicates more fatalities on low humidity days",
            "Negative number indicates fewer fatalities on low humidity days"
          ],
          [
            "Single scalar value rounded to 4 decimal places",
            "Positive value indicates more fatalities in low humidity conditions",
            "Negative value indicates fewer fatalities in low humidity conditions"
          ],
          [
            "Round the final result to four decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5733333333333335
  },
  "wildfire-hard-10": {
    "m_q": {
      "target_metric": {
        "value": "Count of fires by political affiliation of states (Democratic vs Republican)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of fires by political affiliation of states (Democratic vs Republican)",
          "Count of fires by political party affiliation of states (Democratic vs Republican)",
          "Count of fires, grouped by whether the state is predominantly Democratic or Republican"
        ]
      },
      "filters": {
        "value": [
          "Exclude states with missing Total Fires data",
          "Exclude states with missing political affiliation data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude states with missing Total Fires data",
            "Exclude states with missing political affiliation data"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Political affiliation (Democratic/Republican)",
          "political_party_majority",
          "political leaning"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Political affiliation (Democratic/Republican)"
          ],
          [
            "political_party_majority"
          ],
          [
            "political leaning"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to determine if a state is Democratic or Republican?",
          "Which data source contains fire counts?",
          "How to handle states with missing data?",
          "Which states are Democratic and which are Republican based on 2020 election results?",
          "What is the total fire count for each state?",
          "What is the sum of Total Fires for all Democratic states?",
          "What is the sum of Total Fires for all Republican states?",
          "Which group has more fires: Democratic or Republican states?",
          "Determine the political leaning of each state based on the democratic_vs_republican_votes_by_usa_state_2020.csv file.",
          "Count the total number of fires for states leaning Democratic.",
          "Count the total number of fires for states leaning Republican.",
          "Compare the counts of fires between Democratic and Republican leaning states."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How to determine if a state is Democratic or Republican?",
            "Which data source contains fire counts?",
            "How to handle states with missing data?"
          ],
          [
            "Which states are Democratic and which are Republican based on 2020 election results?",
            "What is the total fire count for each state?",
            "What is the sum of Total Fires for all Democratic states?",
            "What is the sum of Total Fires for all Republican states?",
            "Which group has more fires: Democratic or Republican states?"
          ],
          [
            "Determine the political leaning of each state based on the democratic_vs_republican_votes_by_usa_state_2020.csv file.",
            "Count the total number of fires for states leaning Democratic.",
            "Count the total number of fires for states leaning Republican.",
            "Compare the counts of fires between Democratic and Republican leaning states."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Wildfire_Acres_by_State.csv",
          "democratic_vs_republican_votes_by_usa_state_2020.csv",
          "wildfire_total_fires_p45_54.csv"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "Wildfire_Acres_by_State.csv",
            "democratic_vs_republican_votes_by_usa_state_2020.csv",
            "wildfire_total_fires_p45_54.csv"
          ],
          [
            "wildfire_total_fires_p45_54.csv",
            "democratic_vs_republican_votes_by_usa_state_2020.csv"
          ],
          [
            "Wildfire_Acres_by_State.csv",
            "democratic_vs_republican_votes_by_usa_state_2020.csv",
            "wildfire_total_fires_p45_54.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column name mismatch: 'State' vs 'state'",
          "Different number of states in each file (52, 51, 50)",
          "Missing District of Columbia in wildfire_total_fires_p45_54.csv",
          "State column naming differs: 'State' in wildfire data vs 'usa_state' in political data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column name mismatch: 'State' vs 'state'",
            "Different number of states in each file (52, 51, 50)",
            "Missing District of Columbia in wildfire_total_fires_p45_54.csv"
          ],
          [
            "State column naming differs: 'State' in wildfire data vs 'usa_state' in political data"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "total acres burned": "acres",
          "population": "people",
          "total fires": "count",
          "dem": "votes",
          "rep": "votes",
          "percent_democrat": "percentage"
        },
        "confidence": 0.8333333333333334,
        "votes": [
          {
            "Total Acres Burned": "acres",
            "Population": "people",
            "Total Fires": "count",
            "DEM": "votes",
            "REP": "votes"
          },
          {
            "Total Fires": "count",
            "DEM": "votes",
            "REP": "votes",
            "percent_democrat": "percentage"
          },
          {
            "Total Acres Burned": "acres",
            "Population": "people",
            "DEM": "votes",
            "REP": "votes",
            "percent_democrat": "percentage",
            "Total Fires": "number of fires"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Total Acres Burned varies dramatically across states (e.g., 137 vs 1,081,144)",
          "Population varies dramatically across states",
          "Total Fires varies dramatically across states"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total Acres Burned varies dramatically across states (e.g., 137 vs 1,081,144)",
            "Population varies dramatically across states",
            "Total Fires varies dramatically across states"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Wildfire_Acres_by_State.csv has 52 states including District of Columbia, but wildfire_total_fires_p45_54.csv has only 50 states",
          "Need to determine political affiliation threshold (e.g., percent_democrat > 50% = Democratic)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Wildfire_Acres_by_State.csv has 52 states including District of Columbia, but wildfire_total_fires_p45_54.csv has only 50 states",
            "Need to determine political affiliation threshold (e.g., percent_democrat > 50% = Democratic)"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.33,
        "votes": [
          10.0,
          2.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Political affiliation must be determined from 2020 election data",
          "Only states with complete fire count data can be included",
          "States must be classified as either Democratic or Republican (no ties)",
          "Must classify each state as Democratic or Republican based on majority votes (DEM > REP or REP > DEM)",
          "Must sum Total Fires for each political group",
          "Must compare counts and return only 'Democratic' or 'Republican'",
          "Determine political leaning based on 'percent_democrat' column in democratic_vs_republican_votes_by_usa_state_2020.csv. States with >50% are Democratic, <=50% are Republican.",
          "The District of Columbia should be considered Democratic."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Political affiliation must be determined from 2020 election data",
            "Only states with complete fire count data can be included",
            "States must be classified as either Democratic or Republican (no ties)"
          ],
          [
            "Must classify each state as Democratic or Republican based on majority votes (DEM > REP or REP > DEM)",
            "Must sum Total Fires for each political group",
            "Must compare counts and return only 'Democratic' or 'Republican'"
          ],
          [
            "Determine political leaning based on 'percent_democrat' column in democratic_vs_republican_votes_by_usa_state_2020.csv. States with >50% are Democratic, <=50% are Republican.",
            "The District of Columbia should be considered Democratic."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to states where percent_democrat > 50% for Democratic, < 50% for Republican",
          "Exclude states with missing Total Fires values",
          "Democratic states: where DEM > REP",
          "Republican states: where REP > DEM",
          "States leaning Democratic",
          "States leaning Republican"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to states where percent_democrat > 50% for Democratic, < 50% for Republican",
            "Exclude states with missing Total Fires values"
          ],
          [
            "Democratic states: where DEM > REP",
            "Republican states: where REP > DEM"
          ],
          [
            "States leaning Democratic",
            "States leaning Republican"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Sum of Total Fires for Democratic states",
          "Sum of Total Fires for Republican states",
          "Comparison of sums to determine which is larger"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sum of Total Fires for Democratic states",
            "Sum of Total Fires for Republican states",
            "Comparison of sums to determine which is larger"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Answer must be exactly 'Democratic' or 'Republican'",
          "Based on count of fires, not acres burned or population",
          "Output must be exactly one word: 'Democratic' or 'Republican'",
          "No additional explanation or data should be provided",
          "Output either 'Democratic' or 'Republican'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Answer must be exactly 'Democratic' or 'Republican'",
            "Based on count of fires, not acres burned or population"
          ],
          [
            "Output must be exactly one word: 'Democratic' or 'Republican'",
            "No additional explanation or data should be provided"
          ],
          [
            "Output either 'Democratic' or 'Republican'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5859444444444446
  },
  "wildfire-hard-11": {
    "m_q": {
      "target_metric": {
        "value": "Acres burned per capita (Total Acres Burned / Population)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Acres burned per capita (Total Acres Burned / Population)",
          "Land area in acres impacted by fire per capita (Total Acres Burned / Population)",
          "State with the maximum fire-impacted land area (in Acres) per capita"
        ]
      },
      "filters": {
        "value": [
          "Exclude rows with missing Total Acres Burned values",
          "Exclude rows with Population = 0",
          "Exclude states with missing Total Acres Burned or Population data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows with missing Total Acres Burned values",
            "Exclude rows with Population = 0"
          ],
          [
            "Exclude states with missing Total Acres Burned or Population data"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "State"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "State"
          ],
          [
            "State"
          ],
          [
            "State"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which state has the highest acres burned per capita?",
          "What is the distribution of acres burned per capita across states?",
          "Are there states with missing data that need to be handled?",
          "What is the total acres burned for each state?",
          "What is the population for each state?",
          "What is the acres burned per capita for each state?",
          "Which state has the maximum acres burned per capita?",
          "Calculate fire-impacted land area per capita for each state.",
          "Find the maximum fire-impacted land area per capita across all states.",
          "Identify the state corresponding to the maximum fire-impacted land area per capita."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Which state has the highest acres burned per capita?",
            "What is the distribution of acres burned per capita across states?",
            "Are there states with missing data that need to be handled?"
          ],
          [
            "What is the total acres burned for each state?",
            "What is the population for each state?",
            "What is the acres burned per capita for each state?",
            "Which state has the maximum acres burned per capita?"
          ],
          [
            "Calculate fire-impacted land area per capita for each state.",
            "Find the maximum fire-impacted land area per capita across all states.",
            "Identify the state corresponding to the maximum fire-impacted land area per capita."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Wildfire_Acres_by_State.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Wildfire_Acres_by_State.csv"
          ],
          [
            "Wildfire_Acres_by_State.csv"
          ],
          [
            "Wildfire_Acres_by_State.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "total acres burned": "Acres",
          "population": "People",
          "acres_per_capita": "acres per person"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "Total Acres Burned": "Acres",
            "Population": "People"
          },
          {
            "Total Acres Burned": "acres",
            "Population": "persons",
            "acres_per_capita": "acres per person"
          },
          {
            "Total Acres Burned": "Acres",
            "Population": "People"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Population values vary by orders of magnitude (e.g., District of Columbia 702,250 vs California 39,431,263)",
          "Total Acres Burned values vary widely (e.g., Connecticut 339 vs Idaho 996,762)",
          "Per capita calculation requires division of acres by population count",
          "Result will be a small decimal representing acres per person"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Population values vary by orders of magnitude (e.g., District of Columbia 702,250 vs California 39,431,263)",
            "Total Acres Burned values vary widely (e.g., Connecticut 339 vs Idaho 996,762)"
          ],
          [
            "Per capita calculation requires division of acres by population count",
            "Result will be a small decimal representing acres per person"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 3.0,
        "confidence": 1.0,
        "votes": [
          3.0,
          3.0,
          3.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total Acres Burned must be non-negative",
          "Population must be positive integer",
          "Per capita calculation requires non-zero population",
          "Total Acres Burned must be non-null and non-negative",
          "Population must be non-null and greater than zero to avoid division by zero",
          "State field should be non-null",
          "Population must be greater than 0.",
          "Total Acres Burned must be non-negative."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total Acres Burned must be non-negative",
            "Population must be positive integer",
            "Per capita calculation requires non-zero population"
          ],
          [
            "Total Acres Burned must be non-null and non-negative",
            "Population must be non-null and greater than zero to avoid division by zero",
            "State field should be non-null"
          ],
          [
            "Population must be greater than 0.",
            "Total Acres Burned must be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows where Total Acres Burned is null",
          "Filter out rows where Population is 0 or null",
          "Exclude rows where Total Acres Burned is null",
          "Exclude rows where Population is null or zero"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows where Total Acres Burned is null",
            "Filter out rows where Population is 0 or null"
          ],
          [
            "Exclude rows where Total Acres Burned is null",
            "Exclude rows where Population is null or zero"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Total Acres Burned",
          "Check for outliers in Population",
          "Validate per capita calculation doesn't produce extreme values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Total Acres Burned",
            "Check for outliers in Population",
            "Validate per capita calculation doesn't produce extreme values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "State name",
          "Acres burned per capita (calculated)",
          "Ranking by per capita value",
          "Return the state name with the highest acres burned per capita",
          "Optionally include the calculated per capita value for context"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "State name",
            "Acres burned per capita (calculated)",
            "Ranking by per capita value"
          ],
          [
            "Return the state name with the highest acres burned per capita",
            "Optionally include the calculated per capita value for context"
          ],
          [
            "State name"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.608888888888889
  },
  "wildfire-hard-12": {
    "m_q": {
      "target_metric": {
        "value": "temporal shift in fire start day of year distribution over years",
        "confidence": 0.3333333333333333,
        "votes": [
          "temporal shift in fire start day of year distribution over years",
          "Temporal shift in fire start distribution within the year (start_day_of_year) over multiple years",
          "Distribution of fire start dates across years and whether the distribution has shifted earlier or later over time."
        ]
      },
      "filters": {
        "value": [
          "start_year not null",
          "start_day_of_year not null",
          "start_day_of_year between 1 and 366"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "start_year not null",
            "start_day_of_year not null",
            "start_day_of_year between 1 and 366"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "start_year"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "start_year"
          ],
          [
            "start_year"
          ],
          [
            "start_year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "Yes/No"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the average start_day_of_year per year?",
          "Is there a statistically significant trend in start_day_of_year over time?",
          "Does the distribution of start_day_of_year change across years?",
          "What is the distribution of start_day_of_year for each start_year?",
          "Has the central tendency (mean/median) of start_day_of_year changed over time?",
          "Is there a statistically significant trend in start_day_of_year over start_year?",
          "Are fires starting earlier (lower start_day_of_year) or later (higher start_day_of_year) in recent years compared to earlier years?",
          "What is the distribution of 'start_day_of_year' for each 'start_year'?",
          "How does the distribution of 'start_day_of_year' change across different 'start_year' values?",
          "Is there a statistically significant trend in the average or median 'start_day_of_year' over time (across 'start_year')?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the average start_day_of_year per year?",
            "Is there a statistically significant trend in start_day_of_year over time?",
            "Does the distribution of start_day_of_year change across years?"
          ],
          [
            "What is the distribution of start_day_of_year for each start_year?",
            "Has the central tendency (mean/median) of start_day_of_year changed over time?",
            "Is there a statistically significant trend in start_day_of_year over start_year?",
            "Are fires starting earlier (lower start_day_of_year) or later (higher start_day_of_year) in recent years compared to earlier years?"
          ],
          [
            "What is the distribution of 'start_day_of_year' for each 'start_year'?",
            "How does the distribution of 'start_day_of_year' change across different 'start_year' values?",
            "Is there a statistically significant trend in the average or median 'start_day_of_year' over time (across 'start_year')?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Fire_Weather_Data_2002-2014_2016.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ],
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ],
          [
            "Fire_Weather_Data_2002-2014_2016.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "start_day_of_year": "day number (1-366)",
          "start_year": "year",
          "latitude": "degrees",
          "longitude": "degrees",
          "duration": "days",
          "avrh_mean": "relative humidity percentage",
          "wind_med": "wind speed units",
          "erc_med": "energy release component index",
          "rain_sum": "precipitation units",
          "start_date": "date (mm/dd/yyyy format)"
        },
        "confidence": 0.4666666666666667,
        "votes": [
          {
            "start_day_of_year": "day number (1-366)",
            "start_year": "calendar year",
            "latitude": "degrees",
            "longitude": "degrees",
            "duration": "days",
            "avrh_mean": "relative humidity percentage",
            "wind_med": "wind speed units",
            "erc_med": "energy release component index",
            "rain_sum": "precipitation units"
          },
          {
            "start_day_of_year": "day_of_year (1-366)",
            "start_year": "year",
            "start_date": "date (mm/dd/yyyy format)"
          },
          {
            "start_day_of_year": "day of year",
            "start_year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "latitude and longitude stored as integers (likely scaled)",
          "some numeric columns may have missing values encoded as 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "latitude and longitude stored as integers (likely scaled)",
            "some numeric columns may have missing values encoded as 0"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "start_day_of_year must be between 1 and 366",
          "start_year between 2002 and 2016",
          "duration >= 0",
          "control_day_of_year >= start_day_of_year when same year",
          "start_year should be in range 2002-2016",
          "Records with missing or invalid start_day_of_year should be excluded",
          "start_year should be within a reasonable range (e.g., 2002-2016).",
          "start_day_of_year should be between 1 and 366."
        ],
        "confidence": 0.375,
        "votes": [
          [
            "start_day_of_year must be between 1 and 366",
            "start_year between 2002 and 2016",
            "duration >= 0",
            "control_day_of_year >= start_day_of_year when same year"
          ],
          [
            "start_day_of_year must be between 1 and 366",
            "start_year should be in range 2002-2016",
            "Records with missing or invalid start_day_of_year should be excluded"
          ],
          [
            "start_year should be within a reasonable range (e.g., 2002-2016).",
            "start_day_of_year should be between 1 and 366."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "exclude rows where start_day_of_year is 0 or null",
          "consider only years with sufficient sample size"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "exclude rows where start_day_of_year is 0 or null",
            "consider only years with sufficient sample size"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "linear regression of start_day_of_year against start_year",
          "Mann-Kendall trend test",
          "comparison of median start_day_of_year between early and late periods",
          "Linear regression of start_day_of_year vs start_year",
          "Mann-Kendall trend test for temporal trend in start_day_of_year",
          "Comparison of mean/median start_day_of_year between early years (2002-2009) and later years (2010-2016)",
          "Mann-Kendall test for trend in median start_day_of_year over start_year",
          "Kolmogorov-Smirnov test to compare distributions of start_day_of_year between different start_years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "linear regression of start_day_of_year against start_year",
            "Mann-Kendall trend test",
            "comparison of median start_day_of_year between early and late periods"
          ],
          [
            "Linear regression of start_day_of_year vs start_year",
            "Mann-Kendall trend test for temporal trend in start_day_of_year",
            "Comparison of mean/median start_day_of_year between early years (2002-2009) and later years (2010-2016)"
          ],
          [
            "Mann-Kendall test for trend in median start_day_of_year over start_year",
            "Kolmogorov-Smirnov test to compare distributions of start_day_of_year between different start_years"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "binary Yes/No answer only",
          "no explanation text",
          "Answer must be 'Yes' or 'No' only",
          "No explanation or additional text"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "binary Yes/No answer only",
            "no explanation text"
          ],
          [
            "Answer must be 'Yes' or 'No' only",
            "No explanation or additional text"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5865277777777778
  },
  "wildfire-hard-14": {
    "m_q": {
      "target_metric": {
        "value": "Correlation coefficient between proportion of generally unsafe air quality days (Unhealthy, Very Unhealthy, Hazardous) and total acres burned by fire, rounded to 2 decimal places",
        "confidence": 0.3333333333333333,
        "votes": [
          "Correlation coefficient between proportion of generally unsafe air quality days (Unhealthy, Very Unhealthy, Hazardous) and total acres burned by fire, rounded to 2 decimal places",
          "Correlation coefficient between proportion of generally unsafe air quality days (according to EPA) and total acres burned by wildfires in 2024, rounded to 2 decimal places",
          "Pearson correlation coefficient between the proportion of generally unsafe air quality days and the amount of land affected by fires in 2024, rounded to 2 decimal places."
        ]
      },
      "filters": {
        "value": [
          "Year = 2024",
          "State-level aggregation required",
          "Year == 2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year = 2024",
            "State-level aggregation required"
          ],
          [
            "Year == 2024"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "State"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "State"
          ],
          [
            "State"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to calculate proportion of unsafe days from county-level AQI data?",
          "How to aggregate county data to state level?",
          "How to handle missing or incomplete data in wildfire dataset?",
          "What correlation method to use (Pearson, Spearman)?",
          "What constitutes 'generally unsafe air quality days' according to EPA standards?",
          "How to calculate the proportion of unsafe days per state from county-level AQI data?",
          "How to aggregate Total Acres Burned per state?",
          "How to join air quality data with wildfire data by state?",
          "Which correlation method to use (Pearson, Spearman)?",
          "How to handle states with missing data in either dataset?",
          "Calculate the proportion of generally unsafe air quality days for each state.",
          "Calculate the total acres burned for each state.",
          "Merge the air quality data and wildfire data by state.",
          "Calculate the Pearson correlation coefficient between the proportion of generally unsafe air quality days and the total acres burned.",
          "Round the correlation coefficient to 2 decimal places."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "How to calculate proportion of unsafe days from county-level AQI data?",
            "How to aggregate county data to state level?",
            "How to handle missing or incomplete data in wildfire dataset?",
            "What correlation method to use (Pearson, Spearman)?"
          ],
          [
            "What constitutes 'generally unsafe air quality days' according to EPA standards?",
            "How to calculate the proportion of unsafe days per state from county-level AQI data?",
            "How to aggregate Total Acres Burned per state?",
            "How to join air quality data with wildfire data by state?",
            "Which correlation method to use (Pearson, Spearman)?",
            "How to handle states with missing data in either dataset?"
          ],
          [
            "Calculate the proportion of generally unsafe air quality days for each state.",
            "Calculate the total acres burned for each state.",
            "Merge the air quality data and wildfire data by state.",
            "Calculate the Pearson correlation coefficient between the proportion of generally unsafe air quality days and the total acres burned.",
            "Round the correlation coefficient to 2 decimal places."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Wildfire_Acres_by_State.csv",
          "annual_aqi_by_county_2024.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Wildfire_Acres_by_State.csv",
            "annual_aqi_by_county_2024.csv"
          ],
          [
            "annual_aqi_by_county_2024.csv",
            "Wildfire_Acres_by_State.csv"
          ],
          [
            "Wildfire_Acres_by_State.csv",
            "annual_aqi_by_county_2024.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Wildfire data is at state level, AQI data is at county level",
          "Wildfire data has 52 rows (including DC), AQI data has 986 county rows",
          "District of Columbia has missing 'Total Acres Burned' value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Wildfire data is at state level, AQI data is at county level",
            "Wildfire data has 52 rows (including DC), AQI data has 986 county rows",
            "District of Columbia has missing 'Total Acres Burned' value"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "total acres burned": "acres",
          "population": "people",
          "good days": "days",
          "moderate days": "days",
          "unhealthy for sensitive groups days": "days",
          "unhealthy days": "days",
          "very unhealthy days": "days",
          "hazardous days": "days",
          "days with aqi": "days"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Total Acres Burned": "acres",
            "Population": "people",
            "Good Days": "days",
            "Moderate Days": "days",
            "Unhealthy for Sensitive Groups Days": "days",
            "Unhealthy Days": "days",
            "Very Unhealthy Days": "days",
            "Hazardous Days": "days",
            "Days with AQI": "days"
          },
          {
            "Total Acres Burned": "acres",
            "Days with AQI": "days",
            "Good Days": "days",
            "Moderate Days": "days",
            "Unhealthy for Sensitive Groups Days": "days",
            "Unhealthy Days": "days",
            "Very Unhealthy Days": "days",
            "Hazardous Days": "days",
            "Population": "persons"
          },
          {
            "Total Acres Burned": "acres",
            "Population": "people",
            "Days with AQI": "days",
            "Good Days": "days",
            "Moderate Days": "days",
            "Unhealthy for Sensitive Groups Days": "days",
            "Unhealthy Days": "days",
            "Very Unhealthy Days": "days",
            "Hazardous Days": "days"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Wildfire acres vary dramatically by state (from 137 to 1,081,144 acres)",
          "Population varies dramatically by state (from 702,250 to 39,431,263)",
          "Some counties have very few 'Days with AQI' (e.g., Jefferson, AL has only 22 days)",
          "Total Acres Burned varies widely by state (from hundreds to millions of acres)",
          "Number of counties per state varies, affecting aggregation of AQI data",
          "Days with AQI varies by county (monitoring coverage differences)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Wildfire acres vary dramatically by state (from 137 to 1,081,144 acres)",
            "Population varies dramatically by state (from 702,250 to 39,431,263)",
            "Some counties have very few 'Days with AQI' (e.g., Jefferson, AL has only 22 days)"
          ],
          [
            "Total Acres Burned varies widely by state (from hundreds to millions of acres)",
            "Number of counties per state varies, affecting aggregation of AQI data",
            "Days with AQI varies by county (monitoring coverage differences)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "State name formatting may differ between files (e.g., 'District of Columbia' vs potential variations)",
          "Wildfire data includes DC with missing acreage, AQI data may or may not include DC counties",
          "State-level wildfire data vs county-level AQI data requires aggregation",
          "District of Columbia has missing Total Acres Burned value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "State name formatting may differ between files (e.g., 'District of Columbia' vs potential variations)",
            "Wildfire data includes DC with missing acreage, AQI data may or may not include DC counties"
          ],
          [
            "State-level wildfire data vs county-level AQI data requires aggregation",
            "District of Columbia has missing Total Acres Burned value"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "NaN"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.33,
        "votes": [
          10.0,
          18.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year is fixed at 2024 in AQI data",
          "Must aggregate county AQI data to state level before correlation",
          "Unsafe days defined as sum of Unhealthy, Very Unhealthy, and Hazardous Days",
          "Proportion calculated as unsafe days / Days with AQI",
          "Correlation must be rounded to 2 decimal places",
          "Year must be 2024",
          "Generally unsafe days defined as: Unhealthy for Sensitive Groups Days + Unhealthy Days + Very Unhealthy Days + Hazardous Days",
          "Proportion = (unsafe days) / (Days with AQI)",
          "Exclude states with missing Total Acres Burned values",
          "County-level AQI data must be aggregated to state level",
          "The proportion of generally unsafe air quality days is calculated as (Unhealthy for Sensitive Groups Days + Unhealthy Days + Very Unhealthy Days + Hazardous Days) / Days with AQI.",
          "Ensure that 'Days with AQI' is not zero to avoid division by zero errors.",
          "Handle potential missing values in 'Total Acres Burned' column in Wildfire_Acres_by_State.csv."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year is fixed at 2024 in AQI data",
            "Must aggregate county AQI data to state level before correlation",
            "Unsafe days defined as sum of Unhealthy, Very Unhealthy, and Hazardous Days",
            "Proportion calculated as unsafe days / Days with AQI",
            "Correlation must be rounded to 2 decimal places"
          ],
          [
            "Year must be 2024",
            "Generally unsafe days defined as: Unhealthy for Sensitive Groups Days + Unhealthy Days + Very Unhealthy Days + Hazardous Days",
            "Proportion = (unsafe days) / (Days with AQI)",
            "Exclude states with missing Total Acres Burned values",
            "County-level AQI data must be aggregated to state level"
          ],
          [
            "The proportion of generally unsafe air quality days is calculated as (Unhealthy for Sensitive Groups Days + Unhealthy Days + Very Unhealthy Days + Hazardous Days) / Days with AQI.",
            "Ensure that 'Days with AQI' is not zero to avoid division by zero errors.",
            "Handle potential missing values in 'Total Acres Burned' column in Wildfire_Acres_by_State.csv."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out states with missing wildfire acreage data",
          "Consider filtering counties with incomplete AQI data (low 'Days with AQI')",
          "May need to normalize by population or area for fair comparison",
          "Remove District of Columbia if Total Acres Burned is null",
          "States must have at least one county with AQI data",
          "States must have valid Total Acres Burned values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out states with missing wildfire acreage data",
            "Consider filtering counties with incomplete AQI data (low 'Days with AQI')",
            "May need to normalize by population or area for fair comparison"
          ],
          [
            "Remove District of Columbia if Total Acres Burned is null",
            "States must have at least one county with AQI data",
            "States must have valid Total Acres Burned values"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Pearson correlation test between state-level unsafe day proportion and acres burned",
          "Check for normality of both variables",
          "Consider Spearman correlation if data is not normally distributed",
          "Calculate Pearson correlation coefficient between state-level proportion of unsafe days and Total Acres Burned",
          "Verify sufficient sample size (n >= 3) for correlation calculation",
          "Pearson correlation coefficient"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pearson correlation test between state-level unsafe day proportion and acres burned",
            "Check for normality of both variables",
            "Consider Spearman correlation if data is not normally distributed"
          ],
          [
            "Calculate Pearson correlation coefficient between state-level proportion of unsafe days and Total Acres Burned",
            "Verify sufficient sample size (n >= 3) for correlation calculation"
          ],
          [
            "Pearson correlation coefficient"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single correlation coefficient rounded to 2 decimal places",
          "May need to report p-value for significance",
          "Should document which states were included/excluded",
          "Single correlation coefficient value",
          "Rounded to exactly 2 decimal places",
          "Numeric output format: X.XX",
          "The final result should be a single floating-point number rounded to 2 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single correlation coefficient rounded to 2 decimal places",
            "May need to report p-value for significance",
            "Should document which states were included/excluded"
          ],
          [
            "Single correlation coefficient value",
            "Rounded to exactly 2 decimal places",
            "Numeric output format: X.XX"
          ],
          [
            "The final result should be a single floating-point number rounded to 2 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6081666666666667
  },
  "wildfire-hard-16": {
    "m_q": {
      "target_metric": {
        "value": "Chi-square statistic and p-value for independence test between geographical region and fire causes for January-March fires with known causes",
        "confidence": 0.3333333333333333,
        "votes": [
          "Chi-square statistic and p-value for independence test between geographical region and fire causes for January-March fires with known causes",
          "Chi-square test statistic and p-value for the relationship between geographical region and fire cause",
          "Chi-square statistic and p-value for the association between geographical region and fire cause for fires starting in January, February, and March."
        ]
      },
      "filters": {
        "value": [
          "cause != 'U' (exclude unknown causes)",
          "start_date month in [1,2,3] (January, February, March)",
          "cause is not null",
          "region is not null",
          "Fires starting in January, February, or March only",
          "Fires with known causes (exclude unknown causes)",
          "Fires with known causes (cause is not 'U')",
          "Fires starting in January, February, or March (start_date month is 1, 2, or 3)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "cause != 'U' (exclude unknown causes)",
            "start_date month in [1,2,3] (January, February, March)",
            "cause is not null",
            "region is not null"
          ],
          [
            "Fires starting in January, February, or March only",
            "Fires with known causes (exclude unknown causes)"
          ],
          [
            "Fires with known causes (cause is not 'U')",
            "Fires starting in January, February, or March (start_date month is 1, 2, or 3)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "region",
          "cause"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "region",
            "cause"
          ],
          [
            "region",
            "cause"
          ],
          [
            "region",
            "cause"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Breakdown of fire causes by geographical region for January-March fires",
          "Chi-square test of independence between region and cause",
          "Report chi-square statistic and p-value as list of two numbers",
          "What are the unique values in the 'cause' column and which represent known causes?",
          "How to extract the month from 'start_date' to filter for January, February, and March?",
          "What is the contingency table of region vs cause for the filtered data?",
          "What are the chi-square statistic and p-value from the independence test?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Breakdown of fire causes by geographical region for January-March fires",
            "Chi-square test of independence between region and cause",
            "Report chi-square statistic and p-value as list of two numbers"
          ],
          [
            "What are the unique values in the 'cause' column and which represent known causes?",
            "How to extract the month from 'start_date' to filter for January, February, and March?",
            "What is the contingency table of region vs cause for the filtered data?",
            "What are the chi-square statistic and p-value from the independence test?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires.csv",
          "noaa_wildfires_variabledescrip.csv"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv"
          ],
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Variable description file has different structure than main data file",
          "Description file provides metadata but no direct join keys"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Variable description file has different structure than main data file",
            "Description file provides metadata but no direct join keys"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "avrh_mean": "percentage",
          "wind_med": "miles per hour",
          "erc_med": "BTUs/square foot",
          "rain_sum": "total precipitation units",
          "duration": "days",
          "latitude": "degrees",
          "longitude": "degrees",
          "start_date": "date as MM/DD/YYYY",
          "region": "categorical geographic region",
          "cause": "categorical cause code (U=unknown, N=natural, H=human, L=lightning, O=other)"
        },
        "confidence": 0.5666666666666665,
        "votes": [
          {
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs/square foot",
            "rain_sum": "total precipitation units",
            "duration": "days",
            "latitude": "degrees",
            "longitude": "degrees"
          },
          {
            "start_date": "date as MM/DD/YYYY",
            "region": "categorical geographic region",
            "cause": "categorical cause code (U=unknown, N=natural, H=human, L=lightning, O=other)"
          },
          {
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs / square foot",
            "rain_sum": "unspecified",
            "duration": "days",
            "latitude": "degrees",
            "longitude": "degrees"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "start_day_of_year ranges 1-366",
          "control_day_of_year ranges 1-366",
          "cause_ind appears to be numeric encoding of cause"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "start_day_of_year ranges 1-366",
            "control_day_of_year ranges 1-366",
            "cause_ind appears to be numeric encoding of cause"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Variable descriptions may not match actual data usage in analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Variable descriptions may not match actual data usage in analysis"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only fires with known causes (cause != 'U')",
          "Only fires starting in January, February, or March",
          "Region must be non-null",
          "Cause must be non-null",
          "Only include fires where cause is known (exclude cause='U' for unknown)",
          "Only include fires starting in months 1, 2, or 3 (January, February, March)",
          "Must have valid region and cause values for chi-square test",
          "start_date needs to be parsed to extract the month",
          "cause should not be 'U' (unknown)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only fires with known causes (cause != 'U')",
            "Only fires starting in January, February, or March",
            "Region must be non-null",
            "Cause must be non-null"
          ],
          [
            "Only include fires where cause is known (exclude cause='U' for unknown)",
            "Only include fires starting in months 1, 2, or 3 (January, February, March)",
            "Must have valid region and cause values for chi-square test"
          ],
          [
            "start_date needs to be parsed to extract the month",
            "cause should not be 'U' (unknown)"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract month from start_date column",
          "Filter cause column for non-'U' values",
          "Filter for month in [1, 2, 3]",
          "Filter for cause not equal to 'U'",
          "month = start_date.month",
          "known_cause = cause != 'U'",
          "jan_feb_mar = month in [1, 2, 3]"
        ],
        "confidence": 0.38095238095238093,
        "votes": [
          [
            "Extract month from start_date column",
            "Filter cause column for non-'U' values"
          ],
          [
            "Extract month from start_date column",
            "Filter for month in [1, 2, 3]",
            "Filter for cause not equal to 'U'"
          ],
          [
            "month = start_date.month",
            "known_cause = cause != 'U'",
            "jan_feb_mar = month in [1, 2, 3]"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Chi-square test of independence between region and cause categories",
          "Chi-square test of independence between region and cause",
          "Contingency table creation from grouped counts",
          "Chi-square test of independence"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Chi-square test of independence between region and cause categories"
          ],
          [
            "Chi-square test of independence between region and cause",
            "Contingency table creation from grouped counts"
          ],
          [
            "Chi-square test of independence"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of two numbers: [chi-square statistic, p-value]",
          "Return a list of exactly two numbers: [chi-square statistic, p-value]",
          "Chi-square statistic should be the test statistic value",
          "P-value should be the probability value from the test",
          "A list of two numbers: chi-square statistic and p-value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of two numbers: [chi-square statistic, p-value]"
          ],
          [
            "Return a list of exactly two numbers: [chi-square statistic, p-value]",
            "Chi-square statistic should be the test statistic value",
            "P-value should be the probability value from the test"
          ],
          [
            "A list of two numbers: chi-square statistic and p-value"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6390476190476191
  },
  "wildfire-hard-17": {
    "m_q": {
      "target_metric": {
        "value": "average elevation (in feet) of weather stations used for fire site monitoring",
        "confidence": 0.6666666666666666,
        "votes": [
          "average elevation (in feet) of weather stations used for fire site monitoring",
          "average elevation in feet of weather stations used for fire site monitoring",
          "average elevation (in feet) of weather stations used for fire site monitoring"
        ]
      },
      "filters": {
        "value": [
          "stations used for fire site monitoring",
          "stations in NOAA dataset",
          "weather stations used for fire site monitoring (linked to wildfire incidents)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "stations used for fire site monitoring",
            "stations in NOAA dataset"
          ],
          [
            "weather stations used for fire site monitoring (linked to wildfire incidents)"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which stations are used for fire site monitoring?",
          "How to identify elevation data in the station dataset?",
          "How to map weather stations to fire incidents?",
          "What is the unit of elevation measurement?",
          "Which file contains weather station data?",
          "Which column contains elevation information?",
          "What is the unit of measurement for elevation in the source data?",
          "How are weather stations linked to fire incidents?",
          "Are all stations in the RAWS file used for fire monitoring, or do we need to filter?",
          "How to convert elevation to feet if not already in feet?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Which stations are used for fire site monitoring?",
            "How to identify elevation data in the station dataset?",
            "How to map weather stations to fire incidents?",
            "What is the unit of elevation measurement?"
          ],
          [
            "Which file contains weather station data?",
            "Which column contains elevation information?",
            "What is the unit of measurement for elevation in the source data?",
            "How are weather stations linked to fire incidents?",
            "Are all stations in the RAWS file used for fire monitoring, or do we need to filter?",
            "How to convert elevation to feet if not already in feet?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires.csv",
          "noaa_wildfires_variabledescrip.csv",
          "PublicView_RAWS_-3515561676727363726.csv"
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv",
            "PublicView_RAWS_-3515561676727363726.csv"
          ],
          [
            "PublicView_RAWS_-3515561676727363726.csv",
            "noaa_wildfires.csv"
          ],
          [
            "noaa_wildfires.csv",
            "PublicView_RAWS_-3515561676727363726.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column names in PublicView_RAWS file are ambiguous (e.g., '39.47109', '-120.08698', '12.61 inches')",
          "No clear elevation column in any dataset",
          "Station ID column naming mismatch: 'station_verified_in_psa' vs numeric column names like '16968251'",
          "RAWS file has column names that appear to be data values (e.g., '39.47109', '-120.08698', 'SMDC1')",
          "RAWS file structure unclear - column names do not match expected headers",
          "Header row appears to be missing or malformed in RAWS file",
          "Duplicate coordinate columns in RAWS file (e.g., '-120.08698' and '-120.08698.1')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column names in PublicView_RAWS file are ambiguous (e.g., '39.47109', '-120.08698', '12.61 inches')",
            "No clear elevation column in any dataset",
            "Station ID column naming mismatch: 'station_verified_in_psa' vs numeric column names like '16968251'"
          ],
          [
            "RAWS file has column names that appear to be data values (e.g., '39.47109', '-120.08698', 'SMDC1')",
            "RAWS file structure unclear - column names do not match expected headers",
            "Header row appears to be missing or malformed in RAWS file",
            "Duplicate coordinate columns in RAWS file (e.g., '-120.08698' and '-120.08698.1')"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "avrh_mean": "percentage",
          "wind_med": "miles per hour",
          "erc_med": "BTUs/square foot",
          "rain_sum": "inches",
          "latitude": "degrees",
          "longitude": "degrees",
          "12.61 inches": "precipitation",
          "4 mph": "wind speed",
          "35 degrees": "wind direction",
          "56 deg. f": "temperature",
          "73 deg. f": "temperature",
          "17 %": "relative humidity",
          "open pine forest": "potentially feet or meters (needs verification)"
        },
        "confidence": 0.46153846153846145,
        "votes": [
          {
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs/square foot",
            "rain_sum": "inches",
            "latitude": "degrees",
            "longitude": "degrees",
            "12.61 inches": "precipitation",
            "4 mph": "wind speed",
            "35 degrees": "wind direction",
            "56 deg. F": "temperature",
            "73 deg. F": "temperature",
            "17 %": "relative humidity"
          },
          {
            "OPEN PINE FOREST": "potentially feet or meters (needs verification)",
            "12.61 inches": "precipitation in inches",
            "4 mph": "wind speed in miles per hour",
            "56 deg. F": "temperature in Fahrenheit",
            "latitude": "decimal degrees",
            "longitude": "decimal degrees"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Elevation data not clearly identified in any dataset",
          "Potential unit mismatch if elevation found (feet vs meters)",
          "Station IDs appear as numeric column names in PublicView_RAWS file",
          "Elevation unit unclear - could be feet or meters",
          "Need to verify if 'OPEN PINE FOREST' column contains elevation data",
          "Coordinate precision varies between integer and float representations in wildfire data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Elevation data not clearly identified in any dataset",
            "Potential unit mismatch if elevation found (feet vs meters)",
            "Station IDs appear as numeric column names in PublicView_RAWS file"
          ],
          [
            "Elevation unit unclear - could be feet or meters",
            "Need to verify if 'OPEN PINE FOREST' column contains elevation data",
            "Coordinate precision varies between integer and float representations in wildfire data"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "No direct elevation column in any provided dataset",
          "Station identification unclear between datasets",
          "PublicView_RAWS column names are values rather than descriptive names",
          "Wildfire coordinates stored as integers (latitude, longitude) while RAWS coordinates are floats",
          "Precision mismatch may prevent direct joins on coordinates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No direct elevation column in any provided dataset",
            "Station identification unclear between datasets",
            "PublicView_RAWS column names are values rather than descriptive names"
          ],
          [
            "Wildfire coordinates stored as integers (latitude, longitude) while RAWS coordinates are floats",
            "Precision mismatch may prevent direct joins on coordinates"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NO DATA",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.9166666666666666,
        "votes": [
          [
            "NO DATA",
            "",
            "NA",
            "N/A"
          ],
          [
            "NO DATA",
            "NA",
            "",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 0.7631578947368421,
        "votes": [
          37.0,
          28.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Elevation data must be identified or derived",
          "Only stations used for fire monitoring should be included",
          "Average should be computed in feet",
          "Station-to-fire mapping must be established",
          "Only include weather stations that are linked to fire incidents in noaa_wildfires.csv",
          "Elevation must be converted to feet if in different units",
          "Exclude records with missing or invalid elevation values (NO DATA, null)",
          "Must verify correct column contains elevation data in RAWS file"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Elevation data must be identified or derived",
            "Only stations used for fire monitoring should be included",
            "Average should be computed in feet",
            "Station-to-fire mapping must be established"
          ],
          [
            "Only include weather stations that are linked to fire incidents in noaa_wildfires.csv",
            "Elevation must be converted to feet if in different units",
            "Exclude records with missing or invalid elevation values (NO DATA, null)",
            "Must verify correct column contains elevation data in RAWS file"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to stations with valid elevation data",
          "Filter to stations referenced in wildfire incidents",
          "station_verified_in_psa column in wildfire data may indicate station verification status",
          "Filter to stations with valid geographic coordinates matching wildfire incident locations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to stations with valid elevation data",
            "Filter to stations referenced in wildfire incidents"
          ],
          [
            "station_verified_in_psa column in wildfire data may indicate station verification status",
            "Filter to stations with valid geographic coordinates matching wildfire incident locations"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing elevation values",
          "Validate elevation unit consistency",
          "Test station-fire join completeness",
          "Check for outliers in elevation values (unrealistic values)",
          "Verify elevation range is consistent with US geography (below sea level to ~14,500 feet)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing elevation values",
            "Validate elevation unit consistency",
            "Test station-fire join completeness"
          ],
          [
            "Check for outliers in elevation values (unrealistic values)",
            "Verify elevation range is consistent with US geography (below sea level to ~14,500 feet)"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numeric average with units specified",
          "Precision appropriate for elevation measurement",
          "Result should be a single numeric value (average elevation in feet)",
          "Round to appropriate precision (e.g., 1-2 decimal places)",
          "Include count of stations used in calculation",
          "The output should be a single numerical value representing the average elevation in feet."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric average with units specified",
            "Precision appropriate for elevation measurement"
          ],
          [
            "Result should be a single numeric value (average elevation in feet)",
            "Round to appropriate precision (e.g., 1-2 decimal places)",
            "Include count of stations used in calculation"
          ],
          [
            "The output should be a single numerical value representing the average elevation in feet."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5792903733693209
  },
  "wildfire-hard-18": {
    "m_q": {
      "target_metric": {
        "value": "Effect of suppression aggressiveness on fire duration and building impact, controlling for weather conditions",
        "confidence": 0.3333333333333333,
        "votes": [
          "Effect of suppression aggressiveness on fire duration and building impact, controlling for weather conditions",
          "Effect of aggressive suppression on fire duration and buildings affected, controlling for weather variables",
          "The correlation between fire suppression strategy aggressiveness and fire duration/building damage, controlling for weather conditions."
        ]
      },
      "filters": {
        "value": [
          "Remove rows with missing suppression strategy data",
          "Remove rows with missing weather variables (avrh_mean, wind_med, erc_med, rain_sum)",
          "Remove rows with missing duration or building threat columns",
          "Remove records with missing or invalid duration values",
          "Remove records with missing suppression strategy data",
          "Remove records with missing weather control variables (avrh_mean, wind_med, erc_med, rain_sum)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with missing suppression strategy data",
            "Remove rows with missing weather variables (avrh_mean, wind_med, erc_med, rain_sum)",
            "Remove rows with missing duration or building threat columns"
          ],
          [
            "Remove records with missing or invalid duration values",
            "Remove records with missing suppression strategy data",
            "Remove records with missing weather control variables (avrh_mean, wind_med, erc_med, rain_sum)"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "dominant_strategy_25_s",
          "dominant_strategy_50_s",
          "dominant_strategy_75_s",
          "subdom_strategy",
          "dom_strat_ind_25",
          "dom_strat_ind_50",
          "dom_strat_ind_75",
          "avrh_mean",
          "wind_med",
          "erc_med",
          "rain_sum"
        ],
        "confidence": 0.4545454545454545,
        "votes": [
          [
            "dominant_strategy_25_s",
            "dominant_strategy_50_s",
            "dominant_strategy_75_s",
            "subdom_strategy",
            "dom_strat_ind_25",
            "dom_strat_ind_50",
            "dom_strat_ind_75"
          ],
          [
            "dominant_strategy_25_s",
            "dominant_strategy_50_s",
            "dominant_strategy_75_s"
          ],
          [
            "dominant_strategy_75_s",
            "avrh_mean",
            "wind_med",
            "erc_med",
            "rain_sum"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Does higher percentage of full suppression (25% vs 50% vs 75%) correlate with shorter duration?",
          "Do different suppression strategies (Full Suppression vs Confine vs Monitor) affect building threat levels differently?",
          "How do weather variables (avrh_mean, wind_med, erc_med, rain_sum) moderate the relationship between suppression and outcomes?",
          "Are there regional differences in suppression effectiveness?",
          "What defines 'more aggressive suppression' (Full Suppression vs Other strategies at different thresholds)?",
          "How is 'fire ending faster' measured (duration variable)?",
          "How are 'affected buildings' measured (prim_threatened_aggregate, comm_threatened_aggregate, outb_threatened_aggregate)?",
          "What weather variables need to be controlled for (avrh_mean, wind_med, erc_med, rain_sum)?",
          "Is there a causal relationship between suppression strategy and outcomes after controlling for confounders?",
          "What is the appropriate statistical method (regression, matching, propensity score) to control for weather?",
          "How to quantify fire suppression strategy aggressiveness?",
          "How to control for weather conditions?",
          "How to measure fire duration?",
          "How to measure building damage?",
          "What statistical test is appropriate to determine the correlation?"
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Does higher percentage of full suppression (25% vs 50% vs 75%) correlate with shorter duration?",
            "Do different suppression strategies (Full Suppression vs Confine vs Monitor) affect building threat levels differently?",
            "How do weather variables (avrh_mean, wind_med, erc_med, rain_sum) moderate the relationship between suppression and outcomes?",
            "Are there regional differences in suppression effectiveness?"
          ],
          [
            "What defines 'more aggressive suppression' (Full Suppression vs Other strategies at different thresholds)?",
            "How is 'fire ending faster' measured (duration variable)?",
            "How are 'affected buildings' measured (prim_threatened_aggregate, comm_threatened_aggregate, outb_threatened_aggregate)?",
            "What weather variables need to be controlled for (avrh_mean, wind_med, erc_med, rain_sum)?",
            "Is there a causal relationship between suppression strategy and outcomes after controlling for confounders?",
            "What is the appropriate statistical method (regression, matching, propensity score) to control for weather?"
          ],
          [
            "How to quantify fire suppression strategy aggressiveness?",
            "How to control for weather conditions?",
            "How to measure fire duration?",
            "How to measure building damage?",
            "What statistical test is appropriate to determine the correlation?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires.csv",
          "noaa_wildfires_variabledescrip.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Variable description file has different column names than main dataset",
          "Main dataset has 37 columns but description file only shows partial list",
          "No direct conflicts - variabledescrip is metadata, not for analysis join"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Variable description file has different column names than main dataset",
            "Main dataset has 37 columns but description file only shows partial list"
          ],
          [
            "No direct conflicts - variabledescrip is metadata, not for analysis join"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "avrh_mean": "percentage",
          "wind_med": "miles per hour",
          "erc_med": "BTUs/square foot",
          "rain_sum": "total precipitation units",
          "duration": "days",
          "prim_threatened_aggregate": "count",
          "comm_threatened_aggregate": "count",
          "outb_threatened_aggregate": "count",
          "latitude": "degrees",
          "longitude": "degrees",
          "start_day_of_year": "day number (1-366)",
          "control_day_of_year": "day number (1-366)"
        },
        "confidence": 0.6944444444444445,
        "votes": [
          {
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs/square foot",
            "rain_sum": "total precipitation units",
            "duration": "days",
            "prim_threatened_aggregate": "count",
            "comm_threatened_aggregate": "count",
            "outb_threatened_aggregate": "count"
          },
          {
            "duration": "days",
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs per square foot",
            "rain_sum": "precipitation amount (unit not specified in metadata)",
            "prim_threatened_aggregate": "count of primary residences",
            "comm_threatened_aggregate": "count of commercial buildings",
            "outb_threatened_aggregate": "count of outbuildings",
            "latitude": "degrees",
            "longitude": "degrees",
            "start_day_of_year": "day number (1-366)",
            "control_day_of_year": "day number (1-366)"
          },
          {
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs / square foot",
            "rain_sum": "unspecified",
            "duration": "days"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "erc_med has decimal values while other weather variables are integers",
          "rain_sum may represent different time periods (individual fire vs fire cluster)",
          "latitude/longitude are integers but should likely be decimals",
          "erc_med has float type while other weather variables are integers",
          "Longitude values appear to be missing negative signs (should be negative for US West)",
          "Rain_sum aggregation unclear - may be total or average",
          "The scale of 'prim_threatened_aggregate', 'comm_threatened_aggregate', and 'outb_threatened_aggregate' may need to be considered when comparing across different regions or years."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "erc_med has decimal values while other weather variables are integers",
            "rain_sum may represent different time periods (individual fire vs fire cluster)",
            "latitude/longitude are integers but should likely be decimals"
          ],
          [
            "erc_med has float type while other weather variables are integers",
            "Longitude values appear to be missing negative signs (should be negative for US West)",
            "Rain_sum aggregation unclear - may be total or average"
          ],
          [
            "The scale of 'prim_threatened_aggregate', 'comm_threatened_aggregate', and 'outb_threatened_aggregate' may need to be considered when comparing across different regions or years."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Description file mentions 'Young et al. 2020, Table 1' but not all variables have descriptions",
          "Alternative description column has inconsistent formatting"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Description file mentions 'Young et al. 2020, Table 1' but not all variables have descriptions",
            "Alternative description column has inconsistent formatting"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "null"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "duration = control_day_of_year - start_day_of_year (should be verified)",
          "start_year should match year from start_date",
          "control_year should match year from controlled_date",
          "dominant_strategy columns should be consistent with dom_strat_ind columns",
          "duration >= 0",
          "prim_threatened_aggregate >= 0",
          "comm_threatened_aggregate >= 0",
          "outb_threatened_aggregate >= 0",
          "avrh_mean must be between 0 and 100 (percentage)",
          "wind_med >= 0",
          "erc_med >= 0",
          "rain_sum >= 0",
          "controlled_date >= start_date",
          "control_day_of_year must be consistent with controlled_date",
          "The 'dominant_strategy_25_s', 'dominant_strategy_50_s', and 'dominant_strategy_75_s' columns represent different thresholds for suppression strategy. Need to choose one or combine them into a single 'aggressiveness' score.",
          "The 'duration' column is calculated as the difference between 'start_date' and 'controlled_date'. Ensure these dates are properly formatted and that the calculation is accurate.",
          "The 'prim_threatened_aggregate' and 'comm_threatened_aggregate' columns represent the number of buildings threatened. Consider combining these into a single 'buildings_threatened' metric.",
          "The 'cause' column may need to be simplified into broader categories (e.g., 'human' vs. 'natural').",
          "Consider the impact of fire size (e.g., 'gt_100') on the relationship between suppression strategy and fire duration/damage."
        ],
        "confidence": 0.3333333333333332,
        "votes": [
          [
            "duration = control_day_of_year - start_day_of_year (should be verified)",
            "start_year should match year from start_date",
            "control_year should match year from controlled_date",
            "dominant_strategy columns should be consistent with dom_strat_ind columns"
          ],
          [
            "duration >= 0",
            "prim_threatened_aggregate >= 0",
            "comm_threatened_aggregate >= 0",
            "outb_threatened_aggregate >= 0",
            "avrh_mean must be between 0 and 100 (percentage)",
            "wind_med >= 0",
            "erc_med >= 0",
            "rain_sum >= 0",
            "controlled_date >= start_date",
            "control_day_of_year must be consistent with controlled_date"
          ],
          [
            "The 'dominant_strategy_25_s', 'dominant_strategy_50_s', and 'dominant_strategy_75_s' columns represent different thresholds for suppression strategy. Need to choose one or combine them into a single 'aggressiveness' score.",
            "The 'duration' column is calculated as the difference between 'start_date' and 'controlled_date'. Ensure these dates are properly formatted and that the calculation is accurate.",
            "The 'prim_threatened_aggregate' and 'comm_threatened_aggregate' columns represent the number of buildings threatened. Consider combining these into a single 'buildings_threatened' metric.",
            "The 'cause' column may need to be simplified into broader categories (e.g., 'human' vs. 'natural').",
            "Consider the impact of fire size (e.g., 'gt_100') on the relationship between suppression strategy and fire duration/damage."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to fires with complete weather data",
          "Filter to fires with non-zero duration",
          "Consider creating composite building threat metric = prim_threatened_aggregate + comm_threatened_aggregate + outb_threatened_aggregate",
          "Calculate total_threatened_buildings = prim_threatened_aggregate + comm_threatened_aggregate + outb_threatened_aggregate",
          "Create binary indicator for aggressive_suppression based on dominant_strategy thresholds",
          "Exclude fires with duration = 0 or missing values",
          "Flag records where control_year != start_year for cross-year fires"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to fires with complete weather data",
            "Filter to fires with non-zero duration",
            "Consider creating composite building threat metric = prim_threatened_aggregate + comm_threatened_aggregate + outb_threatened_aggregate"
          ],
          [
            "Calculate total_threatened_buildings = prim_threatened_aggregate + comm_threatened_aggregate + outb_threatened_aggregate",
            "Create binary indicator for aggressive_suppression based on dominant_strategy thresholds",
            "Exclude fires with duration = 0 or missing values",
            "Flag records where control_year != start_year for cross-year fires"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "ANCOVA with suppression strategy as factor, weather variables as covariates",
          "Multiple regression with interaction terms between suppression and weather",
          "Chi-square test for suppression strategy distribution across regions",
          "Multiple regression with duration as outcome, suppression strategy as predictor, weather variables as controls",
          "Multiple regression with total_threatened_buildings as outcome, suppression strategy as predictor, weather variables as controls",
          "Test for multicollinearity among weather control variables",
          "Propensity score matching or inverse probability weighting to balance weather covariates across suppression strategies",
          "Robustness checks using different suppression thresholds (25%, 50%, 75%)",
          "Test for interaction effects between suppression strategy and weather variables",
          "Multiple linear regression",
          "ANOVA"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "ANCOVA with suppression strategy as factor, weather variables as covariates",
            "Multiple regression with interaction terms between suppression and weather",
            "Chi-square test for suppression strategy distribution across regions"
          ],
          [
            "Multiple regression with duration as outcome, suppression strategy as predictor, weather variables as controls",
            "Multiple regression with total_threatened_buildings as outcome, suppression strategy as predictor, weather variables as controls",
            "Test for multicollinearity among weather control variables",
            "Propensity score matching or inverse probability weighting to balance weather covariates across suppression strategies",
            "Robustness checks using different suppression thresholds (25%, 50%, 75%)",
            "Test for interaction effects between suppression strategy and weather variables"
          ],
          [
            "Multiple linear regression",
            "ANOVA"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Include confidence intervals for effect sizes",
          "Report R-squared values for models",
          "Show weather-adjusted means for each suppression strategy",
          "Include sample sizes for each suppression category",
          "Report regression coefficients with standard errors and p-values",
          "Provide effect sizes for suppression strategy on duration and buildings affected",
          "Show descriptive statistics by suppression strategy group",
          "Display weather variable distributions across suppression groups to assess balance",
          "Include model diagnostics (R-squared, residual plots) for regression models",
          "Tables showing correlation coefficients and p-values.",
          "Scatter plots visualizing the relationship between suppression strategy and fire duration/damage, with weather conditions as covariates."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Include confidence intervals for effect sizes",
            "Report R-squared values for models",
            "Show weather-adjusted means for each suppression strategy",
            "Include sample sizes for each suppression category"
          ],
          [
            "Report regression coefficients with standard errors and p-values",
            "Provide effect sizes for suppression strategy on duration and buildings affected",
            "Show descriptive statistics by suppression strategy group",
            "Display weather variable distributions across suppression groups to assess balance",
            "Include model diagnostics (R-squared, residual plots) for regression models"
          ],
          [
            "Tables showing correlation coefficients and p-values.",
            "Scatter plots visualizing the relationship between suppression strategy and fire duration/damage, with weather conditions as covariates."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6107828282828283
  },
  "wildfire-hard-19": {
    "m_q": {
      "target_metric": {
        "value": "Percentage of fires in 2016 that were brought under control with moderate/heavy rain (>0.05 inches) in the fire area on the same day or day before control date",
        "confidence": 0.3333333333333333,
        "votes": [
          "Percentage of fires in 2016 that were brought under control with moderate/heavy rain (>0.05 inches) in the fire area on the same day or day before control date",
          "Percentage of fires brought under control with moderate or heavy rain (>0.05 in) in the fire area on the same day or day before the control day",
          "Percentage of fires in 2016 controlled with moderate or heavy rain (>0.05 inches) on the control day or the day before, assuming a 1km fire area diameter."
        ]
      },
      "filters": {
        "value": [
          "start_year = 2016",
          "control_year = 2016",
          "rain_sum > 0.05",
          "fire area diameter = 1km assumption for weather station inclusion",
          "start_year == 2016",
          "Fires where weather station falls within 1km (0.5km radius) of fire location",
          "Rain > 0.05 inches on control_day or control_day - 1",
          "rain_sum > 0.05 on controlled_date or controlled_date - 1 day",
          "Fire area diameter is assumed to be at least 1km, allowing weather station data within the fire area to be used."
        ],
        "confidence": 0.3703703703703704,
        "votes": [
          [
            "start_year = 2016",
            "control_year = 2016",
            "rain_sum > 0.05",
            "fire area diameter = 1km assumption for weather station inclusion"
          ],
          [
            "start_year == 2016",
            "Fires where weather station falls within 1km (0.5km radius) of fire location",
            "Rain > 0.05 inches on control_day or control_day - 1"
          ],
          [
            "start_year = 2016",
            "rain_sum > 0.05 on controlled_date or controlled_date - 1 day",
            "Fire area diameter is assumed to be at least 1km, allowing weather station data within the fire area to be used."
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which fires in 2016 have weather stations within 1km diameter fire area?",
          "For those fires, what was the precipitation on control_date or control_date-1?",
          "How many fires had >0.05 inches precipitation on those days?",
          "What percentage of total 2016 fires does this represent?",
          "How many total fires occurred in 2016?",
          "For each 2016 fire, what is the controlled_date and control_day_of_year?",
          "Which weather stations fall within the fire area (1km diameter circle, 0.5km radius from fire center)?",
          "What was the precipitation at each relevant weather station on the control day?",
          "What was the precipitation at each relevant weather station on the day before the control day?",
          "Did any station within the fire area record >0.05 inches on control day or day before?",
          "How many 2016 fires meet the rain criteria?",
          "What is the percentage (to 2 decimal places) of 2016 fires with rain >0.05 in?",
          "Identify fires that started in 2016.",
          "Determine the control date for each fire.",
          "For each fire, check if rain_sum > 0.05 on the control date or the day before.",
          "Calculate the percentage of fires meeting the rain criteria out of all fires in 2016.",
          "Round the percentage to 2 decimal places."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Which fires in 2016 have weather stations within 1km diameter fire area?",
            "For those fires, what was the precipitation on control_date or control_date-1?",
            "How many fires had >0.05 inches precipitation on those days?",
            "What percentage of total 2016 fires does this represent?"
          ],
          [
            "How many total fires occurred in 2016?",
            "For each 2016 fire, what is the controlled_date and control_day_of_year?",
            "Which weather stations fall within the fire area (1km diameter circle, 0.5km radius from fire center)?",
            "What was the precipitation at each relevant weather station on the control day?",
            "What was the precipitation at each relevant weather station on the day before the control day?",
            "Did any station within the fire area record >0.05 inches on control day or day before?",
            "How many 2016 fires meet the rain criteria?",
            "What is the percentage (to 2 decimal places) of 2016 fires with rain >0.05 in?"
          ],
          [
            "Identify fires that started in 2016.",
            "Determine the control date for each fire.",
            "For each fire, check if rain_sum > 0.05 on the control date or the day before.",
            "Calculate the percentage of fires meeting the rain criteria out of all fires in 2016.",
            "Round the percentage to 2 decimal places."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires.csv",
          "noaa_wildfires_variabledescrip.csv"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv"
          ],
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "rain_sum column appears to be integer but represents precipitation in inches (should be float)",
          "latitude/longitude are integers but should be decimal coordinates",
          "station_verified_in_psa column suggests weather station verification but no station location data provided",
          "Need weather station data with daily precipitation observations to match with fire incidents, but only fire-level aggregated data is available in noaa_wildfires.csv",
          "latitude and longitude columns are int64 instead of float64, which may indicate truncation or encoding issues",
          "No explicit weather station location data provided",
          "No daily precipitation breakdown, only rain_sum (total precipitation summary)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "rain_sum column appears to be integer but represents precipitation in inches (should be float)",
            "latitude/longitude are integers but should be decimal coordinates",
            "station_verified_in_psa column suggests weather station verification but no station location data provided"
          ],
          [
            "Need weather station data with daily precipitation observations to match with fire incidents, but only fire-level aggregated data is available in noaa_wildfires.csv",
            "latitude and longitude columns are int64 instead of float64, which may indicate truncation or encoding issues",
            "No explicit weather station location data provided",
            "No daily precipitation breakdown, only rain_sum (total precipitation summary)"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "rain_sum": "inches",
          "avrh_mean": "percentage",
          "wind_med": "miles_per_hour",
          "erc_med": "BTUs_per_square_foot",
          "latitude": "degrees",
          "longitude": "degrees",
          "duration": "days"
        },
        "confidence": 0.8095238095238095,
        "votes": [
          {
            "rain_sum": "inches",
            "avrh_mean": "percentage",
            "wind_med": "miles_per_hour",
            "erc_med": "BTUs_per_square_foot",
            "latitude": "degrees",
            "longitude": "degrees",
            "duration": "days"
          },
          {
            "rain_sum": "unknown (possibly hundredths of inches based on question threshold of 0.05 in)",
            "latitude": "degrees (integer representation)",
            "longitude": "degrees (integer representation)",
            "duration": "days",
            "wind_med": "miles per hour",
            "erc_med": "BTUs per square foot",
            "avrh_mean": "percentage"
          },
          {
            "rain_sum": "inches",
            "latitude": "degrees",
            "longitude": "degrees"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "rain_sum is integer but precipitation measurements should allow decimals",
          "latitude/longitude as integers loses precision for 1km radius calculations",
          "Missing weather station location data needed for 1km fire area assumption",
          "rain_sum appears to be total precipitation over incident duration, not daily breakdown",
          "latitude/longitude stored as integers suggests possible scaling factor (e.g., multiplied by constant)",
          "Fire area diameter assumption of 1km needs to be converted to radius of 0.5km for distance calculations",
          "Threshold of 0.05 inches needs to match the unit scale of precipitation data",
          "Need to determine if rain_sum unit allows direct comparison to 0.05 inch threshold"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "rain_sum is integer but precipitation measurements should allow decimals",
            "latitude/longitude as integers loses precision for 1km radius calculations",
            "Missing weather station location data needed for 1km fire area assumption"
          ],
          [
            "rain_sum appears to be total precipitation over incident duration, not daily breakdown",
            "latitude/longitude stored as integers suggests possible scaling factor (e.g., multiplied by constant)",
            "Fire area diameter assumption of 1km needs to be converted to radius of 0.5km for distance calculations",
            "Threshold of 0.05 inches needs to match the unit scale of precipitation data",
            "Need to determine if rain_sum unit allows direct comparison to 0.05 inch threshold"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Variable descriptions mention 'fire cluster' aggregation for rain_sum, which may not match individual fire incidents",
          "Missing detailed weather station data with daily precipitation required for the analysis",
          "Cannot identify which weather stations fall within fire area without station location data",
          "Cannot determine precipitation on specific days (control_day and control_day - 1) from aggregated rain_sum"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Variable descriptions mention 'fire cluster' aggregation for rain_sum, which may not match individual fire incidents"
          ],
          [
            "Missing detailed weather station data with daily precipitation required for the analysis",
            "Cannot identify which weather stations fall within fire area without station location data",
            "Cannot determine precipitation on specific days (control_day and control_day - 1) from aggregated rain_sum"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Fire area assumed to be circular with 1km diameter",
          "Weather station must fall within fire area to use its data",
          "Precipitation threshold: >0.05 inches",
          "Time window: control_date or day before control_date",
          "Year filter: 2016 only",
          "Filter to start_year == 2016",
          "Fire area diameter = 1km (radius = 0.5km)",
          "Weather station must fall within fire area (distance from fire center <= 0.5km)",
          "Rain threshold > 0.05 inches",
          "Rain must occur on control_day or control_day - 1",
          "Result must be percentage rounded to 2 decimal places",
          "start_year must be an integer.",
          "control_year must be an integer.",
          "start_date and controlled_date must be valid dates."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Fire area assumed to be circular with 1km diameter",
            "Weather station must fall within fire area to use its data",
            "Precipitation threshold: >0.05 inches",
            "Time window: control_date or day before control_date",
            "Year filter: 2016 only"
          ],
          [
            "Filter to start_year == 2016",
            "Fire area diameter = 1km (radius = 0.5km)",
            "Weather station must fall within fire area (distance from fire center <= 0.5km)",
            "Rain threshold > 0.05 inches",
            "Rain must occur on control_day or control_day - 1",
            "Result must be percentage rounded to 2 decimal places"
          ],
          [
            "start_year must be an integer.",
            "control_year must be an integer.",
            "start_date and controlled_date must be valid dates."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to 2016 fires using start_year and control_year",
          "Calculate which fires have station_verified_in_psa = 1",
          "Check rain_sum > 0.05 for qualifying fires",
          "Calculate control_day as control_day_of_year",
          "Calculate previous_day as control_day_of_year - 1, accounting for year boundaries",
          "Compute haversine distance between fire location (latitude, longitude) and weather stations",
          "Identify fires with at least one weather station within 0.5km radius",
          "Check if daily precipitation on control_day or previous_day exceeds 0.05 inches at any station within fire area",
          "control_date - 1 day must be a valid date."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to 2016 fires using start_year and control_year",
            "Calculate which fires have station_verified_in_psa = 1",
            "Check rain_sum > 0.05 for qualifying fires"
          ],
          [
            "Calculate control_day as control_day_of_year",
            "Calculate previous_day as control_day_of_year - 1, accounting for year boundaries",
            "Compute haversine distance between fire location (latitude, longitude) and weather stations",
            "Identify fires with at least one weather station within 0.5km radius",
            "Check if daily precipitation on control_day or previous_day exceeds 0.05 inches at any station within fire area"
          ],
          [
            "control_date - 1 day must be a valid date."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check percentage calculation: (fires_with_rain_condition / total_2016_fires) * 100",
          "Validate rain_sum distribution for 2016 fires",
          "Check temporal consistency between start_date and controlled_date"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check percentage calculation: (fires_with_rain_condition / total_2016_fires) * 100",
            "Validate rain_sum distribution for 2016 fires",
            "Check temporal consistency between start_date and controlled_date"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Percentage to 2 decimal places",
          "Include only 2016 data",
          "Apply 1km fire area assumption",
          "Output percentage as numeric value with exactly 2 decimal places",
          "Format: XX.XX%",
          "Percentage = (number of 2016 fires with rain > 0.05 in on control day or day before) / (total 2016 fires) * 100",
          "The final answer must be a percentage rounded to 2 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage to 2 decimal places",
            "Include only 2016 data",
            "Apply 1km fire area assumption"
          ],
          [
            "Output percentage as numeric value with exactly 2 decimal places",
            "Format: XX.XX%",
            "Percentage = (number of 2016 fires with rain > 0.05 in on control day or day before) / (total 2016 fires) * 100"
          ],
          [
            "The final answer must be a percentage rounded to 2 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6006613756613758
  },
  "wildfire-hard-20": {
    "m_q": {
      "target_metric": {
        "value": "percentage of wildfires that account for at least 90% of residential houses damaged in 2008",
        "confidence": 0.3333333333333333,
        "votes": [
          "percentage of wildfires that account for at least 90% of residential houses damaged in 2008",
          "Percentage (to 2 decimal places) of wildfires that account for at least 90% of residential houses damaged in 2008",
          "Percentage of wildfires in 2008 that account for at least 90% of residential houses damaged in 2008, rounded to 2 decimal places."
        ]
      },
      "filters": {
        "value": [
          "start_year = 2008",
          "prim_threatened_aggregate > 0",
          "start_year == 2008"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "start_year = 2008",
            "prim_threatened_aggregate > 0"
          ],
          [
            "start_year == 2008"
          ],
          [
            "start_year = 2008"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "incident_number"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "incident_number"
          ],
          [
            "incident_number"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total number of residential houses damaged in 2008?",
          "What is the cumulative sum of houses damaged when sorted by descending damage?",
          "Which wildfires contribute to the top 90% of damage?",
          "What percentage of total wildfires do these represent?",
          "What column represents residential houses damaged?",
          "How many wildfires occurred in 2008?",
          "How to determine which wildfires account for 90% of houses damaged?",
          "Should wildfires be ranked by houses damaged in descending order?",
          "What is the cumulative sum of houses damaged when wildfires are sorted by damage?",
          "How many wildfires are needed until cumulative damage reaches 90% of total?",
          "Which wildfires in 2008 account for at least 90% of the total residential houses damaged?",
          "How many wildfires account for at least 90% of the total residential houses damaged?",
          "What is the percentage of wildfires that account for at least 90% of the total residential houses damaged?"
        ],
        "confidence": 0.3846153846153845,
        "votes": [
          [
            "What is the total number of residential houses damaged in 2008?",
            "What is the cumulative sum of houses damaged when sorted by descending damage?",
            "Which wildfires contribute to the top 90% of damage?",
            "What percentage of total wildfires do these represent?"
          ],
          [
            "What column represents residential houses damaged?",
            "How many wildfires occurred in 2008?",
            "What is the total number of residential houses damaged in 2008?",
            "How to determine which wildfires account for 90% of houses damaged?",
            "Should wildfires be ranked by houses damaged in descending order?",
            "What is the cumulative sum of houses damaged when wildfires are sorted by damage?",
            "How many wildfires are needed until cumulative damage reaches 90% of total?"
          ],
          [
            "What is the total number of residential houses damaged in 2008?",
            "Which wildfires in 2008 account for at least 90% of the total residential houses damaged?",
            "How many wildfires account for at least 90% of the total residential houses damaged?",
            "What is the percentage of wildfires that account for at least 90% of the total residential houses damaged?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires.csv",
          "noaa_wildfires_variabledescrip.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Variable description file has different structure than data file"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Variable description file has different structure than data file"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "avrh_mean": "percentage",
          "wind_med": "miles per hour",
          "erc_med": "BTUs per square foot",
          "rain_sum": "precipitation units",
          "duration": "days",
          "prim_threatened_aggregate": "number of houses",
          "comm_threatened_aggregate": "number of commercial buildings",
          "outb_threatened_aggregate": "number of outbuildings",
          "latitude": "degrees",
          "longitude": "degrees",
          "start_year": "year",
          "percentage": "percent"
        },
        "confidence": 0.5277777777777776,
        "votes": [
          {
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs per square foot",
            "rain_sum": "precipitation units",
            "duration": "days",
            "prim_threatened_aggregate": "number of houses",
            "comm_threatened_aggregate": "number of commercial buildings",
            "outb_threatened_aggregate": "number of outbuildings",
            "latitude": "degrees",
            "longitude": "degrees"
          },
          {
            "prim_threatened_aggregate": "count",
            "start_year": "year",
            "percentage": "percent"
          },
          {
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs / square foot",
            "rain_sum": "unspecified",
            "duration": "days",
            "prim_threatened_aggregate": "houses"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "prim_threatened_aggregate appears to be houses at risk, not necessarily damaged",
          "Need to clarify if 'prim_threatened_aggregate' represents houses damaged or houses at risk/threatened",
          "Variable description says 'Houses at risk' not 'houses damaged' - potential semantic mismatch with question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "prim_threatened_aggregate appears to be houses at risk, not necessarily damaged"
          ],
          [
            "Need to clarify if 'prim_threatened_aggregate' represents houses damaged or houses at risk/threatened",
            "Variable description says 'Houses at risk' not 'houses damaged' - potential semantic mismatch with question"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Data file shows 'prim_threatened_aggregate' but description says 'Houses at risk, contributed to' - unclear if this represents actual damage or potential risk",
          "Question asks for 'houses damaged' but available column is 'prim_threatened_aggregate' which is described as 'houses at risk, contributed to'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data file shows 'prim_threatened_aggregate' but description says 'Houses at risk, contributed to' - unclear if this represents actual damage or potential risk"
          ],
          [
            "Question asks for 'houses damaged' but available column is 'prim_threatened_aggregate' which is described as 'houses at risk, contributed to'"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "nan"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            "",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "prim_threatened_aggregate must be non-negative",
          "start_year must be 2008 for relevant analysis",
          "Percentage result must be to 2 decimal places",
          "start_year must equal 2008",
          "prim_threatened_aggregate >= 0",
          "Each incident_number should be unique per wildfire event in 2008",
          "start_year is valid year"
        ],
        "confidence": 0.380952380952381,
        "votes": [
          [
            "prim_threatened_aggregate must be non-negative",
            "start_year must be 2008 for relevant analysis",
            "Percentage result must be to 2 decimal places"
          ],
          [
            "start_year must equal 2008",
            "prim_threatened_aggregate >= 0",
            "Each incident_number should be unique per wildfire event in 2008"
          ],
          [
            "prim_threatened_aggregate >= 0",
            "start_year is valid year"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to start_year = 2008",
          "Filter to prim_threatened_aggregate > 0 to exclude wildfires with no houses at risk",
          "Filter to start_year == 2008",
          "Sort wildfires by prim_threatened_aggregate descending",
          "Calculate cumulative sum of prim_threatened_aggregate",
          "Calculate cumulative percentage of total houses threatened",
          "Find count of wildfires where cumulative percentage <= 90%",
          "fires_in_2008 = start_year == 2008",
          "houses_damaged_by_fire = prim_threatened_aggregate",
          "ninety_percent_houses = 0.9 * sum(houses_damaged_by_fire where fires_in_2008)",
          "fires_accounting_for_90_percent = fires_in_2008 where sum(houses_damaged_by_fire) >= ninety_percent_houses"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Filter to start_year = 2008",
            "Filter to prim_threatened_aggregate > 0 to exclude wildfires with no houses at risk"
          ],
          [
            "Filter to start_year == 2008",
            "Sort wildfires by prim_threatened_aggregate descending",
            "Calculate cumulative sum of prim_threatened_aggregate",
            "Calculate cumulative percentage of total houses threatened",
            "Find count of wildfires where cumulative percentage <= 90%"
          ],
          [
            "fires_in_2008 = start_year == 2008",
            "houses_damaged_by_fire = prim_threatened_aggregate",
            "ninety_percent_houses = 0.9 * sum(houses_damaged_by_fire where fires_in_2008)",
            "fires_accounting_for_90_percent = fires_in_2008 where sum(houses_damaged_by_fire) >= ninety_percent_houses"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of prim_threatened_aggregate for 2008 data",
          "Calculate cumulative percentage of damage",
          "Pareto analysis to find top wildfires contributing to 90% of damage",
          "Cumulative distribution calculation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of prim_threatened_aggregate for 2008 data",
            "Calculate cumulative percentage of damage"
          ],
          [
            "Pareto analysis to find top wildfires contributing to 90% of damage",
            "Cumulative distribution calculation"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Percentage to 2 decimal places",
          "Result should be a single percentage value",
          "Result must be a percentage",
          "Result must be rounded to 2 decimal places",
          "Result represents percentage of total wildfires in 2008",
          "Percentage value rounded to 2 decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage to 2 decimal places",
            "Result should be a single percentage value"
          ],
          [
            "Result must be a percentage",
            "Result must be rounded to 2 decimal places",
            "Result represents percentage of total wildfires in 2008"
          ],
          [
            "Percentage value rounded to 2 decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.628556166056166
  },
  "wildfire-hard-21": {
    "m_q": {
      "target_metric": {
        "value": "total residential property value lost per state between 2005 and 2010 (inclusive)",
        "confidence": 0.3333333333333333,
        "votes": [
          "total residential property value lost per state between 2005 and 2010 (inclusive)",
          "Total value of residential property lost by state, ranked to identify top 3 states with highest losses",
          "Top 3 states with the largest decrease in residential property value between 2005 and 2010"
        ]
      },
      "filters": {
        "value": [
          "start_year >= 2005",
          "start_year <= 2010",
          "start_year between 2005 and 2010 (inclusive)"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "start_year >= 2005",
            "start_year <= 2010"
          ],
          [
            "start_year >= 2005",
            "start_year <= 2010"
          ],
          [
            "start_year between 2005 and 2010 (inclusive)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "state"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "state"
          ],
          [
            "state"
          ],
          [
            "state"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What column represents residential property value loss?",
          "How to aggregate property loss across multiple incidents per state?",
          "How to convert state abbreviations to full names?",
          "What metric represents residential property loss in the wildfire data?",
          "How to filter records for years 2005-2010 inclusive?",
          "How to aggregate property loss values by state?",
          "How to convert state abbreviations to full state names?",
          "How to rank states by total property loss and select top 3?",
          "Calculate the total residential property value for each state in 2005.",
          "Calculate the total residential property value for each state in 2010.",
          "Calculate the difference in total residential property value between 2005 and 2010 for each state.",
          "Identify the top 3 states with the largest negative difference in residential property value."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What column represents residential property value loss?",
            "How to aggregate property loss across multiple incidents per state?",
            "How to convert state abbreviations to full names?"
          ],
          [
            "What metric represents residential property loss in the wildfire data?",
            "How to filter records for years 2005-2010 inclusive?",
            "How to aggregate property loss values by state?",
            "How to convert state abbreviations to full state names?",
            "How to rank states by total property loss and select top 3?"
          ],
          [
            "Calculate the total residential property value for each state in 2005.",
            "Calculate the total residential property value for each state in 2010.",
            "Calculate the difference in total residential property value between 2005 and 2010 for each state.",
            "Identify the top 3 states with the largest negative difference in residential property value."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires.csv",
          "noaa_wildfires_variabledescrip.csv",
          "state_abbreviation_to_state.json"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "noaa_wildfires.csv",
            "noaa_wildfires_variabledescrip.csv",
            "state_abbreviation_to_state.json"
          ],
          [
            "noaa_wildfires.csv",
            "state_abbreviation_to_state.json",
            "noaa_wildfires_variabledescrip.csv"
          ],
          [
            "noaa_wildfires.csv",
            "state_abbreviation_to_state.json"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "state_abbreviation_to_state.json has non-standard format with abbreviations as column headers",
          "noaa_wildfires.csv uses 2-letter state codes but state_abbreviation_to_state.json needs transformation",
          "state_abbreviation_to_state.json has state codes as column names rather than as row values, requiring transpose or special handling"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "state_abbreviation_to_state.json has non-standard format with abbreviations as column headers",
            "noaa_wildfires.csv uses 2-letter state codes but state_abbreviation_to_state.json needs transformation"
          ],
          [
            "state_abbreviation_to_state.json has state codes as column names rather than as row values, requiring transpose or special handling"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "avrh_mean": "percentage",
          "wind_med": "miles per hour",
          "erc_med": "BTUs/square foot",
          "rain_sum": "total precipitation units",
          "prim_threatened_aggregate": "number of houses",
          "comm_threatened_aggregate": "number of commercial buildings",
          "start_year": "year (YYYY)",
          "duration": "days"
        },
        "confidence": 0.4166666666666667,
        "votes": [
          {
            "avrh_mean": "percentage",
            "wind_med": "miles per hour",
            "erc_med": "BTUs/square foot",
            "rain_sum": "total precipitation units",
            "prim_threatened_aggregate": "number of houses",
            "comm_threatened_aggregate": "number of commercial buildings"
          },
          {
            "prim_threatened_aggregate": "count of houses",
            "start_year": "year (YYYY)",
            "duration": "days"
          },
          {
            "prim_threatened_aggregate": "number of houses"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "prim_threatened_aggregate counts houses at risk, not actual value lost",
          "Need to estimate or find actual property value loss data",
          "prim_threatened_aggregate represents houses 'at risk' or 'threatened', not necessarily destroyed or property value lost - interpretation needed",
          "No direct property value column exists; may need to use threatened count as proxy for loss"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "prim_threatened_aggregate counts houses at risk, not actual value lost",
            "Need to estimate or find actual property value loss data"
          ],
          [
            "prim_threatened_aggregate represents houses 'at risk' or 'threatened', not necessarily destroyed or property value lost - interpretation needed",
            "No direct property value column exists; may need to use threatened count as proxy for loss"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Analytical question asks for 'value lost' but data only has 'properties threatened' counts",
          "The question asks for 'lost property in value' but dataset only contains 'prim_threatened_aggregate' (houses at risk), not actual loss or monetary value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Analytical question asks for 'value lost' but data only has 'properties threatened' counts"
          ],
          [
            "The question asks for 'lost property in value' but dataset only contains 'prim_threatened_aggregate' (houses at risk), not actual loss or monetary value"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 37.0,
        "confidence": 1.0,
        "votes": [
          37.0,
          37.0,
          37.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Must include years 2005-2010 inclusive",
          "Cannot discard rows with missing values unnecessarily",
          "Output must be state full names",
          "Need top 3 states only",
          "Include years 2005 through 2010 (inclusive)",
          "Do not discard rows with missing values unnecessarily",
          "Return state full names, not abbreviations",
          "Return exactly top 3 states",
          "The 'state' column in noaa_wildfires.csv contains state abbreviations. Need to map to full state names using state_abbreviation_to_state.json.",
          "Residential property value is represented by 'prim_threatened_aggregate' column (number of houses at risk).",
          "Missing values in 'prim_threatened_aggregate' should not be discarded if possible."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Must include years 2005-2010 inclusive",
            "Cannot discard rows with missing values unnecessarily",
            "Output must be state full names",
            "Need top 3 states only"
          ],
          [
            "Include years 2005 through 2010 (inclusive)",
            "Do not discard rows with missing values unnecessarily",
            "Return state full names, not abbreviations",
            "Return exactly top 3 states"
          ],
          [
            "The 'state' column in noaa_wildfires.csv contains state abbreviations. Need to map to full state names using state_abbreviation_to_state.json.",
            "Residential property value is represented by 'prim_threatened_aggregate' column (number of houses at risk).",
            "Missing values in 'prim_threatened_aggregate' should not be discarded if possible."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "prim_threatened_aggregate > 0 to focus on actual property threats",
          "state not in ['NA', 'GU', 'AS', 'MP', 'PR', 'VI'] to exclude non-states",
          "Filter on start_year column for range [2005, 2010]",
          "Handle missing values in prim_threatened_aggregate appropriately (treat as 0 or keep)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "prim_threatened_aggregate > 0 to focus on actual property threats",
            "state not in ['NA', 'GU', 'AS', 'MP', 'PR', 'VI'] to exclude non-states"
          ],
          [
            "Filter on start_year column for range [2005, 2010]",
            "Handle missing values in prim_threatened_aggregate appropriately (treat as 0 or keep)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of prim_threatened_aggregate for outliers",
          "Verify temporal coverage 2005-2010 has sufficient data",
          "Aggregate sum of prim_threatened_aggregate by state",
          "Rank states in descending order by total"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of prim_threatened_aggregate for outliers",
            "Verify temporal coverage 2005-2010 has sufficient data"
          ],
          [
            "Aggregate sum of prim_threatened_aggregate by state",
            "Rank states in descending order by total"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of 3 state full names",
          "Sorted descending by total residential property threat/loss",
          "Return full state names (e.g., 'California', not 'CA')",
          "Return list of 3 state names",
          "Maintain all records unless they fail the year filter",
          "Output should be a list of 3 state full names."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of 3 state full names",
            "Sorted descending by total residential property threat/loss"
          ],
          [
            "Return full state names (e.g., 'California', not 'CA')",
            "Return list of 3 state names",
            "Maintain all records unless they fail the year filter"
          ],
          [
            "Output should be a list of 3 state full names."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6347222222222223
  },
  "wildfire-hard-4": {
    "m_q": {
      "target_metric": {
        "value": "year with highest suppression cost per acre of human-caused fire, and the cost per acre rounded to nearest cent",
        "confidence": 0.3333333333333333,
        "votes": [
          "year with highest suppression cost per acre of human-caused fire, and the cost per acre rounded to nearest cent",
          "Suppression cost per acre for human-caused fires, rounded to the nearest cent",
          "Suppression cost per acre of human-caused fire for each year, and the year with the highest cost per acre"
        ]
      },
      "filters": {
        "value": [
          "human-caused fires only",
          "years with both cost and acreage data",
          "Human-caused fires"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "human-caused fires only",
            "years with both cost and acreage data"
          ],
          [
            "Human-caused fires only"
          ],
          [
            "Human-caused fires"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Year"
          ],
          [
            "Year"
          ],
          [
            "Year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total suppression cost per year?",
          "What is the total human-caused acreage per year?",
          "How to calculate cost per acre for each year?",
          "Which year has the maximum cost per acre?",
          "What is the total suppression cost by year?",
          "What is the total acres burned by human-caused fires by year?",
          "What is the cost per acre for each year?",
          "What is that maximum cost per acre value?",
          "Calculate total suppression cost for each year.",
          "Calculate total human-caused acres burned for each year.",
          "Calculate suppression cost per acre for each year (total suppression cost / total human-caused acres burned).",
          "Find the year with the maximum suppression cost per acre."
        ],
        "confidence": 0.3611111111111111,
        "votes": [
          [
            "What is the total suppression cost per year?",
            "What is the total human-caused acreage per year?",
            "How to calculate cost per acre for each year?",
            "Which year has the maximum cost per acre?"
          ],
          [
            "What is the total suppression cost by year?",
            "What is the total acres burned by human-caused fires by year?",
            "What is the cost per acre for each year?",
            "Which year has the maximum cost per acre?",
            "What is that maximum cost per acre value?"
          ],
          [
            "Calculate total suppression cost for each year.",
            "Calculate total human-caused acres burned for each year.",
            "Calculate suppression cost per acre for each year (total suppression cost / total human-caused acres burned).",
            "Find the year with the maximum suppression cost per acre."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "nifc_suppression_costs.csv",
          "nifc_human_caused_acres.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "nifc_suppression_costs.csv",
            "nifc_human_caused_acres.csv"
          ],
          [
            "nifc_suppression_costs.csv",
            "nifc_human_caused_acres.csv"
          ],
          [
            "nifc_suppression_costs.csv",
            "nifc_human_caused_acres.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "nifc_suppression_costs.csv has malformed column names with mixed data types",
          "nifc_suppression_costs.csv column names appear to contain data values",
          "nifc_human_caused_acres.csv has missing values in Western Great Basin column",
          "nifc_suppression_costs.csv has malformed headers - appears to have data in header row",
          "Column names in nifc_suppression_costs.csv contain tab-separated values and mixed data",
          "First column in nifc_suppression_costs.csv appears to be Year but is named '1985\\t82'",
          "nifc_suppression_costs.csv needs complete re-parsing to extract proper columns",
          "Column names in nifc_suppression_costs.csv are not clearly defined and contain combined data. Need to parse the year from the first column.",
          "The suppression cost data is spread across multiple columns and needs to be aggregated."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "nifc_suppression_costs.csv has malformed column names with mixed data types",
            "nifc_suppression_costs.csv column names appear to contain data values",
            "nifc_human_caused_acres.csv has missing values in Western Great Basin column"
          ],
          [
            "nifc_suppression_costs.csv has malformed headers - appears to have data in header row",
            "Column names in nifc_suppression_costs.csv contain tab-separated values and mixed data",
            "First column in nifc_suppression_costs.csv appears to be Year but is named '1985\\t82'",
            "nifc_suppression_costs.csv needs complete re-parsing to extract proper columns"
          ],
          [
            "Column names in nifc_suppression_costs.csv are not clearly defined and contain combined data. Need to parse the year from the first column.",
            "The suppression cost data is spread across multiple columns and needs to be aggregated."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "suppression_cost": "dollars",
          "total_cost": "dollars",
          "acres": "acres",
          "total": "acres",
          "alaska": "acres",
          "northwest": "acres",
          "northern california": "acres",
          "southern california": "acres",
          "northern rockies": "acres",
          "great basin": "acres",
          "western great basin": "acres",
          "southwest": "acres",
          "rocky mountains": "acres",
          "eastern area": "acres",
          "southern area": "acres",
          "suppression_cost_column": "dollars",
          "cost_per_acre": "dollars_per_acre",
          "suppression costs": "USD"
        },
        "confidence": 0.3703703703703703,
        "votes": [
          {
            "Suppression_Cost": "dollars",
            "Total_Cost": "dollars",
            "Acres": "acres",
            "Total": "acres",
            "Alaska": "acres",
            "Northwest": "acres",
            "Northern California": "acres",
            "Southern California": "acres",
            "Northern Rockies": "acres",
            "Great Basin": "acres",
            "Western Great Basin": "acres",
            "Southwest": "acres",
            "Rocky Mountains": "acres",
            "Eastern Area": "acres",
            "Southern Area": "acres"
          },
          {
            "Total": "acres",
            "suppression_cost_column": "dollars",
            "cost_per_acre": "dollars_per_acre"
          },
          {
            "Total": "acres",
            "Suppression Costs": "USD"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Cost columns contain dollar signs and commas that need removal",
          "Acres columns contain commas that need removal",
          "nifc_suppression_costs.csv appears to have truncated values (e.g., $202,778,0 instead of $202,778,000)",
          "Suppression costs appear to be in dollars with values like $161,505,000",
          "Acres are in whole numbers",
          "Final answer must be rounded to nearest cent (2 decimal places)",
          "Suppression costs are in dollars, but represented as strings with '$' and commas. Need to convert to numeric.",
          "Acre values are integers, but may need to be treated as floats for cost per acre calculation."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Cost columns contain dollar signs and commas that need removal",
            "Acres columns contain commas that need removal",
            "nifc_suppression_costs.csv appears to have truncated values (e.g., $202,778,0 instead of $202,778,000)"
          ],
          [
            "Suppression costs appear to be in dollars with values like $161,505,000",
            "Acres are in whole numbers",
            "Final answer must be rounded to nearest cent (2 decimal places)"
          ],
          [
            "Suppression costs are in dollars, but represented as strings with '$' and commas. Need to convert to numeric.",
            "Acre values are integers, but may need to be treated as floats for cost per acre calculation."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Year ranges differ between files (1985-2010 vs 2005-2024)",
          "nifc_suppression_costs.csv has inconsistent formatting with truncated zeros in cost columns",
          "Year ranges may not overlap completely between files",
          "nifc_suppression_costs.csv shows years 1985-2010 with gaps",
          "nifc_human_caused_acres.csv shows years 2005-2024"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year ranges differ between files (1985-2010 vs 2005-2024)",
            "nifc_suppression_costs.csv has inconsistent formatting with truncated zeros in cost columns"
          ],
          [
            "Year ranges may not overlap completely between files",
            "nifc_suppression_costs.csv shows years 1985-2010 with gaps",
            "nifc_human_caused_acres.csv shows years 2005-2024"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 0.6666666666666666,
        "votes": [
          ",",
          "tab-separated or comma-separated depending on file",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.111111111111111,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " "
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.7272727272727273,
        "votes": [
          10.0,
          10.0,
          13.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Cost values must be positive",
          "Acreage values must be positive",
          "Cost per acre must be calculated as Total_Cost / Total_Acres",
          "Only years present in both datasets can be analyzed",
          "Only include human-caused fires (use nifc_human_caused_acres.csv Total column)",
          "Years must match between both datasets",
          "Suppression costs must be positive",
          "Acres must be positive and non-zero to avoid division errors",
          "Year must be a valid year (e.g., between 1985 and 2024).",
          "Suppression costs must be non-negative.",
          "Acres burned must be non-negative."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Cost values must be positive",
            "Acreage values must be positive",
            "Cost per acre must be calculated as Total_Cost / Total_Acres",
            "Only years present in both datasets can be analyzed"
          ],
          [
            "Only include human-caused fires (use nifc_human_caused_acres.csv Total column)",
            "Years must match between both datasets",
            "Suppression costs must be positive",
            "Acres must be positive and non-zero to avoid division errors"
          ],
          [
            "Year must be a valid year (e.g., between 1985 and 2024).",
            "Suppression costs must be non-negative.",
            "Acres burned must be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove dollar signs and commas from cost columns",
          "Remove commas from acreage columns",
          "Fix truncated zeros in nifc_suppression_costs.csv cost columns",
          "Sum regional acreages to get total human-caused acres per year",
          "Filter to years present in both datasets",
          "Exclude years with zero or missing acres",
          "Exclude years with missing suppression cost data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove dollar signs and commas from cost columns",
            "Remove commas from acreage columns",
            "Fix truncated zeros in nifc_suppression_costs.csv cost columns",
            "Sum regional acreages to get total human-caused acres per year"
          ],
          [
            "Filter to years present in both datasets",
            "Exclude years with zero or missing acres",
            "Exclude years with missing suppression cost data"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing years in overlap period",
          "Validate cost per acre calculation for all years",
          "Identify outliers in cost per acre values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing years in overlap period",
            "Validate cost per acre calculation for all years",
            "Identify outliers in cost per acre values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Cost per acre must be rounded to nearest cent",
          "Output should include year and cost per acre value",
          "Return the year with highest cost per acre",
          "Return the cost per acre rounded to nearest cent (2 decimal places)",
          "Format: $X.XX per acre",
          "Cost per acre should be rounded to the nearest cent (two decimal places)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Cost per acre must be rounded to nearest cent",
            "Output should include year and cost per acre value"
          ],
          [
            "Return the year with highest cost per acre",
            "Return the cost per acre rounded to nearest cent (2 decimal places)",
            "Format: $X.XX per acre"
          ],
          [
            "Cost per acre should be rounded to the nearest cent (two decimal places)."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.617382154882155
  },
  "wildfire-hard-5": {
    "m_q": {
      "target_metric": {
        "value": "Average annual difference in number of fires reported by NOAA compared to NIFC since 2000, rounded to nearest whole number",
        "confidence": 0.3333333333333333,
        "votes": [
          "Average annual difference in number of fires reported by NOAA compared to NIFC since 2000, rounded to nearest whole number",
          "Average difference in annual number of fires reported by NOAA compared to NIFC since 2000, rounded to nearest whole number",
          "Average difference in annual reported fires between NOAA and NIFC since 2000"
        ]
      },
      "filters": {
        "value": [
          "Date >= 200001",
          "Year >= 2000"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Date >= 200001",
            "Year >= 2000"
          ],
          [
            "Year >= 2000"
          ],
          [
            "Year >= 2000"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year",
          "Source"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Year"
          ],
          [
            "Year",
            "Source"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the annual total number of fires from NOAA monthly data?",
          "What is the annual number of fires from NIFC data?",
          "What is the difference between NOAA and NIFC annual fire counts for each year since 2000?",
          "What is the average of these annual differences?",
          "What is the total number of fires per year from NOAA data since 2000?",
          "What is the total number of fires per year from NIFC data since 2000?",
          "What is the difference between NOAA and NIFC fire counts for each year?",
          "What is the average of these yearly differences?",
          "Calculate the annual number of fires reported by NOAA since 2000.",
          "Calculate the annual number of fires reported by NIFC since 2000.",
          "Calculate the difference between NOAA and NIFC annual fire counts for each year since 2000.",
          "Calculate the average of the annual differences."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the annual total number of fires from NOAA monthly data?",
            "What is the annual number of fires from NIFC data?",
            "What is the difference between NOAA and NIFC annual fire counts for each year since 2000?",
            "What is the average of these annual differences?"
          ],
          [
            "What is the total number of fires per year from NOAA data since 2000?",
            "What is the total number of fires per year from NIFC data since 2000?",
            "What is the difference between NOAA and NIFC fire counts for each year?",
            "What is the average of these yearly differences?"
          ],
          [
            "Calculate the annual number of fires reported by NOAA since 2000.",
            "Calculate the annual number of fires reported by NIFC since 2000.",
            "Calculate the difference between NOAA and NIFC annual fire counts for each year since 2000.",
            "Calculate the average of the annual differences."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires_monthly_stats.csv",
          "nifc_wildfires.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "noaa_wildfires_monthly_stats.csv",
            "nifc_wildfires.csv"
          ],
          [
            "noaa_wildfires_monthly_stats.csv",
            "nifc_wildfires.csv"
          ],
          [
            "noaa_wildfires_monthly_stats.csv",
            "nifc_wildfires.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "NOAA data has monthly granularity while NIFC data appears to be annual",
          "NOAA Date column is in YYYYMM format while NIFC year column needs extraction",
          "NIFC column names appear corrupted with tab characters and mixed data types",
          "NOAA data has monthly granularity with Date column (YYYYMM format), NIFC has annual granularity",
          "NIFC column names are malformed with tab-separated values in column name '2024\\t64'",
          "NIFC first column contains both year and fire count data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "NOAA data has monthly granularity while NIFC data appears to be annual",
            "NOAA Date column is in YYYYMM format while NIFC year column needs extraction",
            "NIFC column names appear corrupted with tab characters and mixed data types"
          ],
          [
            "NOAA data has monthly granularity with Date column (YYYYMM format), NIFC has annual granularity",
            "NIFC column names are malformed with tab-separated values in column name '2024\\t64'",
            "NIFC first column contains both year and fire count data"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "date": "YYYYMM integer",
          "acres burned": "acres",
          "number of fires": "count",
          "acres burned per fire": "acres per fire"
        },
        "confidence": 0.8333333333333333,
        "votes": [
          {
            "Date": "YYYYMM integer",
            "Acres Burned": "acres",
            "Number of Fires": "count of fires",
            "Acres Burned per Fire": "acres per fire"
          },
          {
            "Number of Fires": "count",
            "Acres Burned": "acres",
            "Acres Burned per Fire": "acres per fire",
            "Date": "YYYYMM integer format"
          },
          {
            "Acres Burned": "acres",
            "Number of Fires": "count"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "NOAA data is monthly, NIFC data appears to be annual - need to aggregate NOAA to annual totals",
          "NIFC data appears to combine year, fire count, and acres in single column with tab separation",
          "NOAA data is monthly, needs aggregation to annual level",
          "NIFC data is already annual"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "NOAA data is monthly, NIFC data appears to be annual - need to aggregate NOAA to annual totals",
            "NIFC data appears to combine year, fire count, and acres in single column with tab separation"
          ],
          [
            "NOAA data is monthly, needs aggregation to annual level",
            "NIFC data is already annual"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "NOAA reports monthly fire counts while NIFC appears to report annual totals directly",
          "NIFC data format suggests fire counts and acres are combined in same column with tab separator",
          "Different reporting granularity: NOAA monthly vs NIFC annual",
          "Potential differences in fire counting methodology between agencies"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "NOAA reports monthly fire counts while NIFC appears to report annual totals directly",
            "NIFC data format suggests fire counts and acres are combined in same column with tab separator"
          ],
          [
            "Different reporting granularity: NOAA monthly vs NIFC annual",
            "Potential differences in fire counting methodology between agencies"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "Missing:-999",
          "*",
          "-999",
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.611111111111111,
        "votes": [
          [
            "Missing:-999",
            "*",
            "-999"
          ],
          [
            "NA",
            "N/A",
            "",
            "-999",
            "*"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only include years >= 2000",
          "NOAA data needs annual aggregation from monthly values",
          "NIFC data needs parsing to extract year and fire count from combined columns",
          "Round final result to nearest whole number",
          "Must aggregate NOAA monthly data to annual totals",
          "Must parse NIFC tab-delimited first column to extract year and fire count",
          "Final answer must be rounded to nearest whole number",
          "Calculate NOAA minus NIFC for comparison",
          "noaa_wildfires_monthly_stats.csv: Need to extract the year from the 'Date' column.",
          "nifc_wildfires.csv: Need to parse the year and number of fires from the first two columns.",
          "nifc_wildfires.csv: The number of fires column contains text and special characters that need to be removed."
        ],
        "confidence": 0.36363636363636365,
        "votes": [
          [
            "Only include years >= 2000",
            "NOAA data needs annual aggregation from monthly values",
            "NIFC data needs parsing to extract year and fire count from combined columns",
            "Round final result to nearest whole number"
          ],
          [
            "Only include years >= 2000",
            "Must aggregate NOAA monthly data to annual totals",
            "Must parse NIFC tab-delimited first column to extract year and fire count",
            "Final answer must be rounded to nearest whole number",
            "Calculate NOAA minus NIFC for comparison"
          ],
          [
            "noaa_wildfires_monthly_stats.csv: Need to extract the year from the 'Date' column.",
            "nifc_wildfires.csv: Need to parse the year and number of fires from the first two columns.",
            "nifc_wildfires.csv: The number of fires column contains text and special characters that need to be removed."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter NOAA data where Date >= 200001",
          "Filter NIFC data where Year >= 2000",
          "Exclude rows with sentinel values like '*', '-999'",
          "Extract year from NOAA Date field: year = Date // 100",
          "Filter NOAA data where year >= 2000",
          "Year >= 2000"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Filter NOAA data where Date >= 200001",
            "Filter NIFC data where Year >= 2000",
            "Exclude rows with sentinel values like '*', '-999'"
          ],
          [
            "Extract year from NOAA Date field: year = Date // 100",
            "Filter NOAA data where year >= 2000",
            "Filter NIFC data where year >= 2000"
          ],
          [
            "Year >= 2000"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing years in either dataset",
          "Validate that NOAA annual sums are reasonable compared to NIFC annual values",
          "Check for outliers in annual differences",
          "Compute annual sum of 'Number of Fires' from NOAA for each year",
          "Extract annual fire count from NIFC first column",
          "Calculate difference: NOAA_annual - NIFC_annual for each year",
          "Compute mean of differences across all years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing years in either dataset",
            "Validate that NOAA annual sums are reasonable compared to NIFC annual values",
            "Check for outliers in annual differences"
          ],
          [
            "Compute annual sum of 'Number of Fires' from NOAA for each year",
            "Extract annual fire count from NIFC first column",
            "Calculate difference: NOAA_annual - NIFC_annual for each year",
            "Compute mean of differences across all years"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Final output should be a single integer rounded to nearest whole number",
          "Calculation: average(NOAA_annual_fires - NIFC_annual_fires) for years >= 2000",
          "Return single integer value",
          "Round to nearest whole number using standard rounding rules",
          "Positive value indicates NOAA reports more fires, negative indicates fewer",
          "Round the final answer to the nearest whole number."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Final output should be a single integer rounded to nearest whole number",
            "Calculation: average(NOAA_annual_fires - NIFC_annual_fires) for years >= 2000"
          ],
          [
            "Return single integer value",
            "Round to nearest whole number using standard rounding rules",
            "Positive value indicates NOAA reports more fires, negative indicates fewer"
          ],
          [
            "Round the final answer to the nearest whole number."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6348484848484849
  },
  "wildfire-hard-6": {
    "m_q": {
      "target_metric": {
        "value": "Pearson correlation coefficient between (1) annual difference in number of fires (NOAA minus NIFC) and (2) annual difference in acres burned (NOAA minus NIFC)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Pearson correlation coefficient between (1) annual difference in number of fires (NOAA minus NIFC) and (2) annual difference in acres burned (NOAA minus NIFC)",
          "Pearson correlation coefficient between annual difference in number of fires (NOAA - NIFC) and annual difference in acres burned (NOAA - NIFC)",
          "Pearson correlation between the annual difference in number of fires reported by NOAA and NIFC, and the annual difference in acres burned reported by NOAA and NIFC."
        ]
      },
      "filters": {
        "value": [
          "Annual aggregation required",
          "Data must be available for same years in both sources",
          "Years where both NOAA and NIFC data exist"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Annual aggregation required",
            "Data must be available for same years in both sources"
          ],
          [
            "Years where both NOAA and NIFC data exist"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Year"
          ],
          [
            "Year"
          ],
          [
            "year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to extract year from NOAA monthly Date column (format YYYYMM)?",
          "How to parse NIFC data with inconsistent formatting?",
          "How to handle missing or malformed values in NIFC data?",
          "What are the annual totals of Number of Fires from NOAA data?",
          "What are the annual totals of Acres Burned from NOAA data?",
          "What are the annual Number of Fires from NIFC data?",
          "What are the annual Acres Burned from NIFC data?",
          "What is the difference between NOAA and NIFC Number of Fires per year?",
          "What is the difference between NOAA and NIFC Acres Burned per year?",
          "What is the correlation between these two difference series?",
          "Calculate the annual number of fires reported by NOAA.",
          "Calculate the annual acres burned reported by NOAA.",
          "Calculate the annual number of fires reported by NIFC.",
          "Calculate the annual acres burned reported by NIFC.",
          "Calculate the annual difference in number of fires (NOAA - NIFC).",
          "Calculate the annual difference in acres burned (NOAA - NIFC)."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "How to extract year from NOAA monthly Date column (format YYYYMM)?",
            "How to parse NIFC data with inconsistent formatting?",
            "How to handle missing or malformed values in NIFC data?"
          ],
          [
            "What are the annual totals of Number of Fires from NOAA data?",
            "What are the annual totals of Acres Burned from NOAA data?",
            "What are the annual Number of Fires from NIFC data?",
            "What are the annual Acres Burned from NIFC data?",
            "What is the difference between NOAA and NIFC Number of Fires per year?",
            "What is the difference between NOAA and NIFC Acres Burned per year?",
            "What is the correlation between these two difference series?"
          ],
          [
            "Calculate the annual number of fires reported by NOAA.",
            "Calculate the annual acres burned reported by NOAA.",
            "Calculate the annual number of fires reported by NIFC.",
            "Calculate the annual acres burned reported by NIFC.",
            "Calculate the annual difference in number of fires (NOAA - NIFC).",
            "Calculate the annual difference in acres burned (NOAA - NIFC)."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "noaa_wildfires_monthly_stats.csv",
          "nifc_wildfires.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "noaa_wildfires_monthly_stats.csv",
            "nifc_wildfires.csv"
          ],
          [
            "noaa_wildfires_monthly_stats.csv",
            "nifc_wildfires.csv"
          ],
          [
            "noaa_wildfires_monthly_stats.csv",
            "nifc_wildfires.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "NOAA data is monthly, NIFC appears annual",
          "NOAA Date format is YYYYMM integer, NIFC year format unclear",
          "NIFC column names appear corrupted with tabs and numbers",
          "NOAA data is monthly with Date as YYYYMM format, NIFC data is annual",
          "NIFC column names appear malformed with tab characters in header",
          "NIFC first column appears to contain Year and Fire count combined",
          "Column names are inconsistent across datasets (e.g., 'Acres Burned' vs implicit columns in nifc_wildfires.csv).",
          "nifc_wildfires.csv has combined columns that need to be split."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "NOAA data is monthly, NIFC appears annual",
            "NOAA Date format is YYYYMM integer, NIFC year format unclear",
            "NIFC column names appear corrupted with tabs and numbers"
          ],
          [
            "NOAA data is monthly with Date as YYYYMM format, NIFC data is annual",
            "NIFC column names appear malformed with tab characters in header",
            "NIFC first column appears to contain Year and Fire count combined"
          ],
          [
            "Column names are inconsistent across datasets (e.g., 'Acres Burned' vs implicit columns in nifc_wildfires.csv).",
            "nifc_wildfires.csv has combined columns that need to be split."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "acres burned (noaa)": "acres",
          "number of fires (noaa)": "count",
          "acres burned per fire (noaa)": "acres per fire",
          "acres burned (nifc)": "acres (likely)",
          "number of fires (nifc)": "count (likely)",
          "acres burned": "acres",
          "number of fires": "count",
          "acres burned per fire": "acres per fire",
          "date": "YYYYMM integer format",
          "noaa_acres_burned": "acres",
          "noaa_number_of_fires": "count",
          "nifc_acres_burned": "acres",
          "nifc_number_of_fires": "count"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {
            "Acres Burned (NOAA)": "acres",
            "Number of Fires (NOAA)": "count",
            "Acres Burned per Fire (NOAA)": "acres per fire",
            "Acres Burned (NIFC)": "acres (likely)",
            "Number of Fires (NIFC)": "count (likely)"
          },
          {
            "Acres Burned": "acres",
            "Number of Fires": "count",
            "Acres Burned per Fire": "acres per fire",
            "Date": "YYYYMM integer format"
          },
          {
            "NOAA_Acres_Burned": "acres",
            "NOAA_Number_of_Fires": "count",
            "NIFC_Acres_Burned": "acres",
            "NIFC_Number_of_Fires": "count"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "NIFC data appears to use commas as thousands separators within numbers",
          "NIFC column 3 (924) and column 4 (884) meaning unclear - possibly acres burned with formatting issues",
          "NOAA data is monthly and needs aggregation to annual",
          "NIFC acres values appear split across multiple columns with comma separators"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "NIFC data appears to use commas as thousands separators within numbers",
            "NIFC column 3 (924) and column 4 (884) meaning unclear - possibly acres burned with formatting issues"
          ],
          [
            "NOAA data is monthly and needs aggregation to annual",
            "NIFC acres values appear split across multiple columns with comma separators"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "NOAA reports monthly, NIFC appears annual - need annual aggregation for NOAA",
          "NIFC data formatting inconsistent with commas and asterisks in values",
          "Different temporal granularity: NOAA is monthly, NIFC is annual",
          "NIFC data format suggests tab-delimited parsing issue resulting in split numeric values",
          "Potential differences in reporting methodologies between NOAA and NIFC."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "NOAA reports monthly, NIFC appears annual - need annual aggregation for NOAA",
            "NIFC data formatting inconsistent with commas and asterisks in values"
          ],
          [
            "Different temporal granularity: NOAA is monthly, NIFC is annual",
            "NIFC data format suggests tab-delimited parsing issue resulting in split numeric values"
          ],
          [
            "Potential differences in reporting methodologies between NOAA and NIFC."
          ]
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "Missing:-999",
          "*",
          "NA",
          "N/A",
          "",
          "-999"
        ],
        "confidence": 0.7222222222222222,
        "votes": [
          [
            "Missing:-999",
            "*",
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            "",
            "-999",
            "*"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 4.0,
        "confidence": 1.0,
        "votes": [
          4.0,
          4.0,
          4.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Correlation must be calculated on annual basis",
          "Result must be to three decimal places",
          "Only years with data in both sources can be used",
          "Output correlation coefficient to exactly three decimal places",
          "Only include years where both NOAA and NIFC data are available",
          "NOAA monthly data must be aggregated to annual totals before comparison",
          "Calculate differences as NOAA minus NIFC for both fires and acres",
          "Ensure that the years are consistent across both datasets.",
          "Handle missing data appropriately."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Correlation must be calculated on annual basis",
            "Result must be to three decimal places",
            "Only years with data in both sources can be used"
          ],
          [
            "Output correlation coefficient to exactly three decimal places",
            "Only include years where both NOAA and NIFC data are available",
            "NOAA monthly data must be aggregated to annual totals before comparison",
            "Calculate differences as NOAA minus NIFC for both fires and acres"
          ],
          [
            "Ensure that the years are consistent across both datasets.",
            "Handle missing data appropriately."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract year from NOAA Date column (first 4 digits)",
          "Clean NIFC data: remove commas, handle asterisks, parse tab-separated values",
          "Aggregate NOAA monthly data to annual totals",
          "Extract year from NOAA Date column (first 4 digits of YYYYMM)",
          "Filter to complete years only (all 12 months present for NOAA data)",
          "Remove or correct malformed NIFC records with asterisks or incorrect comma placement"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract year from NOAA Date column (first 4 digits)",
            "Clean NIFC data: remove commas, handle asterisks, parse tab-separated values",
            "Aggregate NOAA monthly data to annual totals"
          ],
          [
            "Extract year from NOAA Date column (first 4 digits of YYYYMM)",
            "Filter to complete years only (all 12 months present for NOAA data)",
            "Remove or correct malformed NIFC records with asterisks or incorrect comma placement"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Pearson correlation test",
          "Check for normality of differences",
          "Pearson correlation between fire count differences and acres burned differences",
          "Pearson correlation coefficient"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pearson correlation test",
            "Check for normality of differences"
          ],
          [
            "Pearson correlation between fire count differences and acres burned differences"
          ],
          [
            "Pearson correlation coefficient"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single correlation coefficient to three decimal places",
          "Annual data table showing differences for verification",
          "Single numeric value rounded to 3 decimal places",
          "Format: X.XXX",
          "Report the correlation coefficient to three decimal places."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single correlation coefficient to three decimal places",
            "Annual data table showing differences for verification"
          ],
          [
            "Single numeric value rounded to 3 decimal places",
            "Format: X.XXX"
          ],
          [
            "Report the correlation coefficient to three decimal places."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6194444444444445
  },
  "wildfire-hard-7": {
    "m_q": {
      "target_metric": {
        "value": "Z-score of total acres burned per geographic area per year compared to historical annual average for that area",
        "confidence": 0.3333333333333333,
        "votes": [
          "Z-score of total acres burned per geographic area per year compared to historical annual average for that area",
          "Geographic area with the most anomalous year by Z-score of total acres burned compared to its historical annual average",
          "Geographic area and year with the most anomalous total acres burned (human and lightning caused) based on Z-score compared to its historical average."
        ]
      },
      "filters": {
        "value": [
          "Year range: 2005-2024 (based on data availability)",
          "Geographic areas: All 11 regions (excluding 'Total' column)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year range: 2005-2024 (based on data availability)",
            "Geographic areas: All 11 regions (excluding 'Total' column)"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Geographic area (region)",
          "Year",
          "geographic_area"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "Geographic area (region)",
            "Year"
          ],
          [
            "geographic_area",
            "Year"
          ],
          [
            "Year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the historical average acres burned per geographic area?",
          "What is the standard deviation of acres burned per geographic area?",
          "How to calculate Z-score for each year per area: (year_value - area_mean) / area_std",
          "Which area-year combination has the maximum absolute Z-score?",
          "Should we use combined human-caused and lightning-caused acres or analyze separately?",
          "What is the total acres burned per geographic area per year (sum of human-caused and lightning-caused)?",
          "What is the historical annual average of total acres burned for each geographic area?",
          "What is the standard deviation of annual acres burned for each geographic area?",
          "What is the Z-score for each geographic area-year combination?",
          "Which geographic area-year combination has the maximum absolute Z-score?",
          "Calculate total acres burned per geographic area per year by summing human and lightning caused acres.",
          "Calculate the historical average acres burned for each geographic area.",
          "Calculate the Z-score for each year and geographic area based on total acres burned.",
          "Identify the geographic area and year with the highest absolute Z-score."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the historical average acres burned per geographic area?",
            "What is the standard deviation of acres burned per geographic area?",
            "How to calculate Z-score for each year per area: (year_value - area_mean) / area_std",
            "Which area-year combination has the maximum absolute Z-score?",
            "Should we use combined human-caused and lightning-caused acres or analyze separately?"
          ],
          [
            "What is the total acres burned per geographic area per year (sum of human-caused and lightning-caused)?",
            "What is the historical annual average of total acres burned for each geographic area?",
            "What is the standard deviation of annual acres burned for each geographic area?",
            "What is the Z-score for each geographic area-year combination?",
            "Which geographic area-year combination has the maximum absolute Z-score?"
          ],
          [
            "Calculate total acres burned per geographic area per year by summing human and lightning caused acres.",
            "Calculate the historical average acres burned for each geographic area.",
            "Calculate the Z-score for each year and geographic area based on total acres burned.",
            "Identify the geographic area and year with the highest absolute Z-score."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "nifc_human_caused_acres.csv",
          "nifc_lightning_caused_acres.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "nifc_human_caused_acres.csv",
            "nifc_lightning_caused_acres.csv"
          ],
          [
            "nifc_human_caused_acres.csv",
            "nifc_lightning_caused_acres.csv"
          ],
          [
            "nifc_human_caused_acres.csv",
            "nifc_lightning_caused_acres.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column name mismatch: 'Western Great Basin' vs 'Western Great Basin*'",
          "Missing values represented by empty strings in human-caused data",
          "Year 38 appears in lightning data (likely typo for 2018)",
          "Column name mismatch: 'Western Great Basin' in human-caused vs 'Western Great Basin*' in lightning-caused",
          "Column 'Western Great Basin' in nifc_human_caused_acres.csv and 'Western Great Basin*' in nifc_lightning_caused_acres.csv have slightly different names but represent the same geographic area. The asterisk should be removed or the column renamed for consistency."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column name mismatch: 'Western Great Basin' vs 'Western Great Basin*'",
            "Missing values represented by empty strings in human-caused data",
            "Year 38 appears in lightning data (likely typo for 2018)"
          ],
          [
            "Column name mismatch: 'Western Great Basin' in human-caused vs 'Western Great Basin*' in lightning-caused"
          ],
          [
            "Column 'Western Great Basin' in nifc_human_caused_acres.csv and 'Western Great Basin*' in nifc_lightning_caused_acres.csv have slightly different names but represent the same geographic area. The asterisk should be removed or the column renamed for consistency."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "calendar year",
          "alaska": "acres",
          "northwest": "acres",
          "northern california": "acres",
          "southern california": "acres",
          "northern rockies": "acres",
          "great basin": "acres",
          "western great basin": "acres",
          "southwest": "acres",
          "rocky mountains": "acres",
          "eastern area": "acres",
          "southern area": "acres",
          "total": "acres"
        },
        "confidence": 0.9487179487179487,
        "votes": [
          {
            "Year": "calendar year",
            "Alaska": "acres burned",
            "Northwest": "acres burned",
            "Northern California": "acres burned",
            "Southern California": "acres burned",
            "Northern Rockies": "acres burned",
            "Great Basin": "acres burned",
            "Western Great Basin": "acres burned",
            "Southwest": "acres burned",
            "Rocky Mountains": "acres burned",
            "Eastern Area": "acres burned",
            "Southern Area": "acres burned",
            "Total": "acres burned"
          },
          {
            "Alaska": "acres",
            "Northwest": "acres",
            "Northern California": "acres",
            "Southern California": "acres",
            "Northern Rockies": "acres",
            "Great Basin": "acres",
            "Western Great Basin": "acres",
            "Southwest": "acres",
            "Rocky Mountains": "acres",
            "Eastern Area": "acres",
            "Southern Area": "acres",
            "Total": "acres"
          },
          {
            "Alaska": "acres",
            "Northwest": "acres",
            "Northern California": "acres",
            "Southern California": "acres",
            "Northern Rockies": "acres",
            "Great Basin": "acres",
            "Western Great Basin": "acres",
            "Southwest": "acres",
            "Rocky Mountains": "acres",
            "Eastern Area": "acres",
            "Southern Area": "acres",
            "Total": "acres"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Values range from single digits to millions of acres",
          "Some years have missing data (empty strings)",
          "Western Great Basin has float values while others are integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Values range from single digits to millions of acres",
            "Some years have missing data (empty strings)",
            "Western Great Basin has float values while others are integers"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Year 2020 shows identical values for Alaska in both files (180,885 acres) - may be data entry error or actual coincidence",
          "Total columns differ between files - should be summed for combined analysis",
          "Human-caused 2020 Alaska value (180885) equals lightning-caused 2020 Alaska value, suggesting potential data duplication"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year 2020 shows identical values for Alaska in both files (180,885 acres) - may be data entry error or actual coincidence",
            "Total columns differ between files - should be summed for combined analysis"
          ],
          [
            "Human-caused 2020 Alaska value (180885) equals lightning-caused 2020 Alaska value, suggesting potential data duplication"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 13.0,
        "confidence": 1.0,
        "votes": [
          13.0,
          13.0,
          13.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year must be between 2005-2024",
          "Acres burned cannot be negative",
          "Total should equal sum of regional values",
          "Z-score calculation requires at least 2 years of data per area",
          "Year values should be valid years (2005-2024 range expected)",
          "Acres burned values should be non-negative",
          "Z-score calculation requires at least 2 data points per geographic area",
          "Years should be consistent across both datasets.",
          "Acres burned should be non-negative."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year must be between 2005-2024",
            "Acres burned cannot be negative",
            "Total should equal sum of regional values",
            "Z-score calculation requires at least 2 years of data per area"
          ],
          [
            "Year values should be valid years (2005-2024 range expected)",
            "Acres burned values should be non-negative",
            "Z-score calculation requires at least 2 data points per geographic area"
          ],
          [
            "Years should be consistent across both datasets.",
            "Acres burned should be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude 'Total' column from geographic area analysis",
          "Handle missing values before statistical calculations",
          "Fix Year 38 to 2018 in lightning data",
          "Combine human-caused and lightning-caused acres per area per year",
          "Exclude 'Total' column from geographic area analysis to avoid double-counting",
          "Handle Year 38 anomaly in lightning-caused data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude 'Total' column from geographic area analysis",
            "Handle missing values before statistical calculations",
            "Fix Year 38 to 2018 in lightning data",
            "Combine human-caused and lightning-caused acres per area per year"
          ],
          [
            "Exclude 'Total' column from geographic area analysis to avoid double-counting",
            "Handle Year 38 anomaly in lightning-caused data"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Calculate mean and standard deviation per geographic area across years",
          "Compute Z-scores: (x - \u03bc)/\u03c3 for each area-year",
          "Find maximum absolute Z-score across all area-year combinations",
          "Calculate Z-score = (value - mean) / standard_deviation for each geographic area across all years",
          "Identify maximum absolute Z-score across all geographic area-year combinations",
          "Z-score calculation requires a mean and standard deviation for each geographic area."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate mean and standard deviation per geographic area across years",
            "Compute Z-scores: (x - \u03bc)/\u03c3 for each area-year",
            "Find maximum absolute Z-score across all area-year combinations"
          ],
          [
            "Calculate Z-score = (value - mean) / standard_deviation for each geographic area across all years",
            "Identify maximum absolute Z-score across all geographic area-year combinations"
          ],
          [
            "Z-score calculation requires a mean and standard deviation for each geographic area."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Geographic area and year separated by comma",
          "Single answer format: 'Area Name, YYYY'",
          "Output format: <geographic_area>, <year>",
          "Geographic area should match column name from source data",
          "Year should be 4-digit integer",
          "Output should be a string: 'geographic area, year'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Geographic area and year separated by comma",
            "Single answer format: 'Area Name, YYYY'"
          ],
          [
            "Output format: <geographic_area>, <year>",
            "Geographic area should match column name from source data",
            "Year should be 4-digit integer"
          ],
          [
            "Output should be a string: 'geographic area, year'"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.641880341880342
  },
  "dbbench-000": {
    "m_q": {
      "target_metric": {
        "value": "Notes values",
        "confidence": 0.6666666666666666,
        "votes": [
          "Notes values",
          "Notes values",
          "List of Notes when the Method is 'Decision'"
        ]
      },
      "filters": {
        "value": [
          "Method == 'Decision'",
          "Method equals 'Decision'",
          "Method = 'Decision'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Method == 'Decision'"
          ],
          [
            "Method equals 'Decision'"
          ],
          [
            "Method = 'Decision'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the specific Notes entries when Method is 'Decision'?",
          "How many matches were decided by Decision?",
          "What events had Decision outcomes?",
          "Which rows have Method equal to 'Decision'?",
          "What are the corresponding Notes values for those rows?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the specific Notes entries when Method is 'Decision'?",
            "How many matches were decided by Decision?",
            "What events had Decision outcomes?"
          ],
          [
            "Which rows have Method equal to 'Decision'?",
            "What are the corresponding Notes values for those rows?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Jiu-Jitsu Championships Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Jiu-Jitsu Championships Results"
          ],
          [
            "Jiu-Jitsu Championships Results"
          ],
          [
            "Jiu-Jitsu Championships Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "result": "categorical (Win/Loss)",
          "opponent": "text (person name)",
          "method": "categorical (Points/Submission/Decision)",
          "event": "text (event name)",
          "notes": "text (competition details)"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {
            "Result": "categorical (Win/Loss)",
            "Opponent": "text (person name)",
            "Method": "categorical (Points/Submission/Decision)",
            "Event": "text (event name)",
            "Notes": "text (competition details)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Method column contains categorical values like 'Decision', 'Points', 'Submission'",
          "Notes column contains descriptive text about match context",
          "Result column contains 'Win' or 'Loss' values",
          "Method must equal 'Decision' (case-sensitive match)",
          "Notes column must be non-null for filtered results"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Method column contains categorical values like 'Decision', 'Points', 'Submission'",
            "Notes column contains descriptive text about match context",
            "Result column contains 'Win' or 'Loss' values"
          ],
          [
            "Method must equal 'Decision' (case-sensitive match)",
            "Notes column must be non-null for filtered results"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Method exactly equals 'Decision' (case-sensitive)",
          "Exclude rows where Notes is null or empty",
          "Filter rows where Method == 'Decision'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Method exactly equals 'Decision' (case-sensitive)",
            "Exclude rows where Notes is null or empty"
          ],
          [
            "Filter rows where Method == 'Decision'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count of Decision outcomes",
          "Frequency distribution of Notes for Decision method"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count of Decision outcomes",
            "Frequency distribution of Notes for Decision method"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of Notes values",
          "Maintain original text formatting from Notes column",
          "Return list of Notes values where Method is 'Decision'",
          "Preserve original text formatting of Notes"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of Notes values",
            "Maintain original text formatting from Notes column"
          ],
          [
            "Return list of Notes values where Method is 'Decision'",
            "Preserve original text formatting of Notes"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5333333333333334
  },
  "dbbench-001": {
    "m_q": {
      "target_metric": {
        "value": "Identify tournament(s) where the 2007 result is 'sf' (semifinals), the 2008 result is 'sf' (semifinals), and the 2010 result is 'f' (final)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Identify tournament(s) where the 2007 result is 'sf' (semifinals), the 2008 result is 'sf' (semifinals), and the 2010 result is 'f' (final)",
          "Value in the 2007 column",
          "Tournament names that satisfy specific year results"
        ]
      },
      "filters": {
        "value": [
          "2007 == 'SF'",
          "2008 == 'SF'",
          "2010 == 'F'",
          "2008 column equals 'sf' or 'SF'",
          "2010 column equals 'f' or 'F'",
          "2008 equals 'sf'",
          "2010 equals 'f'",
          "Year 2007"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "2007 == 'SF'",
            "2008 == 'SF'",
            "2010 == 'F'"
          ],
          [
            "2008 column equals 'sf' or 'SF'",
            "2010 column equals 'f' or 'F'"
          ],
          [
            "2008 equals 'sf'",
            "2010 equals 'f'",
            "Year 2007"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Tournament"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Tournament"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which tournaments have SF in 2007?",
          "Which of those also have SF in 2008?",
          "Which of those also have F in 2010?",
          "Which tournaments have 'sf' or 'SF' in the 2008 column?",
          "Which tournaments have 'f' or 'F' in the 2010 column?",
          "What is the intersection of these two sets?",
          "What are the corresponding values in the 2007 column for this intersection?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which tournaments have SF in 2007?",
            "Which of those also have SF in 2008?",
            "Which of those also have F in 2010?"
          ],
          [
            "Which tournaments have 'sf' or 'SF' in the 2008 column?",
            "Which tournaments have 'f' or 'F' in the 2010 column?",
            "What is the intersection of these two sets?",
            "What are the corresponding values in the 2007 column for this intersection?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Tournament Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Tournament Results"
          ],
          [
            "Tournament Results"
          ],
          [
            "Tournament Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "2005": "tournament round result",
          "2006": "tournament round result",
          "2007": "tournament round result",
          "2008": "tournament round result",
          "2009": "tournament round result",
          "2010": "tournament round result",
          "2011": "tournament round result",
          "2012": "tournament round result",
          "2013": "tournament round result",
          "win %": "percentage"
        },
        "confidence": 0.6666666666666667,
        "votes": [
          {
            "2005": "tournament round result",
            "2006": "tournament round result",
            "2007": "tournament round result",
            "2008": "tournament round result",
            "2009": "tournament round result",
            "2010": "tournament round result",
            "2011": "tournament round result",
            "2012": "tournament round result",
            "2013": "tournament round result",
            "Win %": "percentage"
          },
          {
            "2005": "tournament result abbreviation",
            "2006": "tournament result abbreviation",
            "2007": "tournament result abbreviation",
            "2008": "tournament result abbreviation",
            "2009": "tournament result abbreviation",
            "2010": "tournament result abbreviation",
            "2011": "tournament result abbreviation",
            "2012": "tournament result abbreviation",
            "2013": "tournament result abbreviation",
            "Win %": "percentage"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Win % column contains percentages but stored as strings with '%' symbol"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Win % column contains percentages but stored as strings with '%' symbol"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "\u2013",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "\u2013",
            "",
            "NA"
          ],
          [
            "\u2013",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Tournament names must be unique",
          "Year columns should contain valid tournament round codes",
          "Win % should be between 0 and 100",
          "Filter rows where 2008 column value matches 'sf' (case-insensitive)",
          "Filter rows where 2010 column value matches 'f' (case-insensitive)",
          "Exclude header or category rows that don't represent actual tournament entries",
          "The values in columns '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013' represent tournament results."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Tournament names must be unique",
            "Year columns should contain valid tournament round codes",
            "Win % should be between 0 and 100"
          ],
          [
            "Filter rows where 2008 column value matches 'sf' (case-insensitive)",
            "Filter rows where 2010 column value matches 'f' (case-insensitive)",
            "Exclude header or category rows that don't represent actual tournament entries"
          ],
          [
            "The values in columns '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013' represent tournament results."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Tournament != 'Grand Slam tournaments'",
          "Tournament != 'Win\u2013Loss'",
          "Apply case-insensitive matching for tournament result abbreviations",
          "Filter the 'Tournament Results' table for rows where '2008' is 'sf' and '2010' is 'f'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Tournament != 'Grand Slam tournaments'",
            "Tournament != 'Win\u2013Loss'"
          ],
          [
            "Apply case-insensitive matching for tournament result abbreviations"
          ],
          [
            "Filter the 'Tournament Results' table for rows where '2008' is 'sf' and '2010' is 'f'."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate tournament names",
          "Validate tournament round codes in year columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate tournament names",
            "Validate tournament round codes in year columns"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of tournament names matching the criteria",
          "Case-sensitive matching for 'SF' and 'F' values",
          "Return the value(s) from the 2007 column for matching rows",
          "If multiple rows match, return all matching 2007 values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of tournament names matching the criteria",
            "Case-sensitive matching for 'SF' and 'F' values"
          ],
          [
            "Return the value(s) from the 2007 column for matching rows",
            "If multiple rows match, return all matching 2007 values"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5666666666666668
  },
  "dbbench-002": {
    "m_q": {
      "target_metric": {
        "value": "Total medals count for United States",
        "confidence": 0.3333333333333333,
        "votes": [
          "Total medals count for United States",
          "total number of medals won by United States",
          "Total number of medals won by the United States"
        ]
      },
      "filters": {
        "value": [
          "Nation contains 'United States'",
          "Nation equals 'United States (USA)'",
          "Nation = 'United States\u00a0(USA)'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Nation contains 'United States'"
          ],
          [
            "Nation equals 'United States (USA)'"
          ],
          [
            "Nation = 'United States\u00a0(USA)'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the value in the 'Total' column for the row where Nation contains 'United States'?",
          "Identify the row corresponding to United States",
          "Extract the Total column value for United States"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the value in the 'Total' column for the row where Nation contains 'United States'?"
          ],
          [
            "Identify the row corresponding to United States",
            "Extract the Total column value for United States"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Olympic Medals"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Olympic Medals"
          ],
          [
            "Olympic Medals"
          ],
          [
            "Olympic Medals"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "gold": "count",
          "silver": "count",
          "bronze": "count",
          "total": "count",
          "rank": "ordinal position"
        },
        "confidence": 0.8666666666666666,
        "votes": [
          {
            "Gold": "count",
            "Silver": "count",
            "Bronze": "count",
            "Total": "count"
          },
          {
            "Gold": "count of medals",
            "Silver": "count of medals",
            "Bronze": "count of medals",
            "Total": "count of medals",
            "Rank": "ordinal position"
          },
          {
            "Gold": "number of medals",
            "Silver": "number of medals",
            "Bronze": "number of medals",
            "Total": "number of medals"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total = Gold + Silver + Bronze for each row",
          "Rank values should be unique or tied",
          "Nation values should be unique",
          "Total column should equal sum of Gold, Silver, and Bronze columns",
          "All medal counts should be non-negative integers",
          "Total medals must be the sum of Gold, Silver, and Bronze medals for each nation."
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Total = Gold + Silver + Bronze for each row",
            "Rank values should be unique or tied",
            "Nation values should be unique"
          ],
          [
            "Total column should equal sum of Gold, Silver, and Bronze columns",
            "Nation values should be unique",
            "All medal counts should be non-negative integers"
          ],
          [
            "Total medals must be the sum of Gold, Silver, and Bronze medals for each nation."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter Nation column where it contains 'United States' or 'USA'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Filter Nation column where it contains 'United States' or 'USA'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single integer value",
          "Return a single integer value representing total medals",
          "Value should be extracted from the Total column for United States row"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single integer value"
          ],
          [
            "Return a single integer value representing total medals",
            "Value should be extracted from the Total column for United States row"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5294444444444446
  },
  "dbbench-003": {
    "m_q": {
      "target_metric": {
        "value": "Difference between Southampton's total goals scored and Sunderland's total goals scored across all matches",
        "confidence": 0.3333333333333333,
        "votes": [
          "Difference between Southampton's total goals scored and Sunderland's total goals scored across all matches",
          "difference between Southampton's score and Sunderland's score",
          "difference between Southampton's total score and Sunderland's total score across all matches"
        ]
      },
      "filters": {
        "value": [
          "Home team = 'Southampton' OR Away team = 'Southampton'",
          "Home team = 'Sunderland' OR Away team = 'Sunderland'",
          "team name = 'Southampton' (as Home team or Away team)",
          "team name = 'Sunderland' (as Home team or Away team)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Home team = 'Southampton' OR Away team = 'Southampton'",
            "Home team = 'Sunderland' OR Away team = 'Sunderland'"
          ],
          [
            "team name = 'Southampton' (as Home team or Away team)",
            "team name = 'Sunderland' (as Home team or Away team)"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Home team",
          "Away team"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Home team",
            "Away team"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is Southampton's total score across all matches?",
          "What is Sunderland's total score across all matches?",
          "How to parse the 'Score' column to extract home and away goals?",
          "Should we consider both home and away matches for each team?",
          "Are there any missing values in the Score column for these teams?",
          "What is Southampton's score in their match(es)?",
          "What is Sunderland's score in their match(es)?",
          "How to parse the 'Score' column to extract individual team scores?",
          "Which matches involve Southampton (home or away)?",
          "Which matches involve Sunderland (home or away)?",
          "Are there multiple matches per team or a single match to consider?",
          "What is the total score for Southampton across all matches?",
          "What is the total score for Sunderland across all matches?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is Southampton's total score across all matches?",
            "What is Sunderland's total score across all matches?",
            "How to parse the 'Score' column to extract home and away goals?",
            "Should we consider both home and away matches for each team?",
            "Are there any missing values in the Score column for these teams?"
          ],
          [
            "What is Southampton's score in their match(es)?",
            "What is Sunderland's score in their match(es)?",
            "How to parse the 'Score' column to extract individual team scores?",
            "Which matches involve Southampton (home or away)?",
            "Which matches involve Sunderland (home or away)?",
            "Are there multiple matches per team or a single match to consider?"
          ],
          [
            "What is the total score for Southampton across all matches?",
            "What is the total score for Sunderland across all matches?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Football Matches"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Football Matches"
          ],
          [
            "Football Matches"
          ],
          [
            "Football Matches"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "score": "goals (format: home_goals\u2013away_goals)",
          "tie no": "match identifier (integer)"
        },
        "confidence": 0.5,
        "votes": [
          {
            "Score": "goals (format: home_goals\u2013away_goals)"
          },
          {
            "Score": "goals (format: home_goals\u2013away_goals)",
            "Tie no": "match identifier (integer)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Score column contains combined home-away scores separated by '\u2013' (en dash), needs parsing"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Score column contains combined home-away scores separated by '\u2013' (en dash), needs parsing"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Score column must be parsed to extract numerical values",
          "Teams may appear in both Home team and Away team columns",
          "Some matches may have multiple rows (replays based on draws)",
          "Score must be parsed to separate home team score and away team score",
          "Must identify whether Southampton/Sunderland played as home or away team",
          "Must handle the en-dash character (\u2013) in Score field",
          "Need to determine which specific match is being referenced if multiple matches exist",
          "The 'Score' column contains two scores separated by a delimiter (e.g., '\u2013').",
          "Need to handle cases where a team appears in both 'Home team' and 'Away team' columns.",
          "Need to parse the score string to extract numerical values."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Score column must be parsed to extract numerical values",
            "Teams may appear in both Home team and Away team columns",
            "Some matches may have multiple rows (replays based on draws)"
          ],
          [
            "Score must be parsed to separate home team score and away team score",
            "Must identify whether Southampton/Sunderland played as home or away team",
            "Must handle the en-dash character (\u2013) in Score field",
            "Need to determine which specific match is being referenced if multiple matches exist"
          ],
          [
            "The 'Score' column contains two scores separated by a delimiter (e.g., '\u2013').",
            "Need to handle cases where a team appears in both 'Home team' and 'Away team' columns.",
            "Need to parse the score string to extract numerical values."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Home team or Away team contains 'Southampton'",
          "Filter rows where Home team or Away team contains 'Sunderland'",
          "Home team = 'Southampton' OR Away team = 'Southampton'",
          "Home team = 'Sunderland' OR Away team = 'Sunderland'",
          "Matches where Southampton is either the 'Home team' or the 'Away team'.",
          "Matches where Sunderland is either the 'Home team' or the 'Away team'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Home team or Away team contains 'Southampton'",
            "Filter rows where Home team or Away team contains 'Sunderland'"
          ],
          [
            "Home team = 'Southampton' OR Away team = 'Southampton'",
            "Home team = 'Sunderland' OR Away team = 'Sunderland'"
          ],
          [
            "Matches where Southampton is either the 'Home team' or the 'Away team'.",
            "Matches where Sunderland is either the 'Home team' or the 'Away team'."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing Score values in filtered data",
          "Validate that all Score values can be parsed into two integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing Score values in filtered data",
            "Validate that all Score values can be parsed into two integers"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numerical difference (integer)",
          "Positive if Southampton scored more, negative if Sunderland scored more",
          "Output should be a single numeric value representing the score difference",
          "Clarify which match is being referenced if ambiguity exists",
          "The output should be a single numerical value representing the difference in total scores."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numerical difference (integer)",
            "Positive if Southampton scored more, negative if Sunderland scored more"
          ],
          [
            "Output should be a single numeric value representing the score difference",
            "Clarify which match is being referenced if ambiguity exists"
          ],
          [
            "The output should be a single numerical value representing the difference in total scores."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.563888888888889
  },
  "dbbench-004": {
    "m_q": {
      "target_metric": {
        "value": "City name corresponding to IATA code 'VIE'",
        "confidence": 0.6666666666666666,
        "votes": [
          "City name corresponding to IATA code 'VIE'",
          "City name corresponding to IATA code 'VIE'",
          "City corresponding to the IATA code 'VIE'"
        ]
      },
      "filters": {
        "value": [
          "IATA = 'VIE'"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "IATA = 'VIE'"
          ],
          [
            "IATA = 'VIE'"
          ],
          [
            "IATA = 'VIE'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the city name in the row where IATA column equals 'VIE'?",
          "Which row in Airport Information has IATA code 'VIE'?",
          "What is the City value for that row?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the city name in the row where IATA column equals 'VIE'?"
          ],
          [
            "Which row in Airport Information has IATA code 'VIE'?",
            "What is the City value for that row?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Airport Information"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Airport Information"
          ],
          [
            "Airport Information"
          ],
          [
            "Airport Information"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "iata": "three-letter airport code",
          "icao": "four-letter airport code"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {
            "IATA": "three-letter airport code",
            "ICAO": "four-letter airport code"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "\u2014",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "\u2014",
            ""
          ],
          [
            "\u2014"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "IATA codes should be unique",
          "Each row represents one airport",
          "IATA must be exactly 3 characters",
          "IATA codes should be unique per airport",
          "City field should not be null or empty",
          "IATA codes are unique"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "IATA codes should be unique",
            "Each row represents one airport"
          ],
          [
            "IATA must be exactly 3 characters",
            "IATA codes should be unique per airport",
            "City field should not be null or empty"
          ],
          [
            "IATA codes are unique"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where IATA column equals 'VIE'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Filter rows where IATA column equals 'VIE'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single city name as string",
          "Return the city name as a simple string value",
          "Expected answer format: 'Vienna'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single city name as string"
          ],
          [
            "Return the city name as a simple string value",
            "Expected answer format: 'Vienna'"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5416666666666667
  },
  "dbbench-005": {
    "m_q": {
      "target_metric": {
        "value": "List of leagues (excluding NPSL) that the NY men's soccer team has played in",
        "confidence": 0.3333333333333333,
        "votes": [
          "List of leagues (excluding NPSL) that the NY men's soccer team has played in",
          "League names other than NPSL that NY men's soccer team has played in",
          "List of leagues, excluding 'NPSL', in which the NY mens soccer team has played."
        ]
      },
      "filters": {
        "value": [
          "League != 'NPSL'",
          "Remove duplicate league entries"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "League != 'NPSL'",
            "Remove duplicate league entries"
          ],
          [
            "League != 'NPSL'"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "League"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "League"
          ],
          [
            "League"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What distinct leagues appear in the data?",
          "Which of those leagues are not NPSL?",
          "Are there any leagues with incomplete or placeholder entries that should be excluded?",
          "What are all unique values in the League column?",
          "Which league values are not equal to 'NPSL'?",
          "How many distinct leagues other than NPSL appear in the dataset?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What distinct leagues appear in the data?",
            "Which of those leagues are not NPSL?",
            "Are there any leagues with incomplete or placeholder entries that should be excluded?"
          ],
          [
            "What are all unique values in the League column?",
            "Which league values are not equal to 'NPSL'?",
            "How many distinct leagues other than NPSL appear in the dataset?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Soccer Team Performance Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Soccer Team Performance Table"
          ],
          [
            "Soccer Team Performance Table"
          ],
          [
            "Soccer Team Performance Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "season/year",
          "division": "soccer division level"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Year": "season/year",
            "Division": "soccer division level"
          },
          {
            "Year": "year or season range",
            "Division": "league division level (numeric)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Year column contains mixed formats: single years (2008) and year ranges (2010-11, 2016-17)",
          "Some Regular Season values contain both ranking and conference information"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column contains mixed formats: single years (2008) and year ranges (2010-11, 2016-17)",
            "Some Regular Season values contain both ranking and conference information"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "N/A",
          "Did not qualify",
          "Did not enter",
          "Season cancelled",
          "In progress",
          "Upcoming",
          "NA",
          ""
        ],
        "confidence": 0.6249999999999999,
        "votes": [
          [
            "N/A",
            "Did not qualify",
            "Did not enter",
            "Season cancelled",
            "In progress",
            "Upcoming"
          ],
          [
            "N/A",
            "Did not qualify",
            "Did not enter",
            "In progress",
            "Upcoming",
            "Season cancelled"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each row represents one season/team performance",
          "League column should contain valid league names",
          "Division values should be integers 4 or 5 based on sample",
          "League column must not be null or empty",
          "Filter out rows where League = 'NPSL'",
          "The 'League' column contains the names of the leagues.",
          "Need to filter out 'NPSL' from the list of leagues."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each row represents one season/team performance",
            "League column should contain valid league names",
            "Division values should be integers 4 or 5 based on sample"
          ],
          [
            "League column must not be null or empty",
            "Filter out rows where League = 'NPSL'"
          ],
          [
            "The 'League' column contains the names of the leagues.",
            "Need to filter out 'NPSL' from the list of leagues."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where League = 'NPSL'",
          "Exclude rows with placeholder league names if any",
          "Exclude NPSL from League values",
          "League != 'NPSL'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where League = 'NPSL'",
            "Exclude rows with placeholder league names if any"
          ],
          [
            "Exclude NPSL from League values"
          ],
          [
            "League != 'NPSL'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct leagues",
          "Verify no null values in League column for filtered rows"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct leagues",
            "Verify no null values in League column for filtered rows"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Alphabetical list of league names",
          "Each league should appear only once",
          "Return distinct league names only",
          "Exclude 'NPSL' from results",
          "Output should be a list of league names"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Alphabetical list of league names",
            "Each league should appear only once"
          ],
          [
            "Return distinct league names only",
            "Exclude 'NPSL' from results",
            "Output should be a list of league names"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5895833333333333
  },
  "dbbench-006": {
    "m_q": {
      "target_metric": {
        "value": "Attendance count for tie number 19",
        "confidence": 0.3333333333333333,
        "votes": [
          "Attendance count for tie number 19",
          "attendance count",
          "Attendance"
        ]
      },
      "filters": {
        "value": [
          "Tie no = '19'",
          "Tie no = 19"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Tie no = '19'"
          ],
          [
            "Tie no = 19"
          ],
          [
            "Tie no = 19"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the attendance value for the row where Tie no equals 19?",
          "Is the attendance value stored as a string with comma separators?",
          "Does the data contain exactly one row with Tie no = 19?",
          "What is the tie number column name?",
          "What is the attendance value for tie number 19?",
          "Is the attendance value in a format that needs parsing (e.g., comma-separated)?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the attendance value for the row where Tie no equals 19?",
            "Is the attendance value stored as a string with comma separators?",
            "Does the data contain exactly one row with Tie no = 19?"
          ],
          [
            "What is the tie number column name?",
            "What is the attendance value for tie number 19?",
            "Is the attendance value in a format that needs parsing (e.g., comma-separated)?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Football Matches"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Football Matches"
          ],
          [
            "Football Matches"
          ],
          [
            "Football Matches"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "attendance": "people"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Attendance": "people"
          },
          {
            "Attendance": "count of people"
          },
          {
            "Attendance": "number of people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Attendance values contain comma separators (e.g., '1,025') that need to be converted to numeric format",
          "Attendance values contain commas as thousands separators (e.g., '1,174')",
          "Attendance is stored as object/string type instead of numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Attendance values contain comma separators (e.g., '1,025') that need to be converted to numeric format"
          ],
          [
            "Attendance values contain commas as thousands separators (e.g., '1,174')",
            "Attendance is stored as object/string type instead of numeric"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Tie no should be unique for each match",
          "Attendance should be a positive integer",
          "There should be exactly one row with Tie no = 19",
          "Tie no must equal 19",
          "Attendance must be a valid numeric value after parsing",
          "Tie no is unique"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Tie no should be unique for each match",
            "Attendance should be a positive integer",
            "There should be exactly one row with Tie no = 19"
          ],
          [
            "Tie no must equal 19",
            "Attendance must be a valid numeric value after parsing"
          ],
          [
            "Tie no is unique"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove comma separators from Attendance values before numeric conversion",
          "Filter rows where 'Tie no' column equals '19' (as string)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove comma separators from Attendance values before numeric conversion"
          ],
          [
            "Filter rows where 'Tie no' column equals '19' (as string)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Tie no values",
          "Verify Attendance values are numeric after cleaning"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Tie no values",
            "Verify Attendance values are numeric after cleaning"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return attendance as a single integer value",
          "Return single numeric value representing attendance",
          "Remove comma separators from attendance before returning"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return attendance as a single integer value"
          ],
          [
            "Return single numeric value representing attendance",
            "Remove comma separators from attendance before returning"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5916666666666668
  },
  "dbbench-007": {
    "m_q": {
      "target_metric": {
        "value": "Last of whiteness (in months) for two specific products",
        "confidence": 0.3333333333333333,
        "votes": [
          "Last of whiteness (in months) for two specific products",
          "lasting whiteness duration in months",
          "lasting whiteness in months"
        ]
      },
      "filters": {
        "value": [
          "Model contains 'Crest 3D Intensive Professional Effects'",
          "Model contains 'Crest Whitestrips 3D Professional Effects'",
          "Model = 'Crest 3D Intensive Professional Effects'",
          "Model = 'Crest Whitestrips 3D Professional Effects'",
          "Model is 'Crest 3D Intensive Professional Effects'",
          "Model is 'Crest Whitestrips 3D Professional Effects'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Model contains 'Crest 3D Intensive Professional Effects'",
            "Model contains 'Crest Whitestrips 3D Professional Effects'"
          ],
          [
            "Model = 'Crest 3D Intensive Professional Effects'",
            "Model = 'Crest Whitestrips 3D Professional Effects'"
          ],
          [
            "Model is 'Crest 3D Intensive Professional Effects'",
            "Model is 'Crest Whitestrips 3D Professional Effects'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Model"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Model"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "list",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the 'Last of whiteness' value for Crest 3D Intensive Professional Effects?",
          "What is the 'Last of whiteness' value for Crest Whitestrips 3D Professional Effects?",
          "Are the values expressed consistently in months?",
          "What is the 'Last of whiteness' value for 'Crest 3D Intensive Professional Effects'?",
          "What is the 'Last of whiteness' value for 'Crest Whitestrips 3D Professional Effects'?",
          "Are both values equal?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the 'Last of whiteness' value for Crest 3D Intensive Professional Effects?",
            "What is the 'Last of whiteness' value for Crest Whitestrips 3D Professional Effects?",
            "Are the values expressed consistently in months?"
          ],
          [
            "What is the 'Last of whiteness' value for 'Crest 3D Intensive Professional Effects'?",
            "What is the 'Last of whiteness' value for 'Crest Whitestrips 3D Professional Effects'?",
            "Are both values equal?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Crest Whitestrips Products"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Crest Whitestrips Products"
          ],
          [
            "Crest Whitestrips Products"
          ],
          [
            "Crest Whitestrips Products"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "last of whiteness": "months",
          "length of use": "days or hours"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Last of whiteness": "months (based on sample data)"
          },
          {
            "Last of whiteness": "months",
            "Length of use": "days or hours"
          },
          {
            "Last of whiteness": "months"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Some 'Last of whiteness' values contain text descriptions instead of numeric months (e.g., 'White after using system')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Some 'Last of whiteness' values contain text descriptions instead of numeric months (e.g., 'White after using system')"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Data contains 23 rows and 5 columns",
          "Target products must exist in the dataset",
          "'Last of whiteness' values must be extractable for both products",
          "Both product models must exist in the dataset",
          "Last of whiteness values must be parseable as numeric months",
          "The 'Last of whiteness' column should be numeric or convertible to numeric representing months."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data contains 23 rows and 5 columns",
            "Target products must exist in the dataset",
            "'Last of whiteness' values must be extractable for both products"
          ],
          [
            "Both product models must exist in the dataset",
            "Last of whiteness values must be parseable as numeric months"
          ],
          [
            "The 'Last of whiteness' column should be numeric or convertible to numeric representing months."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Model contains 'Professional Effects'",
          "Exclude rows where 'Last of whiteness' is not numeric",
          "Extract rows where Model contains 'Crest 3D Intensive Professional Effects' or 'Crest Whitestrips 3D Professional Effects'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Model contains 'Professional Effects'",
            "Exclude rows where 'Last of whiteness' is not numeric"
          ],
          [
            "Extract rows where Model contains 'Crest 3D Intensive Professional Effects' or 'Crest Whitestrips 3D Professional Effects'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check consistency of 'Last of whiteness' format across target products"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check consistency of 'Last of whiteness' format across target products"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Report both product names and their corresponding 'Last of whiteness' values",
          "Values should be expressed in months",
          "Return single numeric value representing months",
          "If values differ, report conflict",
          "If both have same value, return that value",
          "Output a list of the 'Last of whiteness' values for the specified models."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Report both product names and their corresponding 'Last of whiteness' values",
            "Values should be expressed in months"
          ],
          [
            "Return single numeric value representing months",
            "If values differ, report conflict",
            "If both have same value, return that value"
          ],
          [
            "Output a list of the 'Last of whiteness' values for the specified models."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5666666666666668
  },
  "dbbench-008": {
    "m_q": {
      "target_metric": {
        "value": "Apps",
        "confidence": 0.3333333333333333,
        "votes": [
          "Apps",
          "number of apps (appearances)",
          "number of apps"
        ]
      },
      "filters": {
        "value": [
          "Team = 'Spartak Nizhny Novgorod'",
          "Team is Spartak Nizhny Novgorod"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Team = 'Spartak Nizhny Novgorod'"
          ],
          [
            "Team = 'Spartak Nizhny Novgorod'"
          ],
          [
            "Team is Spartak Nizhny Novgorod"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What season(s) does Spartak Nizhny Novgorod appear in?",
          "Is there only one row for this team or multiple seasons?",
          "Find the row where Team is 'Spartak Nizhny Novgorod'",
          "Extract the Apps value for that row"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What season(s) does Spartak Nizhny Novgorod appear in?",
            "Is there only one row for this team or multiple seasons?"
          ],
          [
            "Find the row where Team is 'Spartak Nizhny Novgorod'",
            "Extract the Apps value for that row"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Football_Stats"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Football_Stats"
          ],
          [
            "Football_Stats"
          ],
          [
            "Football_Stats"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "apps": "count",
          "goals": "count",
          "division": "level (1=top division)"
        },
        "confidence": 0.8888888888888888,
        "votes": [
          {
            "Apps": "count",
            "Goals": "count",
            "Division": "level (1=top division)"
          },
          {
            "Apps": "count of appearances",
            "Goals": "count of goals scored",
            "Division": "league tier number"
          },
          {
            "Apps": "number of appearances",
            "Goals": "number of goals"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Season column has inconsistent formats: some years (2004, 2005, 2006), some season ranges (2006/07, 2007/08)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Season column has inconsistent formats: some years (2004, 2005, 2006), some season ranges (2006/07, 2007/08)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Apps >= 0",
          "Goals >= 0",
          "Division should be positive integer",
          "Team name must match exactly 'Spartak Nizhny Novgorod'",
          "Apps column should contain non-negative integer values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Apps >= 0",
            "Goals >= 0",
            "Division should be positive integer"
          ],
          [
            "Team name must match exactly 'Spartak Nizhny Novgorod'",
            "Apps column should contain non-negative integer values"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Country = 'Russia'",
          "Division = 2 (based on sample row)",
          "Team = 'Spartak Nizhny Novgorod'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Country = 'Russia'",
            "Division = 2 (based on sample row)"
          ],
          [],
          [
            "Team = 'Spartak Nizhny Novgorod'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Team-Season combinations",
          "Verify Apps values are reasonable for football statistics"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Team-Season combinations",
            "Verify Apps values are reasonable for football statistics"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric value representing total Apps",
          "Should clarify if asking for sum across seasons or specific season",
          "Return single integer value representing the number of apps"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric value representing total Apps",
            "Should clarify if asking for sum across seasons or specific season"
          ],
          [
            "Return single integer value representing the number of apps"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5861111111111112
  },
  "dbbench-009": {
    "m_q": {
      "target_metric": {
        "value": "Attendance",
        "confidence": 0.6666666666666666,
        "votes": [
          "Attendance value",
          "Attendance",
          "Attendance"
        ]
      },
      "filters": {
        "value": [
          "Round = '52'",
          "Home = 'S\u00f6dert\u00e4lje SK'",
          "Round equals 52",
          "Home equals 's\u00f6dert\u00e4lje sk' (case-insensitive match to 'S\u00f6dert\u00e4lje SK')"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "Round = '52'",
            "Home = 'S\u00f6dert\u00e4lje SK'"
          ],
          [
            "Round equals 52",
            "Home equals 's\u00f6dert\u00e4lje sk' (case-insensitive match to 'S\u00f6dert\u00e4lje SK')"
          ],
          [
            "Round = '52'",
            "Home = 's\u00f6dert\u00e4lje sk'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Find the specific attendance number for the game with Round 52 where S\u00f6dert\u00e4lje SK is the home team",
          "What is the exact match for home team name given case differences?",
          "Are there multiple games matching Round 52 and Home S\u00f6dert\u00e4lje SK?",
          "What is the attendance value for the matching game?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Find the specific attendance number for the game with Round 52 where S\u00f6dert\u00e4lje SK is the home team"
          ],
          [
            "What is the exact match for home team name given case differences?",
            "Are there multiple games matching Round 52 and Home S\u00f6dert\u00e4lje SK?",
            "What is the attendance value for the matching game?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Hockey Games"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Hockey Games"
          ],
          [
            "Hockey Games"
          ],
          [
            "Hockey Games"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "attendance": "number of people",
          "round": "game round number"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Attendance": "number of spectators"
          },
          {
            "Attendance": "number of people",
            "Round": "game round number"
          },
          {
            "Attendance": "number of people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Attendance values contain commas as thousand separators (e.g., '11,497') which need to be parsed as numeric",
          "Attendance values appear to contain commas as thousands separators (e.g., '2,859')",
          "Attendance is stored as string type instead of numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Attendance values contain commas as thousand separators (e.g., '11,497') which need to be parsed as numeric"
          ],
          [
            "Attendance values appear to contain commas as thousands separators (e.g., '2,859')",
            "Attendance is stored as string type instead of numeric"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Round values appear to be strings but represent numeric rounds",
          "Home team names must match exactly (case-sensitive)",
          "There should be exactly one game with Round 52 and Home = 'S\u00f6dert\u00e4lje SK' based on sample data",
          "Round must equal 52",
          "Home must match 'S\u00f6dert\u00e4lje SK' (case-insensitive)",
          "Attendance must be present and non-null",
          "Round values must be strings",
          "Home values must be strings",
          "Attendance values must be strings representing numbers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Round values appear to be strings but represent numeric rounds",
            "Home team names must match exactly (case-sensitive)",
            "There should be exactly one game with Round 52 and Home = 'S\u00f6dert\u00e4lje SK' based on sample data"
          ],
          [
            "Round must equal 52",
            "Home must match 'S\u00f6dert\u00e4lje SK' (case-insensitive)",
            "Attendance must be present and non-null"
          ],
          [
            "Round values must be strings",
            "Home values must be strings",
            "Attendance values must be strings representing numbers"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Round column filtering for value '52'",
          "Home column filtering for value 'S\u00f6dert\u00e4lje SK'",
          "Convert Round from string to numeric for comparison if needed",
          "Normalize Home team name to handle case-insensitive matching"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Round column filtering for value '52'",
            "Home column filtering for value 'S\u00f6dert\u00e4lje SK'"
          ],
          [
            "Convert Round from string to numeric for comparison if needed",
            "Normalize Home team name to handle case-insensitive matching"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Round+Home combinations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Round+Home combinations"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Attendance should be returned as a cleaned numeric value (remove commas and quotes)",
          "Return single attendance value",
          "Remove comma separators from attendance if present",
          "Return numeric attendance value or formatted string as '2,859'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Attendance should be returned as a cleaned numeric value (remove commas and quotes)"
          ],
          [
            "Return single attendance value",
            "Remove comma separators from attendance if present",
            "Return numeric attendance value or formatted string as '2,859'"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5750000000000001
  },
  "dbbench-010": {
    "m_q": {
      "target_metric": {
        "value": "Team name that appears immediately after Manchester City in the dataset",
        "confidence": 0.3333333333333333,
        "votes": [
          "Team name that appears immediately after Manchester City in the dataset",
          "The team name that appears immediately after Manchester City in the list",
          "The team listed immediately after 'Manchester City' in the 'Team' column."
        ]
      },
      "filters": {
        "value": [
          "Manchester City must be present in the Team column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Manchester City must be present in the Team column"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the ordering of teams in the dataset?",
          "Which team follows Manchester City in the sequence?",
          "What is the position/row number of Manchester City in the dataset?",
          "What team appears in the next row after Manchester City?",
          "Are there multiple occurrences of Manchester City in the dataset?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the ordering of teams in the dataset?",
            "Which team follows Manchester City in the sequence?"
          ],
          [
            "What is the position/row number of Manchester City in the dataset?",
            "What team appears in the next row after Manchester City?",
            "Are there multiple occurrences of Manchester City in the dataset?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Manager Changes"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Manager Changes"
          ],
          [
            "Manager Changes"
          ],
          [
            "Manager Changes"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "date of vacancy": "date",
          "date of appointment": "date",
          "position in table": "ordinal_rank"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {
            "Date of vacancy": "date",
            "Date of appointment": "date",
            "Position in table": "ordinal_rank"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Team column must contain 'Manchester City'",
          "Data must maintain original row ordering",
          "There must be at least one row after the Manchester City row(s)",
          "The order of rows in the data file is preserved",
          "The 'Team' column must be ordered as presented in the file."
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Team column must contain 'Manchester City'",
            "Data must maintain original row ordering"
          ],
          [
            "Team column must contain 'Manchester City'",
            "There must be at least one row after the Manchester City row(s)",
            "The order of rows in the data file is preserved"
          ],
          [
            "The 'Team' column must be ordered as presented in the file."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Identify all rows where Team = 'Manchester City'",
          "Determine which Manchester City occurrence is being referenced (first or second)",
          "Select the row immediately following the relevant Manchester City row"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Identify all rows where Team = 'Manchester City'",
            "Determine which Manchester City occurrence is being referenced (first or second)",
            "Select the row immediately following the relevant Manchester City row"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Team name should be returned as a string",
          "Return only the team name as a string",
          "If multiple Manchester City entries exist, clarify which 'after' is meant (likely the first occurrence)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Team name should be returned as a string"
          ],
          [
            "Return only the team name as a string",
            "If multiple Manchester City entries exist, clarify which 'after' is meant (likely the first occurrence)"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5033333333333334
  },
  "dbbench-011": {
    "m_q": {
      "target_metric": {
        "value": "Event name where the fighter fought Masato Shiozawa",
        "confidence": 0.6666666666666666,
        "votes": [
          "Event name where the fighter fought Masato Shiozawa",
          "Event name where the fighter fought Masato Shiozawa",
          "The 'Event' in which the fighter fought 'Masato Shiozawa'"
        ]
      },
      "filters": {
        "value": [
          "Opponent = 'Masato Shiozawa'",
          "Opponent == 'Masato Shiozawa'",
          "Opponent is 'Masato Shiozawa'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Opponent = 'Masato Shiozawa'"
          ],
          [
            "Opponent == 'Masato Shiozawa'"
          ],
          [
            "Opponent is 'Masato Shiozawa'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which fighter's record is this?",
          "Is this the fighter's first professional fight?",
          "What was the result of this fight?",
          "Find the row where Opponent is 'Masato Shiozawa'",
          "Extract the Event value from that row"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which fighter's record is this?",
            "Is this the fighter's first professional fight?",
            "What was the result of this fight?"
          ],
          [
            "Find the row where Opponent is 'Masato Shiozawa'",
            "Extract the Event value from that row"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "MMA Fight Record"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "MMA Fight Record"
          ],
          [
            "MMA Fight Record"
          ],
          [
            "MMA Fight Record"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "round": "fight round number",
          "record": "win-loss_record"
        },
        "confidence": 0.5,
        "votes": [
          {
            "Round": "fight round number"
          },
          {
            "Round": "round_number",
            "Record": "win-loss_record"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Record column uses 'Wins-Losses' format, not actual count"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Record column uses 'Wins-Losses' format, not actual count"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Record column should be consistent with Res. column wins/losses",
          "Round should be a positive integer",
          "Each row represents one professional fight",
          "Opponent column must match 'Masato Shiozawa' exactly",
          "Should return exactly one event if the match exists"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Record column should be consistent with Res. column wins/losses",
            "Round should be a positive integer",
            "Each row represents one professional fight"
          ],
          [
            "Opponent column must match 'Masato Shiozawa' exactly",
            "Should return exactly one event if the match exists"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Res. = 'Win' for the Masato Shiozawa fight",
          "Filter rows where Opponent == 'Masato Shiozawa'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Res. = 'Win' for the Masato Shiozawa fight"
          ],
          [
            "Filter rows where Opponent == 'Masato Shiozawa'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if Record progression is consistent with fight results"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if Record progression is consistent with fight results"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return exact Event string from data",
          "Return the event name as a single string value",
          "If no match found, return null or appropriate indicator"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return exact Event string from data"
          ],
          [
            "Return the event name as a single string value",
            "If no match found, return null or appropriate indicator"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5750000000000002
  },
  "dbbench-012": {
    "m_q": {
      "target_metric": {
        "value": "Name of coach who won promotion to first tier",
        "confidence": 0.3333333333333333,
        "votes": [
          "Name of coach who won promotion to first tier",
          "Name of the coach who won promotion to the first tier",
          "coach name"
        ]
      },
      "filters": {
        "value": [
          "Honours contains 'Won promotion to first tier'",
          "Honours contains 'promotion to first tier' or 'promotion to the first tier'",
          "coach won promotion to the first tier"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Honours contains 'Won promotion to first tier'"
          ],
          [
            "Honours contains 'promotion to first tier' or 'promotion to the first tier'"
          ],
          [
            "coach won promotion to the first tier"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Name"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Name"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which coaches have honours mentioning promotion?",
          "What tier level is mentioned in each honour?",
          "Is there exactly one coach with first tier promotion?",
          "Which coaches have honours related to promotion to first tier?",
          "Is there only one coach with this specific honour?",
          "What is the exact name of that coach?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which coaches have honours mentioning promotion?",
            "What tier level is mentioned in each honour?",
            "Is there exactly one coach with first tier promotion?"
          ],
          [
            "Which coaches have honours related to promotion to first tier?",
            "Is there only one coach with this specific honour?",
            "What is the exact name of that coach?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Coaches"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Coaches"
          ],
          [
            "Coaches"
          ],
          [
            "Coaches"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "from": "date",
          "to": "date"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "From": "date",
            "To": "date"
          },
          {
            "From": "date",
            "To": "date"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Date format varies (e.g., '1 July 2012', '1996')",
          "Some dates are year-only while others are full dates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date format varies (e.g., '1 July 2012', '1996')",
            "Some dates are year-only while others are full dates"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Honours column must be searched for promotion mentions",
          "Coach name must be unique for identification",
          "Only coaches with explicit 'first tier' promotion should be considered",
          "Only one coach should match the criteria",
          "Honours field must contain exact phrase related to first tier promotion",
          "Result must be a single coach name",
          "The 'Honours' column must be parsed to identify coaches who won promotion to the first tier."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Honours column must be searched for promotion mentions",
            "Coach name must be unique for identification",
            "Only coaches with explicit 'first tier' promotion should be considered"
          ],
          [
            "Only one coach should match the criteria",
            "Honours field must contain exact phrase related to first tier promotion",
            "Result must be a single coach name"
          ],
          [
            "The 'Honours' column must be parsed to identify coaches who won promotion to the first tier."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Honours IS NOT NULL",
          "Honours LIKE '%promotion%'",
          "Honours LIKE '%first tier%'",
          "Filter Honours column where text matches 'Won promotion to first tier' (case-insensitive)",
          "Exclude promotions to second tier, third tier, or fourth tier",
          "coach won promotion to the first tier based on the 'Honours' column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Honours IS NOT NULL",
            "Honours LIKE '%promotion%'",
            "Honours LIKE '%first tier%'"
          ],
          [
            "Filter Honours column where text matches 'Won promotion to first tier' (case-insensitive)",
            "Exclude promotions to second tier, third tier, or fourth tier"
          ],
          [
            "coach won promotion to the first tier based on the 'Honours' column"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Count of coaches with first tier promotion should be exactly 1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count of coaches with first tier promotion should be exactly 1"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single coach name as string",
          "Return only the coach name as a single value",
          "Verify cardinality is exactly 1 matching record"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single coach name as string"
          ],
          [
            "Return only the coach name as a single value",
            "Verify cardinality is exactly 1 matching record"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5666666666666668
  },
  "dbbench-013": {
    "m_q": {
      "target_metric": {
        "value": "Team name that won the grand final in 2008",
        "confidence": 0.3333333333333333,
        "votes": [
          "Team name that won the grand final in 2008",
          "Team name that took first place (Premiers)",
          "team that took first place in 2008"
        ]
      },
      "filters": {
        "value": [
          "Year = 2008",
          "Year == 2008"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Year = 2008"
          ],
          [
            "Year == 2008"
          ],
          [
            "Year = 2008"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which team is listed as 'Premiers' for year 2008?",
          "Is there only one record for year 2008?",
          "Does the data confirm 2008 had a valid grand final result?",
          "Which team is listed in the Premiers column for the year 2008?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which team is listed as 'Premiers' for year 2008?",
            "Is there only one record for year 2008?",
            "Does the data confirm 2008 had a valid grand final result?"
          ],
          [
            "Which team is listed in the Premiers column for the year 2008?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "AFL Cairns Grand Final Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "AFL Cairns Grand Final Results"
          ],
          [
            "AFL Cairns Grand Final Results"
          ],
          [
            "AFL Cairns Grand Final Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "premiers score": "Australian rules football score format (goals.behinds (total))",
          "runners up score": "Australian rules football score format (goals.behinds (total))",
          "year": "year"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "Premiers Score": "Australian rules football score format (goals.behinds (total))",
            "Runners Up Score": "Australian rules football score format (goals.behinds (total))"
          },
          {
            "Year": "year",
            "Premiers Score": "goals.behinds (points)",
            "Runners Up Score": "goals.behinds (points)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Year column is object type instead of integer",
          "Score columns are object type instead of numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column is object type instead of integer",
            "Score columns are object type instead of numeric"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be unique (one grand final per year)",
          "Premiers Score should be higher than Runners Up Score for each row",
          "Year values should be sequential from 2003 to 2022",
          "Year must equal 2008",
          "Premiers column contains the first place team",
          "Should return exactly one team name"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be unique (one grand final per year)",
            "Premiers Score should be higher than Runners Up Score for each row",
            "Year values should be sequential from 2003 to 2022"
          ],
          [
            "Year must equal 2008",
            "Premiers column contains the first place team",
            "Should return exactly one team name"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Filter dataset to Year = 2008",
          "Select Premiers column value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter dataset to Year = 2008",
            "Select Premiers column value"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check that 2008 exists in Year column",
          "Verify only one row matches Year = 2008"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check that 2008 exists in Year column",
            "Verify only one row matches Year = 2008"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return team name as string",
          "No additional formatting needed",
          "Return the team name as a string",
          "Answer should be the value from the Premiers column for Year 2008"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return team name as string",
            "No additional formatting needed"
          ],
          [
            "Return the team name as a string",
            "Answer should be the value from the Premiers column for Year 2008"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5694444444444445
  },
  "dbbench-014": {
    "m_q": {
      "target_metric": {
        "value": "Determine which team won the hockey game on December 9, 1993 between Dallas and Ottawa",
        "confidence": 0.3333333333333333,
        "votes": [
          "Determine which team won the hockey game on December 9, 1993 between Dallas and Ottawa",
          "Determine which team (Dallas or Ottawa) won the game on December 9, 1993",
          "The winning team of the hockey game played on December 9, 1993 between Dallas and Ottawa."
        ]
      },
      "filters": {
        "value": [
          "Date = 'December 9, 1993'",
          "Winning Team IN ('Dallas', 'Ottawa')",
          "Losing Team IN ('Dallas', 'Ottawa')",
          "Teams involved are Dallas and Ottawa",
          "Date is December 9, 1993",
          "Either Dallas or Ottawa is the Winning Team or the Losing Team"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Date = 'December 9, 1993'",
            "Winning Team IN ('Dallas', 'Ottawa')",
            "Losing Team IN ('Dallas', 'Ottawa')"
          ],
          [
            "Date = 'December 9, 1993'",
            "Teams involved are Dallas and Ottawa"
          ],
          [
            "Date is December 9, 1993",
            "Either Dallas or Ottawa is the Winning Team or the Losing Team"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What was the final score of the December 9, 1993 game?",
          "Which team is listed as the winning team for that date?",
          "Which team is in the 'Winning Team' column for the December 9, 1993 game?",
          "Is Dallas the Winning Team or Losing Team for this game?",
          "Is Ottawa the Winning Team or Losing Team for this game?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What was the final score of the December 9, 1993 game?",
            "Which team is listed as the winning team for that date?"
          ],
          [
            "Which team is in the 'Winning Team' column for the December 9, 1993 game?",
            "Is Dallas the Winning Team or Losing Team for this game?",
            "Is Ottawa the Winning Team or Losing Team for this game?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Hockey_Game_Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Hockey_Game_Results"
          ],
          [
            "Hockey_Game_Results"
          ],
          [
            "Hockey_Game_Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "winning team score": "goals",
          "losing team score": "goals",
          "attendance": "people"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Winning Team Score": "points",
            "Losing Team Score": "points",
            "Attendance": "people"
          },
          {
            "Winning Team Score": "goals",
            "Losing Team Score": "goals",
            "Attendance": "number of people"
          },
          {
            "Winning Team Score": "goals",
            "Losing Team Score": "goals",
            "Attendance": "people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Attendance column contains commas and asterisks in values (e.g., '7,144', '17,008*')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Attendance column contains commas and asterisks in values (e.g., '7,144', '17,008*')"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 1.0,
        "votes": [
          10.0,
          10.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "For each row, Winning Team Score > Losing Team Score",
          "Winning Team and Losing Team should be different teams",
          "Date values should be valid dates",
          "Date must match exactly 'December 9, 1993'",
          "Either Dallas or Ottawa must appear in Winning Team column",
          "Either Dallas or Ottawa must appear in Losing Team column",
          "A game record must exist for this date with both teams",
          "The date must be a valid date.",
          "Winning Team and Losing Team must be different."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "For each row, Winning Team Score > Losing Team Score",
            "Winning Team and Losing Team should be different teams",
            "Date values should be valid dates"
          ],
          [
            "Date must match exactly 'December 9, 1993'",
            "Either Dallas or Ottawa must appear in Winning Team column",
            "Either Dallas or Ottawa must appear in Losing Team column",
            "A game record must exist for this date with both teams"
          ],
          [
            "The date must be a valid date.",
            "Winning Team and Losing Team must be different."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to rows where Date contains 'December 9, 1993'",
          "Filter to rows where either Winning Team or Losing Team is 'Dallas' or 'Ottawa'",
          "Filter rows where Date = 'December 9, 1993' AND (Winning Team IN ('Dallas', 'Ottawa') OR Losing Team IN ('Dallas', 'Ottawa'))"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to rows where Date contains 'December 9, 1993'",
            "Filter to rows where either Winning Team or Losing Team is 'Dallas' or 'Ottawa'"
          ],
          [
            "Filter rows where Date = 'December 9, 1993' AND (Winning Team IN ('Dallas', 'Ottawa') OR Losing Team IN ('Dallas', 'Ottawa'))"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check that only one row matches the date filter"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check that only one row matches the date filter"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return team name as string",
          "Output should identify which team won: Dallas or Ottawa",
          "Answer should be definitive based on the Winning Team column value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return team name as string"
          ],
          [
            "Output should identify which team won: Dallas or Ottawa",
            "Answer should be definitive based on the Winning Team column value"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5805555555555556
  },
  "dbbench-015": {
    "m_q": {
      "target_metric": {
        "value": "total agricultural value in 2008/09",
        "confidence": 1.0,
        "votes": [
          "total agricultural value in 2008/09",
          "total agricultural value in 2008/09",
          "Total agricultural value in 2008/09"
        ]
      },
      "filters": {
        "value": [
          "IME Exchange (Including spot, credit and forward transactions) = 'Agricultural - Value (billion rials)'",
          "column '2008/09'",
          "Year = 2008/09",
          "Category contains 'Agricultural' and 'Value'",
          "Row contains 'Agricultural - Value (billion rials)'"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "IME Exchange (Including spot, credit and forward transactions) = 'Agricultural - Value (billion rials)'",
            "column '2008/09'"
          ],
          [
            "Year = 2008/09",
            "Category contains 'Agricultural' and 'Value'"
          ],
          [
            "Row contains 'Agricultural - Value (billion rials)'",
            "Column '2008/09'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which row contains agricultural value data?",
          "Is there only one agricultural value entry for 2008/09?",
          "Are there multiple agricultural value rows that need to be summed?",
          "Which rows represent agricultural value data?",
          "Are there multiple agricultural value entries that need to be summed?",
          "Does the dataset contain separate sections (IME Exchange vs other transaction types) that need to be considered?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which row contains agricultural value data?",
            "Is there only one agricultural value entry for 2008/09?",
            "Are there multiple agricultural value rows that need to be summed?"
          ],
          [
            "Which rows represent agricultural value data?",
            "Are there multiple agricultural value entries that need to be summed?",
            "Does the dataset contain separate sections (IME Exchange vs other transaction types) that need to be considered?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "IME Exchange (Including spot, credit and forward transactions)"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "IME Exchange (Including spot, credit and forward transactions)"
          ],
          [
            "IME Exchange (Including spot, credit and forward transactions)"
          ],
          [
            "IME Exchange (Including spot, credit and forward transactions)"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "agricultural - value (billion rials)": "billion rials",
          "2007/08": "year",
          "2008/09": "year",
          "2009/10": "year",
          "2010/11": "year",
          "2011/12": "year"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Agricultural - Value (billion rials)": "billion rials",
            "2007/08": "year",
            "2008/09": "year",
            "2009/10": "year",
            "2010/11": "year",
            "2011/12": "year"
          },
          {
            "2008/09": "billion rials for value rows, thousand tons for volume rows"
          },
          {
            "2007/08": "billion rials",
            "2008/09": "billion rials",
            "2009/10": "billion rials",
            "2010/11": "billion rials",
            "2011/12": "billion rials"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Some values contain commas as thousand separators (e.g., '3,729.7') that need parsing",
          "Value is in billion rials",
          "Volume is in thousand tons",
          "Numbers contain commas as thousand separators"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Some values contain commas as thousand separators (e.g., '3,729.7') that need parsing"
          ],
          [
            "Value is in billion rials",
            "Volume is in thousand tons",
            "Numbers contain commas as thousand separators"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Agricultural value must be numeric after removing thousand separators",
          "Only rows with 'Agricultural - Value' in first column should be considered",
          "Must identify all rows where Category contains 'Agricultural - Value'",
          "Must parse numeric values with comma separators",
          "Must extract data from column '2008/09'",
          "Need to extract the row where 'IME Exchange (Including spot, credit and forward transactions)' is 'Agricultural - Value (billion rials)'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Agricultural value must be numeric after removing thousand separators",
            "Only rows with 'Agricultural - Value' in first column should be considered"
          ],
          [
            "Must identify all rows where Category contains 'Agricultural - Value'",
            "Must parse numeric values with comma separators",
            "Must extract data from column '2008/09'"
          ],
          [
            "Need to extract the row where 'IME Exchange (Including spot, credit and forward transactions)' is 'Agricultural - Value (billion rials)'"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where first column contains 'Agricultural - Value'",
          "Extract value from '2008/09' column",
          "Filter rows where 'Agricultural - Value' appears in the category column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where first column contains 'Agricultural - Value'",
            "Extract value from '2008/09' column"
          ],
          [
            "Filter rows where 'Agricultural - Value' appears in the category column"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate agricultural value rows",
          "Validate numeric conversion of values with commas"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate agricultural value rows",
            "Validate numeric conversion of values with commas"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numeric value in billion rials",
          "Should handle thousand separator parsing",
          "Return single numeric value in billion rials",
          "Sum all agricultural value entries for 2008/09 if multiple sections exist",
          "Output should be a single numerical value representing the total agricultural value in billion rials."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric value in billion rials",
            "Should handle thousand separator parsing"
          ],
          [
            "Return single numeric value in billion rials",
            "Sum all agricultural value entries for 2008/09 if multiple sections exist"
          ],
          [
            "Output should be a single numerical value representing the total agricultural value in billion rials."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5866666666666668
  },
  "dbbench-016": {
    "m_q": {
      "target_metric": {
        "value": "Number of league apps for Ted Davis",
        "confidence": 0.6666666666666666,
        "votes": [
          "League Apps count for Ted Davis",
          "Number of league apps for Ted Davis",
          "Number of league apps for Ted Davis"
        ]
      },
      "filters": {
        "value": [
          "Name = 'Ted Davis'",
          "Name is Ted Davis"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Name = 'Ted Davis'"
          ],
          [
            "Name = 'Ted Davis'"
          ],
          [
            "Name is Ted Davis"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Is Ted Davis present in the data?",
          "What is the exact value of League Apps for Ted Davis?",
          "Which row corresponds to Ted Davis?",
          "What is the value in the 'League Apps' column for Ted Davis?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Is Ted Davis present in the data?",
            "What is the exact value of League Apps for Ted Davis?"
          ],
          [
            "Which row corresponds to Ted Davis?",
            "What is the value in the 'League Apps' column for Ted Davis?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Football_Player_Stats"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Football_Player_Stats"
          ],
          [
            "Football_Player_Stats"
          ],
          [
            "Football_Player_Stats"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "league apps": "count",
          "league goals": "count",
          "fa cup apps": "count",
          "fa cup goals": "count",
          "total apps": "count",
          "total goals": "count"
        },
        "confidence": 0.7222222222222222,
        "votes": [
          {
            "League Apps": "count",
            "League Goals": "count",
            "FA Cup Apps": "count",
            "FA Cup Goals": "count",
            "Total Apps": "count",
            "Total Goals": "count"
          },
          {
            "League Apps": "count",
            "League Goals": "count",
            "FA Cup Apps": "count",
            "FA Cup Goals": "count",
            "Total Apps": "count",
            "Total Goals": "count"
          },
          {
            "League Apps": "number of appearances"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "League Apps should be non-negative integer",
          "Total Apps should equal League Apps + FA Cup Apps",
          "Name column should be unique for each player",
          "Name must match 'Ted Davis' exactly",
          "League Apps must be non-null for Ted Davis",
          "League Apps should be a non-negative integer",
          "Name column should be unique or handle duplicates appropriately"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "League Apps should be non-negative integer",
            "Total Apps should equal League Apps + FA Cup Apps",
            "Name column should be unique for each player"
          ],
          [
            "Name must match 'Ted Davis' exactly",
            "League Apps must be non-null for Ted Davis",
            "League Apps should be a non-negative integer"
          ],
          [
            "Name column should be unique or handle duplicates appropriately"
          ]
        ]
      },
      "derived_filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single integer value",
          "Value should be 27 based on sample data"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Return single integer value"
          ],
          [
            "Return single integer value",
            "Value should be 27 based on sample data"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5527777777777778
  },
  "dbbench-017": {
    "m_q": {
      "target_metric": {
        "value": "Count of schools with website exactly 'http://www.camp.herts.sch.uk/'",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of schools with website exactly 'http://www.camp.herts.sch.uk/'",
          "count of schools",
          "Count of schools with the website 'http://www.camp.herts.sch.uk/'"
        ]
      },
      "filters": {
        "value": [
          "School website = 'http://www.camp.herts.sch.uk/'",
          "School website equals 'http://www.camp.herts.sch.uk/'"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "School website = 'http://www.camp.herts.sch.uk/'"
          ],
          [
            "School website equals 'http://www.camp.herts.sch.uk/'"
          ],
          [
            "School website = 'http://www.camp.herts.sch.uk/'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Is the website URL case-sensitive?",
          "Should trailing slashes be considered in matching?",
          "Are there any missing or malformed website values?",
          "Which schools have the website http://www.camp.herts.sch.uk/?",
          "What is the exact format/string of the website URL to match?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Is the website URL case-sensitive?",
            "Should trailing slashes be considered in matching?",
            "Are there any missing or malformed website values?"
          ],
          [
            "Which schools have the website http://www.camp.herts.sch.uk/?",
            "What is the exact format/string of the website URL to match?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Schools Information"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Schools Information"
          ],
          [
            "Schools Information"
          ],
          [
            "Schools Information"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "school": "school name (text)",
          "school website": "URL (text)"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {
            "School": "school name (text)",
            "School website": "URL (text)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Sample shows truncated website URLs (e.g., 'http://www.bernardsheath.herts' appears incomplete)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sample shows truncated website URLs (e.g., 'http://www.bernardsheath.herts' appears incomplete)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "",
            "N/A",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "School column should be unique",
          "School website should be valid URLs",
          "School website must exactly match 'http://www.camp.herts.sch.uk/'",
          "Comparison should be case-sensitive for URL matching",
          "Account for potential trailing slash variations in URL",
          "The 'School website' column should contain valid URLs."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "School column should be unique",
            "School website should be valid URLs"
          ],
          [
            "School website must exactly match 'http://www.camp.herts.sch.uk/'",
            "Comparison should be case-sensitive for URL matching",
            "Account for potential trailing slash variations in URL"
          ],
          [
            "The 'School website' column should contain valid URLs."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows with missing School website values",
          "Consider exact string match for website URL",
          "Filter rows where School website == 'http://www.camp.herts.sch.uk/'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows with missing School website values",
            "Consider exact string match for website URL"
          ],
          [
            "Filter rows where School website == 'http://www.camp.herts.sch.uk/'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct schools matching the website pattern"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct schools matching the website pattern"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Output should be a single integer representing the count",
          "Count should be >= 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count"
          ],
          [
            "Output should be a single integer representing the count",
            "Count should be >= 0"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5583333333333335
  },
  "dbbench-018": {
    "m_q": {
      "target_metric": {
        "value": "Count of home losses",
        "confidence": 0.6666666666666666,
        "votes": [
          "Count of home losses",
          "Count of home losses",
          "Count of home losses where PF is 13"
        ]
      },
      "filters": {
        "value": [
          "PF = 13",
          "PF equals 13"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "PF = 13"
          ],
          [
            "PF equals 13"
          ],
          [
            "PF = 13"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What does PF represent in this dataset?",
          "Are there any rows where PF equals 13?",
          "How should missing values in Home losses be handled?",
          "Which rows have PF = 13?",
          "What is the value of 'Home losses' for rows where PF = 13?",
          "Sum the home losses values for filtered rows"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What does PF represent in this dataset?",
            "Are there any rows where PF equals 13?",
            "How should missing values in Home losses be handled?"
          ],
          [
            "Which rows have PF = 13?",
            "What is the value of 'Home losses' for rows where PF = 13?",
            "Sum the home losses values for filtered rows"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "NFL Team Records Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NFL Team Records Table"
          ],
          [
            "NFL Team Records Table"
          ],
          [
            "NFL Team Records Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column 'Num' appears to have duplicate values (e.g., 4 appears for multiple teams)",
          "Some teams appear multiple times with different Num values (e.g., Kansas City Chiefs appears with Num=1 and Num=16)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column 'Num' appears to have duplicate values (e.g., 4 appears for multiple teams)",
            "Some teams appear multiple times with different Num values (e.g., Kansas City Chiefs appears with Num=1 and Num=16)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "pf": "Points For (total points scored by team)",
          "pa": "Points Against (total points allowed by team)",
          "pct": "Win percentage",
          "home win pct.": "Percentage",
          "road win pct.": "Percentage",
          "home losses": "count of games lost at home",
          "w": "wins count",
          "l": "losses count"
        },
        "confidence": 0.4583333333333334,
        "votes": [
          {
            "PF": "Points For (total points scored by team)",
            "PA": "Points Against (total points allowed by team)",
            "PCT": "Win percentage",
            "Home Win Pct.": "Percentage",
            "Road Win Pct.": "Percentage"
          },
          {
            "PF": "points (total points scored)",
            "Home losses": "count of games lost at home",
            "W": "wins count",
            "L": "losses count",
            "PCT": "percentage (win percentage)",
            "PA": "points (total points allowed)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "PCT, Home Win Pct., and Road Win Pct. columns contain string values with decimal points (e.g., '.533', '1.000')",
          "Some percentage columns contain '\u2014\u2013' as missing values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "PCT, Home Win Pct., and Road Win Pct. columns contain string values with decimal points (e.g., '.533', '1.000')",
            "Some percentage columns contain '\u2014\u2013' as missing values"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "\u2014\u2013",
          "none"
        ],
        "confidence": 0.7333333333333332,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "\u2014\u2013",
            "none"
          ],
          [
            "N/A",
            "\u2014\u2013",
            "none"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 17.0,
        "confidence": 1.0,
        "votes": [
          17.0,
          17.0,
          17.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Home losses should be non-negative integers",
          "Home losses \u2264 HOME games",
          "PF should be numeric (though stored as object)",
          "PF column must be converted from object to numeric type for comparison",
          "Home losses column must be converted from object to numeric type for aggregation",
          "Filter condition: PF = 13 (exact match)",
          "Handle sentinel values ('\u2014\u2013', 'N/A') appropriately during numeric conversion",
          "PF and Home losses must be numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Home losses should be non-negative integers",
            "Home losses \u2264 HOME games",
            "PF should be numeric (though stored as object)"
          ],
          [
            "PF column must be converted from object to numeric type for comparison",
            "Home losses column must be converted from object to numeric type for aggregation",
            "Filter condition: PF = 13 (exact match)",
            "Handle sentinel values ('\u2014\u2013', 'N/A') appropriately during numeric conversion"
          ],
          [
            "PF and Home losses must be numeric"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where PF column equals '13'",
          "Exclude rows where Home losses is missing or non-numeric",
          "Convert PF to numeric, filter where PF == 13",
          "Home losses > 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where PF column equals '13'",
            "Exclude rows where Home losses is missing or non-numeric"
          ],
          [
            "Convert PF to numeric, filter where PF == 13"
          ],
          [
            "Home losses > 0"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if PF=13 exists in the dataset",
          "Verify Home losses column can be converted to numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if PF=13 exists in the dataset",
            "Verify Home losses column can be converted to numeric"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Handle cases where no rows match PF=13 (return 0)",
          "Return a single numeric value representing total home losses when PF equals 13",
          "If no rows match PF = 13, return 0 or indicate no matching records"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count",
            "Handle cases where no rows match PF=13 (return 0)"
          ],
          [
            "Return a single numeric value representing total home losses when PF equals 13",
            "If no rows match PF = 13, return 0 or indicate no matching records"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5845833333333335
  },
  "dbbench-019": {
    "m_q": {
      "target_metric": {
        "value": "Crude death rate corresponding to a specific death count of 106,000",
        "confidence": 0.3333333333333333,
        "votes": [
          "Crude death rate corresponding to a specific death count of 106,000",
          "Crude Death Rate corresponding to Deaths value of 106 000",
          "Crude Death Rate corresponding to Deaths = 106 000"
        ]
      },
      "filters": {
        "value": [
          "Deaths = 106 000",
          "Deaths == '106 000'"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Deaths = 106 000"
          ],
          [
            "Deaths == '106 000'"
          ],
          [
            "Deaths = 106 000"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Period"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Period"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which period has exactly 106,000 deaths?",
          "What is the crude death rate value in that same row?",
          "Which period(s) have Deaths equal to 106 000?",
          "What is the Crude Death Rate for the period where Deaths equals 106 000?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which period has exactly 106,000 deaths?",
            "What is the crude death rate value in that same row?"
          ],
          [
            "Which period(s) have Deaths equal to 106 000?",
            "What is the Crude Death Rate for the period where Deaths equals 106 000?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Population Dynamics Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Population Dynamics Table"
          ],
          [
            "Population Dynamics Table"
          ],
          [
            "Population Dynamics Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All columns stored as object/string type despite containing numeric data with spaces"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns stored as object/string type despite containing numeric data with spaces"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "births": "count",
          "deaths": "count",
          "difference": "count",
          "crude birth rate": "per 1000 population",
          "crude death rate": "per 1000 population",
          "natural increase": "per 1000 population",
          "total fertility rate": "children per woman",
          "infant mortality rate": "per 1000 live births"
        },
        "confidence": 0.9166666666666666,
        "votes": [
          {
            "Births": "count",
            "Deaths": "count",
            "Difference": "count",
            "Crude Birth Rate": "per 1000 population",
            "Crude Death Rate": "per 1000 population",
            "Natural Increase": "per 1000 population",
            "Total Fertility Rate": "children per woman",
            "Infant Mortality Rate": "per 1000 live births"
          },
          {
            "Deaths": "count with thousands separator (space)",
            "Crude Death Rate": "per 1000 or per 10000 population (needs verification)",
            "Births": "count with thousands separator (space)",
            "Difference": "count with thousands separator (space)",
            "Crude Birth Rate": "rate per population unit",
            "Natural Increase": "rate per population unit",
            "Total Fertility Rate": "per 100 women or scaled value",
            "Infant Mortality Rate": "per 1000 live births or scaled value"
          },
          {
            "Births": "Number of births",
            "Deaths": "Number of deaths",
            "Crude Birth Rate": "Per 1000 population",
            "Crude Death Rate": "Per 1000 population",
            "Total Fertility Rate": "Children per woman",
            "Infant Mortality Rate": "Per 1000 live births"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Numeric values contain spaces (e.g., '106 000') requiring parsing",
          "Crude rates appear to be per 1000 based on typical demographic conventions",
          "Deaths values use space as thousands separator (e.g., '106 000')",
          "Crude Death Rate appears to be scaled (e.g., 91 for Deaths 106 000)",
          "All numeric columns stored as string/object dtype",
          "Rate values appear to be multiplied by 10 or 100"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric values contain spaces (e.g., '106 000') requiring parsing",
            "Crude rates appear to be per 1000 based on typical demographic conventions"
          ],
          [
            "Deaths values use space as thousands separator (e.g., '106 000')",
            "Crude Death Rate appears to be scaled (e.g., 91 for Deaths 106 000)",
            "All numeric columns stored as string/object dtype",
            "Rate values appear to be multiplied by 10 or 100"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Deaths column must be parsed to integer by removing spaces",
          "Only one row should match the filter 'Deaths = 106 000'",
          "Crude death rate should be retrieved from the same row as the matching death count",
          "Deaths value must exactly match '106 000' (with space separator)",
          "Result should be a single Crude Death Rate value",
          "The 'Deaths' column needs to be cleaned by removing spaces and converted to numeric type.",
          "The 'Crude Death Rate' column needs to be cleaned by removing spaces and converted to numeric type."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Deaths column must be parsed to integer by removing spaces",
            "Only one row should match the filter 'Deaths = 106 000'",
            "Crude death rate should be retrieved from the same row as the matching death count"
          ],
          [
            "Deaths value must exactly match '106 000' (with space separator)",
            "Result should be a single Crude Death Rate value"
          ],
          [
            "The 'Deaths' column needs to be cleaned by removing spaces and converted to numeric type.",
            "The 'Crude Death Rate' column needs to be cleaned by removing spaces and converted to numeric type."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Parse 'Deaths' column to integer: replace(' ', '') then convert to int",
          "Filter rows where Deaths column equals '106 000'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Parse 'Deaths' column to integer: replace(' ', '') then convert to int"
          ],
          [
            "Filter rows where Deaths column equals '106 000'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify that crude death rate values are consistent with death counts and population assumptions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify that crude death rate values are consistent with death counts and population assumptions"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single numeric value for crude death rate",
          "Include period context in explanation if needed",
          "Return the Crude Death Rate value for the matching period",
          "Handle string-to-numeric conversion if needed",
          "Verify only one period matches the Deaths criterion"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single numeric value for crude death rate",
            "Include period context in explanation if needed"
          ],
          [
            "Return the Crude Death Rate value for the matching period",
            "Handle string-to-numeric conversion if needed",
            "Verify only one period matches the Deaths criterion"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6208333333333333
  },
  "dbbench-020": {
    "m_q": {
      "target_metric": {
        "value": "total number of represents for Clary Sermina Delgado Cid",
        "confidence": 0.3333333333333333,
        "votes": [
          "total number of represents for Clary Sermina Delgado Cid",
          "Total number of represents (regions/locations)",
          "Count of distinct values in the 'Represents' column for the contestant 'Clary Sermina Delgado Cid'"
        ]
      },
      "filters": {
        "value": [
          "Contestant = 'Clary Sermina Delgado Cid'",
          "Contestant is 'Clary Sermina Delgado Cid'"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Contestant = 'Clary Sermina Delgado Cid'"
          ],
          [
            "Contestant = 'Clary Sermina Delgado Cid'"
          ],
          [
            "Contestant is 'Clary Sermina Delgado Cid'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which location does Clary Sermina Delgado Cid represent?",
          "How many times does this contestant appear in the data?",
          "Find the row(s) where Contestant is 'Clary Sermina Delgado Cid'",
          "Count the number of distinct 'Represents' values for this contestant"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which location does Clary Sermina Delgado Cid represent?",
            "How many times does this contestant appear in the data?"
          ],
          [
            "Find the row(s) where Contestant is 'Clary Sermina Delgado Cid'",
            "Count the number of distinct 'Represents' values for this contestant"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "contestants"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "contestants"
          ],
          [
            "contestants"
          ],
          [
            "contestants"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "height (cm)": "centimeters",
          "height (ft)": "feet and inches",
          "age": "years"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Height (cm)": "meters (incorrectly labeled as cm)",
            "Height (ft)": "feet and inches",
            "Age": "years"
          },
          {
            "Height (cm)": "centimeters",
            "Height (ft)": "feet and inches",
            "Age": "years"
          },
          {
            "Age": "years",
            "Height (cm)": "centimeters",
            "Height (ft)": "feet"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Height (cm) column appears to contain values in meters (e.g., 1.71) rather than centimeters"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Height (cm) column appears to contain values in meters (e.g., 1.71) rather than centimeters"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each contestant should have exactly one 'Represents' value",
          "Contestant names should be unique",
          "Contestant name must match exactly: 'Clary Sermina Delgado Cid'",
          "Each contestant should have only one 'Represents' value",
          "Result should be 1 if contestant exists with one representation",
          "The 'Contestant' column should be unique, but it is not enforced in this question.",
          "The 'Represents' column should contain valid locations."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each contestant should have exactly one 'Represents' value",
            "Contestant names should be unique"
          ],
          [
            "Contestant name must match exactly: 'Clary Sermina Delgado Cid'",
            "Each contestant should have only one 'Represents' value",
            "Result should be 1 if contestant exists with one representation"
          ],
          [
            "The 'Contestant' column should be unique, but it is not enforced in this question.",
            "The 'Represents' column should contain valid locations."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Case-sensitive exact match on Contestant column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Case-sensitive exact match on Contestant column"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate contestant entries"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate contestant entries"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric value representing count",
          "Return a single integer count",
          "Count should be 0 if contestant not found, otherwise typically 1",
          "The output should be a single integer representing the count."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric value representing count"
          ],
          [
            "Return a single integer count",
            "Count should be 0 if contestant not found, otherwise typically 1"
          ],
          [
            "The output should be a single integer representing the count."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5916666666666668
  },
  "dbbench-021": {
    "m_q": {
      "target_metric": {
        "value": "count of locomotives with operational status",
        "confidence": 0.3333333333333333,
        "votes": [
          "count of locomotives with operational status",
          "count of locomotives",
          "Count of locomotives with 'Operational' status"
        ]
      },
      "filters": {
        "value": [
          "Status = 'Operational'",
          "Status == 'Operational'"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Status = 'Operational'"
          ],
          [
            "Status == 'Operational'"
          ],
          [
            "Status = 'Operational'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What does 'Operational' status mean in this dataset?",
          "Are there any locomotives with ambiguous operational status?",
          "Should locomotives 'Under restoration' or 'Under Repair' be considered operational?",
          "What values exist in the Status column?",
          "How many rows have Status equal to 'Operational'?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What does 'Operational' status mean in this dataset?",
            "Are there any locomotives with ambiguous operational status?",
            "Should locomotives 'Under restoration' or 'Under Repair' be considered operational?"
          ],
          [
            "What values exist in the Status column?",
            "How many rows have Status equal to 'Operational'?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Locomotive Inventory"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Locomotive Inventory"
          ],
          [
            "Locomotive Inventory"
          ],
          [
            "Locomotive Inventory"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "number": "locomotive identifier",
          "status": "categorical status"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {
            "Number": "locomotive identifier",
            "Status": "categorical status"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Status column contains categorical values that need consistent interpretation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Status column contains categorical values that need consistent interpretation"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "~",
          "NA",
          "N/A"
        ],
        "confidence": 0.5833333333333333,
        "votes": [
          [
            "",
            "~"
          ],
          [
            "~",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Status column must contain consistent values",
          "Each locomotive should have a unique identifier (Number or Name)",
          "Status column must contain valid operational states",
          "Each row represents one locomotive",
          "The 'Status' column must contain a valid status value (e.g., Operational, Under Repair, Stored, Static Display)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Status column must contain consistent values",
            "Each locomotive should have a unique identifier (Number or Name)"
          ],
          [
            "Status column must contain valid operational states",
            "Each row represents one locomotive"
          ],
          [
            "The 'Status' column must contain a valid status value (e.g., Operational, Under Repair, Stored, Static Display)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where Status is not 'Operational'",
          "Consider if 'Under restoration' or 'Under Repair' should be excluded",
          "Filter rows where Status equals 'Operational' (case-sensitive)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Status is not 'Operational'",
            "Consider if 'Under restoration' or 'Under Repair' should be excluded"
          ],
          [
            "Filter rows where Status equals 'Operational' (case-sensitive)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct values in Status column",
          "Check for missing values in Status column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct values in Status column",
            "Check for missing values in Status column"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Should include locomotives with empty Number but valid Name",
          "Return a single integer count",
          "Count only locomotives with Status='Operational'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count",
            "Should include locomotives with empty Number but valid Name"
          ],
          [
            "Return a single integer count",
            "Count only locomotives with Status='Operational'"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5375000000000002
  },
  "dbbench-022": {
    "m_q": {
      "target_metric": {
        "value": "count of games played by the Badgers in October",
        "confidence": 0.6666666666666666,
        "votes": [
          "count of games played by the Badgers in October",
          "count of games played in October",
          "Count of games played by the Badgers in October"
        ]
      },
      "filters": {
        "value": [
          "Date column contains 'October'",
          "Team is Badgers (implied from context)",
          "Date contains 'October'",
          "Date is in October",
          "Team is the Badgers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date column contains 'October'",
            "Team is Badgers (implied from context)"
          ],
          [
            "Date contains 'October'"
          ],
          [
            "Date is in October",
            "Team is the Badgers"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How to identify Badgers games from the schedule?",
          "How to extract month from Date column?",
          "Are there multiple seasons in the data?",
          "Which rows have dates in October?",
          "How many rows match the October filter?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How to identify Badgers games from the schedule?",
            "How to extract month from Date column?",
            "Are there multiple seasons in the data?"
          ],
          [
            "Which rows have dates in October?",
            "How many rows match the October filter?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Football Schedule"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Football Schedule"
          ],
          [
            "Football Schedule"
          ],
          [
            "Football Schedule"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Date format varies (e.g., 'October 6' vs 'January 1, 2013')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date format varies (e.g., 'October 6' vs 'January 1, 2013')"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "attendance": "number of people",
          "time": "time of day",
          "date": "string representation of date (Month Day format)"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "Attendance": "number of people",
            "Time": "time of day"
          },
          {
            "Date": "string representation of date (Month Day format)",
            "Attendance": "number of people (string with comma separator)"
          },
          {
            "Attendance": "number of attendees"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Date column contains both simple month-day and full date formats"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date column contains both simple month-day and full date formats"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 8.0,
        "confidence": 1.0,
        "votes": [
          8.0,
          8.0,
          8.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "All games in the file are Badgers games (implied from context)",
          "Date column must be parsed to extract month",
          "Date must be parseable to identify month",
          "Only count games where Date starts with 'October'",
          "The 'Date' column must be parsed to determine the month.",
          "The team name 'Badgers' needs to be identified either in the 'Opponent#' or 'Site' column."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All games in the file are Badgers games (implied from context)",
            "Date column must be parsed to extract month"
          ],
          [
            "Date must be parseable to identify month",
            "Only count games where Date starts with 'October'"
          ],
          [
            "The 'Date' column must be parsed to determine the month.",
            "The team name 'Badgers' needs to be identified either in the 'Opponent#' or 'Site' column."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Date contains 'October'",
          "Exclude rows with null/empty Date",
          "Filter rows where Date column starts with 'October'",
          "Month of 'Date' = October"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Date contains 'October'",
            "Exclude rows with null/empty Date"
          ],
          [
            "Filter rows where Date column starts with 'October'"
          ],
          [
            "Month of 'Date' = October"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct October dates",
          "Verify no duplicate dates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct October dates",
            "Verify no duplicate dates"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Return single integer representing count of October games"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count"
          ],
          [
            "Return single integer representing count of October games"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.588888888888889
  },
  "dbbench-023": {
    "m_q": {
      "target_metric": {
        "value": "total count of distinct website entries",
        "confidence": 0.3333333333333333,
        "votes": [
          "total count of distinct website entries",
          "total count of websites",
          "Total number of websites listed in the Educational Websites Table"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Are there duplicate website names?",
          "Should we count all rows regardless of data quality?",
          "Does 'Name' column uniquely identify websites?",
          "How many rows are in the Educational Websites Table?",
          "Does each row represent a unique website?",
          "Are there any duplicate entries that should be excluded?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Are there duplicate website names?",
            "Should we count all rows regardless of data quality?",
            "Does 'Name' column uniquely identify websites?"
          ],
          [
            "How many rows are in the Educational Websites Table?",
            "Does each row represent a unique website?",
            "Are there any duplicate entries that should be excluded?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Educational Websites Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Educational Websites Table"
          ],
          [
            "Educational Websites Table"
          ],
          [
            "Educational Websites Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "cost": "currency/year or 'Free'",
          "target age": "age range or grade level",
          "name": "website name (text)",
          "topic": "subject area (text)",
          "advertising": "advertising presence (categorical text)"
        },
        "confidence": 0.4666666666666666,
        "votes": [
          {
            "Cost": "currency/year or 'Free'",
            "Target age": "age range or grade level"
          },
          {
            "Name": "website name (text)",
            "Topic": "subject area (text)",
            "Cost": "pricing information (text with currency)",
            "Target age": "age range (text with numeric ranges)",
            "Advertising": "advertising presence (categorical text)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Cost column mixes 'Free' with specific currency amounts",
          "Target age uses inconsistent formats (5+, 4-17, K-8, etc.)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Cost column mixes 'Free' with specific currency amounts",
            "Target age uses inconsistent formats (5+, 4-17, K-8, etc.)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "?",
          "None",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.5333333333333334,
        "votes": [
          [
            "?",
            "None",
            ""
          ],
          [
            "?",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Name column should be non-null",
          "Each row represents one website",
          "Each row should represent a unique website",
          "Name column should contain non-null values for counting",
          "Total count should match the shape information (27 rows)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Name column should be non-null",
            "Each row represents one website"
          ],
          [
            "Each row should represent a unique website",
            "Name column should contain non-null values for counting",
            "Total count should match the shape information (27 rows)"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where Name is missing or empty"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Name is missing or empty"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct values in Name column",
          "Check for duplicate website names"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct values in Name column",
            "Check for duplicate website names"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer representing total count",
          "Return a single integer representing the total number of websites",
          "The count should include all rows in the dataset"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer representing total count"
          ],
          [
            "Return a single integer representing the total number of websites",
            "The count should include all rows in the dataset"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5166666666666668
  },
  "dbbench-024": {
    "m_q": {
      "target_metric": {
        "value": "Count of distinct college teams that have at least one player with Russian nationality",
        "confidence": 0.6666666666666666,
        "votes": [
          "Count of distinct college teams that have at least one player with Russian nationality",
          "Count of distinct college teams that have at least one player with Russian nationality",
          "Count of distinct college teams that have at least one player with a nationality of Russia"
        ]
      },
      "filters": {
        "value": [
          "Nationality == 'Russia'",
          "College/junior/club team contains 'NCAA' or university name pattern",
          "Nationality equals 'Russia'",
          "Nationality is Russia",
          "College/junior/club team is a college team"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Nationality == 'Russia'",
            "College/junior/club team contains 'NCAA' or university name pattern"
          ],
          [
            "Nationality equals 'Russia'"
          ],
          [
            "Nationality is Russia",
            "College/junior/club team is a college team"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "College/junior/club team"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "College/junior/club team"
          ],
          [
            "College/junior/club team"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which players have Russian nationality?",
          "Which of those players are on college teams?",
          "What are the distinct college teams from those players?",
          "Which players have nationality of Russia?",
          "What are the college/junior/club teams associated with Russian players?",
          "How many unique college teams are there among Russian players?",
          "Identify players with nationality Russia.",
          "Determine the college teams associated with those players.",
          "Count the distinct college teams."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which players have Russian nationality?",
            "Which of those players are on college teams?",
            "What are the distinct college teams from those players?"
          ],
          [
            "Which players have nationality of Russia?",
            "What are the college/junior/club teams associated with Russian players?",
            "How many unique college teams are there among Russian players?"
          ],
          [
            "Identify players with nationality Russia.",
            "Determine the college teams associated with those players.",
            "Count the distinct college teams."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "NHL_draft_picks"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NHL_draft_picks"
          ],
          [
            "NHL_draft_picks"
          ],
          [
            "NHL_draft_picks"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "pick": "draft pick number",
          "player": "player name",
          "position": "hockey position",
          "nationality": "country of origin",
          "nhl team": "NHL franchise name",
          "college/junior/club team": "team name with league identifier"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {
            "Pick": "draft pick number",
            "Player": "player name",
            "Position": "hockey position",
            "Nationality": "country of origin",
            "NHL team": "NHL franchise name",
            "College/junior/club team": "team name with league identifier"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "College/junior/club team column contains mixed types: college teams, junior teams, and club teams from different countries"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "College/junior/club team column contains mixed types: college teams, junior teams, and club teams from different countries"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Nationality column must contain 'Russia' values",
          "College/junior/club team column must be parsed to identify college teams vs other types",
          "Nationality must exactly match 'Russia' (case-sensitive check may be needed)",
          "College/junior/club team values must be non-null for counting",
          "Each unique team name should be counted only once regardless of how many Russian players from that team",
          "The 'College/junior/club team' column may contain values that are not college teams. Need to filter to only include college teams."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Nationality column must contain 'Russia' values",
            "College/junior/club team column must be parsed to identify college teams vs other types"
          ],
          [
            "Nationality must exactly match 'Russia' (case-sensitive check may be needed)",
            "College/junior/club team values must be non-null for counting",
            "Each unique team name should be counted only once regardless of how many Russian players from that team"
          ],
          [
            "The 'College/junior/club team' column may contain values that are not college teams. Need to filter to only include college teams."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Nationality = 'Russia'",
          "Filter rows where College/junior/club team indicates NCAA or university affiliation",
          "Filter rows where Nationality == 'Russia'",
          "Extract distinct College/junior/club team values from filtered results",
          "College/junior/club team is a college team"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Nationality = 'Russia'",
            "Filter rows where College/junior/club team indicates NCAA or university affiliation"
          ],
          [
            "Filter rows where Nationality == 'Russia'",
            "Extract distinct College/junior/club team values from filtered results"
          ],
          [
            "College/junior/club team is a college team"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct college teams after filtering"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct college teams after filtering"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Return a single integer representing the count of unique college teams",
          "Count should be zero if no Russian players exist in dataset"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count"
          ],
          [
            "Return a single integer representing the count of unique college teams",
            "Count should be zero if no Russian players exist in dataset"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6000000000000001
  },
  "dbbench-025": {
    "m_q": {
      "target_metric": {
        "value": "total count of films",
        "confidence": 0.6666666666666666,
        "votes": [
          "count of distinct films",
          "total count of films",
          "Total count of films"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Are there duplicate film titles?",
          "Should films with missing ratings be included?",
          "Should multi-part films (like Nymphomaniac: Volume I and II) count as one or two films?",
          "How many rows are in the Film Ratings dataset?",
          "Are there any duplicate films in the dataset?",
          "Should films with N/A ratings still be counted?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Are there duplicate film titles?",
            "Should films with missing ratings be included?",
            "Should multi-part films (like Nymphomaniac: Volume I and II) count as one or two films?"
          ],
          [
            "How many rows are in the Film Ratings dataset?",
            "Are there any duplicate films in the dataset?",
            "Should films with N/A ratings still be counted?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Film Ratings"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Film Ratings"
          ],
          [
            "Film Ratings"
          ],
          [
            "Film Ratings"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "rotten tomatoes": "percentage",
          "metacritic": "score out of 100",
          "imdb": "score out of 10",
          "year": "year",
          "film": "film_title"
        },
        "confidence": 0.8,
        "votes": [
          {
            "Rotten Tomatoes": "percentage",
            "Metacritic": "score out of 100",
            "IMDb": "score out of 10"
          },
          {
            "Year": "year",
            "Film": "film_title",
            "Rotten Tomatoes": "percentage",
            "Metacritic": "percentage",
            "IMDb": "rating_out_of_10"
          },
          {
            "Year": "year",
            "Rotten Tomatoes": "percentage",
            "Metacritic": "score",
            "IMDb": "rating"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Rotten Tomatoes and Metacritic columns contain 'N/A' values",
          "IMDb column contains scores with '/10' suffix"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rotten Tomatoes and Metacritic columns contain 'N/A' values",
            "IMDb column contains scores with '/10' suffix"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "N/A",
            ""
          ],
          [
            "N/A"
          ],
          [
            "N/A"
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Film column should not contain null values",
          "Year should be between reasonable film production years",
          "Each row represents one film",
          "Film column should contain unique titles",
          "Year should be a valid year value",
          "The 'Film' column should contain unique values for each film."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Film column should not contain null values",
            "Year should be between reasonable film production years"
          ],
          [
            "Each row represents one film",
            "Film column should contain unique titles",
            "Year should be a valid year value"
          ],
          [
            "The 'Film' column should contain unique values for each film."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where Film is null or empty",
          "Consider filtering by Year range if needed"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Film is null or empty",
            "Consider filtering by Year range if needed"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Film entries",
          "Verify Year distribution",
          "Check for duplicate film titles",
          "Verify row count matches unique film count"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Film entries",
            "Verify Year distribution"
          ],
          [
            "Check for duplicate film titles",
            "Verify row count matches unique film count"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Return single integer value representing total number of films",
          "The output should be a single integer representing the total number of films."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count"
          ],
          [
            "Return single integer value representing total number of films"
          ],
          [
            "The output should be a single integer representing the total number of films."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5566666666666669
  },
  "dbbench-026": {
    "m_q": {
      "target_metric": {
        "value": "count of nations with zero silver medals",
        "confidence": 1.0,
        "votes": [
          "count of nations with zero silver medals",
          "count of nations with zero silver medals",
          "count of nations with zero silver medals"
        ]
      },
      "filters": {
        "value": [
          "Silver = 0"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Silver = 0"
          ],
          [
            "Silver = 0"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Nation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Nation"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which nations have Silver = 0?",
          "How many nations meet this condition?",
          "How many such nations exist?"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "Which nations have Silver = 0?",
            "How many nations meet this condition?"
          ],
          [
            "Which nations have Silver = 0?",
            "How many such nations exist?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Olympic Medal Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Olympic Medal Table"
          ],
          [
            "Olympic Medal Table"
          ],
          [
            "Olympic Medal Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "gold": "medal count",
          "silver": "medal count",
          "bronze": "medal count",
          "total": "medal count",
          "rank": "ordinal position"
        },
        "confidence": 0.8666666666666666,
        "votes": [
          {
            "Gold": "medal count",
            "Silver": "medal count",
            "Bronze": "medal count",
            "Total": "medal count"
          },
          {
            "Rank": "ordinal position",
            "Gold": "count of gold medals",
            "Silver": "count of silver medals",
            "Bronze": "count of bronze medals",
            "Total": "count of total medals"
          },
          {
            "Gold": "number of medals",
            "Silver": "number of medals",
            "Bronze": "number of medals",
            "Total": "number of medals"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Silver >= 0",
          "Total = Gold + Silver + Bronze",
          "Rank values should be unique",
          "Silver column must be non-negative integer",
          "Total should equal Gold + Silver + Bronze",
          "All nations in table should have at least one medal (Total >= 1)",
          "Silver column must be non-negative"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Silver >= 0",
            "Total = Gold + Silver + Bronze",
            "Rank values should be unique"
          ],
          [
            "Silver column must be non-negative integer",
            "Total should equal Gold + Silver + Bronze",
            "All nations in table should have at least one medal (Total >= 1)"
          ],
          [
            "Silver column must be non-negative"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Silver = 0",
          "Nation with Silver == 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Filter rows where Silver = 0"
          ],
          [
            "Nation with Silver == 0"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify Silver column contains only non-negative integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify Silver column contains only non-negative integers"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Return a single integer count"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count"
          ],
          [
            "Return a single integer count"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.598888888888889
  },
  "dbbench-027": {
    "m_q": {
      "target_metric": {
        "value": "Count of award-winning films where the opening film is 'Encounters at the End of the World'",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of award-winning films where the opening film is 'Encounters at the End of the World'",
          "Count of award-winning films",
          "Count of award-winning films where 'Encounters at the End of the World' was the opening film"
        ]
      },
      "filters": {
        "value": [
          "Opening Film = 'Encounters at the End of the World'",
          "Award-Winning Film is not null",
          "Opening Film equals 'Encounters at the End of the World'",
          "'Opening Film' is 'Encounters at the End of the World'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Opening Film = 'Encounters at the End of the World'",
            "Award-Winning Film is not null"
          ],
          [
            "Opening Film equals 'Encounters at the End of the World'"
          ],
          [
            "'Opening Film' is 'Encounters at the End of the World'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which year had 'Encounters at the End of the World' as opening film?",
          "What is the award-winning film for that year?",
          "Is there exactly one matching record?",
          "Which year had 'Encounters at the End of the World' as the opening film?",
          "How many award-winning films are listed for that specific row?"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Which year had 'Encounters at the End of the World' as opening film?",
            "What is the award-winning film for that year?",
            "Is there exactly one matching record?"
          ],
          [
            "Which year had 'Encounters at the End of the World' as the opening film?",
            "What is the award-winning film for that year?",
            "How many award-winning films are listed for that specific row?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "International Documentary Film Festival Amsterdam (IDFA)"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "International Documentary Film Festival Amsterdam (IDFA)"
          ],
          [
            "International Documentary Film Festival Amsterdam (IDFA)"
          ],
          [
            "International Documentary Film Festival Amsterdam (IDFA)"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "number of screening": "films and televised counts",
          "year": "year",
          "date (opening)": "date string (Month Day)",
          "date (closing)": "date string (Month Day)",
          "award-winning film": "film title (text)"
        },
        "confidence": 0.4666666666666666,
        "votes": [
          {
            "Number of Screening": "films and televised counts",
            "Year": "year"
          },
          {
            "Year": "calendar year",
            "Date (Opening)": "date string (Month Day)",
            "Date (Closing)": "date string (Month Day)",
            "Number of Screening": "count of films with mixed format text",
            "Award-Winning Film": "film title (text)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "'Number of Screening' column contains combined film and televised counts as text",
          "Date columns are stored as text objects"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Number of Screening' column contains combined film and televised counts as text",
            "Date columns are stored as text objects"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each row represents one festival year",
          "Opening Film should be unique per year",
          "Award-Winning Film should be unique per year",
          "Opening Film must match 'Encounters at the End of the World' exactly",
          "Award-Winning Film column may contain single film or multiple films",
          "The 'Opening Film' column must be checked for exact string match with 'Encounters at the End of the World'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each row represents one festival year",
            "Opening Film should be unique per year",
            "Award-Winning Film should be unique per year"
          ],
          [
            "Opening Film must match 'Encounters at the End of the World' exactly",
            "Award-Winning Film column may contain single film or multiple films"
          ],
          [
            "The 'Opening Film' column must be checked for exact string match with 'Encounters at the End of the World'"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Year = 2008 (based on data sample showing 'Encounters at the End of the World' in 2008)",
          "Filter rows where Opening Film = 'Encounters at the End of the World'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year = 2008 (based on data sample showing 'Encounters at the End of the World' in 2008)"
          ],
          [
            "Filter rows where Opening Film = 'Encounters at the End of the World'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if exactly one row matches the filter condition"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if exactly one row matches the filter condition"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Return a single numeric count",
          "If Award-Winning Film contains multiple films separated by delimiters, count them appropriately",
          "If only one film is listed, return 1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count"
          ],
          [
            "Return a single numeric count",
            "If Award-Winning Film contains multiple films separated by delimiters, count them appropriately",
            "If only one film is listed, return 1"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5488888888888891
  },
  "dbbench-028": {
    "m_q": {
      "target_metric": {
        "value": "total number of participants",
        "confidence": 0.6666666666666666,
        "votes": [
          "total number of participants",
          "total number of participants",
          "Total number of participants from Norway and France"
        ]
      },
      "filters": {
        "value": [
          "Nationality in ['Norway', 'France']",
          "Nationality is Norway",
          "Nationality is France"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "Nationality in ['Norway', 'France']"
          ],
          [
            "Nationality in ['Norway', 'France']"
          ],
          [
            "Nationality is Norway",
            "Nationality is France"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Nationality"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Nationality"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How many participants from Norway?",
          "How many participants from France?",
          "Are there any missing Nationality values that need to be handled?",
          "How many participants are from Norway?",
          "How many participants are from France?",
          "What is the sum of participants from Norway and France?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How many participants from Norway?",
            "How many participants from France?",
            "Are there any missing Nationality values that need to be handled?"
          ],
          [
            "How many participants are from Norway?",
            "How many participants are from France?",
            "What is the sum of participants from Norway and France?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Biathlon Results Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Biathlon Results Table"
          ],
          [
            "Biathlon Results Table"
          ],
          [
            "Biathlon Results Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "start": "time offset (minutes:seconds)",
          "time": "race time (minutes:seconds.milliseconds)",
          "deficit": "time deficit (minutes:seconds.milliseconds or seconds)"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Start": "time offset (minutes:seconds)",
            "Time": "race time (minutes:seconds.milliseconds)",
            "Deficit": "time deficit (minutes:seconds.milliseconds or seconds)"
          },
          {
            "Time": "MM:SS.s",
            "Deficit": "MM:SS.s or +SS.s",
            "Start": "MM:SS"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Time and Deficit columns use mixed time formats (seconds vs minutes:seconds)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time and Deficit columns use mixed time formats (seconds vs minutes:seconds)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 8.0,
        "confidence": 1.0,
        "votes": [
          8.0,
          8.0,
          8.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each row represents one participant",
          "Nationality column should contain country names",
          "Bib numbers should be unique per participant",
          "Count only rows where Nationality equals 'Norway' or 'France'",
          "Exclude any rows with null or missing Name or Nationality values",
          "Nationality column should only contain valid country names"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Each row represents one participant",
            "Nationality column should contain country names",
            "Bib numbers should be unique per participant"
          ],
          [
            "Count only rows where Nationality equals 'Norway' or 'France'",
            "Each row represents one participant",
            "Exclude any rows with null or missing Name or Nationality values"
          ],
          [
            "Nationality column should only contain valid country names"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows with missing Nationality",
          "Count distinct participants by Nationality",
          "Nationality == 'Norway'",
          "Nationality == 'France'",
          "Participants from Norway",
          "Participants from France"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows with missing Nationality",
            "Count distinct participants by Nationality"
          ],
          [
            "Nationality == 'Norway'",
            "Nationality == 'France'"
          ],
          [
            "Participants from Norway",
            "Participants from France"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Bib numbers",
          "Verify Nationality values are consistent"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Bib numbers",
            "Verify Nationality values are consistent"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer total",
          "Should include participants from both Norway and France",
          "Return a single integer representing the total count",
          "Sum the counts from both countries"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer total",
            "Should include participants from both Norway and France"
          ],
          [
            "Return a single integer representing the total count",
            "Sum the counts from both countries"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6027777777777779
  },
  "dbbench-029": {
    "m_q": {
      "target_metric": {
        "value": "total count of trains that operate every day",
        "confidence": 0.3333333333333333,
        "votes": [
          "total count of trains that operate every day",
          "Count of trains that run daily",
          "Total number of trains that run daily"
        ]
      },
      "filters": {
        "value": [
          "Day column must contain 'Daily' or equivalent daily operation indicator",
          "Day == 'Daily'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Day column must contain 'Daily' or equivalent daily operation indicator"
          ],
          [
            "Day == 'Daily'"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How many trains have 'Daily' in their Day column?",
          "Are there any trains with multiple day patterns that include all days of the week?",
          "Do trains with 'N/a' in Departure time still count as running?",
          "Which column indicates the frequency/schedule of train runs?",
          "What value in the Day column represents daily trains?",
          "How many unique trains have 'Daily' in the Day column?",
          "How to handle trains that run on multiple days?",
          "How to interpret the 'Day' column to identify daily trains?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How many trains have 'Daily' in their Day column?",
            "Are there any trains with multiple day patterns that include all days of the week?",
            "Do trains with 'N/a' in Departure time still count as running?"
          ],
          [
            "Which column indicates the frequency/schedule of train runs?",
            "What value in the Day column represents daily trains?",
            "How many unique trains have 'Daily' in the Day column?"
          ],
          [
            "How to handle trains that run on multiple days?",
            "How to interpret the 'Day' column to identify daily trains?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Train Schedule"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Train Schedule"
          ],
          [
            "Train Schedule"
          ],
          [
            "Train Schedule"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "arrival": "time (HH:MM)",
          "departure": "time (HH:MM)",
          "sr. no.": "sequential_number",
          "train no.": "train_identifier",
          "day": "frequency_schedule"
        },
        "confidence": 0.4666666666666666,
        "votes": [
          {
            "Arrival": "time (HH:MM)",
            "Departure": "time (HH:MM)"
          },
          {
            "Sr. No.": "sequential_number",
            "Train No.": "train_identifier",
            "Arrival": "time_HH:MM",
            "Departure": "time_HH:MM",
            "Day": "frequency_schedule"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Day column contains inconsistent formats: single days (M), multiple days (M,Tu,W,T,F,St), 'Daily', and 'N/a' in Departure"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Day column contains inconsistent formats: single days (M), multiple days (M,Tu,W,T,F,St), 'Daily', and 'N/a' in Departure"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.9999999999999999,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "N/a"
          ],
          [
            "N/a",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Train No. should be unique",
          "Sr. No. should be sequential",
          "Arrival and Departure times should be valid time formats",
          "Day column must be checked for exact match 'Daily' (case-sensitive)",
          "Each Train No. should be unique per record",
          "Result should be a non-negative integer count",
          "The 'Day' column must be parsed to determine if a train runs daily. 'Daily' indicates the train runs every day.",
          "If 'Day' contains 'M,Tu,W,T,F,St,S' or similar combinations covering all days, it should be considered a daily train."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Train No. should be unique",
            "Sr. No. should be sequential",
            "Arrival and Departure times should be valid time formats"
          ],
          [
            "Day column must be checked for exact match 'Daily' (case-sensitive)",
            "Each Train No. should be unique per record",
            "Result should be a non-negative integer count"
          ],
          [
            "The 'Day' column must be parsed to determine if a train runs daily. 'Daily' indicates the train runs every day.",
            "If 'Day' contains 'M,Tu,W,T,F,St,S' or similar combinations covering all days, it should be considered a daily train."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude trains with 'N/a' in Departure if they don't actually run",
          "Consider trains with day patterns covering all 7 days as equivalent to 'Daily'",
          "Filter where Day column exactly equals 'Daily' (not partial match with comma-separated days)",
          "Train runs daily: 'Day' column contains 'Daily' or 'M,Tu,W,T,F,St,S' or equivalent combinations."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude trains with 'N/a' in Departure if they don't actually run",
            "Consider trains with day patterns covering all 7 days as equivalent to 'Daily'"
          ],
          [
            "Filter where Day column exactly equals 'Daily' (not partial match with comma-separated days)"
          ],
          [
            "Train runs daily: 'Day' column contains 'Daily' or 'M,Tu,W,T,F,St,S' or equivalent combinations."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Count of trains with 'Daily' vs other day patterns",
          "Check for duplicate Train No. values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count of trains with 'Daily' vs other day patterns",
            "Check for duplicate Train No. values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Should clarify whether partial day trains are included",
          "Return single integer value representing total count of daily trains",
          "Include verification that count matches number of rows where Day='Daily'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count",
            "Should clarify whether partial day trains are included"
          ],
          [
            "Return single integer value representing total count of daily trains",
            "Include verification that count matches number of rows where Day='Daily'"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5566666666666669
  },
  "dbbench-030": {
    "m_q": {
      "target_metric": {
        "value": "Count of distinct values of 'Raiders first downs' for game number 9",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of distinct values of 'Raiders first downs' for game number 9",
          "count of different values of Raiders first downs",
          "Count of distinct values in 'Raiders first downs' for 'Game' equal to 9"
        ]
      },
      "filters": {
        "value": [
          "Game = '9'",
          "Game == 9",
          "Game = 9"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Game = '9'"
          ],
          [
            "Game == 9"
          ],
          [
            "Game = 9"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the value of 'Raiders first downs' for game 9?",
          "Is there only one row for game 9?",
          "Are there any duplicate or conflicting values for game 9?",
          "How many distinct values exist for that entry?"
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "What is the value of 'Raiders first downs' for game 9?",
            "Is there only one row for game 9?",
            "Are there any duplicate or conflicting values for game 9?"
          ],
          [
            "What is the value of 'Raiders first downs' for Game 9?",
            "How many distinct values exist for that entry?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "raiders_game_stats"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "raiders_game_stats"
          ],
          [
            "raiders_game_stats"
          ],
          [
            "raiders_game_stats"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "raiders first downs": "count",
          "raiders points": "points",
          "opponents": "points",
          "attendance": "people"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Raiders first downs": "count",
            "Raiders points": "points",
            "Opponents": "points",
            "Attendance": "people"
          },
          {
            "Raiders first downs": "count",
            "Raiders points": "points",
            "Opponents": "points",
            "Attendance": "people"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "All columns stored as object/string type despite numeric content"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns stored as object/string type despite numeric content"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Game values should be unique",
          "Raiders first downs should be numeric",
          "Game 9 should exist in the data",
          "Game must equal 9",
          "Raiders first downs must be a valid numeric value",
          "The 'Game' column should contain integer-like values.",
          "'Raiders first downs' column should contain integer-like values."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Game values should be unique",
            "Raiders first downs should be numeric",
            "Game 9 should exist in the data"
          ],
          [
            "Game must equal 9",
            "Raiders first downs must be a valid numeric value"
          ],
          [
            "The 'Game' column should contain integer-like values.",
            "'Raiders first downs' column should contain integer-like values."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Game = '9'",
          "Filter rows where Game == '9' (string comparison) or Game == 9 (numeric comparison)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Game = '9'"
          ],
          [
            "Filter rows where Game == '9' (string comparison) or Game == 9 (numeric comparison)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Game values",
          "Verify Raiders first downs can be converted to numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Game values",
            "Verify Raiders first downs can be converted to numeric"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric value representing count of distinct first downs values for game 9",
          "Return a single integer representing the count of distinct Raiders first downs values for Game 9",
          "Since Game 9 is a single row, the answer should be 1 (one distinct value)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric value representing count of distinct first downs values for game 9"
          ],
          [
            "Return a single integer representing the count of distinct Raiders first downs values for Game 9",
            "Since Game 9 is a single row, the answer should be 1 (one distinct value)"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5708333333333334
  },
  "dbbench-031": {
    "m_q": {
      "target_metric": {
        "value": "count of goals scored by Brazil",
        "confidence": 0.6666666666666666,
        "votes": [
          "count of goals scored by Brazil",
          "Number of goals scored by Brazil",
          "Count of goals scored by Brazil"
        ]
      },
      "filters": {
        "value": [
          "Date = 'November 6, 1968'",
          "Opposition = 'FIFA XI'",
          "Date equals November 6, 1968",
          "Date is November 6th"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date = 'November 6, 1968'",
            "Opposition = 'FIFA XI'"
          ],
          [
            "Date equals November 6, 1968"
          ],
          [
            "Date is November 6th"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the Score column value for the November 6, 1968 game?",
          "How many goals are listed in the Brazil scorers column for that game?",
          "Does the Score column format allow direct extraction of Brazil's goals?",
          "What was the date format for November 6th in the dataset?",
          "What was the Score for the game on November 6, 1968?",
          "How to extract Brazil's goal count from the Score column?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the Score column value for the November 6, 1968 game?",
            "How many goals are listed in the Brazil scorers column for that game?",
            "Does the Score column format allow direct extraction of Brazil's goals?"
          ],
          [
            "What was the date format for November 6th in the dataset?",
            "What was the Score for the game on November 6, 1968?",
            "How to extract Brazil's goal count from the Score column?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Brazil National Football Team Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Brazil National Football Team Results"
          ],
          [
            "Brazil National Football Team Results"
          ],
          [
            "Brazil National Football Team Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "score": "goals (format: Brazil-Opposition)",
          "date": "text date format (Month Day, Year)"
        },
        "confidence": 0.5,
        "votes": [
          {
            "Score": "goals (format: Brazil-Opposition)"
          },
          {
            "Score": "goals (format: Brazil goals-Opposition goals)",
            "Date": "text date format (Month Day, Year)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Date format uses month name and day with year (e.g., 'November 6, 1968')",
          "Brazil scorers column contains player names with possible goal counts in parentheses"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date format uses month name and day with year (e.g., 'November 6, 1968')",
            "Brazil scorers column contains player names with possible goal counts in parentheses"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "-",
          "NA",
          "",
          "N/A"
        ],
        "confidence": 0.5833333333333334,
        "votes": [
          [
            "-"
          ],
          [
            "-",
            "NA",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only one row should match Date = 'November 6, 1968'",
          "Score column should be in format 'X-Y' where X is Brazil's goals",
          "Brazil scorers column should list all goal scorers for Brazil",
          "Date must match 'November 6, 1968'",
          "Score must be parseable as 'X-Y' format",
          "Result field should be W, L, or D",
          "Date column must be parsed to datetime format to filter by November 6th"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only one row should match Date = 'November 6, 1968'",
            "Score column should be in format 'X-Y' where X is Brazil's goals",
            "Brazil scorers column should list all goal scorers for Brazil"
          ],
          [
            "Date must match 'November 6, 1968'",
            "Score must be parseable as 'X-Y' format",
            "Result field should be W, L, or D"
          ],
          [
            "Date column must be parsed to datetime format to filter by November 6th"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Result = 'W' (Brazil won 2-1)",
          "Extract first number from Score column (before hyphen) for Brazil's goals",
          "Extract the year from the Date column to ensure the correct November 6th is selected if multiple exist."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Result = 'W' (Brazil won 2-1)"
          ],
          [
            "Extract first number from Score column (before hyphen) for Brazil's goals"
          ],
          [
            "Extract the year from the Date column to ensure the correct November 6th is selected if multiple exist."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify Brazil scorers count matches first number in Score column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify Brazil scorers count matches first number in Score column"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Integer count of goals",
          "Return a single integer representing the number of goals Brazil scored"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Integer count of goals"
          ],
          [
            "Return a single integer representing the number of goals Brazil scored"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5541666666666668
  },
  "dbbench-032": {
    "m_q": {
      "target_metric": {
        "value": "count of episodes",
        "confidence": 0.6666666666666666,
        "votes": [
          "count of episodes",
          "count of episodes",
          "Count of episodes that aired on Saturday, July 11, 2009"
        ]
      },
      "filters": {
        "value": [
          "US air date = 'Saturday, July 11, 2009'"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "US air date = 'Saturday, July 11, 2009'"
          ],
          [
            "US air date = 'Saturday, July 11, 2009'"
          ],
          [
            "US air date = 'Saturday, July 11, 2009'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Is the date format consistent across all rows?",
          "Are there any missing values in the US air date column?",
          "Does the dataset contain all episodes that aired on that date?",
          "Which rows have US air date equal to Saturday, July 11, 2009?",
          "How many such rows exist?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Is the date format consistent across all rows?",
            "Are there any missing values in the US air date column?",
            "Does the dataset contain all episodes that aired on that date?"
          ],
          [
            "Which rows have US air date equal to Saturday, July 11, 2009?",
            "How many such rows exist?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Episode Ratings"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Episode Ratings"
          ],
          [
            "Episode Ratings"
          ],
          [
            "Episode Ratings"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "rating": "percentage points",
          "share": "percentage points",
          "viewers (millions)": "millions of viewers"
        },
        "confidence": 0.7777777777777777,
        "votes": [
          {
            "Rating": "percentage points",
            "Share": "percentage points",
            "Viewers (millions)": "millions of viewers"
          },
          {
            "Rating": "percentage points",
            "Share": "percentage points",
            "Viewers (millions)": "millions of viewers"
          },
          {
            "Viewers (millions)": "millions"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Rating/Share (18\u201349) column contains combined values with '/' separator that need parsing"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rating/Share (18\u201349) column contains combined values with '/' separator that need parsing"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "TBA",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "TBA"
          ],
          [
            "TBA",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Episode # should be unique",
          "US air date should be valid dates",
          "Viewers (millions) should be numeric",
          "US air date must exactly match 'Saturday, July 11, 2009' including formatting",
          "Result should be a non-negative integer count"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Episode # should be unique",
            "US air date should be valid dates",
            "Viewers (millions) should be numeric"
          ],
          [
            "US air date must exactly match 'Saturday, July 11, 2009' including formatting",
            "Result should be a non-negative integer count"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where US air date exactly matches 'Saturday, July 11, 2009'",
          "Parse US air date as string and perform exact match on 'Saturday, July 11, 2009'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where US air date exactly matches 'Saturday, July 11, 2009'"
          ],
          [
            "Parse US air date as string and perform exact match on 'Saturday, July 11, 2009'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct Episode # values for the filtered date"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct Episode # values for the filtered date"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer count",
          "Single integer value representing the count of episodes"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer count"
          ],
          [
            "Single integer value representing the count of episodes"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5972222222222223
  },
  "dbbench-033": {
    "m_q": {
      "target_metric": {
        "value": "count of surfaces that are grass",
        "confidence": 0.3333333333333333,
        "votes": [
          "count of surfaces that are grass",
          "count of records where Surface is 'Grass'",
          "Count of tennis match surfaces that are grass"
        ]
      },
      "filters": {
        "value": [
          "Surface == 'Grass'",
          "Surface is 'Grass'"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Surface == 'Grass'"
          ],
          [
            "Surface == 'Grass'"
          ],
          [
            "Surface is 'Grass'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total count of matches?",
          "What percentage of matches are on grass?",
          "Are there any missing values in the Surface column?",
          "What are the unique values in the Surface column?",
          "How many records have Surface equal to 'Grass'?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total count of matches?",
            "What percentage of matches are on grass?",
            "Are there any missing values in the Surface column?"
          ],
          [
            "What are the unique values in the Surface column?",
            "How many records have Surface equal to 'Grass'?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Tennis Matches"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Tennis Matches"
          ],
          [
            "Tennis Matches"
          ],
          [
            "Tennis Matches"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "surface": "categorical (surface type)"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {
            "Surface": "categorical (surface type)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Surface values appear to be categorical (Clay, Hard, Grass) but may have inconsistent capitalization or whitespace"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Surface values appear to be categorical (Clay, Hard, Grass) but may have inconsistent capitalization or whitespace"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "w/o"
        ],
        "confidence": 0.7499999999999999,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "w/o"
          ],
          [
            "w/o",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Surface column should contain only valid surface types",
          "Each row represents one tennis match",
          "Surface values should be consistent across the dataset",
          "Surface column should contain only valid surface type values",
          "Count result should be non-negative integer",
          "Count should be <= total number of records (21)",
          "Surface column must contain valid surface types (e.g., Grass, Clay, Hard)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Surface column should contain only valid surface types",
            "Each row represents one tennis match",
            "Surface values should be consistent across the dataset"
          ],
          [
            "Surface column should contain only valid surface type values",
            "Count result should be non-negative integer",
            "Count should be <= total number of records (21)"
          ],
          [
            "Surface column must contain valid surface types (e.g., Grass, Clay, Hard)"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows with missing Surface values",
          "Consider case-insensitive matching for 'Grass'",
          "Filter records where Surface column exactly matches 'Grass' (case-sensitive)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows with missing Surface values",
            "Consider case-insensitive matching for 'Grass'"
          ],
          [
            "Filter records where Surface column exactly matches 'Grass' (case-sensitive)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct surface types",
          "Check for null/missing values in Surface column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct surface types",
            "Check for null/missing values in Surface column"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Integer count of grass surfaces",
          "Should handle potential variations like 'grass', 'Grass', 'GRASS'",
          "Return single integer value representing count of grass surface matches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Integer count of grass surfaces",
            "Should handle potential variations like 'grass', 'Grass', 'GRASS'"
          ],
          [
            "Return single integer value representing count of grass surface matches"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5458333333333335
  },
  "dbbench-034": {
    "m_q": {
      "target_metric": {
        "value": "Country of origin",
        "confidence": 1.0,
        "votes": [
          "Country of origin",
          "Country of origin",
          "Country of Origin"
        ]
      },
      "filters": {
        "value": [
          "Type = 'Disposable'",
          "Primary Cartridge = '105mm'",
          "Year of Intro < 2008",
          "Type equals 'Disposable'",
          "Primary cartridge equals '105mm'",
          "Year of intro is less than 2008",
          "Type is Disposable",
          "Primary Cartridge is 105mm"
        ],
        "confidence": 0.375,
        "votes": [
          [
            "Type = 'Disposable'",
            "Primary Cartridge = '105mm'",
            "Year of Intro < 2008"
          ],
          [
            "Type equals 'Disposable'",
            "Primary cartridge equals '105mm'",
            "Year of intro is less than 2008"
          ],
          [
            "Type is Disposable",
            "Primary Cartridge is 105mm",
            "Year of Intro is less than 2008"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the specific countries that produced disposable RPGs with 105mm primary cartridges introduced before 2008?",
          "Are there multiple entries meeting these criteria?",
          "Which records have Type = 'Disposable'?",
          "Which records have Primary cartridge = '105mm'?",
          "Which records have Year of intro < 2008?",
          "What is the intersection of all three filter conditions?",
          "What are the Country of origin values for the filtered records?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the specific countries that produced disposable RPGs with 105mm primary cartridges introduced before 2008?",
            "Are there multiple entries meeting these criteria?"
          ],
          [
            "Which records have Type = 'Disposable'?",
            "Which records have Primary cartridge = '105mm'?",
            "Which records have Year of intro < 2008?",
            "What is the intersection of all three filter conditions?",
            "What are the Country of origin values for the filtered records?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Rocket Propelled Grenade (RPG) Weapons"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Rocket Propelled Grenade (RPG) Weapons"
          ],
          [
            "Rocket Propelled Grenade (RPG) Weapons"
          ],
          [
            "Rocket Propelled Grenade (RPG) Weapons"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "primary cartridge": "millimeters (mm)",
          "year of intro": "year"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Primary cartridge": "millimeters (mm)",
            "Year of intro": "year"
          },
          {
            "Year of intro": "year",
            "Primary cartridge": "millimeters"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Year of intro is stored as object/string type instead of numeric, requiring type conversion for comparison operations",
          "Year of intro stored as object type, may need conversion to numeric for comparison",
          "Primary cartridge includes 'mm' suffix in some values, may need parsing"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year of intro is stored as object/string type instead of numeric, requiring type conversion for comparison operations"
          ],
          [
            "Year of intro stored as object type, may need conversion to numeric for comparison",
            "Primary cartridge includes 'mm' suffix in some values, may need parsing"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year of intro must be convertible to numeric for comparison",
          "Primary cartridge values must match exactly '105mm' (case-sensitive)",
          "Type values must match exactly 'Disposable' (case-sensitive)",
          "Year of intro must be numeric and less than 2008",
          "Type must exactly match 'Disposable'",
          "Primary cartridge must exactly match '105mm'",
          "All three conditions must be satisfied simultaneously",
          "Year of intro must be a valid year (integer)",
          "Primary Cartridge must be a valid cartridge size"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year of intro must be convertible to numeric for comparison",
            "Primary cartridge values must match exactly '105mm' (case-sensitive)",
            "Type values must match exactly 'Disposable' (case-sensitive)"
          ],
          [
            "Year of intro must be numeric and less than 2008",
            "Type must exactly match 'Disposable'",
            "Primary cartridge must exactly match '105mm'",
            "All three conditions must be satisfied simultaneously"
          ],
          [
            "Year of intro must be a valid year (integer)",
            "Primary Cartridge must be a valid cartridge size"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Year of intro < 2008 AND Type = 'Disposable' AND Primary Cartridge = '105mm'",
          "Convert Year of intro from object to numeric type before applying < 2008 filter",
          "Ensure case-sensitive matching for Type = 'Disposable'",
          "Ensure exact string matching for Primary cartridge = '105mm'",
          "Year of Intro < 2008"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year of intro < 2008 AND Type = 'Disposable' AND Primary Cartridge = '105mm'"
          ],
          [
            "Convert Year of intro from object to numeric type before applying < 2008 filter",
            "Ensure case-sensitive matching for Type = 'Disposable'",
            "Ensure exact string matching for Primary cartridge = '105mm'"
          ],
          [
            "Year of Intro < 2008"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate entries with same criteria",
          "Validate year values are within reasonable range (e.g., 1900-2024)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate entries with same criteria",
            "Validate year values are within reasonable range (e.g., 1900-2024)"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of distinct country names",
          "Should include any relevant metadata like weapon names if multiple results",
          "Return Country of origin value(s) that satisfy all filter conditions",
          "Handle case where no records match the criteria",
          "Return distinct Country of origin values if multiple records match"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of distinct country names",
            "Should include any relevant metadata like weapon names if multiple results"
          ],
          [
            "Return Country of origin value(s) that satisfy all filter conditions",
            "Handle case where no records match the criteria",
            "Return distinct Country of origin values if multiple records match"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6020833333333334
  },
  "dbbench-035": {
    "m_q": {
      "target_metric": {
        "value": "Builder names with GT numbers above 7",
        "confidence": 0.3333333333333333,
        "votes": [
          "Builder names with GT numbers above 7",
          "Name of builder(s) that had more than 7 GT numbers",
          "Builders with more than 7 GT numbers"
        ]
      },
      "filters": {
        "value": [
          "GT numbers > 7",
          "GT numbers count > 7"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "GT numbers > 7"
          ],
          [
            "GT numbers count > 7"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Builder"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Builder"
          ],
          [
            "Builder"
          ],
          [
            "Builder"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the threshold for 'above 7'? (e.g., >7, >=8)",
          "Should we consider maximum GT number per builder or average?",
          "How to handle GT numbers that are ranges or lists?",
          "How many GT numbers does each builder have?",
          "Which builders have a count of GT numbers greater than 7?",
          "How to parse the 'GT numbers' column to count individual numbers?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the threshold for 'above 7'? (e.g., >7, >=8)",
            "Should we consider maximum GT number per builder or average?",
            "How to handle GT numbers that are ranges or lists?"
          ],
          [
            "How many GT numbers does each builder have?",
            "Which builders have a count of GT numbers greater than 7?",
            "How to parse the 'GT numbers' column to count individual numbers?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Train Builders and their Works Numbers"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Train Builders and their Works Numbers"
          ],
          [
            "Train Builders and their Works Numbers"
          ],
          [
            "Train Builders and their Works Numbers"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "GT numbers column has object dtype but contains numeric ranges and lists"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "GT numbers column has object dtype but contains numeric ranges and lists"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "gt numbers": "locomotive identification numbers (likely sequential)",
          "works numbers": "range notation with possible newline separators",
          "cn numbers": "range notation with possible newline separators",
          "dates": "year"
        },
        "confidence": 0.41666666666666663,
        "votes": [
          {
            "GT numbers": "locomotive identification numbers (likely sequential)"
          },
          {
            "GT numbers": "range notation (e.g., '651-705' represents multiple sequential numbers)",
            "Works numbers": "range notation with possible newline separators",
            "CN numbers": "range notation with possible newline separators",
            "Dates": "year"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "GT numbers stored as strings with newline-separated ranges (e.g., '651-705')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "GT numbers stored as strings with newline-separated ranges (e.g., '651-705')"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            ""
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "GT numbers must be parsed from string ranges to numeric values",
          "Need to handle multiple GT number ranges per builder",
          "Dates are integers representing years",
          "GT numbers must be parsed from range notation to count individual values",
          "Ranges like '651-705' represent consecutive numbers from 651 to 705 inclusive",
          "Multiple ranges in a cell (separated by \\n) should all be counted",
          "Count threshold is greater than 7 (not greater than or equal to 7)",
          "GT numbers column contains ranges of numbers, which need to be expanded into individual numbers before counting."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "GT numbers must be parsed from string ranges to numeric values",
            "Need to handle multiple GT number ranges per builder",
            "Dates are integers representing years"
          ],
          [
            "GT numbers must be parsed from range notation to count individual values",
            "Ranges like '651-705' represent consecutive numbers from 651 to 705 inclusive",
            "Multiple ranges in a cell (separated by \\n) should all be counted",
            "Count threshold is greater than 7 (not greater than or equal to 7)"
          ],
          [
            "GT numbers column contains ranges of numbers, which need to be expanded into individual numbers before counting."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract all GT numbers from ranges like '651-705'",
          "Convert GT numbers to integers for comparison",
          "Filter builders where any GT number > 7",
          "Parse GT numbers ranges to expand into individual counts",
          "Calculate total count per builder by summing all ranges",
          "Filter builders where total GT numbers count > 7",
          "Count the number of GT numbers for each builder.",
          "Filter builders with a count greater than 7."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract all GT numbers from ranges like '651-705'",
            "Convert GT numbers to integers for comparison",
            "Filter builders where any GT number > 7"
          ],
          [
            "Parse GT numbers ranges to expand into individual counts",
            "Calculate total count per builder by summing all ranges",
            "Filter builders where total GT numbers count > 7"
          ],
          [
            "Count the number of GT numbers for each builder.",
            "Filter builders with a count greater than 7."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if GT numbers are sequential within ranges",
          "Verify no overlapping GT number ranges between builders"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if GT numbers are sequential within ranges",
            "Verify no overlapping GT number ranges between builders"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of builder names meeting criteria",
          "Should include supporting GT numbers as evidence",
          "Return builder name(s) as text",
          "If multiple builders qualify, return all qualifying names"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of builder names meeting criteria",
            "Should include supporting GT numbers as evidence"
          ],
          [
            "Return builder name(s) as text",
            "If multiple builders qualify, return all qualifying names"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6041666666666667
  },
  "dbbench-036": {
    "m_q": {
      "target_metric": {
        "value": "Round value(s)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Round value(s)",
          "Round number(s)",
          "Count of Rounds that satisfy the conditions"
        ]
      },
      "filters": {
        "value": [
          "School/Club Team contains 'indiana' (case-insensitive)",
          "Pick < 198 (numeric comparison)",
          "School/Club Team = 'Indiana'",
          "Pick < 198",
          "School/Club Team is 'Indiana'",
          "Pick is smaller than 198"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "School/Club Team contains 'indiana' (case-insensitive)",
            "Pick < 198 (numeric comparison)"
          ],
          [
            "School/Club Team = 'Indiana'",
            "Pick < 198"
          ],
          [
            "School/Club Team is 'Indiana'",
            "Pick is smaller than 198"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "list",
          "list",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the exact string value in 'School/Club Team' column for Indiana?",
          "Should 'Pick' be treated as numeric despite being object dtype?",
          "Are there multiple rows matching these criteria?",
          "What records have School/Club Team equal to 'Indiana'?",
          "Among those Indiana records, which have Pick values less than 198?",
          "What are the Round values for the filtered records?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the exact string value in 'School/Club Team' column for Indiana?",
            "Should 'Pick' be treated as numeric despite being object dtype?",
            "Are there multiple rows matching these criteria?"
          ],
          [
            "What records have School/Club Team equal to 'Indiana'?",
            "Among those Indiana records, which have Pick values less than 198?",
            "What are the Round values for the filtered records?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "NFL Draft Picks"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NFL Draft Picks"
          ],
          [
            "NFL Draft Picks"
          ],
          [
            "NFL Draft Picks"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Pick column has object dtype but needs numeric comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pick column has object dtype but needs numeric comparison"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "pick": "draft position number",
          "round": "draft round number"
        },
        "confidence": 0.5,
        "votes": [
          {
            "Pick": "draft position number"
          },
          {
            "Round": "draft round number",
            "Pick": "overall pick number in draft"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Pick values need conversion from string to integer for comparison",
          "Round and Pick columns are stored as 'object' dtype but represent numeric values",
          "Need to convert Round and Pick to numeric types for comparison operations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pick values need conversion from string to integer for comparison"
          ],
          [
            "Round and Pick columns are stored as 'object' dtype but represent numeric values",
            "Need to convert Round and Pick to numeric types for comparison operations"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Pick values must be convertible to integers",
          "School/Club Team values should be consistent capitalization",
          "School/Club Team must equal 'Indiana' (case-insensitive)",
          "Pick must be numeric and less than 198",
          "Round must exist and be valid for filtered records",
          "Pick must be a valid integer",
          "Round must be a valid value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pick values must be convertible to integers",
            "School/Club Team values should be consistent capitalization"
          ],
          [
            "School/Club Team must equal 'Indiana' (case-insensitive)",
            "Pick must be numeric and less than 198",
            "Round must exist and be valid for filtered records"
          ],
          [
            "Pick must be a valid integer",
            "Round must be a valid value"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert Pick to numeric before filtering",
          "Case-insensitive search for 'indiana' in School/Club Team",
          "Convert Pick column to numeric type before applying < 198 filter",
          "Apply case-insensitive string matching for 'Indiana' in School/Club Team"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert Pick to numeric before filtering",
            "Case-insensitive search for 'indiana' in School/Club Team"
          ],
          [
            "Convert Pick column to numeric type before applying < 198 filter",
            "Apply case-insensitive string matching for 'Indiana' in School/Club Team"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if Pick column contains only numeric strings",
          "Verify School/Club Team values for Indiana variations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if Pick column contains only numeric strings",
            "Verify School/Club Team values for Indiana variations"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return Round value(s) as list",
          "Include both Round and Pick values in output",
          "Return distinct Round values that satisfy both conditions",
          "If no records match, return empty result",
          "If multiple records match, return all unique Round values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return Round value(s) as list",
            "Include both Round and Pick values in output"
          ],
          [
            "Return distinct Round values that satisfy both conditions",
            "If no records match, return empty result",
            "If multiple records match, return all unique Round values"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5583333333333335
  },
  "dbbench-037": {
    "m_q": {
      "target_metric": {
        "value": "Title of the episode",
        "confidence": 0.3333333333333333,
        "votes": [
          "Title of the episode",
          "Episode title (name)",
          "episode title"
        ]
      },
      "filters": {
        "value": [
          "Directed by = 'Michael Pressman'",
          "No. in season < 10.0",
          "director is Michael Pressman",
          "episode number in season is less than 10.0"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "Directed by = 'Michael Pressman'",
            "No. in season < 10.0"
          ],
          [
            "Directed by = 'Michael Pressman'",
            "No. in season < 10.0"
          ],
          [
            "director is Michael Pressman",
            "episode number in season is less than 10.0"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which episodes were directed by Michael Pressman?",
          "Among those, which have season number less than 10.0?",
          "What is the title of the matching episode?",
          "Which of those episodes have No. in season less than 10.0?",
          "What are the titles of those episodes?"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Which episodes were directed by Michael Pressman?",
            "Among those, which have season number less than 10.0?",
            "What is the title of the matching episode?"
          ],
          [
            "Which episodes were directed by Michael Pressman?",
            "Which of those episodes have No. in season less than 10.0?",
            "What are the titles of those episodes?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "episode_table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "episode_table"
          ],
          [
            "episode_table"
          ],
          [
            "episode_table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "u.s. viewers (millions)": "millions",
          "no. in series": "episode number (series-wide)",
          "no. in season": "episode number (within season)"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "U.S. viewers (millions)": "millions"
          },
          {
            "No. in series": "episode number (series-wide)",
            "No. in season": "episode number (within season)",
            "U.S. viewers (millions)": "millions of viewers"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "No. in series and No. in season are stored as objects but contain numeric data",
          "U.S. viewers (millions) is stored as object but contains numeric data",
          "No. in season is stored as object/string type but needs numeric comparison",
          "Question specifies comparison with 10.0 (float) but column may contain string values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No. in series and No. in season are stored as objects but contain numeric data",
            "U.S. viewers (millions) is stored as object but contains numeric data"
          ],
          [
            "No. in season is stored as object/string type but needs numeric comparison",
            "Question specifies comparison with 10.0 (float) but column may contain string values"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 8.0,
        "confidence": 1.0,
        "votes": [
          8.0,
          8.0,
          8.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "No. in season should be numeric for comparison with 10.0",
          "Directed by column should contain director names",
          "Title should be unique for each episode",
          "No. in season must be convertible to numeric for comparison",
          "Directed by must match exactly 'Michael Pressman'",
          "No. in season < 10.0 (strict inequality)",
          "\"No. in season\" should be converted to numeric type for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No. in season should be numeric for comparison with 10.0",
            "Directed by column should contain director names",
            "Title should be unique for each episode"
          ],
          [
            "No. in season must be convertible to numeric for comparison",
            "Directed by must match exactly 'Michael Pressman'",
            "No. in season < 10.0 (strict inequality)"
          ],
          [
            "\"No. in season\" should be converted to numeric type for comparison"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert No. in season to numeric type before filtering",
          "Trim whitespace from Title and Directed by columns",
          "CAST(No. in season AS FLOAT) < 10.0",
          "Directed by = 'Michael Pressman'",
          "episode_table[\"Directed by\"] == \"Michael Pressman\"",
          "episode_table[\"No. in season\"].astype(float) < 10.0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert No. in season to numeric type before filtering",
            "Trim whitespace from Title and Directed by columns"
          ],
          [
            "CAST(No. in season AS FLOAT) < 10.0",
            "Directed by = 'Michael Pressman'"
          ],
          [
            "episode_table[\"Directed by\"] == \"Michael Pressman\"",
            "episode_table[\"No. in season\"].astype(float) < 10.0"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in Directed by and No. in season columns",
          "Verify numeric conversion of No. in season column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in Directed by and No. in season columns",
            "Verify numeric conversion of No. in season column"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single episode title as string",
          "Handle case where multiple episodes might match criteria",
          "Return episode title(s) as list of strings",
          "If multiple episodes match, return all matching titles",
          "If no episodes match, return empty result"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single episode title as string",
            "Handle case where multiple episodes might match criteria"
          ],
          [
            "Return episode title(s) as list of strings",
            "If multiple episodes match, return all matching titles",
            "If no episodes match, return empty result"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5505555555555557
  },
  "dbbench-038": {
    "m_q": {
      "target_metric": {
        "value": "Year of First Appearance",
        "confidence": 0.6666666666666666,
        "votes": [
          "Year of First Appearance",
          "Year of First Appearance for the Black Knights drum corps",
          "Year of first appearance"
        ]
      },
      "filters": {
        "value": [
          "Corps Name contains 'Black Knights'",
          "Number of Finals Appearances < 1",
          "Corps Name equals 'Black Knights'",
          "Number of Finals Appearances is less than 1",
          "Corps Name is 'Black Knights'"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Corps Name contains 'Black Knights'",
            "Number of Finals Appearances < 1"
          ],
          [
            "Corps Name equals 'Black Knights'",
            "Number of Finals Appearances is less than 1"
          ],
          [
            "Corps Name is 'Black Knights'",
            "Number of Finals Appearances is less than 1"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Corps Name"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Corps Name"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which corps are named 'Black Knights'?",
          "What is the 'Number of Finals Appearances' for Black Knights?",
          "What is the 'Year of First Appearance' for Black Knights?",
          "Which drum corps is named 'Black Knights'?",
          "What is the Number of Finals Appearances for the Black Knights?",
          "Is the Number of Finals Appearances for Black Knights less than 1?",
          "What is the Year of First Appearance for the Black Knights if they meet the filter criteria?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which corps are named 'Black Knights'?",
            "What is the 'Number of Finals Appearances' for Black Knights?",
            "What is the 'Year of First Appearance' for Black Knights?"
          ],
          [
            "Which drum corps is named 'Black Knights'?",
            "What is the Number of Finals Appearances for the Black Knights?",
            "Is the Number of Finals Appearances for Black Knights less than 1?",
            "What is the Year of First Appearance for the Black Knights if they meet the filter criteria?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Finals Appearances by Drum Corps"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Finals Appearances by Drum Corps"
          ],
          [
            "Finals Appearances by Drum Corps"
          ],
          [
            "Finals Appearances by Drum Corps"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "'Number of Finals Appearances' is stored as object/string but contains numeric data",
          "'Year of First Appearance' is stored as object/string but contains numeric data",
          "'Year of Most Recent Appearance' is stored as object/string but contains numeric data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Number of Finals Appearances' is stored as object/string but contains numeric data",
            "'Year of First Appearance' is stored as object/string but contains numeric data",
            "'Year of Most Recent Appearance' is stored as object/string but contains numeric data"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "number of finals appearances": "count",
          "year of first appearance": "year",
          "year of most recent appearance": "year"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Number of Finals Appearances": "count",
            "Year of First Appearance": "year",
            "Year of Most Recent Appearance": "year"
          },
          {
            "Number of Finals Appearances": "count",
            "Year of First Appearance": "year",
            "Year of Most Recent Appearance": "year"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "'All Years of Finals Appearances' contains complex string formatting with ranges and lists",
          "Year ranges use different separators: '\u2013', '-', ', '"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'All Years of Finals Appearances' contains complex string formatting with ranges and lists",
            "Year ranges use different separators: '\u2013', '-', ', '"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.111111111111111,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " "
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Corps Name must be unique",
          "Year of First Appearance \u2264 Year of Most Recent Appearance",
          "Number of Finals Appearances must match count of years in All Years of Finals Appearances",
          "Corps Name must match exactly 'Black Knights' (case-sensitive check recommended)",
          "Number of Finals Appearances must be convertible to numeric type for comparison",
          "Filter condition 'less than 1' means Number of Finals Appearances must be 0 or negative (likely 0)",
          "Year of First Appearance must exist for filtered records",
          "Number of Finals Appearances must be converted to numeric type for filtering"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Corps Name must be unique",
            "Year of First Appearance \u2264 Year of Most Recent Appearance",
            "Number of Finals Appearances must match count of years in All Years of Finals Appearances"
          ],
          [
            "Corps Name must match exactly 'Black Knights' (case-sensitive check recommended)",
            "Number of Finals Appearances must be convertible to numeric type for comparison",
            "Filter condition 'less than 1' means Number of Finals Appearances must be 0 or negative (likely 0)",
            "Year of First Appearance must exist for filtered records"
          ],
          [
            "Number of Finals Appearances must be converted to numeric type for filtering"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert 'Number of Finals Appearances' to numeric for comparison",
          "Extract 'Black Knights' from Corps Name column",
          "Convert 'Number of Finals Appearances' from string to integer before applying < 1 filter"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert 'Number of Finals Appearances' to numeric for comparison",
            "Extract 'Black Knights' from Corps Name column"
          ],
          [
            "Convert 'Number of Finals Appearances' from string to integer before applying < 1 filter"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in key columns",
          "Validate year format consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in key columns",
            "Validate year format consistency"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single year value",
          "Handle case where no Black Knights meet criteria",
          "Return a single year value",
          "If no records match the filter criteria, return null or appropriate indicator",
          "Output should be a year in integer format"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single year value",
            "Handle case where no Black Knights meet criteria"
          ],
          [
            "Return a single year value",
            "If no records match the filter criteria, return null or appropriate indicator",
            "Output should be a year in integer format"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6255555555555556
  },
  "dbbench-039": {
    "m_q": {
      "target_metric": {
        "value": "Number of households",
        "confidence": 1.0,
        "votes": [
          "Number of households",
          "Number of households",
          "Number of households"
        ]
      },
      "filters": {
        "value": [
          "County = 'Carroll'",
          "Population > 146,445",
          "County is 'Carroll'"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "County = 'Carroll'",
            "Population > 146,445"
          ],
          [
            "County = 'Carroll'",
            "Population > 146,445"
          ],
          [
            "County is 'Carroll'",
            "Population > 146,445"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the population of Carroll county?",
          "Does Carroll county have population > 146,445?",
          "If yes, what is the corresponding number of households?",
          "What is the population value for Carroll county?",
          "Is the population for Carroll county greater than 146,445?",
          "If the condition is met, what is the number of households for Carroll county?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the population of Carroll county?",
            "Does Carroll county have population > 146,445?",
            "If yes, what is the corresponding number of households?"
          ],
          [
            "What is the population value for Carroll county?",
            "Is the population for Carroll county greater than 146,445?",
            "If the condition is met, what is the number of households for Carroll county?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Income and Population by County in New Hampshire"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Income and Population by County in New Hampshire"
          ],
          [
            "Income and Population by County in New Hampshire"
          ],
          [
            "Income and Population by County in New Hampshire"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Duplicate county names in data (Grafton, Hillsborough, Sullivan, Merrimack, Belknap, Rockingham, Strafford, Coos appear multiple times)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Duplicate county names in data (Grafton, Hillsborough, Sullivan, Merrimack, Belknap, Rockingham, Strafford, Coos appear multiple times)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "per capita income": "dollars",
          "median household income": "dollars",
          "median family income": "dollars",
          "population": "count",
          "number of households": "count"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Per capita income": "dollars",
            "Median household income": "dollars",
            "Median family income": "dollars",
            "Population": "count",
            "Number of households": "count"
          },
          {
            "Per capita income": "US dollars",
            "Median household income": "US dollars",
            "Median family income": "US dollars",
            "Population": "count of individuals",
            "Number of households": "count of households"
          },
          {
            "Per capita income": "USD",
            "Median household income": "USD",
            "Median family income": "USD",
            "Population": "people",
            "Number of households": "households"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Income values have dollar signs and commas that need to be removed for numerical operations",
          "Population and household counts have commas that need to be removed",
          "Numeric columns are stored as strings with formatting characters (e.g., '$' for income, ',' for thousands separator)",
          "Population and Number of households contain comma separators that need to be removed for numeric comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Income values have dollar signs and commas that need to be removed for numerical operations",
            "Population and household counts have commas that need to be removed"
          ],
          [
            "Numeric columns are stored as strings with formatting characters (e.g., '$' for income, ',' for thousands separator)",
            "Population and Number of households contain comma separators that need to be removed for numeric comparison"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Population must be converted to numeric by removing commas and dollar signs",
          "Need to identify which Carroll county row(s) to use given duplicates",
          "Filter condition 'Population > 146,445' must be evaluated after data cleaning",
          "County must equal 'Carroll'",
          "Population must be greater than 146,445",
          "Population and Number of households must be converted from string to numeric for comparison",
          "County column should contain valid county names.",
          "Population and Number of households should be non-negative integers.",
          "Per capita income, Median household income, and Median family income should be non-negative numbers."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Population must be converted to numeric by removing commas and dollar signs",
            "Need to identify which Carroll county row(s) to use given duplicates",
            "Filter condition 'Population > 146,445' must be evaluated after data cleaning"
          ],
          [
            "County must equal 'Carroll'",
            "Population must be greater than 146,445",
            "Population and Number of households must be converted from string to numeric for comparison"
          ],
          [
            "County column should contain valid county names.",
            "Population and Number of households should be non-negative integers.",
            "Per capita income, Median household income, and Median family income should be non-negative numbers."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove non-numeric characters from Population column",
          "Convert Population to integer",
          "Filter to County = 'Carroll'",
          "Apply Population > 146445 condition",
          "Convert Population column from string to integer by removing commas",
          "Apply numeric comparison: Population > 146445"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove non-numeric characters from Population column",
            "Convert Population to integer",
            "Filter to County = 'Carroll'",
            "Apply Population > 146445 condition"
          ],
          [
            "Convert Population column from string to integer by removing commas",
            "Apply numeric comparison: Population > 146445"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if any Carroll county rows have Population > 146,445",
          "If multiple Carroll rows exist, determine which one(s) to use"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if any Carroll county rows have Population > 146,445",
            "If multiple Carroll rows exist, determine which one(s) to use"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer value representing number of households",
          "If no Carroll county has population > 146,445, return null or appropriate indicator",
          "Return the number of households as a single value if conditions are met",
          "Return null or empty result if no rows match the conditions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer value representing number of households",
            "If no Carroll county has population > 146,445, return null or appropriate indicator"
          ],
          [
            "Return the number of households as a single value if conditions are met",
            "Return null or empty result if no rows match the conditions"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6500000000000001
  },
  "dbbench-040": {
    "m_q": {
      "target_metric": {
        "value": "Nation names that have Gold > 0 and Silver = 0",
        "confidence": 0.3333333333333333,
        "votes": [
          "Nation names that have Gold > 0 and Silver = 0",
          "Nations that won at least one gold medal but won zero silver medals",
          "Nations that won gold but did not win silver medals"
        ]
      },
      "filters": {
        "value": [
          "Gold > 0",
          "Silver = 0",
          "Silver == 0"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Gold > 0",
            "Silver = 0"
          ],
          [
            "Gold > 0",
            "Silver = 0"
          ],
          [
            "Gold > 0",
            "Silver == 0"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Nation"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Nation"
          ],
          [
            "Nation"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which nations have at least one gold medal?",
          "Which nations have zero silver medals?",
          "What is the intersection of these two sets?",
          "Which nations have Gold medal count greater than 0?",
          "Which nations have Silver medal count equal to 0?",
          "What is the intersection of nations with gold medals and no silver medals?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which nations have at least one gold medal?",
            "Which nations have zero silver medals?",
            "What is the intersection of these two sets?"
          ],
          [
            "Which nations have Gold medal count greater than 0?",
            "Which nations have Silver medal count equal to 0?",
            "What is the intersection of nations with gold medals and no silver medals?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Olympic Medals"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Olympic Medals"
          ],
          [
            "Olympic Medals"
          ],
          [
            "Olympic Medals"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "rank": "ordinal position",
          "gold": "count of gold medals",
          "silver": "count of silver medals",
          "bronze": "count of bronze medals",
          "total": "sum of all medals"
        },
        "confidence": 0.9333333333333332,
        "votes": [
          {
            "Rank": "ordinal position",
            "Gold": "count of gold medals",
            "Silver": "count of silver medals",
            "Bronze": "count of bronze medals",
            "Total": "sum of all medals"
          },
          {
            "Rank": "ordinal ranking",
            "Gold": "count of medals",
            "Silver": "count of medals",
            "Bronze": "count of medals",
            "Total": "count of medals"
          },
          {
            "Gold": "number of gold medals",
            "Silver": "number of silver medals",
            "Bronze": "number of bronze medals",
            "Total": "total number of medals"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Rank column appears to restart at 1 in the sample data (e.g., row 1 is Cuba, row 10 is Japan with Rank 1), suggesting possible grouping by region or event type not shown in sample"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rank column appears to restart at 1 in the sample data (e.g., row 1 is Cuba, row 10 is Japan with Rank 1), suggesting possible grouping by region or event type not shown in sample"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total = Gold + Silver + Bronze for each row",
          "All medal counts should be non-negative integers",
          "Rank should be positive integer",
          "Gold >= 0",
          "Silver >= 0",
          "Bronze >= 0",
          "Total = Gold + Silver + Bronze",
          "Rank >= 1",
          "Gold, Silver, Bronze, and Total are non-negative integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total = Gold + Silver + Bronze for each row",
            "All medal counts should be non-negative integers",
            "Rank should be positive integer"
          ],
          [
            "Gold >= 0",
            "Silver >= 0",
            "Bronze >= 0",
            "Total = Gold + Silver + Bronze",
            "Rank >= 1"
          ],
          [
            "Gold, Silver, Bronze, and Total are non-negative integers"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Gold > 0 AND Silver = 0"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Gold > 0 AND Silver = 0"
          ],
          [
            "Gold > 0 AND Silver = 0"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify Total column consistency: sum(Gold + Silver + Bronze) = Total for each row"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify Total column consistency: sum(Gold + Silver + Bronze) = Total for each row"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of nation names",
          "Should include all nations meeting criteria across entire dataset",
          "Return list of nation names",
          "Only include nations meeting both criteria: Gold > 0 AND Silver = 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of nation names",
            "Should include all nations meeting criteria across entire dataset"
          ],
          [
            "Return list of nation names",
            "Only include nations meeting both criteria: Gold > 0 AND Silver = 0"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6466666666666667
  },
  "dbbench-041": {
    "m_q": {
      "target_metric": {
        "value": "US Cash Box value",
        "confidence": 0.6666666666666666,
        "votes": [
          "US Cash Box value",
          "US Cash Box value",
          "Count of songs that meet the specified criteria"
        ]
      },
      "filters": {
        "value": [
          "Year < 1978",
          "US Billboard = 35",
          "Year is before 1978",
          "US Billboard is 35",
          "US Cash Box is not null"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [
            "Year < 1978",
            "US Billboard = 35"
          ],
          [
            "Year < 1978",
            "US Billboard = 35"
          ],
          [
            "Year is before 1978",
            "US Billboard is 35",
            "US Cash Box is not null"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Find all records with US Billboard value of 35",
          "Filter to only records before 1978",
          "Extract the US Cash Box value from the matching record",
          "Which records have US Billboard value equal to 35?",
          "Among those records, which have Year before 1978?",
          "What is the US Cash Box value for the matching record?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Find all records with US Billboard value of 35",
            "Filter to only records before 1978",
            "Extract the US Cash Box value from the matching record"
          ],
          [
            "Which records have US Billboard value equal to 35?",
            "Among those records, which have Year before 1978?",
            "What is the US Cash Box value for the matching record?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "US Music Charts"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "US Music Charts"
          ],
          [
            "US Music Charts"
          ],
          [
            "US Music Charts"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "year",
          "us billboard": "chart position",
          "us cash box": "chart position",
          "us ac": "chart position",
          "can ac": "chart position"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Year": "year",
            "US Billboard": "chart position",
            "US Cash Box": "chart position",
            "US AC": "chart position",
            "CAN AC": "chart position"
          },
          {
            "Year": "year",
            "US Billboard": "chart_position",
            "US Cash Box": "chart_position",
            "US AC": "chart_position",
            "CAN AC": "chart_position"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Year stored as object/string instead of integer",
          "Chart positions stored as objects/strings instead of integers",
          "Missing values represented by '\u2013' (en dash) instead of standard null indicators"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year stored as object/string instead of integer",
            "Chart positions stored as objects/strings instead of integers",
            "Missing values represented by '\u2013' (en dash) instead of standard null indicators"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "\u2013",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.9166666666666666,
        "votes": [
          [
            "\u2013",
            "",
            "NA",
            "N/A"
          ],
          [
            "\u2013",
            "NA",
            ""
          ],
          [
            "NA",
            "N/A",
            "",
            "\u2013"
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be between 1972-1988 based on sample",
          "Chart positions should be integers between 1-100 (or null)",
          "Each row represents a unique song in a specific year",
          "Year must be convertible to integer for comparison",
          "US Billboard must be convertible to integer for exact match",
          "US Cash Box may contain sentinel values (\u2013) indicating no chart position",
          "Year < 1978 means Year <= 1977",
          "Year must be parsed as an integer",
          "US Billboard must be parsed as an integer",
          "US Cash Box must be parsed as an integer"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Year values should be between 1972-1988 based on sample",
            "Chart positions should be integers between 1-100 (or null)",
            "Each row represents a unique song in a specific year"
          ],
          [
            "Year must be convertible to integer for comparison",
            "US Billboard must be convertible to integer for exact match",
            "US Cash Box may contain sentinel values (\u2013) indicating no chart position",
            "Year < 1978 means Year <= 1977"
          ],
          [
            "Year must be parsed as an integer",
            "US Billboard must be parsed as an integer",
            "US Cash Box must be parsed as an integer"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert Year to numeric for comparison",
          "Convert US Billboard to numeric for comparison",
          "Handle '\u2013' as null/missing values",
          "Convert Year from object to integer where Year < 1978",
          "Convert US Billboard from object to integer where US Billboard = 35",
          "Handle sentinel values in US Cash Box before returning result",
          "Year < 1978"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert Year to numeric for comparison",
            "Convert US Billboard to numeric for comparison",
            "Handle '\u2013' as null/missing values"
          ],
          [
            "Convert Year from object to integer where Year < 1978",
            "Convert US Billboard from object to integer where US Billboard = 35",
            "Handle sentinel values in US Cash Box before returning result"
          ],
          [
            "Year < 1978"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Year-Title combinations",
          "Validate Year range consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Year-Title combinations",
            "Validate Year range consistency"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric value or null if no match found",
          "Return single US Cash Box value matching the criteria",
          "If no match found, indicate no result",
          "If multiple matches found, return all matching values or clarify ambiguity"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric value or null if no match found"
          ],
          [
            "Return single US Cash Box value matching the criteria",
            "If no match found, indicate no result",
            "If multiple matches found, return all matching values or clarify ambiguity"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5858333333333334
  },
  "dbbench-042": {
    "m_q": {
      "target_metric": {
        "value": "Count of wins for Dunfermline Athletic where total final appearances is less than 2",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of wins for Dunfermline Athletic where total final appearances is less than 2",
          "Number of wins for Dunfermline Athletic",
          "Number of wins for Dunfermline Athletic when their total final appearances is less than 2"
        ]
      },
      "filters": {
        "value": [
          "Club = 'Dunfermline Athletic'",
          "Total final appearances < 2",
          "Club equals 'Dunfermline Athletic'",
          "Total final appearances less than 2",
          "Club is Dunfermline Athletic",
          "Total final appearances is less than 2"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Club = 'Dunfermline Athletic'",
            "Total final appearances < 2"
          ],
          [
            "Club equals 'Dunfermline Athletic'",
            "Total final appearances less than 2"
          ],
          [
            "Club is Dunfermline Athletic",
            "Total final appearances is less than 2"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the value in the 'Wins' column for Dunfermline Athletic?",
          "What is the value in the 'Total final appearances' column for Dunfermline Athletic?",
          "Does the 'Total final appearances' value satisfy the condition (<2)?",
          "Does Dunfermline Athletic have Total final appearances less than 2?"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "What is the value in the 'Wins' column for Dunfermline Athletic?",
            "What is the value in the 'Total final appearances' column for Dunfermline Athletic?",
            "Does the 'Total final appearances' value satisfy the condition (<2)?"
          ],
          [
            "What is the value in the 'Wins' column for Dunfermline Athletic?",
            "What is the value in the 'Total final appearances' column for Dunfermline Athletic?",
            "Does Dunfermline Athletic have Total final appearances less than 2?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Scottish Challenge Cup Finalists"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Scottish Challenge Cup Finalists"
          ],
          [
            "Scottish Challenge Cup Finalists"
          ],
          [
            "Scottish Challenge Cup Finalists"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "wins": "count",
          "runners-up": "count",
          "total final appearances": "count",
          "last final won": "year",
          "last final lost": "year"
        },
        "confidence": 0.6666666666666667,
        "votes": [
          {
            "Wins": "count",
            "Runners-up": "count",
            "Total final appearances": "count"
          },
          {
            "Wins": "count",
            "Runners-up": "count",
            "Total final appearances": "count",
            "Last final won": "year",
            "Last final lost": "year"
          },
          {
            "Wins": "number of wins",
            "Total final appearances": "number of appearances"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "All numeric columns (Wins, Runners-up, Total final appearances) are stored as object dtype, requiring type conversion for numerical comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All numeric columns (Wins, Runners-up, Total final appearances) are stored as object dtype, requiring type conversion for numerical comparison"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "\u2014",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.7499999999999999,
        "votes": [
          [
            "\u2014",
            ""
          ],
          [
            "\u2014",
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total final appearances = Wins + Runners-up (should hold for each row)",
          "Wins and Runners-up should be non-negative integers",
          "Total final appearances should be positive integer",
          "Total final appearances must be numeric and convertible from object type",
          "Wins must be numeric and convertible from object type",
          "Total final appearances should equal Wins plus Runners-up",
          "Club name must match exactly 'Dunfermline Athletic'",
          "Club names should be unique",
          "Wins, Runners-up, and Total final appearances should be non-negative integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total final appearances = Wins + Runners-up (should hold for each row)",
            "Wins and Runners-up should be non-negative integers",
            "Total final appearances should be positive integer"
          ],
          [
            "Total final appearances must be numeric and convertible from object type",
            "Wins must be numeric and convertible from object type",
            "Total final appearances should equal Wins plus Runners-up",
            "Club name must match exactly 'Dunfermline Athletic'"
          ],
          [
            "Club names should be unique",
            "Wins, Runners-up, and Total final appearances should be non-negative integers"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert 'Wins' column from string to integer",
          "Convert 'Total final appearances' column from string to integer",
          "Filter to Club == 'Dunfermline Athletic'",
          "Convert 'Total final appearances' to numeric type",
          "Filter to Total final appearances < 2",
          "Total final appearances < 2"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert 'Wins' column from string to integer",
            "Convert 'Total final appearances' column from string to integer"
          ],
          [
            "Filter to Club == 'Dunfermline Athletic'",
            "Convert 'Total final appearances' to numeric type",
            "Filter to Total final appearances < 2"
          ],
          [
            "Total final appearances < 2"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify data type conversion for numerical columns",
          "Check that Dunfermline Athletic exists in the dataset"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify data type conversion for numerical columns",
            "Check that Dunfermline Athletic exists in the dataset"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return a single integer value representing the answer",
          "Return a single numeric value representing the number of wins",
          "If no records match the filters, return 0 or indicate no match"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return a single integer value representing the answer"
          ],
          [
            "Return a single numeric value representing the number of wins",
            "If no records match the filters, return 0 or indicate no match"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5625000000000001
  },
  "dbbench-043": {
    "m_q": {
      "target_metric": {
        "value": "List of major competitions where Yoichiro Kakitani had at least 3 starts",
        "confidence": 0.3333333333333333,
        "votes": [
          "List of major competitions where Yoichiro Kakitani had at least 3 starts",
          "major competitions where Yoichiro Kakitani had at least 3 starts",
          "List of major competitions where Yoichiro Kakitani had at least 3 starts."
        ]
      },
      "filters": {
        "value": [
          "Team = 'Japan'",
          "Appearances Start >= 3",
          "player is Yoichiro Kakitani"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Team = 'Japan'",
            "Appearances Start >= 3"
          ],
          [
            "player is Yoichiro Kakitani",
            "Appearances Start >= 3"
          ],
          [
            "Player is Yoichiro Kakitani",
            "Appearances Start >= 3"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Competition"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Competition"
          ],
          [
            "Competition"
          ],
          [
            "Competition"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which competitions are considered 'major'?",
          "Does the data include all competitions for Yoichiro Kakitani?",
          "Are there any competitions with partial data?",
          "Which rows in the dataset belong to Yoichiro Kakitani?",
          "Which competitions have Appearances Start >= 3?",
          "What are the names of those competitions?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which competitions are considered 'major'?",
            "Does the data include all competitions for Yoichiro Kakitani?",
            "Are there any competitions with partial data?"
          ],
          [
            "Which rows in the dataset belong to Yoichiro Kakitani?",
            "Which competitions have Appearances Start >= 3?",
            "What are the names of those competitions?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "team_competition_stats"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "team_competition_stats"
          ],
          [
            "team_competition_stats"
          ],
          [
            "team_competition_stats"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "The sample shows a row with 'Team,Competition,Category,,,,Team Record' which appears to be a malformed header or data row",
          "Data contains entries for multiple teams/players mixed together without a player identifier column",
          "Rows 7 onward appear to be example data for different teams, not related to Kakitani"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "The sample shows a row with 'Team,Competition,Category,,,,Team Record' which appears to be a malformed header or data row"
          ],
          [
            "Data contains entries for multiple teams/players mixed together without a player identifier column",
            "Rows 7 onward appear to be example data for different teams, not related to Kakitani"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "appearances start": "count",
          "appearances sub": "count",
          "goals": "count"
        },
        "confidence": 0.8888888888888888,
        "votes": [
          {
            "Appearances Start": "count",
            "Appearances Sub": "count",
            "Goals": "count"
          },
          {
            "Appearances Start": "count",
            "Appearances Sub": "count",
            "Goals": "count"
          },
          {
            "Appearances Start": "number of appearances",
            "Goals": "number of goals"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Data appears to mix different national teams and competitions - unclear if this is comprehensive for Yoichiro Kakitani"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data appears to mix different national teams and competitions - unclear if this is comprehensive for Yoichiro Kakitani"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Appearances Start must be non-negative integer",
          "Goals must be non-negative integer",
          "Team must be a valid country name",
          "Must identify which rows belong to Yoichiro Kakitani (likely the Japan team entries)",
          "Appearances Start must be numeric and >= 3",
          "Competition names must be non-null",
          "Need to filter the data for Yoichiro Kakitani's appearances.",
          "Need to handle missing values in 'Appearances Start' column."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Appearances Start must be non-negative integer",
            "Goals must be non-negative integer",
            "Team must be a valid country name"
          ],
          [
            "Must identify which rows belong to Yoichiro Kakitani (likely the Japan team entries)",
            "Appearances Start must be numeric and >= 3",
            "Competition names must be non-null"
          ],
          [
            "Need to filter the data for Yoichiro Kakitani's appearances.",
            "Need to handle missing values in 'Appearances Start' column."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to rows where Team = 'Japan' to find Yoichiro Kakitani's competitions",
          "Filter to rows where Appearances Start >= 3",
          "Team = 'Japan' to isolate Kakitani's records",
          "Filter out invalid/header rows (e.g., row 7)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to rows where Team = 'Japan' to find Yoichiro Kakitani's competitions",
            "Filter to rows where Appearances Start >= 3"
          ],
          [
            "Team = 'Japan' to isolate Kakitani's records",
            "Filter out invalid/header rows (e.g., row 7)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of Appearances Start values",
          "Verify no duplicate competition entries for Japan"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of Appearances Start values",
            "Verify no duplicate competition entries for Japan"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of competition names",
          "Include number of starts for each qualifying competition",
          "Only include competitions where starts >= 3"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "List of competition names",
            "Include number of starts for each qualifying competition"
          ],
          [
            "List of competition names",
            "Only include competitions where starts >= 3"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6611111111111112
  },
  "dbbench-044": {
    "m_q": {
      "target_metric": {
        "value": "Home team score",
        "confidence": 1.0,
        "votes": [
          "Home team score",
          "Home team score",
          "Home team score"
        ]
      },
      "filters": {
        "value": [
          "Venue = 'Victoria Park'",
          "Crowd > 12000",
          "Venue equals 'Victoria Park'",
          "Crowd greater than 12,000",
          "Venue is Victoria Park",
          "Crowd is greater than 12,000"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Venue = 'Victoria Park'",
            "Crowd > 12000"
          ],
          [
            "Venue equals 'Victoria Park'",
            "Crowd greater than 12,000"
          ],
          [
            "Venue is Victoria Park",
            "Crowd is greater than 12,000"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the format of Home team score values?",
          "How is Crowd data formatted (with commas)?",
          "Are there multiple matches at Victoria Park with attendance > 12000?",
          "Which matches were played at Victoria Park?",
          "Which matches had attendance (Crowd) greater than 12,000?",
          "What is the home team score for matches meeting both conditions?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the format of Home team score values?",
            "How is Crowd data formatted (with commas)?",
            "Are there multiple matches at Victoria Park with attendance > 12000?"
          ],
          [
            "Which matches were played at Victoria Park?",
            "Which matches had attendance (Crowd) greater than 12,000?",
            "What is the home team score for matches meeting both conditions?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Australian Rules Football Matches"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Australian Rules Football Matches"
          ],
          [
            "Australian Rules Football Matches"
          ],
          [
            "Australian Rules Football Matches"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "crowd": "number of people",
          "home team score": "Australian rules football score format (goals.behinds (total))",
          "away team score": "goals.behinds (total_points)",
          "date": "date"
        },
        "confidence": 0.49999999999999994,
        "votes": [
          {
            "Crowd": "number of people",
            "Home team score": "Australian rules football score format (goals.behinds (total))"
          },
          {
            "Home team score": "goals.behinds (total_points)",
            "Away team score": "goals.behinds (total_points)",
            "Crowd": "number of people",
            "Date": "date"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Crowd values contain commas that need removal for numeric comparison",
          "Home team score has complex format needing parsing",
          "Crowd values contain commas and need to be parsed as integers for comparison",
          "Home team score is in format 'goals.behinds (points)' and may need parsing depending on desired output"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Crowd values contain commas that need removal for numeric comparison",
            "Home team score has complex format needing parsing"
          ],
          [
            "Crowd values contain commas and need to be parsed as integers for comparison",
            "Home team score is in format 'goals.behinds (points)' and may need parsing depending on desired output"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only one data source needed",
          "Must parse Crowd as numeric by removing commas and quotes",
          "Must filter for Victoria Park venue",
          "Must filter for Crowd > 12000 after numeric conversion",
          "Venue must equal 'Victoria Park'",
          "Crowd must be greater than 12,000 (numeric comparison required)",
          "All columns contain string data types that may need conversion",
          "Venue must be Victoria Park",
          "Crowd must be greater than 12000"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only one data source needed",
            "Must parse Crowd as numeric by removing commas and quotes",
            "Must filter for Victoria Park venue",
            "Must filter for Crowd > 12000 after numeric conversion"
          ],
          [
            "Venue must equal 'Victoria Park'",
            "Crowd must be greater than 12,000 (numeric comparison required)",
            "All columns contain string data types that may need conversion"
          ],
          [
            "Venue must be Victoria Park",
            "Crowd must be greater than 12000"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Crowd_numeric = CAST(REPLACE(REPLACE(Crowd, ',', ''), '\"', '') AS INTEGER)",
          "Crowd_numeric > 12000",
          "Venue = 'Victoria Park'",
          "Parse Crowd column to integer by removing commas before comparison",
          "Filter rows where Venue == 'Victoria Park' AND parsed_Crowd > 12000",
          "Crowd > 12000"
        ],
        "confidence": 0.38888888888888884,
        "votes": [
          [
            "Crowd_numeric = CAST(REPLACE(REPLACE(Crowd, ',', ''), '\"', '') AS INTEGER)",
            "Crowd_numeric > 12000",
            "Venue = 'Victoria Park'"
          ],
          [
            "Parse Crowd column to integer by removing commas before comparison",
            "Filter rows where Venue == 'Victoria Park' AND parsed_Crowd > 12000"
          ],
          [
            "Venue = 'Victoria Park'",
            "Crowd > 12000"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if any Victoria Park matches exist in data",
          "Check distribution of Crowd values at Victoria Park"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if any Victoria Park matches exist in data",
            "Check distribution of Crowd values at Victoria Park"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return list of Home team score values",
          "Consider returning both raw score format and parsed total points",
          "Return Home team score value(s) for matching rows",
          "May return multiple scores if multiple matches meet criteria",
          "Score format should preserve original notation (goals.behinds (points))"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return list of Home team score values",
            "Consider returning both raw score format and parsed total points"
          ],
          [
            "Return Home team score value(s) for matching rows",
            "May return multiple scores if multiple matches meet criteria",
            "Score format should preserve original notation (goals.behinds (points))"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5777777777777778
  },
  "dbbench-045": {
    "m_q": {
      "target_metric": {
        "value": "List of competition names",
        "confidence": 0.3333333333333333,
        "votes": [
          "List of competition names",
          "List of competitions competed in before the 2001 World Championships",
          "List of competitions the athlete participated in before the 2001 World Championships."
        ]
      },
      "filters": {
        "value": [
          "Year < 2001",
          "Competition != 'World Championships'",
          "Year < 2001 OR (Year = 2001 AND Competition occurs before World Championships)",
          "Competition != 'World Championships' or Year != 2001"
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "Year < 2001",
            "Competition != 'World Championships'"
          ],
          [
            "Year < 2001 OR (Year = 2001 AND Competition occurs before World Championships)"
          ],
          [
            "Year < 2001",
            "Competition != 'World Championships' or Year != 2001"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the earliest competition year?",
          "Are there multiple competitions in the same year before 2001?",
          "Does 'before 2001 world championships' include competitions in 2001 but before the world championships event?",
          "What is the chronological order of competitions in 2001?",
          "Which competitions occurred before 2001?",
          "How to identify the 2001 World Championships row?",
          "What competitions are listed in rows prior to 2001 World Championships?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the earliest competition year?",
            "Are there multiple competitions in the same year before 2001?",
            "Does 'before 2001 world championships' include competitions in 2001 but before the world championships event?"
          ],
          [
            "What is the chronological order of competitions in 2001?",
            "Which competitions occurred before 2001?",
            "How to identify the 2001 World Championships row?",
            "What competitions are listed in rows prior to 2001 World Championships?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Athletics Performances"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Athletics Performances"
          ],
          [
            "Athletics Performances"
          ],
          [
            "Athletics Performances"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "notes": "meters (m) for throw distances",
          "year": "calendar_year"
        },
        "confidence": 0.5,
        "votes": [
          {
            "Notes": "meters (m) for throw distances"
          },
          {
            "Year": "calendar_year",
            "Notes": "meters (m) for performance distances"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Year column uses Int64 type which may handle missing values differently than standard int"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column uses Int64 type which may handle missing values differently than standard int"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be between 1999 and 2015 based on sample",
          "Position values follow ordinal pattern (1st, 2nd, 3rd, etc.)",
          "Competition names are categorical",
          "Year must be less than or equal to 2001",
          "For Year = 2001, only include competitions that chronologically occur before World Championships",
          "2001 World Championships is identified by Year=2001 AND Competition='World Championships'",
          "The 'Year' column must be an integer.",
          "The 'Competition' column must be a string.",
          "The 'Venue' column must be a string.",
          "The 'Position' column must be a string.",
          "The 'Notes' column must be a string."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Year values should be between 1999 and 2015 based on sample",
            "Position values follow ordinal pattern (1st, 2nd, 3rd, etc.)",
            "Competition names are categorical"
          ],
          [
            "Year must be less than or equal to 2001",
            "For Year = 2001, only include competitions that chronologically occur before World Championships",
            "2001 World Championships is identified by Year=2001 AND Competition='World Championships'"
          ],
          [
            "The 'Year' column must be an integer.",
            "The 'Competition' column must be a string.",
            "The 'Venue' column must be a string.",
            "The 'Position' column must be a string.",
            "The 'Notes' column must be a string."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude 2001 World Championships from results",
          "Include only rows where Year < 2001 OR (Year = 2001 AND Competition != 'World Championships')",
          "Filter rows where Year < 2001",
          "Filter rows where Year = 2001 AND Competition in ['World Youth Championships', 'World Junior Championships', 'European Junior Championships'] based on temporal ordering",
          "Year < 2001"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude 2001 World Championships from results",
            "Include only rows where Year < 2001 OR (Year = 2001 AND Competition != 'World Championships')"
          ],
          [
            "Filter rows where Year < 2001",
            "Filter rows where Year = 2001 AND Competition in ['World Youth Championships', 'World Junior Championships', 'European Junior Championships'] based on temporal ordering"
          ],
          [
            "Year < 2001"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate competition entries in same year",
          "Verify chronological order of competitions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate competition entries in same year",
            "Verify chronological order of competitions"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List competitions in chronological order",
          "Include year with each competition name",
          "Return list of Competition names",
          "Include Venue information for context",
          "Maintain chronological order by Year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List competitions in chronological order",
            "Include year with each competition name"
          ],
          [
            "Return list of Competition names",
            "Include Venue information for context",
            "Maintain chronological order by Year"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5569444444444446
  },
  "dbbench-046": {
    "m_q": {
      "target_metric": {
        "value": "Round",
        "confidence": 0.6666666666666666,
        "votes": [
          "Round",
          "Round value(s)",
          "Round"
        ]
      },
      "filters": {
        "value": [
          "Winner = 'Ivanovic'",
          "Year < 2008",
          "Tournament = 'Paris'",
          "Winner is \"Ivanovic\"",
          "Year is less than 2008",
          "Tournament is \"Paris\""
        ],
        "confidence": 0.5000000000000001,
        "votes": [
          [
            "Winner = 'Ivanovic'",
            "Year < 2008",
            "Tournament = 'Paris'"
          ],
          [
            "Winner = 'Ivanovic'",
            "Year < 2008",
            "Tournament = 'Paris'"
          ],
          [
            "Winner is \"Ivanovic\"",
            "Year is less than 2008",
            "Tournament is \"Paris\""
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the round value when all three conditions are satisfied?",
          "Are there multiple matches that satisfy these conditions?",
          "Filter records where Winner is 'Ivanovic'",
          "Filter records where Year is less than 2008",
          "Filter records where Tournament is 'Paris'",
          "Extract Round value from filtered results"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the round value when all three conditions are satisfied?",
            "Are there multiple matches that satisfy these conditions?"
          ],
          [
            "Filter records where Winner is 'Ivanovic'",
            "Filter records where Year is less than 2008",
            "Filter records where Tournament is 'Paris'",
            "Extract Round value from filtered results"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "tennis_results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "tennis_results"
          ],
          [
            "tennis_results"
          ],
          [
            "tennis_results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "calendar year",
          "round": "tournament_stage_code",
          "score": "game_score_format"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "Year": "calendar year"
          },
          {
            "Year": "calendar_year",
            "Round": "tournament_stage_code",
            "Score": "game_score_format"
          },
          {
            "Year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Year column is stored as object/string type but contains numeric years",
          "Score column contains inconsistent formatting with spaces around commas"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column is stored as object/string type but contains numeric years",
            "Score column contains inconsistent formatting with spaces around commas"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be numeric years",
          "Round values should be valid tennis round notations",
          "Winner and Tournament should be non-empty strings",
          "Year must be numeric and less than 2008",
          "Winner must exactly match 'Ivanovic'",
          "Tournament must exactly match 'Paris'",
          "Year must be a valid year format",
          "Tournament values must be consistent",
          "Winner values must be consistent",
          "Round values must be consistent"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Year values should be numeric years",
            "Round values should be valid tennis round notations",
            "Winner and Tournament should be non-empty strings"
          ],
          [
            "Year must be numeric and less than 2008",
            "Winner must exactly match 'Ivanovic'",
            "Tournament must exactly match 'Paris'"
          ],
          [
            "Year must be a valid year format",
            "Tournament values must be consistent",
            "Winner values must be consistent",
            "Round values must be consistent"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Winner = 'Ivanovic'",
          "Filter rows where Year < 2008",
          "Filter rows where Tournament = 'Paris'",
          "Year column needs conversion from object to numeric for comparison Year < 2008",
          "Case-sensitive string matching for Winner and Tournament",
          "Year < 2008"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Winner = 'Ivanovic'",
            "Filter rows where Year < 2008",
            "Filter rows where Tournament = 'Paris'"
          ],
          [
            "Year column needs conversion from object to numeric for comparison Year < 2008",
            "Case-sensitive string matching for Winner and Tournament"
          ],
          [
            "Year < 2008"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if exactly one row satisfies all three conditions",
          "Verify Year values can be converted to integers for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if exactly one row satisfies all three conditions",
            "Verify Year values can be converted to integers for comparison"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single Round value if exactly one match found",
          "Return list of Round values if multiple matches found",
          "Return null/empty if no matches found",
          "Return the Round value as a string",
          "If multiple matches exist, return all matching Round values",
          "If no matches exist, return null or empty result"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single Round value if exactly one match found",
            "Return list of Round values if multiple matches found",
            "Return null/empty if no matches found"
          ],
          [
            "Return the Round value as a string",
            "If multiple matches exist, return all matching Round values",
            "If no matches exist, return null or empty result"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.563888888888889
  },
  "dbbench-047": {
    "m_q": {
      "target_metric": {
        "value": "Result (score outcome) for a specific match",
        "confidence": 0.3333333333333333,
        "votes": [
          "Result (score outcome) for a specific match",
          "Result value",
          "The 'Result' of basketball matches"
        ]
      },
      "filters": {
        "value": [
          "Arena = 'A'",
          "League = 'SL'",
          "Match > 16",
          "Arena is 'A Arena'",
          "League is 'SL'",
          "Match number is greater than 16"
        ],
        "confidence": 0.5000000000000001,
        "votes": [
          [
            "Arena = 'A'",
            "League = 'SL'",
            "Match > 16"
          ],
          [
            "Arena = 'A'",
            "League = 'SL'",
            "Match > 16"
          ],
          [
            "Arena is 'A Arena'",
            "League is 'SL'",
            "Match number is greater than 16"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the match number?",
          "What is the exact result (score) for that match?",
          "Which teams were involved in that match?",
          "Which rows have Arena equal to 'A'?",
          "Which rows have League equal to 'SL'?",
          "Which rows have Match number greater than 16?",
          "What is the Result value for rows matching all three conditions?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the match number?",
            "What is the exact result (score) for that match?",
            "Which teams were involved in that match?"
          ],
          [
            "Which rows have Arena equal to 'A'?",
            "Which rows have League equal to 'SL'?",
            "Which rows have Match number greater than 16?",
            "What is the Result value for rows matching all three conditions?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Basketball Matches"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Basketball Matches"
          ],
          [
            "Basketball Matches"
          ],
          [
            "Basketball Matches"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "date & time ( cet )": "CET timezone",
          "team and score": "Team names and score separated by dash",
          "match": "sequential match number",
          "result": "final score in format Score1-Score2"
        },
        "confidence": 0.49999999999999994,
        "votes": [
          {
            "Date & Time ( CET )": "CET timezone",
            "Team and Score": "Team names and score separated by dash"
          },
          {
            "Match": "sequential match number",
            "Date & Time ( CET )": "date and time in Central European Time format dd.mm.yy, HH:MM",
            "Team and Score": "team names and scores in format 'Team1 - Team2, Score1-Score2'",
            "Result": "final score in format Score1-Score2"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Match numbers appear to be sequential but may have gaps",
          "Score format uses en-dash (\u2013) not hyphen (-)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Match numbers appear to be sequential but may have gaps",
            "Score format uses en-dash (\u2013) not hyphen (-)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Match numbers should be unique",
          "Each row represents one match",
          "Scores should be numeric pairs separated by en-dash",
          "Match must be numeric and greater than 16",
          "Arena must equal 'A'",
          "League must equal 'SL'",
          "The 'Match' column needs to be converted to a numerical type to filter for values greater than 16."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Match numbers should be unique",
            "Each row represents one match",
            "Scores should be numeric pairs separated by en-dash"
          ],
          [
            "Match must be numeric and greater than 16",
            "Arena must equal 'A'",
            "League must equal 'SL'"
          ],
          [
            "The 'Match' column needs to be converted to a numerical type to filter for values greater than 16."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Arena = 'A' AND League = 'SL' AND Match > 16",
          "Extract the single matching row",
          "Convert Match column from string/object to numeric for comparison",
          "Filter where Arena == 'A' AND League == 'SL' AND Match > 16",
          "Convert 'Match' column to integer type."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Arena = 'A' AND League = 'SL' AND Match > 16",
            "Extract the single matching row"
          ],
          [
            "Convert Match column from string/object to numeric for comparison",
            "Filter where Arena == 'A' AND League == 'SL' AND Match > 16"
          ],
          [
            "Convert 'Match' column to integer type."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if exactly one match satisfies all conditions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if exactly one match satisfies all conditions"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return the complete 'Result' field value",
          "Include match number and teams for context",
          "Return the Result value(s) matching all filter conditions",
          "If multiple matches exist, return all Results",
          "If no matches exist, indicate no results found"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return the complete 'Result' field value",
            "Include match number and teams for context"
          ],
          [
            "Return the Result value(s) matching all filter conditions",
            "If multiple matches exist, return all Results",
            "If no matches exist, indicate no results found"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.538888888888889
  },
  "dbbench-048": {
    "m_q": {
      "target_metric": {
        "value": "Score against Portland in games where Game number is less than 20",
        "confidence": 0.3333333333333333,
        "votes": [
          "Score against Portland in games where Game number is less than 20",
          "Score value for games against Portland with game numbers under 20",
          "The score of the 'Team' against 'Portland' in games where the 'Game' number is less than 20."
        ]
      },
      "filters": {
        "value": [
          "Team = 'Portland'",
          "Game < 20",
          "Team equals 'Portland'",
          "Game number less than 20",
          "Team played against Portland",
          "Game number is less than 20"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Team = 'Portland'",
            "Game < 20"
          ],
          [
            "Team equals 'Portland'",
            "Game number less than 20"
          ],
          [
            "Team played against Portland",
            "Game number is less than 20"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Game",
          "Date"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Game",
            "Date"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "list",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the Game number for each Portland game?",
          "What is the Score for each Portland game with Game number under 20?",
          "Is the Score format consistent for parsing?",
          "Which rows have 'Portland' in the Team column?",
          "Which rows have Game values less than 20?",
          "What is the intersection of these two conditions?",
          "What is the Score value for the filtered row(s)?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the Game number for each Portland game?",
            "What is the Score for each Portland game with Game number under 20?",
            "Is the Score format consistent for parsing?"
          ],
          [
            "Which rows have 'Portland' in the Team column?",
            "Which rows have Game values less than 20?",
            "What is the intersection of these two conditions?",
            "What is the Score value for the filtered row(s)?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "game_results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "game_results"
          ],
          [
            "game_results"
          ],
          [
            "game_results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Score column contains both result indicator (W/L) and score values in format like 'L 93\u2013132 (OT)'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Score column contains both result indicator (W/L) and score values in format like 'L 93\u2013132 (OT)'"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "location attendance": "number of people",
          "score": "points",
          "game": "game_number",
          "high points": "points",
          "high rebounds": "rebounds",
          "high assists": "assists"
        },
        "confidence": 0.5555555555555556,
        "votes": [
          {
            "Location Attendance": "number of people",
            "Score": "points"
          },
          {
            "Game": "game_number",
            "Score": "points_outcome",
            "High points": "points",
            "High rebounds": "rebounds",
            "High assists": "assists",
            "Location Attendance": "venue_and_count"
          },
          {
            "Score": "points",
            "Game": "game number"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Score column contains combined data: win/loss indicator, score values, and overtime indicator"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Score column contains combined data: win/loss indicator, score values, and overtime indicator"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Game column should be numeric for comparison with 20",
          "Only one row per game per team needs to be considered",
          "Portland games appear both as home and away teams",
          "Game number must be numeric and less than 20",
          "Team value must exactly match 'Portland'",
          "Score format needs parsing to extract numeric values",
          "The 'Game' column needs to be converted to numeric type to filter games under 20.",
          "The 'Team' column needs to be checked for consistent naming of 'Portland'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Game column should be numeric for comparison with 20",
            "Only one row per game per team needs to be considered",
            "Portland games appear both as home and away teams"
          ],
          [
            "Game number must be numeric and less than 20",
            "Team value must exactly match 'Portland'",
            "Score format needs parsing to extract numeric values"
          ],
          [
            "The 'Game' column needs to be converted to numeric type to filter games under 20.",
            "The 'Team' column needs to be checked for consistent naming of 'Portland'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract numeric Game value from Game column",
          "Parse Score to extract actual score values",
          "Identify Portland games regardless of home/away status",
          "Convert Game column from string to integer for numeric comparison",
          "Extract score components from Score column (W/L indicator and point values)",
          "Game < 20",
          "Opponent = 'Portland'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract numeric Game value from Game column",
            "Parse Score to extract actual score values",
            "Identify Portland games regardless of home/away status"
          ],
          [
            "Convert Game column from string to integer for numeric comparison",
            "Extract score components from Score column (W/L indicator and point values)"
          ],
          [
            "Game < 20",
            "Opponent = 'Portland'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if Game values are sequential and complete",
          "Verify Score format consistency across all rows"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if Game values are sequential and complete",
            "Verify Score format consistency across all rows"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Should show Game number, Date, and Score for each qualifying game",
          "Score should be presented in readable format",
          "Return the complete Score string value (e.g., 'L 97\u201398 (OT)')",
          "If multiple matches exist, return all matching scores",
          "If no matches exist, indicate no results found"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Should show Game number, Date, and Score for each qualifying game",
            "Score should be presented in readable format"
          ],
          [
            "Return the complete Score string value (e.g., 'L 97\u201398 (OT)')",
            "If multiple matches exist, return all matching scores",
            "If no matches exist, indicate no results found"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5777777777777778
  },
  "dbbench-049": {
    "m_q": {
      "target_metric": {
        "value": "Identify November values where Game < 16 and Points > 22",
        "confidence": 0.3333333333333333,
        "votes": [
          "Identify November values where Game < 16 and Points > 22",
          "November dates that meet specified conditions",
          "November values where Game is smaller than 16 and Points is larger than 22"
        ]
      },
      "filters": {
        "value": [
          "Game < 16",
          "Points > 22"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Game < 16",
            "Points > 22"
          ],
          [
            "Game < 16",
            "Points > 22"
          ],
          [
            "Game < 16",
            "Points > 22"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "November"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "November"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the valid November values in the dataset?",
          "Are Game and Points columns numeric or need conversion?",
          "Do we need to handle any missing values in these columns?",
          "Which rows have Game values less than 16?",
          "Which rows have Points values greater than 22?",
          "Which November values satisfy both conditions simultaneously?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the valid November values in the dataset?",
            "Are Game and Points columns numeric or need conversion?",
            "Do we need to handle any missing values in these columns?"
          ],
          [
            "Which rows have Game values less than 16?",
            "Which rows have Points values greater than 22?",
            "Which November values satisfy both conditions simultaneously?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "game_results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "game_results"
          ],
          [
            "game_results"
          ],
          [
            "game_results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "game": "game number",
          "points": "team points",
          "november": "day_of_month (November date)",
          "score": "game_score (goals for\u2013goals against)"
        },
        "confidence": 0.49999999999999994,
        "votes": [
          {
            "Game": "game number",
            "Points": "team points"
          },
          {
            "Game": "game_number (sequential identifier)",
            "November": "day_of_month (November date)",
            "Points": "team_points (cumulative season points)",
            "Score": "game_score (goals for\u2013goals against)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Game and Points columns are stored as object dtype but contain numeric values",
          "Score column contains mixed formats (e.g., '4-3', '5-5 OT')",
          "Game column appears to contain numeric values but dtype is object",
          "November column appears to contain numeric values but dtype is object",
          "Points column appears to contain numeric values but dtype is object"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Game and Points columns are stored as object dtype but contain numeric values",
            "Score column contains mixed formats (e.g., '4-3', '5-5 OT')"
          ],
          [
            "Game column appears to contain numeric values but dtype is object",
            "November column appears to contain numeric values but dtype is object",
            "Points column appears to contain numeric values but dtype is object"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Game values should be sequential integers",
          "November values should be integers representing days of November",
          "Points should be non-negative integers",
          "Game must be convertible to numeric type for comparison",
          "Points must be convertible to numeric type for comparison",
          "Game < 16 (strict inequality)",
          "Points > 22 (strict inequality)",
          "Game and Points columns should be of numeric type for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Game values should be sequential integers",
            "November values should be integers representing days of November",
            "Points should be non-negative integers"
          ],
          [
            "Game must be convertible to numeric type for comparison",
            "Points must be convertible to numeric type for comparison",
            "Game < 16 (strict inequality)",
            "Points > 22 (strict inequality)"
          ],
          [
            "Game and Points columns should be of numeric type for comparison"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert Game column to numeric type",
          "Convert Points column to numeric type",
          "Filter rows where Game < 16 AND Points > 22",
          "Convert Game column from object to numeric type",
          "Convert Points column from object to numeric type",
          "Filter where numeric(Game) < 16 AND numeric(Points) > 22"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert Game column to numeric type",
            "Convert Points column to numeric type",
            "Filter rows where Game < 16 AND Points > 22"
          ],
          [
            "Convert Game column from object to numeric type",
            "Convert Points column from object to numeric type",
            "Filter where numeric(Game) < 16 AND numeric(Points) > 22"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in Game and Points columns",
          "Verify numeric ranges for Game and Points columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in Game and Points columns",
            "Verify numeric ranges for Game and Points columns"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of November values meeting criteria",
          "Should include Game and Points values for verification",
          "Return list of November values that satisfy both conditions",
          "Handle type conversion errors gracefully",
          "Return empty result if no rows match criteria"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of November values meeting criteria",
            "Should include Game and Points values for verification"
          ],
          [
            "Return list of November values that satisfy both conditions",
            "Handle type conversion errors gracefully",
            "Return empty result if no rows match criteria"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6083333333333334
  },
  "dbbench-050": {
    "m_q": {
      "target_metric": {
        "value": "Platform names",
        "confidence": 0.3333333333333333,
        "votes": [
          "Platform names",
          "Platform name",
          "Platform and Developer"
        ]
      },
      "filters": {
        "value": [
          "Year > 2006",
          "Developer = '3G Studios'"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Year > 2006",
            "Developer = '3G Studios'"
          ],
          [
            "Year > 2006",
            "Developer = '3G Studios'"
          ],
          [
            "Year > 2006",
            "Developer = '3G Studios'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "list",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Filter games where Year > 2006",
          "Filter games where Developer is '3G Studios'",
          "Extract Platform values from filtered rows",
          "Which records have Year greater than 2006?",
          "Which records have Developer equal to '3G Studios'?",
          "What is the Platform value for records matching both conditions?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter games where Year > 2006",
            "Filter games where Developer is '3G Studios'",
            "Extract Platform values from filtered rows"
          ],
          [
            "Which records have Year greater than 2006?",
            "Which records have Developer equal to '3G Studios'?",
            "What is the Platform value for records matching both conditions?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "SWAT Games"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "SWAT Games"
          ],
          [
            "SWAT Games"
          ],
          [
            "SWAT Games"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "calendar year",
          "platform": "gaming platform name",
          "developer": "company name",
          "publisher": "company name",
          "title": "game title"
        },
        "confidence": 0.4666666666666666,
        "votes": [
          {
            "Year": "calendar year"
          },
          {
            "Year": "calendar year",
            "Platform": "gaming platform name",
            "Developer": "company name",
            "Publisher": "company name",
            "Title": "game title"
          },
          {
            "Year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Year column is object type instead of numeric, may contain non-numeric values or need type conversion"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column is object type instead of numeric, may contain non-numeric values or need type conversion"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year must be convertible to numeric for comparison > 2006",
          "Developer name matching must be exact (case-sensitive: '3G Studios' vs '3g studios' in question)",
          "Year must be convertible to integer for comparison",
          "Developer string match must be exact or case-insensitive",
          "Year > 2006 means Year >= 2007",
          "Year column should be numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year must be convertible to numeric for comparison > 2006",
            "Developer name matching must be exact (case-sensitive: '3G Studios' vs '3g studios' in question)"
          ],
          [
            "Year must be convertible to integer for comparison",
            "Developer string match must be exact or case-insensitive",
            "Year > 2006 means Year >= 2007"
          ],
          [
            "Year column should be numeric"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Year > 2006",
          "Developer = '3G Studios'",
          "Convert Year column from string to integer before applying Year > 2006 filter",
          "Apply both filters conjunctively (AND logic)",
          "Year > 2006 is derived from the Year column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year > 2006",
            "Developer = '3G Studios'"
          ],
          [
            "Convert Year column from string to integer before applying Year > 2006 filter",
            "Apply both filters conjunctively (AND logic)"
          ],
          [
            "Year > 2006 is derived from the Year column"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check Year column for non-numeric values",
          "Check Developer column for variations of '3G Studios'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check Year column for non-numeric values",
            "Check Developer column for variations of '3G Studios'"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List of unique platform names",
          "Case-sensitive platform names as they appear in data",
          "Return the Platform value as a string",
          "If no records match, return null or empty result",
          "If multiple records match, return all matching Platform values",
          "Output should be a list of platforms and developers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List of unique platform names",
            "Case-sensitive platform names as they appear in data"
          ],
          [
            "Return the Platform value as a string",
            "If no records match, return null or empty result",
            "If multiple records match, return all matching Platform values"
          ],
          [
            "Output should be a list of platforms and developers"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5733333333333335
  },
  "dbbench-051": {
    "m_q": {
      "target_metric": {
        "value": "Player(s) with the minimum number of matches where Runs equals 276",
        "confidence": 0.3333333333333333,
        "votes": [
          "Player(s) with the minimum number of matches where Runs equals 276",
          "minimum value of Matches column",
          "Minimum value of 'Matches' where 'Runs' is 276"
        ]
      },
      "filters": {
        "value": [
          "Runs = 276",
          "Runs == 276"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Runs = 276"
          ],
          [
            "Runs == 276"
          ],
          [
            "Runs = 276"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Player",
          "Team"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Player",
            "Team"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "list",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which players have exactly 276 runs?",
          "Among those players, who has the smallest Matches value?",
          "If multiple players have the same minimum Matches, list all of them",
          "Which rows have Runs equal to 276?",
          "What is the Matches value for rows where Runs equals 276?",
          "What is the minimum Matches value among filtered rows?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which players have exactly 276 runs?",
            "Among those players, who has the smallest Matches value?",
            "If multiple players have the same minimum Matches, list all of them"
          ],
          [
            "Which rows have Runs equal to 276?",
            "What is the Matches value for rows where Runs equals 276?",
            "What is the minimum Matches value among filtered rows?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Cricket Bowling Statistics"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Cricket Bowling Statistics"
          ],
          [
            "Cricket Bowling Statistics"
          ],
          [
            "Cricket Bowling Statistics"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "overs": "overs bowled",
          "runs": "runs conceded",
          "wickets": "wickets taken",
          "econ": "economy rate (runs per over)",
          "s/rate": "bowling strike rate (balls per wicket)",
          "average": "bowling average (runs per wicket)",
          "matches": "count",
          "4/inns": "count of 4-wicket innings",
          "5+/inns": "count of 5-or-more-wicket innings"
        },
        "confidence": 0.8148148148148148,
        "votes": [
          {
            "Overs": "cricket overs (e.g., 44 means 44 overs, 34.1 means 34 overs and 1 ball)",
            "Runs": "runs conceded",
            "Wickets": "wickets taken",
            "Econ": "economy rate (runs per over)",
            "S/Rate": "bowling strike rate (balls per wicket)",
            "Average": "bowling average (runs per wicket)"
          },
          {
            "Matches": "count",
            "Overs": "overs bowled",
            "Runs": "runs conceded",
            "Wickets": "wickets taken",
            "Econ": "economy rate (runs per over)",
            "S/Rate": "strike rate (balls per wicket)",
            "4/inns": "count of 4-wicket innings",
            "5+/inns": "count of 5-or-more-wicket innings",
            "Average": "bowling average (runs per wicket)"
          },
          {
            "Matches": "number of matches",
            "Runs": "runs scored",
            "Wickets": "number of wickets",
            "Overs": "overs bowled",
            "Econ": "runs per over",
            "S/Rate": "balls per wicket",
            "Average": "runs per wicket"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "All columns are stored as object/string dtypes, numerical columns need conversion for calculations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns are stored as object/string dtypes, numerical columns need conversion for calculations"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Runs column must be convertible to numeric",
          "Matches column must be convertible to numeric",
          "Only consider rows where Runs equals exactly 276",
          "Runs column must be converted to numeric type for filtering",
          "Matches column must be converted to numeric type for finding minimum",
          "Only rows with Runs exactly equal to 276 should be considered",
          "Matches must be a positive integer",
          "Runs must be a non-negative integer"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Runs column must be convertible to numeric",
            "Matches column must be convertible to numeric",
            "Only consider rows where Runs equals exactly 276"
          ],
          [
            "Runs column must be converted to numeric type for filtering",
            "Matches column must be converted to numeric type for finding minimum",
            "Only rows with Runs exactly equal to 276 should be considered"
          ],
          [
            "Matches must be a positive integer",
            "Runs must be a non-negative integer"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter dataset to rows where Runs == 276",
          "Among filtered rows, find minimum Matches value",
          "Return all rows with that minimum Matches value",
          "Filter records where Runs == 276"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter dataset to rows where Runs == 276",
            "Among filtered rows, find minimum Matches value",
            "Return all rows with that minimum Matches value"
          ],
          [
            "Filter records where Runs == 276"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate player entries",
          "Verify Runs values are consistent numeric format"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate player entries",
            "Verify Runs values are consistent numeric format"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output should be list of player names",
          "Include Team information if multiple players have same minimum matches",
          "Return the minimum Matches value as a scalar number",
          "Handle case where no rows match the Runs filter"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Output should be list of player names",
            "Include Team information if multiple players have same minimum matches"
          ],
          [
            "Return the minimum Matches value as a scalar number",
            "Handle case where no rows match the Runs filter"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5824074074074075
  },
  "dbbench-052": {
    "m_q": {
      "target_metric": {
        "value": "Identify the first player selected in the draft who attended a school located in California",
        "confidence": 0.3333333333333333,
        "votes": [
          "Identify the first player selected in the draft who attended a school located in California",
          "First player picked who attended a school in California",
          "Players who were the first pick in the draft and attended school in California"
        ]
      },
      "filters": {
        "value": [
          "School/Club Team contains 'California'",
          "Pick is the minimum value among filtered records",
          "School/Club Team is located in California",
          "Pick is minimum among California schools",
          "Pick == 1"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "School/Club Team contains 'California'",
            "Pick is the minimum value among filtered records"
          ],
          [
            "School/Club Team is located in California",
            "Pick is minimum among California schools"
          ],
          [
            "Pick == 1",
            "School/Club Team contains 'California'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What constitutes a 'school in California'? (e.g., contains 'California', 'CA', specific California school names)",
          "Does 'first player picked' mean earliest pick number overall or first in chronological order?",
          "Should we consider only US colleges or include international schools/clubs?",
          "Which schools in the School/Club Team column are located in California?",
          "Among players who attended California schools, what is the minimum Pick number?",
          "What is the Player name corresponding to that minimum Pick?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What constitutes a 'school in California'? (e.g., contains 'California', 'CA', specific California school names)",
            "Does 'first player picked' mean earliest pick number overall or first in chronological order?",
            "Should we consider only US colleges or include international schools/clubs?"
          ],
          [
            "Which schools in the School/Club Team column are located in California?",
            "Among players who attended California schools, what is the minimum Pick number?",
            "What is the Player name corresponding to that minimum Pick?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "NBA Draft History"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NBA Draft History"
          ],
          [
            "NBA Draft History"
          ],
          [
            "NBA Draft History"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "round": "draft round number",
          "pick": "overall selection number"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Round": "draft round number",
            "Pick": "overall selection number"
          },
          {
            "Round": "draft round number",
            "Pick": "overall draft pick number"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Pick numbers may not be sequential across all rows (e.g., jumps from 221 to 47)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pick numbers may not be sequential across all rows (e.g., jumps from 221 to 47)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Pick values should be positive integers",
          "Round should be between 1-10 typically",
          "Player names should be non-empty",
          "Pick number must be valid (non-null)",
          "School/Club Team must be identifiable as California institution",
          "Return single player with earliest/lowest pick number",
          "Pick column should contain integer values",
          "School/Club Team column may contain missing values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pick values should be positive integers",
            "Round should be between 1-10 typically",
            "Player names should be non-empty"
          ],
          [
            "Pick number must be valid (non-null)",
            "School/Club Team must be identifiable as California institution",
            "Return single player with earliest/lowest pick number"
          ],
          [
            "Pick column should contain integer values",
            "School/Club Team column may contain missing values"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where School/Club Team contains 'California' (case-insensitive)",
          "Among filtered rows, find row with minimum Pick value",
          "Identify California schools by matching patterns: 'California', 'Stanford', 'USC', 'UCLA', 'San Diego State', etc.",
          "Filter to only rows where School/Club Team matches California institution"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where School/Club Team contains 'California' (case-insensitive)",
            "Among filtered rows, find row with minimum Pick value"
          ],
          [
            "Identify California schools by matching patterns: 'California', 'Stanford', 'USC', 'UCLA', 'San Diego State', etc.",
            "Filter to only rows where School/Club Team matches California institution"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Pick values",
          "Verify chronological ordering of picks"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Pick values",
            "Verify chronological ordering of picks"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single player name with supporting details (Pick, School)",
          "Return player name as string",
          "Should be the player with minimum Pick value among California school attendees"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single player name with supporting details (Pick, School)"
          ],
          [
            "Return player name as string",
            "Should be the player with minimum Pick value among California school attendees"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5533333333333335
  },
  "dbbench-053": {
    "m_q": {
      "target_metric": {
        "value": "Area (km\u00b2) or Area (sq mi) value for each island",
        "confidence": 0.3333333333333333,
        "votes": [
          "Area (km\u00b2) or Area (sq mi) value for each island",
          "Island with the largest area between Tiree and Kasos",
          "area of Tiree and Kasos islands"
        ]
      },
      "filters": {
        "value": [
          "Island = 'tiree'",
          "Island = 'kasos'",
          "Island = 'Tiree' OR Island = 'Kasos'",
          "Island is Tiree",
          "Island is Kasos"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Island = 'tiree'",
            "Island = 'kasos'"
          ],
          [
            "Island = 'Tiree' OR Island = 'Kasos'"
          ],
          [
            "Island is Tiree",
            "Island is Kasos"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Island"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Island"
          ],
          [],
          [
            "Island"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the area of Tiree in km\u00b2?",
          "What is the area of Kasos in km\u00b2?",
          "Which island has the larger area value?",
          "What is the area of Tiree?",
          "What is the area of Kasos?",
          "Which island has the greater area value?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the area of Tiree in km\u00b2?",
            "What is the area of Kasos in km\u00b2?",
            "Which island has the larger area value?"
          ],
          [
            "What is the area of Tiree?",
            "What is the area of Kasos?",
            "Which island has the greater area value?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Island Area by Country/Region"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Island Area by Country/Region"
          ],
          [
            "Island Area by Country/Region"
          ],
          [
            "Island Area by Country/Region"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "area (km\u00b2)": "square kilometers",
          "area (sq mi)": "square miles"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Area (km\u00b2)": "square kilometers",
            "Area (sq mi)": "square miles"
          },
          {
            "Area (km\u00b2)": "square kilometers",
            "Area (sq mi)": "square miles"
          },
          {
            "Area (km\u00b2)": "km\u00b2",
            "Area (sq mi)": "sq mi"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Two different area units provided - need to ensure consistent comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Two different area units provided - need to ensure consistent comparison"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Island names must match exactly (case-sensitive)",
          "Area values should be positive numbers",
          "Only two specific islands need to be compared",
          "Island names must be filtered case-insensitively to match 'Tiree' and 'Kasos'",
          "Both islands must exist in the dataset to produce valid comparison",
          "Area comparison should use same unit (either km\u00b2 or sq mi)",
          "Island names must be exact matches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Island names must match exactly (case-sensitive)",
            "Area values should be positive numbers",
            "Only two specific islands need to be compared"
          ],
          [
            "Island names must be filtered case-insensitively to match 'Tiree' and 'Kasos'",
            "Both islands must exist in the dataset to produce valid comparison",
            "Area comparison should use same unit (either km\u00b2 or sq mi)"
          ],
          [
            "Island names must be exact matches"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter dataset to only rows where Island contains 'tiree' or 'kasos' (case-insensitive)",
          "Filter rows where Island column matches 'Tiree' or 'Kasos' (case-insensitive)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter dataset to only rows where Island contains 'tiree' or 'kasos' (case-insensitive)"
          ],
          [
            "Filter rows where Island column matches 'Tiree' or 'Kasos' (case-insensitive)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Direct numerical comparison of Area (km\u00b2) values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Direct numerical comparison of Area (km\u00b2) values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Clear statement of which island has larger area",
          "Include both area values for verification",
          "Return the name of the island with larger area",
          "Optionally include the area values for both islands for verification",
          "Report the island with the largest area in km\u00b2"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Clear statement of which island has larger area",
            "Include both area values for verification"
          ],
          [
            "Return the name of the island with larger area",
            "Optionally include the area values for both islands for verification"
          ],
          [
            "Report the island with the largest area in km\u00b2"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6166666666666668
  },
  "dbbench-054": {
    "m_q": {
      "target_metric": {
        "value": "maximum number of goals scored",
        "confidence": 0.3333333333333333,
        "votes": [
          "maximum number of goals scored",
          "Name of the player with the most goals during the period 1972-1975, 1976-1982",
          "Maximum goals scored by a player"
        ]
      },
      "filters": {
        "value": [
          "Period contains '1972 \u2013 1975'",
          "Period contains '1976 \u2013 1982'",
          "Period contains '1972 \u2013 1975, 1976 \u2013 1982'",
          "Period is between 1972 and 1975 (inclusive)",
          "Period is between 1976 and 1982 (inclusive)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Period contains '1972 \u2013 1975'",
            "Period contains '1976 \u2013 1982'"
          ],
          [
            "Period contains '1972 \u2013 1975, 1976 \u2013 1982'"
          ],
          [
            "Period is between 1972 and 1975 (inclusive)",
            "Period is between 1976 and 1982 (inclusive)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Name"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Name"
          ],
          [
            "Name"
          ],
          [
            "Name"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which player(s) had the highest goals count during the specified periods?",
          "Are there players who played in both periods?",
          "How should we handle players with multiple periods listed?",
          "Which players have a Period that exactly matches '1972 \u2013 1975, 1976 \u2013 1982'?",
          "What is the Goals\u00b9 value for each player matching this period?",
          "Which player has the maximum Goals\u00b9 value among the filtered results?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which player(s) had the highest goals count during the specified periods?",
            "Are there players who played in both periods?",
            "How should we handle players with multiple periods listed?"
          ],
          [
            "Which players have a Period that exactly matches '1972 \u2013 1975, 1976 \u2013 1982'?",
            "What is the Goals\u00b9 value for each player matching this period?",
            "Which player has the maximum Goals\u00b9 value among the filtered results?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "football_player_statistics"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "football_player_statistics"
          ],
          [
            "football_player_statistics"
          ],
          [
            "football_player_statistics"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Period column contains multiple date ranges separated by commas",
          "Appearances\u00b9 and Goals\u00b9 columns are object type but contain numeric data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Period column contains multiple date ranges separated by commas",
            "Appearances\u00b9 and Goals\u00b9 columns are object type but contain numeric data"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "appearances\u00b9": "count",
          "goals\u00b9": "count"
        },
        "confidence": 0.8333333333333333,
        "votes": [
          {
            "Appearances\u00b9": "count",
            "Goals\u00b9": "count"
          },
          {
            "Goals\u00b9": "count",
            "Appearances\u00b9": "count"
          },
          {
            "Goals\u00b9": "goals"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Period values contain multiple ranges (e.g., '1972 \u2013 1975, 1976 \u2013 1982')",
          "Some Period values contain \u2020 symbol"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Period values contain multiple ranges (e.g., '1972 \u2013 1975, 1976 \u2013 1982')",
            "Some Period values contain \u2020 symbol"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Goals\u00b9 must be convertible to numeric",
          "Period must be parsed to identify overlapping date ranges",
          "Need to handle players with multiple periods in their career",
          "Period must exactly match '1972 \u2013 1975, 1976 \u2013 1982' format",
          "Goals\u00b9 must be converted to numeric type for comparison",
          "Goals\u00b9 must be numeric",
          "Period needs to be parsed to determine if it falls within the specified ranges (1972-1975 and 1976-1982).  Periods can be a single year or a range of years."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Goals\u00b9 must be convertible to numeric",
            "Period must be parsed to identify overlapping date ranges",
            "Need to handle players with multiple periods in their career"
          ],
          [
            "Period must exactly match '1972 \u2013 1975, 1976 \u2013 1982' format",
            "Goals\u00b9 must be converted to numeric type for comparison"
          ],
          [
            "Goals\u00b9 must be numeric",
            "Period needs to be parsed to determine if it falls within the specified ranges (1972-1975 and 1976-1982).  Periods can be a single year or a range of years."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert Goals\u00b9 to numeric type",
          "Filter rows where Period contains either '1972 \u2013 1975' or '1976 \u2013 1982'",
          "Exclude rows where Goals\u00b9 cannot be converted to numeric",
          "Period == '1972 \u2013 1975, 1976 \u2013 1982'",
          "Period_1972_1975: Period falls within 1972-1975",
          "Period_1976_1982: Period falls within 1976-1982"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert Goals\u00b9 to numeric type",
            "Filter rows where Period contains either '1972 \u2013 1975' or '1976 \u2013 1982'",
            "Exclude rows where Goals\u00b9 cannot be converted to numeric"
          ],
          [
            "Period == '1972 \u2013 1975, 1976 \u2013 1982'"
          ],
          [
            "Period_1972_1975: Period falls within 1972-1975",
            "Period_1976_1982: Period falls within 1976-1982"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Goals\u00b9 values",
          "Verify distribution of Goals\u00b9 across periods"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Goals\u00b9 values",
            "Verify distribution of Goals\u00b9 across periods"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return player name(s) with maximum goals",
          "Include actual goals count",
          "Handle ties if multiple players have same maximum",
          "Return the Name of the player with maximum Goals\u00b9",
          "Handle case where multiple players may have the same maximum goals",
          "List of players with the most goals for each period (1972-1975 and 1976-1982)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return player name(s) with maximum goals",
            "Include actual goals count",
            "Handle ties if multiple players have same maximum"
          ],
          [
            "Return the Name of the player with maximum Goals\u00b9",
            "Handle case where multiple players may have the same maximum goals"
          ],
          [
            "List of players with the most goals for each period (1972-1975 and 1976-1982)"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6250000000000001
  },
  "dbbench-055": {
    "m_q": {
      "target_metric": {
        "value": "Identify the athlete(s) with the highest rank (lowest numerical rank value)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Identify the athlete(s) with the highest rank (lowest numerical rank value)",
          "Athlete with the highest rank (lowest numeric rank value)",
          "Athlete with the minimum 'Total Rank' value"
        ]
      },
      "filters": {
        "value": [
          "Exclude rows where Total Rank is '\u2013' or 'DNF' or 'DSQ'",
          "Consider only valid numerical rank values",
          "Exclude rows where Total Rank is '\u2013' or DNF or DSQ"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Total Rank is '\u2013' or 'DNF' or 'DSQ'",
            "Consider only valid numerical rank values"
          ],
          [
            "Exclude rows where Total Rank is '\u2013' or DNF or DSQ"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Athlete"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Athlete"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "list",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What does '\u2013' in Total Rank column mean?",
          "Are DNF/DSQ considered ranks?",
          "Should we consider all events or specific events?",
          "How to handle ties in rank?",
          "What does 'highest rank' mean - lowest numeric value or highest numeric value?",
          "Should we consider only completed races (valid Total Rank values)?",
          "Are there multiple athletes tied for the highest rank?",
          "Should results be filtered by Event type?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What does '\u2013' in Total Rank column mean?",
            "Are DNF/DSQ considered ranks?",
            "Should we consider all events or specific events?",
            "How to handle ties in rank?"
          ],
          [
            "What does 'highest rank' mean - lowest numeric value or highest numeric value?",
            "Should we consider only completed races (valid Total Rank values)?",
            "Are there multiple athletes tied for the highest rank?",
            "Should results be filtered by Event type?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Athlete Event Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Athlete Event Results"
          ],
          [
            "Athlete Event Results"
          ],
          [
            "Athlete Event Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "race 1 time": "minutes:seconds.hundredths",
          "race 2 time": "minutes:seconds.hundredths",
          "total time": "minutes:seconds.hundredths",
          "total rank": "ordinal rank (lower is better)"
        },
        "confidence": 0.5833333333333334,
        "votes": [
          {
            "Race 1 Time": "minutes:seconds.hundredths",
            "Race 2 Time": "minutes:seconds.hundredths",
            "Total Time": "minutes:seconds.hundredths"
          },
          {
            "Race 1 Time": "seconds (MM:SS.SS format)",
            "Race 2 Time": "seconds (MM:SS.SS format)",
            "Total Time": "seconds (MM:SS.SS format)",
            "Total Rank": "ordinal rank (lower is better)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Time values mix different formats (MM:SS.ss vs SS.ss)",
          "Total Rank contains both numerical values and special codes ('\u2013', 'DNF', 'DSQ')",
          "Total Rank is stored as string/object type but represents numeric ordinal values",
          "Time values are in MM:SS.SS format stored as strings, need conversion for calculations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time values mix different formats (MM:SS.ss vs SS.ss)",
            "Total Rank contains both numerical values and special codes ('\u2013', 'DNF', 'DSQ')"
          ],
          [
            "Total Rank is stored as string/object type but represents numeric ordinal values",
            "Time values are in MM:SS.SS format stored as strings, need conversion for calculations"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "\u2013",
          "DNF",
          "DSQ",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.6111111111111112,
        "votes": [
          [
            "\u2013",
            "DNF",
            "DSQ",
            ""
          ],
          [
            "DNF",
            "DSQ",
            "\u2013",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total Rank should be numeric for valid race completions",
          "Total Time should be sum of Race 1 Time and Race 2 Time for valid races",
          "Athlete names should be unique per event",
          "Total Rank must be numeric and valid (not DNF, DSQ, or '\u2013')",
          "Highest rank means lowest numeric value in Total Rank column",
          "Each athlete may have multiple events, need to determine if looking for best rank across all events or per event",
          "'Total Rank' column should be converted to numeric type for comparison.",
          "Handle non-numeric values (e.g., '\u2013', 'DNF', 'DSQ') in 'Total Rank' appropriately (e.g., convert to NaN or a very large number) before finding the minimum."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total Rank should be numeric for valid race completions",
            "Total Time should be sum of Race 1 Time and Race 2 Time for valid races",
            "Athlete names should be unique per event"
          ],
          [
            "Total Rank must be numeric and valid (not DNF, DSQ, or '\u2013')",
            "Highest rank means lowest numeric value in Total Rank column",
            "Each athlete may have multiple events, need to determine if looking for best rank across all events or per event"
          ],
          [
            "'Total Rank' column should be converted to numeric type for comparison.",
            "Handle non-numeric values (e.g., '\u2013', 'DNF', 'DSQ') in 'Total Rank' appropriately (e.g., convert to NaN or a very large number) before finding the minimum."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to rows where Total Rank is numeric",
          "Consider minimum rank value across all events per athlete",
          "Filter out rows where Total Rank is non-numeric",
          "Convert Total Rank to integer for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to rows where Total Rank is numeric",
            "Consider minimum rank value across all events per athlete"
          ],
          [
            "Filter out rows where Total Rank is non-numeric",
            "Convert Total Rank to integer for comparison"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check consistency between Total Time and sum of Race 1 + Race 2 Times",
          "Validate rank ordering matches time ordering"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check consistency between Total Time and sum of Race 1 + Race 2 Times",
            "Validate rank ordering matches time ordering"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List athlete name(s) with highest rank",
          "Include rank value",
          "Handle multiple athletes if tied",
          "Return athlete name(s) with the minimum Total Rank value",
          "Handle ties if multiple athletes have the same rank",
          "Include Event and Total Rank in output for context",
          "Output the name of the athlete with the highest rank (lowest 'Total Rank' value)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List athlete name(s) with highest rank",
            "Include rank value",
            "Handle multiple athletes if tied"
          ],
          [
            "Return athlete name(s) with the minimum Total Rank value",
            "Handle ties if multiple athletes have the same rank",
            "Include Event and Total Rank in output for context"
          ],
          [
            "Output the name of the athlete with the highest rank (lowest 'Total Rank' value)."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5430555555555557
  },
  "dbbench-056": {
    "m_q": {
      "target_metric": {
        "value": "Identify two hospitals with consecutive rankings of 8 and 9 that both have exactly 1200 beds",
        "confidence": 0.3333333333333333,
        "votes": [
          "Identify two hospitals with consecutive rankings of 8 and 9 that both have exactly 1200 beds",
          "Two hospitals with consecutive rankings of 8 and 9 that both have 1200 beds",
          "Identify two hospitals with consecutive ranks (8 and 9) that both have 1200 beds."
        ]
      },
      "filters": {
        "value": [
          "Rank IN (8, 9)",
          "# Beds = 1200",
          "Rank equals 8 or 9",
          "# Beds equals 1200",
          "Rank is 8 or 9",
          "# Beds is 1200"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rank IN (8, 9)",
            "# Beds = 1200"
          ],
          [
            "Rank equals 8 or 9",
            "# Beds equals 1200"
          ],
          [
            "Rank is 8 or 9",
            "# Beds is 1200"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Rank",
          "Hospital"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rank",
            "Hospital"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Which hospitals have rank 8?",
          "Which hospitals have rank 9?",
          "Do both hospitals have exactly 1200 beds?",
          "Are ranks 8 and 9 consecutive in the dataset?",
          "Which hospital has Rank = 8?",
          "Which hospital has Rank = 9?",
          "Does the hospital at Rank 8 have exactly 1200 beds?",
          "Does the hospital at Rank 9 have exactly 1200 beds?",
          "What are the names of these two hospitals?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which hospitals have rank 8?",
            "Which hospitals have rank 9?",
            "Do both hospitals have exactly 1200 beds?",
            "Are ranks 8 and 9 consecutive in the dataset?"
          ],
          [
            "Which hospital has Rank = 8?",
            "Which hospital has Rank = 9?",
            "Does the hospital at Rank 8 have exactly 1200 beds?",
            "Does the hospital at Rank 9 have exactly 1200 beds?",
            "What are the names of these two hospitals?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Hospital Information"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Hospital Information"
          ],
          [
            "Hospital Information"
          ],
          [
            "Hospital Information"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "# beds": "count of hospital beds",
          "rank": "ordinal position"
        },
        "confidence": 0.8333333333333333,
        "votes": [
          {
            "# Beds": "count of hospital beds"
          },
          {
            "# Beds": "count",
            "Rank": "ordinal position"
          },
          {
            "Rank": "ordinal",
            "# Beds": "number of beds"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Rank values should be unique and sequential",
          "# Beds should be positive integers",
          "Hospital names should be unique",
          "Rank must equal 8 or 9",
          "# Beds must equal exactly 1200",
          "Rankings must be consecutive (8 and 9)",
          "Must return exactly two hospitals",
          "Rank must be an integer",
          "# Beds must be an integer"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rank values should be unique and sequential",
            "# Beds should be positive integers",
            "Hospital names should be unique"
          ],
          [
            "Rank must equal 8 or 9",
            "# Beds must equal exactly 1200",
            "Rankings must be consecutive (8 and 9)",
            "Must return exactly two hospitals"
          ],
          [
            "Rank must be an integer",
            "# Beds must be an integer"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Rank = 8 AND # Beds = 1200",
          "Filter rows where Rank = 9 AND # Beds = 1200",
          "Filter where Rank IN (8, 9) AND # Beds = 1200"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Rank = 8 AND # Beds = 1200",
            "Filter rows where Rank = 9 AND # Beds = 1200"
          ],
          [
            "Filter where Rank IN (8, 9) AND # Beds = 1200"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if ranks 8 and 9 exist in the dataset",
          "Verify bed count consistency for ranks 8 and 9"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if ranks 8 and 9 exist in the dataset",
            "Verify bed count consistency for ranks 8 and 9"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "List hospital names for ranks 8 and 9",
          "Confirm both have 1200 beds",
          "Verify ranks are consecutive in the dataset",
          "Return the names of both hospitals",
          "Verify both hospitals meet the bed count requirement",
          "Confirm rankings are consecutive"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "List hospital names for ranks 8 and 9",
            "Confirm both have 1200 beds",
            "Verify ranks are consecutive in the dataset"
          ],
          [
            "Return the names of both hospitals",
            "Verify both hospitals meet the bed count requirement",
            "Confirm rankings are consecutive"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5750000000000002
  },
  "dbbench-057": {
    "m_q": {
      "target_metric": {
        "value": "earliest game date where the score is exactly '99-89'",
        "confidence": 0.3333333333333333,
        "votes": [
          "earliest game date where the score is exactly '99-89'",
          "earliest game number with a score of 99-89",
          "earliest date of a game with a score of 99-89"
        ]
      },
      "filters": {
        "value": [
          "Score == '99-89'",
          "Score = '99-89'",
          "Score is equal to '99-89'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Score == '99-89'"
          ],
          [
            "Score = '99-89'"
          ],
          [
            "Score is equal to '99-89'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which games have score 99-89?",
          "Among those games, which has the earliest date?",
          "How should date strings be parsed for chronological ordering?",
          "Which games have a score of exactly 99-89?",
          "Among games with score 99-89, which has the smallest game number?",
          "What does 'earliest' mean - chronologically by date or by game sequence number?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which games have score 99-89?",
            "Among those games, which has the earliest date?",
            "How should date strings be parsed for chronological ordering?"
          ],
          [
            "Which games have a score of exactly 99-89?",
            "Among games with score 99-89, which has the smallest game number?",
            "What does 'earliest' mean - chronologically by date or by game sequence number?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Game Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Game Results"
          ],
          [
            "Game Results"
          ],
          [
            "Game Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Duplicate 'Game' column values (e.g., Game 3 appears twice with different data)",
          "Inconsistent ordering - rows not sorted by Game number or Date"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Duplicate 'Game' column values (e.g., Game 3 appears twice with different data)",
            "Inconsistent ordering - rows not sorted by Game number or Date"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "score": "points-team1-points-team2",
          "record": "wins-losses",
          "game": "game_number",
          "date": "date_string"
        },
        "confidence": 0.49999999999999994,
        "votes": [
          {
            "Score": "points-team1-points-team2",
            "Record": "wins-losses"
          },
          {
            "Game": "game_number",
            "Date": "date_string",
            "Score": "points_home_vs_away",
            "Record": "wins_losses"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Date format varies (e.g., 'Fri. Nov. 2', 'Thu. Nov. 29') - need consistent parsing",
          "Score column contains OT notation (e.g., '84-88 (OT)')",
          "Game column contains non-numeric values (may need parsing)",
          "Date format varies (abbreviated month names with day and year)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date format varies (e.g., 'Fri. Nov. 2', 'Thu. Nov. 29') - need consistent parsing",
            "Score column contains OT notation (e.g., '84-88 (OT)')"
          ],
          [
            "Game column contains non-numeric values (may need parsing)",
            "Date format varies (abbreviated month names with day and year)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Score format: 'XX-YY' or 'XX-YY (OT)'",
          "Date format: 'Day. Month. DayNumber'",
          "Game numbers should be unique but aren't in this sample",
          "Score must exactly match '99-89' format",
          "Game number should be interpretable as integer for comparison",
          "Result should be a single game number",
          "The 'Date' column needs to be parsed into a datetime object to determine the earliest date."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Score format: 'XX-YY' or 'XX-YY (OT)'",
            "Date format: 'Day. Month. DayNumber'",
            "Game numbers should be unique but aren't in this sample"
          ],
          [
            "Score must exactly match '99-89' format",
            "Game number should be interpretable as integer for comparison",
            "Result should be a single game number"
          ],
          [
            "The 'Date' column needs to be parsed into a datetime object to determine the earliest date."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Score contains '99-89' (exact match)",
          "Filter rows where Score == '99-89'",
          "Parse Game column as integer if needed"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Score contains '99-89' (exact match)"
          ],
          [
            "Filter rows where Score == '99-89'",
            "Parse Game column as integer if needed"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Game values",
          "Validate date parsing logic"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Game values",
            "Validate date parsing logic"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single date value",
          "If multiple games with score 99-89, return earliest",
          "If no games with score 99-89, return null/empty",
          "Return the game number as integer or string",
          "If multiple games have score 99-89, return the one with minimum game number",
          "If no games match, indicate no result found",
          "Output should be a date in a standard format (e.g., YYYY-MM-DD)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single date value",
            "If multiple games with score 99-89, return earliest",
            "If no games with score 99-89, return null/empty"
          ],
          [
            "Return the game number as integer or string",
            "If multiple games have score 99-89, return the one with minimum game number",
            "If no games match, indicate no result found"
          ],
          [
            "Output should be a date in a standard format (e.g., YYYY-MM-DD)."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5750000000000002
  },
  "dbbench-058": {
    "m_q": {
      "target_metric": {
        "value": "Compare the arrival order of Tony Maggs and Jo Siffert in the race",
        "confidence": 0.3333333333333333,
        "votes": [
          "Compare the arrival order of Tony Maggs and Jo Siffert in the race",
          "Compare finishing positions to determine who came in earlier between Tony Maggs and Jo Siffert",
          "Determine who finished earlier between Tony Maggs and Jo Siffert in a Grand Prix."
        ]
      },
      "filters": {
        "value": [
          "Driver in ['Tony Maggs', 'Jo Siffert']",
          "Driver equals 'Tony Maggs' OR Driver equals 'Jo Siffert'",
          "Driver is either 'Tony Maggs' or 'Jo Siffert'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Driver in ['Tony Maggs', 'Jo Siffert']"
          ],
          [
            "Driver equals 'Tony Maggs' OR Driver equals 'Jo Siffert'"
          ],
          [
            "Driver is either 'Tony Maggs' or 'Jo Siffert'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Driver"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Driver"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What were the finishing positions of Tony Maggs and Jo Siffert?",
          "Which driver had the lower position number (indicating earlier arrival)?",
          "What is the 'Pos' value for Tony Maggs?",
          "What is the 'Pos' value for Jo Siffert?",
          "Which driver has the lower 'Pos' value (earlier finish)?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What were the finishing positions of Tony Maggs and Jo Siffert?",
            "Which driver had the lower position number (indicating earlier arrival)?"
          ],
          [
            "What is the 'Pos' value for Tony Maggs?",
            "What is the 'Pos' value for Jo Siffert?",
            "Which driver has the lower 'Pos' value (earlier finish)?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Grand Prix Result"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Grand Prix Result"
          ],
          [
            "Grand Prix Result"
          ],
          [
            "Grand Prix Result"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "time/retired": "race time or retirement reason",
          "grid": "starting position",
          "pos": "race finishing position (ordinal rank)"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "Time/Retired": "race time or retirement reason",
            "Grid": "starting position"
          },
          {
            "Pos": "race finishing position (ordinal rank)",
            "Time/Retired": "time difference from winner or reason for retirement",
            "Grid": "starting grid position (ordinal rank)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Time/Retired column contains mixed data types: time durations (e.g., '2.02:58.6') and retirement reasons (e.g., 'Engine')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time/Retired column contains mixed data types: time durations (e.g., '2.02:58.6') and retirement reasons (e.g., 'Engine')"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Pos values should be integers for finished drivers",
          "Grid values should be integers between 1 and starting grid size",
          "Both Tony Maggs and Jo Siffert must exist in the Driver column",
          "Pos values determine finishing order where lower values indicate earlier (better) finish",
          "Null Pos values indicate did not finish (DNF) which is ranked after all finishers",
          "Need to handle 'Time/Retired' column which contains both finishing times and reasons for retirement.",
          "Need to parse 'Time/Retired' to determine finishing order. If 'Time/Retired' contains 'laps', the lower the number of laps, the earlier the driver came in. If it contains a time, the lower the time, the earlier the driver came in."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pos values should be integers for finished drivers",
            "Grid values should be integers between 1 and starting grid size"
          ],
          [
            "Both Tony Maggs and Jo Siffert must exist in the Driver column",
            "Pos values determine finishing order where lower values indicate earlier (better) finish",
            "Null Pos values indicate did not finish (DNF) which is ranked after all finishers"
          ],
          [
            "Need to handle 'Time/Retired' column which contains both finishing times and reasons for retirement.",
            "Need to parse 'Time/Retired' to determine finishing order. If 'Time/Retired' contains 'laps', the lower the number of laps, the earlier the driver came in. If it contains a time, the lower the time, the earlier the driver came in."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Pos not null (to filter out retired drivers)",
          "Driver contains 'Tony Maggs' or 'Jo Siffert'",
          "Tony Maggs has Pos = 5",
          "Jo Siffert has Pos = 11",
          "Driver = 'Tony Maggs'",
          "Driver = 'Jo Siffert'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pos not null (to filter out retired drivers)",
            "Driver contains 'Tony Maggs' or 'Jo Siffert'"
          ],
          [
            "Tony Maggs has Pos = 5",
            "Jo Siffert has Pos = 11"
          ],
          [
            "Driver = 'Tony Maggs'",
            "Driver = 'Jo Siffert'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Compare Pos values for the two drivers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Compare Pos values for the two drivers"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return driver name who came earlier",
          "If both retired, compare lap counts from Time/Retired column",
          "Return the name of the driver who came in earlier (had lower Pos value)",
          "If one driver has a Pos value and the other does not, the one with Pos value came in earlier",
          "Output the name of the driver who came in earlier."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return driver name who came earlier",
            "If both retired, compare lap counts from Time/Retired column"
          ],
          [
            "Return the name of the driver who came in earlier (had lower Pos value)",
            "If one driver has a Pos value and the other does not, the one with Pos value came in earlier"
          ],
          [
            "Output the name of the driver who came in earlier."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5777777777777778
  },
  "dbbench-059": {
    "m_q": {
      "target_metric": {
        "value": "minimum Time value",
        "confidence": 0.3333333333333333,
        "votes": [
          "minimum Time value",
          "fastest time among Chinese athletes",
          "fastest time"
        ]
      },
      "filters": {
        "value": [
          "Nationality == 'China'",
          "Nationality is China"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Nationality == 'China'"
          ],
          [
            "Nationality == 'China'"
          ],
          [
            "Nationality is China"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Athlete"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Athlete"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the fastest time among Chinese athletes?",
          "Which Chinese athlete achieved that time?",
          "Which athletes have Nationality equal to 'China'?",
          "What are the Time values for Chinese athletes?",
          "Which Chinese athlete has the minimum Time value?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the fastest time among Chinese athletes?",
            "Which Chinese athlete achieved that time?"
          ],
          [
            "Which athletes have Nationality equal to 'China'?",
            "What are the Time values for Chinese athletes?",
            "Which Chinese athlete has the minimum Time value?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "race_results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "race_results"
          ],
          [
            "race_results"
          ],
          [
            "race_results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "time": "hours:minutes:seconds"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Time": "hours:minutes:seconds"
          },
          {
            "Time": "hours:minutes:seconds (H:MM:SS)"
          },
          {
            "Time": "minutes:seconds"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Time column is string format, needs conversion to time duration for comparison",
          "Time is stored as string object, needs parsing to compare values properly"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time column is string format, needs conversion to time duration for comparison"
          ],
          [
            "Time is stored as string object, needs parsing to compare values properly"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Time must be convertible to duration",
          "Nationality must include 'China' values",
          "Athlete names must be unique per row",
          "Nationality must equal 'China'",
          "Time values must be valid and parseable",
          "At least one Chinese athlete must exist in the dataset",
          "Time column needs to be converted to seconds for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time must be convertible to duration",
            "Nationality must include 'China' values",
            "Athlete names must be unique per row"
          ],
          [
            "Nationality must equal 'China'",
            "Time values must be valid and parseable",
            "At least one Chinese athlete must exist in the dataset"
          ],
          [
            "Time column needs to be converted to seconds for comparison"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Nationality contains 'China'",
          "Exclude rows with missing Time values",
          "Convert Time from string to comparable datetime/timedelta format",
          "Filter rows where Nationality == 'China'",
          "Nationality = 'China'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Nationality contains 'China'",
            "Exclude rows with missing Time values"
          ],
          [
            "Convert Time from string to comparable datetime/timedelta format",
            "Filter rows where Nationality == 'China'"
          ],
          [
            "Nationality = 'China'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate athlete entries",
          "Validate Time format consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate athlete entries",
            "Validate Time format consistency"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return athlete name as string",
          "Include corresponding Time value",
          "Return the Athlete name with the fastest (minimum) Time value",
          "Output should be a single athlete name",
          "Report the athlete's name and time"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return athlete name as string",
            "Include corresponding Time value"
          ],
          [
            "Return the Athlete name with the fastest (minimum) Time value",
            "Output should be a single athlete name"
          ],
          [
            "Report the athlete's name and time"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6027777777777779
  },
  "dbbench-060": {
    "m_q": {
      "target_metric": {
        "value": "maximum value of 'No. of Titles' column",
        "confidence": 0.3333333333333333,
        "votes": [
          "maximum value of 'No. of Titles' column",
          "Player with highest number of titles",
          "maximum value of 'No. of Titles'"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Player Name"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Player Name"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which player(s) have the highest number of titles?",
          "Are there multiple players tied for the highest number of titles?",
          "What is the maximum value in 'No. of Titles' column?",
          "Which player(s) have this maximum number of titles?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which player(s) have the highest number of titles?",
            "Are there multiple players tied for the highest number of titles?"
          ],
          [
            "What is the maximum value in 'No. of Titles' column?",
            "Which player(s) have this maximum number of titles?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Squash Players Ranking"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Squash Players Ranking"
          ],
          [
            "Squash Players Ranking"
          ],
          [
            "Squash Players Ranking"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "no. of titles": "count",
          "runner-up": "count",
          "final appearances": "count",
          "rank": "ordinal"
        },
        "confidence": 0.9166666666666666,
        "votes": [
          {
            "No. of Titles": "count",
            "Runner-up": "count",
            "Final Appearances": "count"
          },
          {
            "No. of Titles": "count",
            "Runner-up": "count",
            "Final Appearances": "count",
            "Rank": "ordinal"
          },
          {
            "Rank": "ranking",
            "No. of Titles": "number of titles",
            "Runner-up": "number of times runner-up",
            "Final Appearances": "number of final appearances"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "No. of Titles >= 0",
          "Final Appearances = No. of Titles + Runner-up",
          "Rank values should be unique but data shows duplicates",
          "No. of Titles must be non-negative integer",
          "Rank should be unique or tied",
          "Final Appearances should be >= No. of Titles + Runner-up",
          "'No. of Titles', 'Runner-up', and 'Final Appearances' should be non-negative integers.",
          "'Final Appearances' should be greater than or equal to 'No. of Titles' and 'Runner-up'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No. of Titles >= 0",
            "Final Appearances = No. of Titles + Runner-up",
            "Rank values should be unique but data shows duplicates"
          ],
          [
            "No. of Titles must be non-negative integer",
            "Rank should be unique or tied",
            "Final Appearances should be >= No. of Titles + Runner-up"
          ],
          [
            "'No. of Titles', 'Runner-up', and 'Final Appearances' should be non-negative integers.",
            "'Final Appearances' should be greater than or equal to 'No. of Titles' and 'Runner-up'"
          ]
        ]
      },
      "derived_filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate player names",
          "Verify consistency: Final Appearances = No. of Titles + Runner-up"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate player names",
            "Verify consistency: Final Appearances = No. of Titles + Runner-up"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Player name(s) with highest No. of Titles value",
          "Include the actual number of titles",
          "Return player name(s) with highest No. of Titles",
          "Include the number of titles in output",
          "player name with the highest number of titles"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Player name(s) with highest No. of Titles value",
            "Include the actual number of titles"
          ],
          [
            "Return player name(s) with highest No. of Titles",
            "Include the number of titles in output"
          ],
          [
            "player name with the highest number of titles"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5291666666666667
  },
  "dbbench-061": {
    "m_q": {
      "target_metric": {
        "value": "Association name",
        "confidence": 0.3333333333333333,
        "votes": [
          "Association name",
          "The association that entered last (most recent joining year)",
          "The association with the latest joining year"
        ]
      },
      "filters": {
        "value": [
          "Joining year must be valid (not null)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Joining year must be valid (not null)"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Find maximum Joining year",
          "Retrieve Association with that Joining year",
          "What is the maximum (most recent) Joining year?",
          "Which Association has that maximum Joining year?",
          "How to handle NULL/missing Joining year values?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Find maximum Joining year",
            "Retrieve Association with that Joining year"
          ],
          [
            "What is the maximum (most recent) Joining year?",
            "Which Association has that maximum Joining year?",
            "How to handle NULL/missing Joining year values?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Football Teams"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Football Teams"
          ],
          [
            "Football Teams"
          ],
          [
            "Football Teams"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "joining year": "year"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Joining year": "year"
          },
          {
            "Joining year": "year"
          },
          {
            "Joining year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Joining year appears to be categorical (specific years only)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Joining year appears to be categorical (specific years only)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Joining year should be integer between 1999-2007 based on sample",
          "Each Association appears once",
          "Joining year must be a valid year value",
          "Only consider records with non-null Joining year values",
          "'Joining year' must be an integer",
          "The latest year should be determined by the maximum value in the 'Joining year' column."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Joining year should be integer between 1999-2007 based on sample",
            "Each Association appears once"
          ],
          [
            "Joining year must be a valid year value",
            "Only consider records with non-null Joining year values"
          ],
          [
            "'Joining year' must be an integer",
            "The latest year should be determined by the maximum value in the 'Joining year' column."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where Joining year is null",
          "Filter out rows where Joining year is NULL/NA"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Joining year is null"
          ],
          [
            "Filter out rows where Joining year is NULL/NA"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Association entries"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Association entries"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single association name as string",
          "Return the Association name as a string",
          "If multiple associations have the same maximum year, clarify how to handle (return all or one)",
          "The output should be the 'Association' corresponding to the maximum 'Joining year'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single association name as string"
          ],
          [
            "Return the Association name as a string",
            "If multiple associations have the same maximum year, clarify how to handle (return all or one)"
          ],
          [
            "The output should be the 'Association' corresponding to the maximum 'Joining year'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5833333333333334
  },
  "dbbench-062": {
    "m_q": {
      "target_metric": {
        "value": "Box office revenue from national films in 2013",
        "confidence": 0.3333333333333333,
        "votes": [
          "Box office revenue from national films in 2013",
          "Country with highest box office revenue from national films in 2013",
          "total box office revenue from national films"
        ]
      },
      "filters": {
        "value": [
          "Year = 2013",
          "Country != 'World'",
          "Box office from national films != '\u2013'",
          "Year == 2013",
          "Box office from national films is not null or missing",
          "Year is 2013"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year = 2013",
            "Country != 'World'",
            "Box office from national films != '\u2013'"
          ],
          [
            "Year == 2013",
            "Box office from national films is not null or missing"
          ],
          [
            "Year is 2013"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Country"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Country"
          ],
          [
            "Country"
          ],
          [
            "Country"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total box office revenue for each country in 2013?",
          "What percentage of each country's box office comes from national films in 2013?",
          "Which country has the highest absolute revenue from national films in 2013?",
          "Which rows have Year = 2013?",
          "How to parse 'Box office from national films' to extract percentage and year?",
          "How to calculate absolute revenue from national films using Box Office and percentage?",
          "Which country has the maximum calculated national films revenue?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total box office revenue for each country in 2013?",
            "What percentage of each country's box office comes from national films in 2013?",
            "Which country has the highest absolute revenue from national films in 2013?"
          ],
          [
            "Which rows have Year = 2013?",
            "How to parse 'Box office from national films' to extract percentage and year?",
            "How to calculate absolute revenue from national films using Box Office and percentage?",
            "Which country has the maximum calculated national films revenue?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "national films"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "national films"
          ],
          [
            "national films"
          ],
          [
            "national films"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple rows with same Rank value (e.g., Rank 1 appears twice)",
          "Inconsistent Year values within same dataset",
          "World entry appears in middle of country list",
          "Multiple entries per country with different years",
          "Box office from national films contains percentage and year in format 'XX% (YYYY)'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple rows with same Rank value (e.g., Rank 1 appears twice)",
            "Inconsistent Year values within same dataset",
            "World entry appears in middle of country list"
          ],
          [
            "Multiple entries per country with different years",
            "Box office from national films contains percentage and year in format 'XX% (YYYY)'"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "box office": "USD billions",
          "box office from national films": "percentage with year reference"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Box Office": "USD billions",
            "Box office from national films": "percentage with year reference"
          },
          {
            "Box Office": "dollars (billions)",
            "Box office from national films": "percentage with year annotation"
          },
          {
            "Box Office": "USD",
            "Box office from national films": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Box Office values contain '$' and 'billion' text",
          "Box office from national films contains percentage values with year references in parentheses",
          "Need to convert text values to numeric for calculation",
          "Box Office values are in billions (e.g., '$3.6 billion')",
          "Need to parse dollar amounts and convert to numeric",
          "Need to extract percentage from 'Box office from national films' format 'XX% (YYYY)'",
          "Box Office and Box office from national films are in different scales (billions vs percentage).",
          "Box Office needs to be converted to a numerical value (remove '$' and 'billion').",
          "Box office from national films needs to be converted to a numerical value (remove '% (year)')."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Box Office values contain '$' and 'billion' text",
            "Box office from national films contains percentage values with year references in parentheses",
            "Need to convert text values to numeric for calculation"
          ],
          [
            "Box Office values are in billions (e.g., '$3.6 billion')",
            "Need to parse dollar amounts and convert to numeric",
            "Need to extract percentage from 'Box office from national films' format 'XX% (YYYY)'"
          ],
          [
            "Box Office and Box office from national films are in different scales (billions vs percentage).",
            "Box Office needs to be converted to a numerical value (remove '$' and 'billion').",
            "Box office from national films needs to be converted to a numerical value (remove '% (year)')."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "\u2013",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "\u2013",
            "",
            "NA"
          ],
          [
            "\u2013",
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year must be 2013",
          "Box office from national films must be a valid percentage for 2013",
          "Country must not be 'World'",
          "Box Office must be convertible to numeric value",
          "Only consider rows where Year == 2013",
          "Exclude rows where 'Box office from national films' is '\u2013' or missing",
          "The year in parentheses in 'Box office from national films' should match 2013",
          "The 'Box Office' column needs to be converted to a numerical type.",
          "The 'Box office from national films' column needs to be converted to a numerical type.",
          "Handle missing values in 'Box office from national films' appropriately (e.g., imputation or removal).",
          "The 'Country' column may contain combined countries (e.g., 'Canada/United States'). Need to decide how to handle these."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Year must be 2013",
            "Box office from national films must be a valid percentage for 2013",
            "Country must not be 'World'",
            "Box Office must be convertible to numeric value"
          ],
          [
            "Only consider rows where Year == 2013",
            "Exclude rows where 'Box office from national films' is '\u2013' or missing",
            "The year in parentheses in 'Box office from national films' should match 2013"
          ],
          [
            "The 'Box Office' column needs to be converted to a numerical type.",
            "The 'Box office from national films' column needs to be converted to a numerical type.",
            "Handle missing values in 'Box office from national films' appropriately (e.g., imputation or removal).",
            "The 'Country' column may contain combined countries (e.g., 'Canada/United States'). Need to decide how to handle these."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Year = 2013",
          "Filter rows where Box office from national films contains '(2013)'",
          "Remove rows with '\u2013' in Box office from national films",
          "Remove 'World' country entry",
          "Parse percentage from 'Box office from national films' where year annotation is (2013)",
          "Calculate absolute revenue: Box Office * (percentage / 100)",
          "Extract the year from the 'Box office from national films' column if needed for filtering.",
          "Create a numerical column for 'Box Office' by removing '$' and 'billion' and converting to float.",
          "Create a numerical column for 'Box office from national films' by extracting the percentage and converting to float."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Year = 2013",
            "Filter rows where Box office from national films contains '(2013)'",
            "Remove rows with '\u2013' in Box office from national films",
            "Remove 'World' country entry"
          ],
          [
            "Parse percentage from 'Box office from national films' where year annotation is (2013)",
            "Calculate absolute revenue: Box Office * (percentage / 100)"
          ],
          [
            "Extract the year from the 'Box office from national films' column if needed for filtering.",
            "Create a numerical column for 'Box Office' by removing '$' and 'billion' and converting to float.",
            "Create a numerical column for 'Box office from national films' by extracting the percentage and converting to float."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate countries in 2013 data",
          "Validate percentage values are between 0-100",
          "Verify year consistency in percentage references"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate countries in 2013 data",
            "Validate percentage values are between 0-100",
            "Verify year consistency in percentage references"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Country name",
          "Calculated national films revenue in USD",
          "Supporting calculation: Box Office * (National_films_percentage/100)",
          "Return single country name",
          "Country should be the one with maximum calculated national films revenue in 2013",
          "The output should be a single country name."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Country name",
            "Calculated national films revenue in USD",
            "Supporting calculation: Box Office * (National_films_percentage/100)"
          ],
          [
            "Return single country name",
            "Country should be the one with maximum calculated national films revenue in 2013"
          ],
          [
            "The output should be a single country name."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6416666666666668
  },
  "dbbench-063": {
    "m_q": {
      "target_metric": {
        "value": "minimum value of 'Int yards' column",
        "confidence": 0.3333333333333333,
        "votes": [
          "minimum value of 'Int yards' column",
          "minimum value of 'Int yards' (interception yards)",
          "minimum 'Int yards'"
        ]
      },
      "filters": {
        "value": [
          "Sacks = 11.5",
          "Sacks == 11.5",
          "'Sacks' is equal to 11.5"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sacks = 11.5"
          ],
          [
            "Sacks == 11.5"
          ],
          [
            "'Sacks' is equal to 11.5"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Find all rows where Sacks equals 11.5",
          "Among those rows, identify the minimum value in the 'Int yards' column",
          "Return that minimum value",
          "Which players have exactly 11.5 sacks?",
          "What are the interception yards for players with 11.5 sacks?",
          "What is the minimum interception yards among those players?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Find all rows where Sacks equals 11.5",
            "Among those rows, identify the minimum value in the 'Int yards' column",
            "Return that minimum value"
          ],
          [
            "Which players have exactly 11.5 sacks?",
            "What are the interception yards for players with 11.5 sacks?",
            "What is the minimum interception yards among those players?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Player Stats"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Player Stats"
          ],
          [
            "Player Stats"
          ],
          [
            "Player Stats"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "sacks": "count (decimal allowed)",
          "int yards": "yards",
          "int avg.": "yards per interception",
          "g": "games",
          "tackles": "count",
          "solo": "count",
          "assts": "count",
          "int": "count",
          "int td": "count",
          "fum. rec": "count",
          "fum. rec td": "count"
        },
        "confidence": 0.4242424242424242,
        "votes": [
          {
            "Sacks": "count (decimal allowed)",
            "Int yards": "yards"
          },
          {
            "Int yards": "yards",
            "Int avg.": "yards per interception",
            "Sacks": "count",
            "G": "games",
            "Tackles": "count",
            "Solo": "count",
            "Assts": "count",
            "Int": "count",
            "Int TD": "count",
            "Fum. rec": "count",
            "Fum. rec TD": "count"
          },
          {
            "Int yards": "yards"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "All numeric columns are stored as object/string dtype, requiring type conversion for numerical operations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All numeric columns are stored as object/string dtype, requiring type conversion for numerical operations"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "0",
          "0.0"
        ],
        "confidence": 0.5333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "0",
            "0.0"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Sacks column contains decimal values like 11.5, 1.0, 0.0",
          "Int yards column contains integer values",
          "Only one data source needed for this analysis",
          "Sacks must equal exactly 11.5",
          "Int yards must be a valid numeric value",
          "Result should be the minimum value among all matching records",
          "'Sacks' and 'Int yards' should be numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sacks column contains decimal values like 11.5, 1.0, 0.0",
            "Int yards column contains integer values",
            "Only one data source needed for this analysis"
          ],
          [
            "Sacks must equal exactly 11.5",
            "Int yards must be a valid numeric value",
            "Result should be the minimum value among all matching records"
          ],
          [
            "'Sacks' and 'Int yards' should be numeric"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert Sacks column to numeric type for comparison with 11.5",
          "Convert Int yards column to numeric type for finding minimum",
          "Filter rows where Sacks == 11.5",
          "Exclude non-numeric or null values in Int yards column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert Sacks column to numeric type for comparison with 11.5",
            "Convert Int yards column to numeric type for finding minimum"
          ],
          [
            "Filter rows where Sacks == 11.5",
            "Exclude non-numeric or null values in Int yards column"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in Sacks and Int yards columns",
          "Verify that Sacks=11.5 filter returns at least one row"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in Sacks and Int yards columns",
            "Verify that Sacks=11.5 filter returns at least one row"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric value representing the least Int yards when Sacks is 11.5",
          "Return a single numeric value representing the minimum Int yards",
          "If multiple players have 11.5 sacks, return only the minimum Int yards value among them",
          "If no players have 11.5 sacks, return null or appropriate indication"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric value representing the least Int yards when Sacks is 11.5"
          ],
          [
            "Return a single numeric value representing the minimum Int yards",
            "If multiple players have 11.5 sacks, return only the minimum Int yards value among them",
            "If no players have 11.5 sacks, return null or appropriate indication"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5312121212121214
  },
  "dbbench-064": {
    "m_q": {
      "target_metric": {
        "value": "Determine which individual (John F. Williams or William A. Mann) held their position first based on the 'From' date",
        "confidence": 0.3333333333333333,
        "votes": [
          "Determine which individual (John F. Williams or William A. Mann) held their position first based on the 'From' date",
          "Determine temporal order - which person held their position first",
          "Determine which of the two individuals, John F. Williams or William A. Mann, held their military leadership position first based on the 'From' date."
        ]
      },
      "filters": {
        "value": [
          "Name contains 'John F. Williams'",
          "Name contains 'William A. Mann'",
          "Name contains 'John F. Williams' or 'William A. Mann'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Name contains 'John F. Williams'",
            "Name contains 'William A. Mann'"
          ],
          [
            "Name contains 'John F. Williams' or 'William A. Mann'"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Name"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Name"
          ],
          [
            "Name"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the earliest 'From' dates for each individual?",
          "Are there multiple entries for the same person that need to be considered?",
          "How should we handle missing 'Number' values when comparing individuals?",
          "What is the earliest 'From' date for John F. Williams?",
          "What is the earliest 'From' date for William A. Mann?",
          "Which date comes first chronologically?",
          "Extract the 'From' date for John F. Williams.",
          "Extract the 'From' date for William A. Mann.",
          "Compare the two 'From' dates to determine which is earlier."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the earliest 'From' dates for each individual?",
            "Are there multiple entries for the same person that need to be considered?",
            "How should we handle missing 'Number' values when comparing individuals?"
          ],
          [
            "What is the earliest 'From' date for John F. Williams?",
            "What is the earliest 'From' date for William A. Mann?",
            "Which date comes first chronologically?"
          ],
          [
            "Extract the 'From' date for John F. Williams.",
            "Extract the 'From' date for William A. Mann.",
            "Compare the two 'From' dates to determine which is earlier."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Military Leaders Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Military Leaders Table"
          ],
          [
            "Military Leaders Table"
          ],
          [
            "Military Leaders Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Missing 'Number' values for some rows (null entries)",
          "Inconsistent rank prefixes in 'Name' column (COL, BG, MG)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Missing 'Number' values for some rows (null entries)",
            "Inconsistent rank prefixes in 'Name' column (COL, BG, MG)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "from": "date",
          "to": "date"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "From": "date",
            "To": "date"
          },
          {
            "From": "date (format: Month Day, Year)",
            "To": "date (format: Month Day, Year)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Date format appears as 'Month DD, YYYY' but stored as object/string type",
          "Need to parse dates for chronological comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date format appears as 'Month DD, YYYY' but stored as object/string type",
            "Need to parse dates for chronological comparison"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each row represents a leadership tenure period",
          "Same person may appear multiple times with different date ranges",
          "Date ranges may overlap or have gaps",
          "From dates must be parseable as valid dates",
          "Comparison requires chronological date ordering",
          "The 'From' column must be parsed as a date.",
          "Multiple entries for the same person may exist; the earliest 'From' date should be used."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each row represents a leadership tenure period",
            "Same person may appear multiple times with different date ranges",
            "Date ranges may overlap or have gaps"
          ],
          [
            "From dates must be parseable as valid dates",
            "Comparison requires chronological date ordering"
          ],
          [
            "The 'From' column must be parsed as a date.",
            "Multiple entries for the same person may exist; the earliest 'From' date should be used."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to rows where Name contains 'Williams' or 'Mann'",
          "Extract earliest 'From' date for each unique individual",
          "Filter rows where Name column contains 'John F. Williams'",
          "Filter rows where Name column contains 'William A. Mann'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to rows where Name contains 'Williams' or 'Mann'",
            "Extract earliest 'From' date for each unique individual"
          ],
          [
            "Filter rows where Name column contains 'John F. Williams'",
            "Filter rows where Name column contains 'William A. Mann'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for date format consistency across all rows",
          "Validate that 'From' date is always earlier than 'To' date"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for date format consistency across all rows",
            "Validate that 'From' date is always earlier than 'To' date"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single name of person who came first",
          "Include supporting date evidence if needed",
          "Return the name of the person who came first",
          "Answer should be either 'William A. Mann' or 'John F. Williams'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single name of person who came first",
            "Include supporting date evidence if needed"
          ],
          [
            "Return the name of the person who came first",
            "Answer should be either 'William A. Mann' or 'John F. Williams'"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6055555555555557
  },
  "dbbench-065": {
    "m_q": {
      "target_metric": {
        "value": "Band value (numerical)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Band value (numerical)",
          "The highest band value among records with System='gsm-450'",
          "the band with the highest value where the system is 'GSM-450'"
        ]
      },
      "filters": {
        "value": [
          "System = 'GSM-450'",
          "System equals 'gsm-450' (case-insensitive)",
          "System is 'GSM-450'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "System = 'GSM-450'"
          ],
          [
            "System equals 'gsm-450' (case-insensitive)"
          ],
          [
            "System is 'GSM-450'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "System",
          "Band"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "System",
            "Band"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the highest band number among all rows where System is GSM-450?",
          "Are there multiple bands with System GSM-450?",
          "How should 'highest' be interpreted - numerically or by some other measure?",
          "What records have System='gsm-450'?",
          "What Band values exist for gsm-450 system?",
          "Which band value is numerically highest?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the highest band number among all rows where System is GSM-450?",
            "Are there multiple bands with System GSM-450?",
            "How should 'highest' be interpreted - numerically or by some other measure?"
          ],
          [
            "What records have System='gsm-450'?",
            "What Band values exist for gsm-450 system?",
            "Which band value is numerically highest?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "cellular_frequency_bands_table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "cellular_frequency_bands_table"
          ],
          [
            "cellular_frequency_bands_table"
          ],
          [
            "cellular_frequency_bands_table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Duplicate rows with same System and Band values (e.g., GSM-850 appears twice with different channel numbers)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Duplicate rows with same System and Band values (e.g., GSM-850 appears twice with different channel numbers)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "uplink (mhz)": "MHz",
          "downlink (mhz)": "MHz",
          "band": "MHz (implied frequency band designation)"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "Uplink (MHz)": "MHz",
            "Downlink (MHz)": "MHz"
          },
          {
            "Band": "MHz (implied frequency band designation)",
            "Uplink (MHz)": "MHz",
            "Downlink (MHz)": "MHz"
          },
          {
            "Uplink (MHz)": "MHz",
            "Downlink (MHz)": "MHz"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Band column contains numeric values but is stored as object dtype",
          "Uplink and Downlink columns contain range strings with commas and dashes that need parsing",
          "Band column contains numeric values that may be stored as strings",
          "Band values appear to be central frequencies or band designations (e.g., 450, 480)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Band column contains numeric values but is stored as object dtype",
            "Uplink and Downlink columns contain range strings with commas and dashes that need parsing"
          ],
          [
            "Band column contains numeric values that may be stored as strings",
            "Band values appear to be central frequencies or band designations (e.g., 450, 480)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "dynamic",
          "dynamic^",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.4666666666666666,
        "votes": [
          [
            "dynamic",
            "dynamic^"
          ],
          [
            "dynamic",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Band should be convertible to numeric for comparison",
          "Only rows where System exactly matches 'GSM-450' should be considered",
          "Highest band means maximum numeric value of Band column",
          "System value must match 'gsm-450' case-insensitively",
          "Band column must be interpretable as numeric for comparison",
          "The 'Band' column should be numeric or convertible to numeric for comparison."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Band should be convertible to numeric for comparison",
            "Only rows where System exactly matches 'GSM-450' should be considered",
            "Highest band means maximum numeric value of Band column"
          ],
          [
            "System value must match 'gsm-450' case-insensitively",
            "Band column must be interpretable as numeric for comparison"
          ],
          [
            "The 'Band' column should be numeric or convertible to numeric for comparison."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "System == 'GSM-450'",
          "Filter rows where System column equals 'GSM-450' (case-insensitive match)",
          "Convert Band column to numeric type for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "System == 'GSM-450'"
          ],
          [
            "Filter rows where System column equals 'GSM-450' (case-insensitive match)",
            "Convert Band column to numeric type for comparison"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if Band values are unique for GSM-450 system"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if Band values are unique for GSM-450 system"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single band number as integer or float",
          "Return single band value that is highest",
          "Output should be the Band value (e.g., '450') not the entire row",
          "Output should be a single band value."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single band number as integer or float"
          ],
          [
            "Return single band value that is highest",
            "Output should be the Band value (e.g., '450') not the entire row"
          ],
          [
            "Output should be a single band value."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.578888888888889
  },
  "dbbench-066": {
    "m_q": {
      "target_metric": {
        "value": "Model with the highest total Quantity",
        "confidence": 0.3333333333333333,
        "votes": [
          "Model with the highest total Quantity",
          "Maximum quantity value and the corresponding model",
          "Total quantity of each model"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Model"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Model"
          ],
          [
            "Model"
          ],
          [
            "Model"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total quantity for each model?",
          "Which model has the maximum total quantity?",
          "What is the quantity for each model?",
          "Which model has the highest quantity value?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total quantity for each model?",
            "Which model has the maximum total quantity?"
          ],
          [
            "What is the quantity for each model?",
            "Which model has the highest quantity value?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Vehicle_Data"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Vehicle_Data"
          ],
          [
            "Vehicle_Data"
          ],
          [
            "Vehicle_Data"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "length (feet)": "feet",
          "quantity": "count",
          "year": "year"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "Length (feet)": "feet",
            "Quantity": "count"
          },
          {
            "Quantity": "count",
            "Length (feet)": "feet"
          },
          {
            "Year": "year",
            "Length (feet)": "feet",
            "Quantity": "number of vehicles"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Length (feet) column contains text values with units (e.g., '60 (articulated)')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Length (feet) column contains text values with units (e.g., '60 (articulated)')"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 8.0,
        "confidence": 1.0,
        "votes": [
          8.0,
          8.0,
          8.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Quantity must be non-negative integer",
          "Year should be reasonable (e.g., 1900-2024)",
          "Quantity column must be numeric (Int64)",
          "Model column must be non-null for comparison",
          "Quantity must be a non-negative integer"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Quantity must be non-negative integer",
            "Year should be reasonable (e.g., 1900-2024)"
          ],
          [
            "Quantity column must be numeric (Int64)",
            "Model column must be non-null for comparison"
          ],
          [
            "Quantity must be a non-negative integer"
          ]
        ]
      },
      "derived_filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Sum of Quantity by Model",
          "Maximum of aggregated Quantity"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Sum of Quantity by Model",
            "Maximum of aggregated Quantity"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Model name",
          "Total Quantity",
          "Return the model name with the highest quantity",
          "Include the quantity value in the output",
          "The output should be a list of models and their total quantities, sorted by quantity in descending order."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Model name",
            "Total Quantity"
          ],
          [
            "Return the model name with the highest quantity",
            "Include the quantity value in the output"
          ],
          [
            "The output should be a list of models and their total quantities, sorted by quantity in descending order."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5722222222222223
  },
  "dbbench-067": {
    "m_q": {
      "target_metric": {
        "value": "maximum population among satellite cities",
        "confidence": 0.3333333333333333,
        "votes": [
          "maximum population among satellite cities",
          "area with the highest population among satellite cities",
          "population of the satellite city with the highest population"
        ]
      },
      "filters": {
        "value": [
          "Name contains 'Satellite cities' category",
          "Exclude rows with 'City Proper' or district names",
          "rows where Name is 'Satellite cities' header or rows 4-17 (satellite cities section)",
          "filter out rows that are not satellite cities"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Name contains 'Satellite cities' category",
            "Exclude rows with 'City Proper' or district names"
          ],
          [
            "rows where Name is 'Satellite cities' header or rows 4-17 (satellite cities section)"
          ],
          [
            "filter out rows that are not satellite cities"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Name"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Name"
          ],
          [
            "Name"
          ],
          [
            "Name"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What defines a satellite city in this dataset?",
          "Are all rows after row 3 considered satellite cities?",
          "Should we include only rows with '\u5e02' (city) in Hanzi column?",
          "Which rows represent satellite cities?",
          "What is the population value for each satellite city?",
          "Which satellite city has the maximum population?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What defines a satellite city in this dataset?",
            "Are all rows after row 3 considered satellite cities?",
            "Should we include only rows with '\u5e02' (city) in Hanzi column?"
          ],
          [
            "Which rows represent satellite cities?",
            "What is the population value for each satellite city?",
            "Which satellite city has the maximum population?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "City Population and Area"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "City Population and Area"
          ],
          [
            "City Population and Area"
          ],
          [
            "City Population and Area"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Mixed data types in Population and Area columns (object instead of numeric)",
          "Header row appears to have descriptive text 'Satellite cities' as a row",
          "Header rows mixed with data rows (rows with 'City Proper' and 'Satellite cities' values)",
          "Row index '#' column contains null values for header rows"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Mixed data types in Population and Area columns (object instead of numeric)",
            "Header row appears to have descriptive text 'Satellite cities' as a row"
          ],
          [
            "Header rows mixed with data rows (rows with 'City Proper' and 'Satellite cities' values)",
            "Row index '#' column contains null values for header rows"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "population (2010 census)": "people",
          "area (km\u00b2)": "square kilometers",
          "density (/km\u00b2)": "people per square kilometer"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Population (2010 Census)": "people",
            "Area (km\u00b2)": "square kilometers",
            "Density (/km\u00b2)": "people per square kilometer"
          },
          {
            "Population (2010 Census)": "count",
            "Area (km\u00b2)": "square kilometers",
            "Density (/km\u00b2)": "persons per square kilometer"
          },
          {
            "Population (2010 Census)": "people",
            "Area (km\u00b2)": "km\u00b2",
            "Density (/km\u00b2)": "people/km\u00b2"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Population values contain commas (e.g., '3,121,275')",
          "Area values contain commas (e.g., '1,698')",
          "Some density values have decimal points while others have commas",
          "Population values contain comma thousands separators",
          "Density values contain comma decimal separators"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Population values contain commas (e.g., '3,121,275')",
            "Area values contain commas (e.g., '1,698')",
            "Some density values have decimal points while others have commas"
          ],
          [
            "Population values contain comma thousands separators",
            "Density values contain comma decimal separators"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "City Proper",
          "Satellite cities"
        ],
        "confidence": 0.6,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "City Proper",
            "Satellite cities"
          ],
          [
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Population must be positive",
          "Area must be positive",
          "Density should equal Population/Area",
          "Satellite cities should have '\u5e02' (city) in Hanzi column",
          "Only consider rows marked as satellite cities (rows 4-17 based on sample)",
          "Exclude header rows where Name='Satellite cities'",
          "Population values must be parsed as numeric after removing commas",
          "The 'Population (2010 Census)' and 'Area (km\u00b2)' columns should be converted to numeric types after removing commas."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Population must be positive",
            "Area must be positive",
            "Density should equal Population/Area",
            "Satellite cities should have '\u5e02' (city) in Hanzi column"
          ],
          [
            "Only consider rows marked as satellite cities (rows 4-17 based on sample)",
            "Exclude header rows where Name='Satellite cities'",
            "Population values must be parsed as numeric after removing commas"
          ],
          [
            "The 'Population (2010 Census)' and 'Area (km\u00b2)' columns should be converted to numeric types after removing commas."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows where Name equals 'City Proper' or 'Satellite cities'",
          "Filter to rows where Hanzi contains '\u5e02' (city character)",
          "# >= 4 AND # <= 17 to identify satellite cities",
          "Name != 'Satellite cities' to exclude section headers",
          "Identify satellite cities based on the 'Name' column. The rows with 'Satellite cities' in the 'Name' column are headers, and the subsequent rows until the next header are satellite cities."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows where Name equals 'City Proper' or 'Satellite cities'",
            "Filter to rows where Hanzi contains '\u5e02' (city character)"
          ],
          [
            "# >= 4 AND # <= 17 to identify satellite cities",
            "Name != 'Satellite cities' to exclude section headers"
          ],
          [
            "Identify satellite cities based on the 'Name' column. The rows with 'Satellite cities' in the 'Name' column are headers, and the subsequent rows until the next header are satellite cities."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check population distribution among satellite cities",
          "Verify density calculations match population/area"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check population distribution among satellite cities",
            "Verify density calculations match population/area"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single city name with highest population",
          "Include population value in output",
          "Return the Name of the satellite city with maximum population",
          "Include the population value for context"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single city name with highest population",
            "Include population value in output"
          ],
          [
            "Return the Name of the satellite city with maximum population",
            "Include the population value for context"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6300000000000001
  },
  "dbbench-068": {
    "m_q": {
      "target_metric": {
        "value": "total pick # for D position from Chilliwack Bruins",
        "confidence": 0.3333333333333333,
        "votes": [
          "total pick # for D position from Chilliwack Bruins",
          "total (sum) of Pick # values",
          "Sum of 'Pick #' for players with 'Position' as 'D' from 'Team from' 'Chilliwack Bruins'"
        ]
      },
      "filters": {
        "value": [
          "Position = 'D'",
          "Team from = 'Chilliwack Bruins'"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Position = 'D'",
            "Team from = 'Chilliwack Bruins'"
          ],
          [
            "Position = 'D'",
            "Team from = 'Chilliwack Bruins'"
          ],
          [
            "Position = 'D'",
            "Team from = 'Chilliwack Bruins'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the sum of Pick # values for all players with Position D and Team from Chilliwack Bruins?",
          "How many D players came from Chilliwack Bruins?",
          "What are the individual Pick # values for D players from Chilliwack Bruins?",
          "Which players have Position = 'D'?",
          "Which players have Team from = 'Chilliwack Bruins'?",
          "What are the Pick # values for players matching both conditions?",
          "What is the sum of those Pick # values?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the sum of Pick # values for all players with Position D and Team from Chilliwack Bruins?",
            "How many D players came from Chilliwack Bruins?",
            "What are the individual Pick # values for D players from Chilliwack Bruins?"
          ],
          [
            "Which players have Position = 'D'?",
            "Which players have Team from = 'Chilliwack Bruins'?",
            "What are the Pick # values for players matching both conditions?",
            "What is the sum of those Pick # values?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "hockey_players"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "hockey_players"
          ],
          [
            "hockey_players"
          ],
          [
            "hockey_players"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "pick #": "ordinal selection number",
          "position": "player position code (categorical)",
          "team from": "team name (categorical)"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "Pick #": "ordinal selection number"
          },
          {
            "Pick #": "draft pick number (integer)",
            "Position": "player position code (categorical)",
            "Team from": "team name (categorical)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Pick # column is stored as object/string type but contains numeric values that should be summed",
          "Pick # is stored as object dtype but should be numeric for summation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pick # column is stored as object/string type but contains numeric values that should be summed"
          ],
          [
            "Pick # is stored as object dtype but should be numeric for summation"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Pick # values should be convertible to numeric for summation",
          "Position values should be exact matches (case-sensitive)",
          "Team from values should be exact matches (case-sensitive)",
          "Pick # must be convertible to numeric type for summation",
          "Position must exactly match 'D' (case-sensitive)",
          "Team from must exactly match 'Chilliwack Bruins' (case-sensitive)",
          "Pick # should be numeric for summation, handle non-numeric values appropriately (e.g., convert to numeric, remove, or replace with a default value)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pick # values should be convertible to numeric for summation",
            "Position values should be exact matches (case-sensitive)",
            "Team from values should be exact matches (case-sensitive)"
          ],
          [
            "Pick # must be convertible to numeric type for summation",
            "Position must exactly match 'D' (case-sensitive)",
            "Team from must exactly match 'Chilliwack Bruins' (case-sensitive)"
          ],
          [
            "Pick # should be numeric for summation, handle non-numeric values appropriately (e.g., convert to numeric, remove, or replace with a default value)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Position contains 'D'",
          "Filter rows where Team from contains 'Chilliwack Bruins'",
          "Filter rows where Position == 'D' AND Team from == 'Chilliwack Bruins'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Position contains 'D'",
            "Filter rows where Team from contains 'Chilliwack Bruins'"
          ],
          [
            "Filter rows where Position == 'D' AND Team from == 'Chilliwack Bruins'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing Pick # values in filtered subset",
          "Verify all Pick # values are numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing Pick # values in filtered subset",
            "Verify all Pick # values are numeric"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric total",
          "Should handle potential multiple D players from Chilliwack Bruins",
          "Return a single numeric value representing the sum",
          "If no rows match filters, return 0 or null"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric total",
            "Should handle potential multiple D players from Chilliwack Bruins"
          ],
          [
            "Return a single numeric value representing the sum",
            "If no rows match filters, return 0 or null"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5888888888888889
  },
  "dbbench-069": {
    "m_q": {
      "target_metric": {
        "value": "sum of RR2 Pts.",
        "confidence": 0.3333333333333333,
        "votes": [
          "sum of RR2 Pts.",
          "total number of RR2 Pts.",
          "Sum of 'RR2 Pts.' where 'Won' is equal to 11"
        ]
      },
      "filters": {
        "value": [
          "Won = 11",
          "Filter rows where 'Won' equals 11"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Won = 11"
          ],
          [
            "Won = 11"
          ],
          [
            "Filter rows where 'Won' equals 11"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Identify rows where Won column equals 11",
          "Extract RR2 Pts. values from those rows",
          "Sum the RR2 Pts. values",
          "Which teams have Won = 11?",
          "What are the RR2 Pts. values for teams with Won = 11?",
          "Sum all RR2 Pts. values where Won = 11"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify rows where Won column equals 11",
            "Extract RR2 Pts. values from those rows",
            "Sum the RR2 Pts. values"
          ],
          [
            "Which teams have Won = 11?",
            "What are the RR2 Pts. values for teams with Won = 11?",
            "Sum all RR2 Pts. values where Won = 11"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "team_racing_results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "team_racing_results"
          ],
          [
            "team_racing_results"
          ],
          [
            "team_racing_results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All columns are stored as object dtype despite containing numeric data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns are stored as object dtype despite containing numeric data"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "rr2 pts.": "points",
          "won": "race wins count",
          "races": "count",
          "rr1 pts.": "points",
          "rr3 pts.": "points",
          "total pts.": "points",
          "ranking": "ordinal"
        },
        "confidence": 0.42857142857142855,
        "votes": [
          {
            "RR2 Pts.": "points",
            "Won": "race wins count"
          },
          {
            "Races": "count",
            "Won": "count",
            "RR1 Pts.": "points",
            "RR2 Pts.": "points",
            "RR3 Pts.": "points",
            "Total Pts.": "points",
            "Ranking": "ordinal"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Won column contains numeric values but stored as object dtype",
          "RR2 Pts. column contains numeric values but stored as object dtype"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Won column contains numeric values but stored as object dtype",
            "RR2 Pts. column contains numeric values but stored as object dtype"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 8.0,
        "confidence": 1.0,
        "votes": [
          8.0,
          8.0,
          8.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Won values should be integers",
          "RR2 Pts. values should be numeric",
          "Each team appears only once in the dataset",
          "Won column must be converted to numeric for filtering",
          "RR2 Pts. column must be converted to numeric for summation",
          "Filter condition: Won == 11",
          "The columns 'Won' and 'RR2 Pts.' should be of numeric type for filtering and summation."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Won values should be integers",
            "RR2 Pts. values should be numeric",
            "Each team appears only once in the dataset"
          ],
          [
            "Won column must be converted to numeric for filtering",
            "RR2 Pts. column must be converted to numeric for summation",
            "Filter condition: Won == 11"
          ],
          [
            "The columns 'Won' and 'RR2 Pts.' should be of numeric type for filtering and summation."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Won column equals '11' (string representation due to object dtype)",
          "CAST(Won AS INTEGER) = 11"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Won column equals '11' (string representation due to object dtype)"
          ],
          [
            "CAST(Won AS INTEGER) = 11"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Type conversion needed: Won and RR2 Pts. columns must be converted to numeric types"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Type conversion needed: Won and RR2 Pts. columns must be converted to numeric types"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric value representing total RR2 points",
          "Return a single scalar numeric value representing the total",
          "Sum of all RR2 Pts. where Won equals 11"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric value representing total RR2 points"
          ],
          [
            "Return a single scalar numeric value representing the total",
            "Sum of all RR2 Pts. where Won equals 11"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5797619047619049
  },
  "dbbench-070": {
    "m_q": {
      "target_metric": {
        "value": "sum of crowd numbers for matches at Glenferrie Oval",
        "confidence": 0.3333333333333333,
        "votes": [
          "sum of crowd numbers for matches at Glenferrie Oval",
          "total crowd attendance",
          "Total crowd attendance for matches played at Glenferrie Oval"
        ]
      },
      "filters": {
        "value": [
          "Venue = 'Glenferrie Oval'",
          "Venue equals 'Glenferrie Oval'",
          "Venue is Glenferrie Oval"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Venue = 'Glenferrie Oval'"
          ],
          [
            "Venue equals 'Glenferrie Oval'"
          ],
          [
            "Venue is Glenferrie Oval"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total crowd attendance across all matches at Glenferrie Oval?",
          "How many matches were played at Glenferrie Oval?",
          "What is the average crowd size at Glenferrie Oval?",
          "Which rows have Venue = 'Glenferrie Oval'?",
          "What are the Crowd values for those rows?",
          "Sum all Crowd values for Glenferrie Oval matches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total crowd attendance across all matches at Glenferrie Oval?",
            "How many matches were played at Glenferrie Oval?",
            "What is the average crowd size at Glenferrie Oval?"
          ],
          [
            "Which rows have Venue = 'Glenferrie Oval'?",
            "What are the Crowd values for those rows?",
            "Sum all Crowd values for Glenferrie Oval matches"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Aussie Rules Football Match Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Aussie Rules Football Match Results"
          ],
          [
            "Aussie Rules Football Match Results"
          ],
          [
            "Aussie Rules Football Match Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Crowd column is stored as object/string type but contains numeric data with commas",
          "Score columns (Home team score, Away team score) contain both points format (e.g., '7.9 (51)') and numeric values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Crowd column is stored as object/string type but contains numeric data with commas",
            "Score columns (Home team score, Away team score) contain both points format (e.g., '7.9 (51)') and numeric values"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "crowd": "number of people",
          "date": "date",
          "home team score": "Australian rules football score format",
          "away team score": "Australian rules football score format"
        },
        "confidence": 0.5833333333333333,
        "votes": [
          {
            "Crowd": "number of people",
            "Date": "date",
            "Home team score": "Australian rules football score format",
            "Away team score": "Australian rules football score format"
          },
          {
            "Crowd": "number of people (count)",
            "Date": "date format"
          },
          {
            "Crowd": "number of people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Crowd numbers contain commas as thousands separators (e.g., '17,000') which need to be removed for numerical operations",
          "Date format appears inconsistent (historical dates like '1 July 1939' vs modern dates like '12 May 2021')",
          "Crowd values contain commas as thousands separators (e.g., '8,000')",
          "Crowd column has dtype 'object' but represents numeric values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Crowd numbers contain commas as thousands separators (e.g., '17,000') which need to be removed for numerical operations",
            "Date format appears inconsistent (historical dates like '1 July 1939' vs modern dates like '12 May 2021')"
          ],
          [
            "Crowd values contain commas as thousands separators (e.g., '8,000')",
            "Crowd column has dtype 'object' but represents numeric values"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Crowd values must be non-negative integers",
          "Venue names should be consistent (exact match required for 'Glenferrie Oval')",
          "Date values should be valid dates",
          "Venue must exactly match 'Glenferrie Oval' (case-sensitive)",
          "Crowd values must be parsed from string format with commas removed",
          "Only sum non-null Crowd values",
          "Crowd column must be converted to numeric type after removing commas."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Crowd values must be non-negative integers",
            "Venue names should be consistent (exact match required for 'Glenferrie Oval')",
            "Date values should be valid dates"
          ],
          [
            "Venue must exactly match 'Glenferrie Oval' (case-sensitive)",
            "Crowd values must be parsed from string format with commas removed",
            "Only sum non-null Crowd values"
          ],
          [
            "Crowd column must be converted to numeric type after removing commas."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Venue column contains 'Glenferrie Oval' (case-sensitive exact match)",
          "Remove commas from Crowd column before summation",
          "Convert Crowd column to numeric type",
          "Filter rows where Venue == 'Glenferrie Oval'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Venue column contains 'Glenferrie Oval' (case-sensitive exact match)",
            "Remove commas from Crowd column before summation",
            "Convert Crowd column to numeric type"
          ],
          [
            "Filter rows where Venue == 'Glenferrie Oval'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in Crowd column for Glenferrie Oval matches",
          "Verify that all filtered rows have valid numeric crowd values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in Crowd column for Glenferrie Oval matches",
            "Verify that all filtered rows have valid numeric crowd values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Total should be presented as a single integer",
          "Consider formatting the result with thousands separators for readability",
          "Return total as numeric value",
          "Total should be sum of all matching crowd values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total should be presented as a single integer",
            "Consider formatting the result with thousands separators for readability"
          ],
          [
            "Return total as numeric value",
            "Total should be sum of all matching crowd values"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5791666666666668
  },
  "dbbench-071": {
    "m_q": {
      "target_metric": {
        "value": "total population count for districts where foreign nationals percentage equals 23.3%",
        "confidence": 0.3333333333333333,
        "votes": [
          "total population count for districts where foreign nationals percentage equals 23.3%",
          "Total population for city districts where foreign nationals percentage equals 23.3%",
          "Total population of foreign nationals in city districts where the percentage of foreign nationals is 23.3%"
        ]
      },
      "filters": {
        "value": [
          "Foreign nationals in % = '23,3%'",
          "Foreign nationals in % == 23.3%",
          "Filter city districts where 'Foreign nationals in %' is equal to 23.3%"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Foreign nationals in % = '23,3%'"
          ],
          [
            "Foreign nationals in % == 23.3%"
          ],
          [
            "Filter city districts where 'Foreign nationals in %' is equal to 23.3%"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which city districts have exactly 23.3% foreign nationals?",
          "What is the sum of Population for those districts?",
          "What is the population value for those districts?",
          "What is the sum of population across matching districts?"
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "Which city districts have exactly 23.3% foreign nationals?",
            "What is the sum of Population for those districts?"
          ],
          [
            "Which city districts have exactly 23.3% foreign nationals?",
            "What is the population value for those districts?",
            "What is the sum of population across matching districts?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "city_district_statistics"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "city_district_statistics"
          ],
          [
            "city_district_statistics"
          ],
          [
            "city_district_statistics"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All columns stored as object/string type despite numeric content"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns stored as object/string type despite numeric content"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "area in km\u00b2": "square kilometers",
          "population": "count of people",
          "foreign nationals": "count of people",
          "foreign nationals in %": "percentage"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Area in km\u00b2": "square kilometers",
            "Population": "count of people",
            "Foreign nationals": "count of people",
            "Foreign nationals in %": "percentage"
          },
          {
            "Area in km\u00b2": "square kilometers",
            "Population": "count",
            "Foreign nationals": "count",
            "Foreign nationals in %": "percentage"
          },
          {
            "Area in km\u00b2": "km\u00b2",
            "Population": "people",
            "Foreign nationals": "people",
            "Foreign nationals in %": "percentage"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Decimal separator is comma in numeric strings (e.g., '0,51 km\u00b2', '32,3%')",
          "Thousands separator is period in numeric strings (e.g., '3.475')",
          "Area in km\u00b2 column contains unit suffix 'km\u00b2' embedded in values",
          "Foreign nationals in % column contains percentage symbol '%' embedded in values",
          "Numeric columns stored as object/string type instead of numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Decimal separator is comma in numeric strings (e.g., '0,51 km\u00b2', '32,3%')",
            "Thousands separator is period in numeric strings (e.g., '3.475')"
          ],
          [
            "Area in km\u00b2 column contains unit suffix 'km\u00b2' embedded in values",
            "Foreign nationals in % column contains percentage symbol '%' embedded in values",
            "Numeric columns stored as object/string type instead of numeric"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Population \u2265 0",
          "0% \u2264 Foreign nationals in % \u2264 100%",
          "Foreign nationals \u2264 Population",
          "Foreign nationals in % must equal exactly 23.3%",
          "Population values must be valid positive integers",
          "Handle potential floating point comparison issues with percentage values",
          "The 'Area in km\u00b2', 'Population', 'Foreign nationals', and 'Foreign nationals in %' columns need to be converted to numeric types.",
          "The 'Area in km\u00b2' column needs to have the 'km\u00b2' suffix removed and the comma replaced with a period for proper numeric conversion.",
          "The 'Foreign nationals in %' column needs to have the '%' suffix removed for proper numeric conversion."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Population \u2265 0",
            "0% \u2264 Foreign nationals in % \u2264 100%",
            "Foreign nationals \u2264 Population"
          ],
          [
            "Foreign nationals in % must equal exactly 23.3%",
            "Population values must be valid positive integers",
            "Handle potential floating point comparison issues with percentage values"
          ],
          [
            "The 'Area in km\u00b2', 'Population', 'Foreign nationals', and 'Foreign nationals in %' columns need to be converted to numeric types.",
            "The 'Area in km\u00b2' column needs to have the 'km\u00b2' suffix removed and the comma replaced with a period for proper numeric conversion.",
            "The 'Foreign nationals in %' column needs to have the '%' suffix removed for proper numeric conversion."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert 'Foreign nationals in %' to numeric for comparison with 23.3%",
          "Convert 'Population' to numeric for summation",
          "Parse 'Foreign nationals in %' to extract numeric value without '%' symbol",
          "Convert European decimal format (comma) to standard format (dot)",
          "Filter where parsed percentage equals 23.3"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert 'Foreign nationals in %' to numeric for comparison with 23.3%",
            "Convert 'Population' to numeric for summation"
          ],
          [
            "Parse 'Foreign nationals in %' to extract numeric value without '%' symbol",
            "Convert European decimal format (comma) to standard format (dot)",
            "Filter where parsed percentage equals 23.3"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify percentage calculation: Foreign nationals / Population \u2248 Foreign nationals in %"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify percentage calculation: Foreign nationals / Population \u2248 Foreign nationals in %"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Numeric result without formatting",
          "Consider rounding to nearest integer",
          "Return total population as integer",
          "Handle case where no districts match the criteria (return 0 or null)",
          "Handle case where multiple districts match (sum their populations)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Numeric result without formatting",
            "Consider rounding to nearest integer"
          ],
          [
            "Return total population as integer",
            "Handle case where no districts match the criteria (return 0 or null)",
            "Handle case where multiple districts match (sum their populations)"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5875000000000001
  },
  "dbbench-072": {
    "m_q": {
      "target_metric": {
        "value": "sum of Total medals",
        "confidence": 0.3333333333333333,
        "votes": [
          "sum of Total medals",
          "total number of medals",
          "total number of medals for South Korea, North Korea, Sweden, and Brazil"
        ]
      },
      "filters": {
        "value": [
          "Nation in ['South Korea', 'North Korea', 'Sweden', 'Brazil']",
          "Nation is South Korea",
          "Nation is North Korea",
          "Nation is Sweden",
          "Nation is Brazil"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Nation in ['South Korea', 'North Korea', 'Sweden', 'Brazil']"
          ],
          [
            "Nation in ['South Korea', 'North Korea', 'Sweden', 'Brazil']"
          ],
          [
            "Nation is South Korea",
            "Nation is North Korea",
            "Nation is Sweden",
            "Nation is Brazil"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Identify rows for each specified nation",
          "Extract Total column values",
          "Sum the extracted values",
          "What is the total number of medals for South Korea?",
          "What is the total number of medals for North Korea?",
          "What is the total number of medals for Sweden?",
          "What is the total number of medals for Brazil?",
          "What is the sum of these four totals?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Identify rows for each specified nation",
            "Extract Total column values",
            "Sum the extracted values"
          ],
          [
            "What is the total number of medals for South Korea?",
            "What is the total number of medals for North Korea?",
            "What is the total number of medals for Sweden?",
            "What is the total number of medals for Brazil?",
            "What is the sum of these four totals?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Medal Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Medal Table"
          ],
          [
            "Medal Table"
          ],
          [
            "Medal Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "gold": "medals",
          "silver": "medals",
          "bronze": "medals",
          "total": "medals",
          "rank": "ordinal"
        },
        "confidence": 0.9333333333333333,
        "votes": [
          {
            "Gold": "medals",
            "Silver": "medals",
            "Bronze": "medals",
            "Total": "medals"
          },
          {
            "Gold": "count",
            "Silver": "count",
            "Bronze": "count",
            "Total": "count",
            "Rank": "ordinal"
          },
          {
            "Rank": "rank",
            "Gold": "medals",
            "Silver": "medals",
            "Bronze": "medals",
            "Total": "medals"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total = Gold + Silver + Bronze for each row",
          "Rank values are unique",
          "Nation values are unique",
          "Total column should equal Gold + Silver + Bronze for each nation",
          "All medal counts should be non-negative integers",
          "Nation names must match exactly: 'South Korea', 'North Korea', 'Sweden', 'Brazil'",
          "The 'Total' column should be equal to the sum of 'Gold', 'Silver', and 'Bronze' columns for each row."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total = Gold + Silver + Bronze for each row",
            "Rank values are unique",
            "Nation values are unique"
          ],
          [
            "Total column should equal Gold + Silver + Bronze for each nation",
            "All medal counts should be non-negative integers",
            "Nation names must match exactly: 'South Korea', 'North Korea', 'Sweden', 'Brazil'"
          ],
          [
            "The 'Total' column should be equal to the sum of 'Gold', 'Silver', and 'Bronze' columns for each row."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Nation == 'South Korea' OR Nation == 'North Korea' OR Nation == 'Sweden' OR Nation == 'Brazil'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Filter rows where Nation == 'South Korea' OR Nation == 'North Korea' OR Nation == 'Sweden' OR Nation == 'Brazil'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric total",
          "Return a single numeric value representing the sum of Total column for the four specified nations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric total"
          ],
          [
            "Return a single numeric value representing the sum of Total column for the four specified nations"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5333333333333334
  },
  "dbbench-073": {
    "m_q": {
      "target_metric": {
        "value": "sum of total goals scored by Chipperfield and Viduka",
        "confidence": 0.6666666666666666,
        "votes": [
          "sum of goals scored by players named 'Chipperfield' and 'Viduka'",
          "sum of total goals scored by Chipperfield and Viduka",
          "sum of total goals scored by Chipperfield and Viduka"
        ]
      },
      "filters": {
        "value": [
          "Player in ['Chipperfield', 'Viduka']",
          "Player = 'Chipperfield'",
          "Player = 'Viduka'",
          "Player is Chipperfield",
          "Player is Viduka"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Player in ['Chipperfield', 'Viduka']"
          ],
          [
            "Player = 'Chipperfield'",
            "Player = 'Viduka'"
          ],
          [
            "Player is Chipperfield",
            "Player is Viduka"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Player"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Player"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the individual goal counts for Chipperfield?",
          "What are the individual goal counts for Viduka?",
          "How should missing values in goal columns be handled?",
          "What is the total number of goals scored by Chipperfield?",
          "What is the total number of goals scored by Viduka?",
          "What is the sum of goals for both players?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the individual goal counts for Chipperfield?",
            "What are the individual goal counts for Viduka?",
            "How should missing values in goal columns be handled?"
          ],
          [
            "What is the total number of goals scored by Chipperfield?",
            "What is the total number of goals scored by Viduka?",
            "What is the sum of goals for both players?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Player Goals in International Tournaments"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Player Goals in International Tournaments"
          ],
          [
            "Player Goals in International Tournaments"
          ],
          [
            "Player Goals in International Tournaments"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column 'FIFA World Cup Qual.' has dtype 'object' while other goal columns are 'Int64' - contains non-numeric values like '-'",
          "Player names appear multiple times in the dataset, suggesting multiple records per player"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column 'FIFA World Cup Qual.' has dtype 'object' while other goal columns are 'Int64' - contains non-numeric values like '-'"
          ],
          [
            "Player names appear multiple times in the dataset, suggesting multiple records per player"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "friendlies": "goals",
          "fifa confederations cup": "goals",
          "fifa world cup qual.": "goals",
          "total goals": "goals"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Friendlies": "goals",
            "FIFA Confederations Cup": "goals",
            "FIFA World Cup Qual.": "goals",
            "Total Goals": "goals"
          },
          {
            "Friendlies": "count of goals",
            "FIFA Confederations Cup": "count of goals",
            "FIFA World Cup Qual.": "count of goals",
            "Total Goals": "count of goals"
          },
          {
            "Friendlies": "goals",
            "FIFA Confederations Cup": "goals",
            "FIFA World Cup Qual.": "goals",
            "Total Goals": "goals"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Missing values represented differently: empty strings in some columns, '-' in FIFA World Cup Qual."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Missing values represented differently: empty strings in some columns, '-' in FIFA World Cup Qual."
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "-",
          "NA",
          "N/A"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "",
            "-"
          ],
          [
            "-",
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total Goals should equal sum of Friendlies + FIFA Confederations Cup + FIFA World Cup Qual. for each player",
          "Goal counts must be non-negative integers",
          "Player names must match exactly: 'Chipperfield' and 'Viduka'",
          "Multiple rows exist for same player names - need to sum across all rows per player",
          "Total Goals must be an integer",
          "Friendlies must be an integer",
          "FIFA Confederations Cup must be an integer"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total Goals should equal sum of Friendlies + FIFA Confederations Cup + FIFA World Cup Qual. for each player",
            "Goal counts must be non-negative integers"
          ],
          [
            "Player names must match exactly: 'Chipperfield' and 'Viduka'",
            "Multiple rows exist for same player names - need to sum across all rows per player"
          ],
          [
            "Total Goals must be an integer",
            "Friendlies must be an integer",
            "FIFA Confederations Cup must be an integer"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Player matches 'Chipperfield' or 'Viduka' (case-sensitive)",
          "Filter rows where Player IN ('Chipperfield', 'Viduka')",
          "Group by Player and sum Total Goals column",
          "Sum the resulting totals for both players",
          "Player is Chipperfield OR Player is Viduka"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Player matches 'Chipperfield' or 'Viduka' (case-sensitive)"
          ],
          [
            "Filter rows where Player IN ('Chipperfield', 'Viduka')",
            "Group by Player and sum Total Goals column",
            "Sum the resulting totals for both players"
          ],
          [
            "Player is Chipperfield OR Player is Viduka"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if Total Goals column matches sum of component columns for each player"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if Total Goals column matches sum of component columns for each player"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single numeric result representing total sum",
          "Handle missing values appropriately (treat as 0)",
          "Single numeric value representing the sum of total goals",
          "Must account for all rows matching Chipperfield or Viduka"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single numeric result representing total sum",
            "Handle missing values appropriately (treat as 0)"
          ],
          [
            "Single numeric value representing the sum of total goals",
            "Must account for all rows matching Chipperfield or Viduka"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6166666666666667
  },
  "dbbench-074": {
    "m_q": {
      "target_metric": {
        "value": "total number of offensive awards for Colin Doyle",
        "confidence": 0.3333333333333333,
        "votes": [
          "total number of offensive awards for Colin Doyle",
          "total count of times Colin Doyle appears in the Offensive column",
          "Count of rows where 'Offensive' column equals 'Colin Doyle'"
        ]
      },
      "filters": {
        "value": [
          "Offensive column contains 'Colin Doyle'",
          "Offensive column equals 'Colin Doyle'",
          "Offensive == 'Colin Doyle'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Offensive column contains 'Colin Doyle'"
          ],
          [
            "Offensive column equals 'Colin Doyle'"
          ],
          [
            "Offensive == 'Colin Doyle'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How many times does 'Colin Doyle' appear in the Offensive column?",
          "Is there any other data source needed to verify this count?",
          "Does the data cover the entire period needed for a complete count?",
          "Which rows have 'Colin Doyle' in the Offensive column?",
          "How many such rows exist?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How many times does 'Colin Doyle' appear in the Offensive column?",
            "Is there any other data source needed to verify this count?",
            "Does the data cover the entire period needed for a complete count?"
          ],
          [
            "Which rows have 'Colin Doyle' in the Offensive column?",
            "How many such rows exist?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "lacrosse_stats"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "lacrosse_stats"
          ],
          [
            "lacrosse_stats"
          ],
          [
            "lacrosse_stats"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "offensive": "player name",
          "month": "month name",
          "week": "week number",
          "overall": "player name",
          "defensive": "player name",
          "transition": "player name",
          "rookie": "player name"
        },
        "confidence": 0.4761904761904763,
        "votes": [
          {
            "Offensive": "player name",
            "Month": "month name",
            "Week": "week number"
          },
          {
            "Month": "calendar month name",
            "Week": "week number in season",
            "Overall": "player name",
            "Offensive": "player name",
            "Defensive": "player name",
            "Transition": "player name",
            "Rookie": "player name"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "All columns stored as object/string type including Week which appears numeric",
          "No explicit year column - temporal scope unclear"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns stored as object/string type including Week which appears numeric",
            "No explicit year column - temporal scope unclear"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Offensive column must contain only player names",
          "Each row represents one week's awards",
          "Player names should be consistent across occurrences",
          "Offensive column must be checked for exact match with 'Colin Doyle'",
          "Count must be non-negative integer",
          "The 'Offensive' column must contain string values."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Offensive column must contain only player names",
            "Each row represents one week's awards",
            "Player names should be consistent across occurrences"
          ],
          [
            "Offensive column must be checked for exact match with 'Colin Doyle'",
            "Count must be non-negative integer"
          ],
          [
            "The 'Offensive' column must contain string values."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Offensive = 'Colin Doyle'",
          "Filter rows where Offensive == 'Colin Doyle'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Offensive = 'Colin Doyle'"
          ],
          [
            "Filter rows where Offensive == 'Colin Doyle'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distinct occurrences of 'Colin Doyle' in Offensive column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distinct occurrences of 'Colin Doyle' in Offensive column"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single integer value representing total count",
          "Return a single integer representing the count",
          "Result should be the total number of occurrences"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single integer value representing total count"
          ],
          [
            "Return a single integer representing the count",
            "Result should be the total number of occurrences"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5571428571428573
  },
  "dbbench-075": {
    "m_q": {
      "target_metric": {
        "value": "sum of Playoffs appearances across all teams",
        "confidence": 0.3333333333333333,
        "votes": [
          "sum of Playoffs appearances across all teams",
          "sum of playoffs appearances from all teams",
          "Total number of playoffs appearances across all teams"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Are there any teams with 'Never Played' status that should be excluded?",
          "Should we include teams with 0 seasons played?",
          "Do we need to handle missing or invalid Playoffs appearances values?",
          "What is the 'Playoffs appearances' value for each team?",
          "How should teams with 0 playoff appearances be handled?",
          "Should all teams (active and inactive) be included in the total?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Are there any teams with 'Never Played' status that should be excluded?",
            "Should we include teams with 0 seasons played?",
            "Do we need to handle missing or invalid Playoffs appearances values?"
          ],
          [
            "What is the 'Playoffs appearances' value for each team?",
            "How should teams with 0 playoff appearances be handled?",
            "Should all teams (active and inactive) be included in the total?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Basketball Teams History"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Basketball Teams History"
          ],
          [
            "Basketball Teams History"
          ],
          [
            "Basketball Teams History"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "playoffs appearances": "count",
          "seasons played": "count",
          "win%": "percentage"
        },
        "confidence": 0.7777777777777777,
        "votes": [
          {
            "Playoffs appearances": "count",
            "Seasons played": "count",
            "Win%": "percentage"
          },
          {
            "Playoffs appearances": "count",
            "Seasons played": "count",
            "Win%": "percentage"
          },
          {
            "Playoffs appearances": "number of appearances"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Win\u2013loss record contains commas in some values (e.g., '3,445\u20132,318')",
          "Win% column contains string values like '.578' and 'N/A'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Win\u2013loss record contains commas in some values (e.g., '3,445\u20132,318')",
            "Win% column contains string values like '.578' and 'N/A'"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "N/A",
            "NA",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Playoffs appearances should be non-negative integers",
          "Seasons played should be >= 0",
          "Playoffs appearances should be <= Seasons played for active teams",
          "Playoffs appearances must be non-negative integers",
          "Playoffs appearances cannot exceed Seasons played for any team",
          "Sum should include all 27 teams in the dataset",
          "'Playoffs appearances' column must contain non-negative integers."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Playoffs appearances should be non-negative integers",
            "Seasons played should be >= 0",
            "Playoffs appearances should be <= Seasons played for active teams"
          ],
          [
            "Playoffs appearances must be non-negative integers",
            "Playoffs appearances cannot exceed Seasons played for any team",
            "Sum should include all 27 teams in the dataset"
          ],
          [
            "'Playoffs appearances' column must contain non-negative integers."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where 'Years active' = 'Never Played'",
          "Consider filtering out teams with 0 seasons played"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where 'Years active' = 'Never Played'",
            "Consider filtering out teams with 0 seasons played"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for consistency between Seasons played and Years active ranges",
          "Validate that Playoffs appearances sum is reasonable given total seasons"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for consistency between Seasons played and Years active ranges",
            "Validate that Playoffs appearances sum is reasonable given total seasons"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Integer total",
          "Consider providing context about number of teams included",
          "Output should be a single integer value representing the total count",
          "Result should be the sum of all values in the 'Playoffs appearances' column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Integer total",
            "Consider providing context about number of teams included"
          ],
          [
            "Output should be a single integer value representing the total count",
            "Result should be the sum of all values in the 'Playoffs appearances' column"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5555555555555557
  },
  "dbbench-076": {
    "m_q": {
      "target_metric": {
        "value": "minimum year of birth among all players",
        "confidence": 0.3333333333333333,
        "votes": [
          "minimum year of birth among all players",
          "Minimum value of 'Year born' across all players",
          "minimum year born"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the minimum value in the 'Year born' column?",
          "Which player has the earliest birth year?",
          "Are there any missing or invalid birth years that need to be handled?",
          "What is the data type of the 'Year born' column?",
          "Are there any missing or invalid values in 'Year born'?",
          "What is the range of birth years in the dataset?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the minimum value in the 'Year born' column?",
            "Which player has the earliest birth year?",
            "Are there any missing or invalid birth years that need to be handled?"
          ],
          [
            "What is the data type of the 'Year born' column?",
            "Are there any missing or invalid values in 'Year born'?",
            "What is the range of birth years in the dataset?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Basketball Players"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Basketball Players"
          ],
          [
            "Basketball Players"
          ],
          [
            "Basketball Players"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "height": "meters",
          "year born": "year",
          "no": "jersey number (unitless)"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "Height": "meters",
            "Year born": "year"
          },
          {
            "Year born": "year (YYYY format)",
            "Height": "meters",
            "No": "jersey number (unitless)"
          },
          {
            "Height": "meters",
            "Year born": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Year born is stored as object/string instead of integer",
          "Height values use decimal notation with '.' as decimal separator",
          "'Year born' is stored as object dtype but should be numeric for minimum calculation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year born is stored as object/string instead of integer",
            "Height values use decimal notation with '.' as decimal separator"
          ],
          [
            "'Year born' is stored as object dtype but should be numeric for minimum calculation"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year born values should be valid integers representing years",
          "Year born should be within reasonable range (e.g., 1900-2024)",
          "Each player should have exactly one birth year",
          "'Year born' values should be four-digit years",
          "'Year born' should be between reasonable bounds (e.g., 1900-2010 for active players)",
          "No missing values expected in 'Year born' column",
          "'Year born' column should contain valid year values.",
          "'Year born' column should be of numeric type."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year born values should be valid integers representing years",
            "Year born should be within reasonable range (e.g., 1900-2024)",
            "Each player should have exactly one birth year"
          ],
          [
            "'Year born' values should be four-digit years",
            "'Year born' should be between reasonable bounds (e.g., 1900-2010 for active players)",
            "No missing values expected in 'Year born' column"
          ],
          [
            "'Year born' column should contain valid year values.",
            "'Year born' column should be of numeric type."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where 'Year born' is null or non-numeric",
          "Consider only valid year values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where 'Year born' is null or non-numeric",
            "Consider only valid year values"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in 'Year born' column",
          "Verify distribution of birth years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in 'Year born' column",
            "Verify distribution of birth years"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output should be a single integer year",
          "Consider whether to output just the year or include player information",
          "Return earliest birth year as a single integer value",
          "Result should be in YYYY format"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Output should be a single integer year",
            "Consider whether to output just the year or include player information"
          ],
          [
            "Return earliest birth year as a single integer value",
            "Result should be in YYYY format"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5555555555555557
  },
  "dbbench-077": {
    "m_q": {
      "target_metric": {
        "value": "best starting position (lowest Start value) for each car across all years",
        "confidence": 0.3333333333333333,
        "votes": [
          "best starting position (lowest Start value) for each car across all years",
          "minimum value of the 'Start' column (best/lowest starting position)",
          "The best starting position, defined as the starting position that resulted in the best (lowest) finishing position."
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Car",
          "Start"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Car"
          ],
          [],
          [
            "Start"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which car number had the best starting position?",
          "What was the actual best starting position value?",
          "In which year did each car achieve its best starting position?",
          "What is the minimum value in the 'Start' column?",
          "What does 'its' refer to - a specific car, driver, or all entries in the dataset?",
          "What is the finishing position for each race?",
          "What was the starting position for each race?",
          "For each starting position, what was the best finishing position achieved?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which car number had the best starting position?",
            "What was the actual best starting position value?",
            "In which year did each car achieve its best starting position?"
          ],
          [
            "What is the minimum value in the 'Start' column?",
            "What does 'its' refer to - a specific car, driver, or all entries in the dataset?"
          ],
          [
            "What is the finishing position for each race?",
            "What was the starting position for each race?",
            "For each starting position, what was the best finishing position achieved?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "car_race_results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "car_race_results"
          ],
          [
            "car_race_results"
          ],
          [
            "car_race_results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "qual": "mph (qualifying speed)",
          "laps": "count",
          "led": "laps led",
          "start": "grid position (integer, lower is better)",
          "rank": "qualifying rank",
          "finish": "finishing position"
        },
        "confidence": 0.5000000000000001,
        "votes": [
          {
            "Qual": "mph (qualifying speed)",
            "Laps": "count",
            "Led": "laps led"
          },
          {
            "Start": "grid position (integer, lower is better)",
            "Qual": "qualifying speed (likely mph or km/h)",
            "Rank": "qualifying rank",
            "Finish": "finishing position",
            "Laps": "number of laps completed",
            "Led": "number of laps led"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Year column is object type but contains year values - should be integer",
          "Retired column contains categorical status values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column is object type but contains year values - should be integer",
            "Retired column contains categorical status values"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Start values should be positive integers (grid positions)",
          "Start values should be less than or equal to total cars in race",
          "Best starting position means minimum Start value (position 1 is best)",
          "Start position should be positive integer",
          "Lower Start values indicate better starting positions",
          "'Start' and 'Finish' columns should contain integer values representing positions in the race."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Start values should be positive integers (grid positions)",
            "Start values should be less than or equal to total cars in race",
            "Best starting position means minimum Start value (position 1 is best)"
          ],
          [
            "Start position should be positive integer",
            "Lower Start values indicate better starting positions"
          ],
          [
            "'Start' and 'Finish' columns should contain integer values representing positions in the race."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to find minimum Start value per Car",
          "Consider only races where car actually started (Start not null)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to find minimum Start value per Car",
            "Consider only races where car actually started (Start not null)"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Start positions",
          "Verify Start values are within reasonable range (1-33 typical for Indy 500)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Start positions",
            "Verify Start values are within reasonable range (1-33 typical for Indy 500)"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Should show Car number and its best Start position",
          "Include Year when best Start was achieved",
          "Sort by best Start position ascending",
          "Return the minimum Start value as a single integer",
          "Optionally include the Year and other context for the race with the best starting position",
          "The output should be a single integer representing the best starting position."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Should show Car number and its best Start position",
            "Include Year when best Start was achieved",
            "Sort by best Start position ascending"
          ],
          [
            "Return the minimum Start value as a single integer",
            "Optionally include the Year and other context for the race with the best starting position"
          ],
          [
            "The output should be a single integer representing the best starting position."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5416666666666667
  },
  "dbbench-078": {
    "m_q": {
      "target_metric": {
        "value": "lowest game number where the score is 108-85",
        "confidence": 0.3333333333333333,
        "votes": [
          "lowest game number where the score is 108-85",
          "Game number with the lowest value where the Score is 108-85",
          "The lowest game number where the score was 108-85"
        ]
      },
      "filters": {
        "value": [
          "Score = '108-85'",
          "Score equals '108-85'",
          "Score is exactly '108-85'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Score = '108-85'"
          ],
          [
            "Score equals '108-85'"
          ],
          [
            "Score is exactly '108-85'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the Game number when Score equals '108-85'?",
          "Is there only one game with score 108-85?",
          "Which game has the lowest Game number among those with score 108-85?",
          "Which rows have Score = '108-85'?",
          "What is the Game value for the row with Score = '108-85'?",
          "If multiple games have Score = '108-85', what is the minimum Game number?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the Game number when Score equals '108-85'?",
            "Is there only one game with score 108-85?",
            "Which game has the lowest Game number among those with score 108-85?"
          ],
          [
            "Which rows have Score = '108-85'?",
            "What is the Game value for the row with Score = '108-85'?",
            "If multiple games have Score = '108-85', what is the minimum Game number?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "game score"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "game score"
          ],
          [
            "game score"
          ],
          [
            "game score"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "game": "game number",
          "score": "team_score-opponent_score",
          "date": "date_string",
          "record": "wins-losses"
        },
        "confidence": 0.49999999999999994,
        "votes": [
          {
            "Game": "game number",
            "Score": "team_score-opponent_score"
          },
          {
            "Game": "game_number",
            "Date": "date_string",
            "Score": "points_home-points_away",
            "Record": "wins-losses"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Score column contains combined score values (e.g., '108-85') that need parsing",
          "Date values have inconsistent formatting with day abbreviations and month abbreviations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Score column contains combined score values (e.g., '108-85') that need parsing",
            "Date values have inconsistent formatting with day abbreviations and month abbreviations"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Game numbers should be unique",
          "Score should follow pattern 'XXX-XXX' or 'XXX-XXX (OT)'",
          "Record should follow pattern 'XX-XX'",
          "Game column should be convertible to integer for comparison",
          "Score must match exactly '108-85' format",
          "At least one record should match the filter condition",
          "The 'Game' column should be numeric or convertible to numeric for comparison."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Game numbers should be unique",
            "Score should follow pattern 'XXX-XXX' or 'XXX-XXX (OT)'",
            "Record should follow pattern 'XX-XX'"
          ],
          [
            "Game column should be convertible to integer for comparison",
            "Score must match exactly '108-85' format",
            "At least one record should match the filter condition"
          ],
          [
            "The 'Game' column should be numeric or convertible to numeric for comparison."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Score column exactly matches '108-85'",
          "Extract numeric Game value from filtered rows",
          "Filter rows where Score = '108-85'",
          "Extract Game values from filtered rows",
          "Find minimum Game number"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Score column exactly matches '108-85'",
            "Extract numeric Game value from filtered rows"
          ],
          [
            "Filter rows where Score = '108-85'",
            "Extract Game values from filtered rows",
            "Find minimum Game number"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if only one row has Score = '108-85'",
          "Verify Game numbers are sequential"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if only one row has Score = '108-85'",
            "Verify Game numbers are sequential"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single integer value representing the Game number",
          "If multiple games have score 108-85, return the minimum Game number",
          "Return the Game number as an integer or numeric value",
          "If no match found, indicate no result",
          "If multiple matches, return the lowest Game number"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single integer value representing the Game number",
            "If multiple games have score 108-85, return the minimum Game number"
          ],
          [
            "Return the Game number as an integer or numeric value",
            "If no match found, indicate no result",
            "If multiple matches, return the lowest Game number"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5416666666666667
  },
  "dbbench-079": {
    "m_q": {
      "target_metric": {
        "value": "The name of the building that was first designated as the tallest",
        "confidence": 0.3333333333333333,
        "votes": [
          "The name of the building that was first designated as the tallest",
          "The name of the first building that held the title of tallest",
          "Name of the building that was the tallest first"
        ]
      },
      "filters": {
        "value": [
          "Years as tallest must contain a valid year range",
          "Height ft (m) must contain valid height data",
          "Buildings that were tallest at some point in time"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Years as tallest must contain a valid year range",
            "Height ft (m) must contain valid height data"
          ],
          [
            "Buildings that were tallest at some point in time"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Years as tallest"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Years as tallest"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the earliest start year in Years as tallest column?",
          "Which building corresponds to that earliest start year?",
          "How should we handle buildings with the same start year?",
          "Which buildings have valid 'Years as tallest' values?",
          "What is the earliest year in the 'Years as tallest' column?",
          "Which building corresponds to the earliest year as tallest?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the earliest start year in Years as tallest column?",
            "Which building corresponds to that earliest start year?",
            "How should we handle buildings with the same start year?"
          ],
          [
            "Which buildings have valid 'Years as tallest' values?",
            "What is the earliest year in the 'Years as tallest' column?",
            "Which building corresponds to the earliest year as tallest?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Tall Buildings in Minneapolis"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Tall Buildings in Minneapolis"
          ],
          [
            "Tall Buildings in Minneapolis"
          ],
          [
            "Tall Buildings in Minneapolis"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "height ft (m)": "feet (meters)",
          "floors": "count",
          "years as tallest": "year range"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Height ft (m)": "feet (meters)",
            "Floors": "count"
          },
          {
            "Height ft (m)": "feet and meters",
            "Floors": "count",
            "Years as tallest": "year range"
          },
          {
            "Height ft (m)": "feet (meters)"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Height ft (m) column contains both feet and meters in same string",
          "Years as tallest contains ranges with en dash separator",
          "Height ft (m) contains both feet and meters in a single string",
          "Years as tallest contains year ranges (e.g., '1882\u20131886') and 'present' keyword"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Height ft (m) column contains both feet and meters in same string",
            "Years as tallest contains ranges with en dash separator"
          ],
          [
            "Height ft (m) contains both feet and meters in a single string",
            "Years as tallest contains year ranges (e.g., '1882\u20131886') and 'present' keyword"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "-",
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.7499999999999999,
        "votes": [
          [
            "-",
            ""
          ],
          [
            "NA",
            "N/A",
            "",
            "-"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Years as tallest must be parsed to extract start year",
          "Rows with '-' in Years as tallest should be excluded",
          "First building is determined by minimum start year",
          "Years as tallest must be parseable to extract start year",
          "Rows with missing or dash values in Years as tallest should be excluded",
          "The first building is determined by the earliest start year in Years as tallest",
          "The 'Years as tallest' column needs to be parsed to determine the start year.",
          "Need to find the minimum start year from the 'Years as tallest' column."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Years as tallest must be parsed to extract start year",
            "Rows with '-' in Years as tallest should be excluded",
            "First building is determined by minimum start year"
          ],
          [
            "Years as tallest must be parseable to extract start year",
            "Rows with missing or dash values in Years as tallest should be excluded",
            "The first building is determined by the earliest start year in Years as tallest"
          ],
          [
            "The 'Years as tallest' column needs to be parsed to determine the start year.",
            "Need to find the minimum start year from the 'Years as tallest' column."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows where Years as tallest = '-'",
          "Exclude rows where Height ft (m) = '-'",
          "Filter out rows where Years as tallest is '-' or empty",
          "Extract start year from Years as tallest range"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows where Years as tallest = '-'",
            "Exclude rows where Height ft (m) = '-'"
          ],
          [
            "Filter out rows where Years as tallest is '-' or empty",
            "Extract start year from Years as tallest range"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate start years",
          "Validate year range format"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate start years",
            "Validate year range format"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single building name",
          "Include supporting year information",
          "Return the Name value of the building with the earliest start year",
          "Output should be a single building name as a string"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single building name",
            "Include supporting year information"
          ],
          [
            "Return the Name value of the building with the earliest start year",
            "Output should be a single building name as a string"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5708333333333334
  },
  "dbbench-080": {
    "m_q": {
      "target_metric": {
        "value": "smallest crowd number",
        "confidence": 0.6666666666666666,
        "votes": [
          "minimum crowd number",
          "smallest crowd number",
          "smallest Crowd number"
        ]
      },
      "filters": {
        "value": [
          "Away team score = '15.11 (101)'",
          "Away team score = 15.11 (101)",
          "Away team score is 15.11 (101)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Away team score = '15.11 (101)'"
          ],
          [
            "Away team score = 15.11 (101)"
          ],
          [
            "Away team score is 15.11 (101)"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Away team"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Away team"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which teams have an Away team score of 15.11 (101)?",
          "What are the crowd numbers for those matches?",
          "What is the smallest crowd number among those matches?",
          "Filter rows where Away team score equals 15.11 (101)",
          "Extract Crowd values for filtered rows",
          "Convert Crowd from string with commas to numeric",
          "Find minimum Crowd value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which teams have an Away team score of 15.11 (101)?",
            "What are the crowd numbers for those matches?",
            "What is the smallest crowd number among those matches?"
          ],
          [
            "Filter rows where Away team score equals 15.11 (101)",
            "Extract Crowd values for filtered rows",
            "Convert Crowd from string with commas to numeric",
            "Find minimum Crowd value"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "football_matches"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "football_matches"
          ],
          [
            "football_matches"
          ],
          [
            "football_matches"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "crowd": "people (formatted with commas)",
          "home team score": "goals.behinds (total points)",
          "away team score": "goals.behinds (total points)",
          "date": "date"
        },
        "confidence": 0.5833333333333334,
        "votes": [
          {
            "Crowd": "people (formatted with commas)",
            "Home team score": "goals.behinds (total points)",
            "Away team score": "goals.behinds (total points)"
          },
          {
            "Home team score": "goals.behinds (total_points)",
            "Away team score": "goals.behinds (total_points)",
            "Crowd": "number of people",
            "Date": "date"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Crowd column is stored as object/string with comma formatting (e.g., '4,000') - needs conversion to numeric for comparison",
          "Crowd values contain comma separators (e.g., '16,000')",
          "Crowd dtype is object/string, needs conversion to numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Crowd column is stored as object/string with comma formatting (e.g., '4,000') - needs conversion to numeric for comparison"
          ],
          [
            "Crowd values contain comma separators (e.g., '16,000')",
            "Crowd dtype is object/string, needs conversion to numeric"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Away team score must exactly match '15.11 (101)' including formatting",
          "Crowd values must be converted from string to integer for numerical comparison",
          "Only consider matches where Away team score is exactly '15.11 (101)'",
          "Away team score must exactly match '15.11 (101)'",
          "Crowd values must be convertible to numeric after removing commas",
          "Result should be a single numeric value representing minimum crowd",
          "Crowd must be a number",
          "Away team score must be parsed to extract the numerical score"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Away team score must exactly match '15.11 (101)' including formatting",
            "Crowd values must be converted from string to integer for numerical comparison",
            "Only consider matches where Away team score is exactly '15.11 (101)'"
          ],
          [
            "Away team score must exactly match '15.11 (101)'",
            "Crowd values must be convertible to numeric after removing commas",
            "Result should be a single numeric value representing minimum crowd"
          ],
          [
            "Crowd must be a number",
            "Away team score must be parsed to extract the numerical score"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert Crowd from string to integer by removing commas",
          "Filter rows where Away team score = '15.11 (101)'",
          "Filter: Away team score == '15.11 (101)'",
          "Away team score == 101"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert Crowd from string to integer by removing commas",
            "Filter rows where Away team score = '15.11 (101)'"
          ],
          [
            "Filter: Away team score == '15.11 (101)'"
          ],
          [
            "Away team score == 101"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate matches with same Away team score",
          "Verify crowd number distribution for filtered matches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate matches with same Away team score",
            "Verify crowd number distribution for filtered matches"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single integer value representing smallest crowd",
          "If multiple teams have same minimum crowd, return that value",
          "Return single numeric value",
          "Value should represent the minimum crowd attendance"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single integer value representing smallest crowd",
            "If multiple teams have same minimum crowd, return that value"
          ],
          [
            "Return single numeric value",
            "Value should represent the minimum crowd attendance"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5958333333333334
  },
  "dbbench-081": {
    "m_q": {
      "target_metric": {
        "value": "minimum release year",
        "confidence": 0.3333333333333333,
        "votes": [
          "minimum release year",
          "earliest year from Release column",
          "the earliest year a title was released"
        ]
      },
      "filters": {
        "value": [
          "non-null Release values",
          "valid year format"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "non-null Release values",
            "valid year format"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the earliest year in the Release column?",
          "Are there any missing or invalid values in the Release column that need to be handled?",
          "Does the data contain releases from multiple artists or only specific ones?",
          "What is the minimum value in the Release column?",
          "Are all Release values valid years?",
          "What title(s) correspond to the earliest year?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the earliest year in the Release column?",
            "Are there any missing or invalid values in the Release column that need to be handled?",
            "Does the data contain releases from multiple artists or only specific ones?"
          ],
          [
            "What is the minimum value in the Release column?",
            "Are all Release values valid years?",
            "What title(s) correspond to the earliest year?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Music Releases"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Music Releases"
          ],
          [
            "Music Releases"
          ],
          [
            "Music Releases"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "release": "year"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Release": "year"
          },
          {
            "Release": "year (YYYY format)"
          },
          {
            "Release": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Release column appears to contain years as strings/objects rather than numeric values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Release column appears to contain years as strings/objects rather than numeric values"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Release column should contain valid year values",
          "Each row represents a music release with associated metadata",
          "Release values should be valid years",
          "Release values should be numeric or convertible to numeric",
          "Result should be a single year value",
          "The 'Release' column should contain valid year values."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Release column should contain valid year values",
            "Each row represents a music release with associated metadata"
          ],
          [
            "Release values should be valid years",
            "Release values should be numeric or convertible to numeric",
            "Result should be a single year value"
          ],
          [
            "The 'Release' column should contain valid year values."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows with missing Release values",
          "Convert Release column to numeric type for comparison",
          "Exclude rows where Release is null or empty",
          "Exclude rows where Release cannot be converted to a valid year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows with missing Release values",
            "Convert Release column to numeric type for comparison"
          ],
          [
            "Exclude rows where Release is null or empty",
            "Exclude rows where Release cannot be converted to a valid year"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for minimum value in Release column after conversion to numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for minimum value in Release column after conversion to numeric"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output should be a single year value",
          "Year should be formatted as integer",
          "Return the minimum year as a single integer or year value",
          "Optionally include the title associated with that year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Output should be a single year value",
            "Year should be formatted as integer"
          ],
          [
            "Return the minimum year as a single integer or year value",
            "Optionally include the title associated with that year"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5833333333333334
  },
  "dbbench-082": {
    "m_q": {
      "target_metric": {
        "value": "minimum value of 'Commenced operations' column",
        "confidence": 0.3333333333333333,
        "votes": [
          "minimum value of 'Commenced operations' column",
          "The year when the oldest airline started operations",
          "Earliest year in which an airline commenced operations"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the earliest year in the 'Commenced operations' column?",
          "Which airline(s) have this earliest commencement year?",
          "What is the format of the 'Commenced operations' column?",
          "Which airline has the earliest 'Commenced operations' value?",
          "What is the minimum year in 'Commenced operations' column?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the earliest year in the 'Commenced operations' column?",
            "Which airline(s) have this earliest commencement year?"
          ],
          [
            "What is the format of the 'Commenced operations' column?",
            "Which airline has the earliest 'Commenced operations' value?",
            "What is the minimum year in 'Commenced operations' column?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "airlines"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "airlines"
          ],
          [
            "airlines"
          ],
          [
            "airlines"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "commenced operations": "year",
          "fleet size": "number of aircraft"
        },
        "confidence": 0.5,
        "votes": [
          {
            "Commenced operations": "year"
          },
          {
            "Commenced operations": "year",
            "Fleet size": "number of aircraft"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "'Commenced operations' column is stored as object/string type but contains numeric year values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Commenced operations' column is stored as object/string type but contains numeric year values"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "'Commenced operations' values should be valid years (4-digit numbers)",
          "Years should be positive integers",
          "Current year should be upper bound for valid years",
          "Commenced operations must be a valid year",
          "Commenced operations should be in the past (not future)",
          "Commenced operations should be reasonable for airline industry (post-1900s)",
          "'Commenced operations' must be a valid year or convertible to a year."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Commenced operations' values should be valid years (4-digit numbers)",
            "Years should be positive integers",
            "Current year should be upper bound for valid years"
          ],
          [
            "Commenced operations must be a valid year",
            "Commenced operations should be in the past (not future)",
            "Commenced operations should be reasonable for airline industry (post-1900s)"
          ],
          [
            "'Commenced operations' must be a valid year or convertible to a year."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows where 'Commenced operations' is not a valid year",
          "Filter out any rows with missing or invalid 'Commenced operations' values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows where 'Commenced operations' is not a valid year"
          ],
          [
            "Filter out any rows with missing or invalid 'Commenced operations' values"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in 'Commenced operations' column",
          "Validate year format consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in 'Commenced operations' column",
            "Validate year format consistency"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return year as integer",
          "Consider returning airline name(s) with the oldest commencement year",
          "Return the year as a single numeric value or date",
          "Optionally include the airline name that started in that year",
          "Output must be a year (integer)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return year as integer",
            "Consider returning airline name(s) with the oldest commencement year"
          ],
          [
            "Return the year as a single numeric value or date",
            "Optionally include the airline name that started in that year"
          ],
          [
            "Output must be a year (integer)."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5416666666666667
  },
  "dbbench-083": {
    "m_q": {
      "target_metric": {
        "value": "minimum value of 'Games played' column",
        "confidence": 0.3333333333333333,
        "votes": [
          "minimum value of 'Games played' column",
          "The minimum value in the 'Games played' column",
          "minimum value of 'Games played'"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Are there any clubs with fewer games played than others?",
          "Is the 'Games played' value consistent across all rows?",
          "Do we need to handle any non-numeric values in 'Games played' column?",
          "What are all the values in the 'Games played' column?",
          "What is the minimum of those values?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Are there any clubs with fewer games played than others?",
            "Is the 'Games played' value consistent across all rows?",
            "Do we need to handle any non-numeric values in 'Games played' column?"
          ],
          [
            "What are all the values in the 'Games played' column?",
            "What is the minimum of those values?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "football_standings"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "football_standings"
          ],
          [
            "football_standings"
          ],
          [
            "football_standings"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column 'Games played' has object dtype but contains numeric data",
          "All columns have object dtype despite containing numeric values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column 'Games played' has object dtype but contains numeric data",
            "All columns have object dtype despite containing numeric values"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "games played": "count of matches",
          "goals scored": "count of goals",
          "goals conceded": "count of goals",
          "points": "league points",
          "wins": "count",
          "draws": "count",
          "loses": "count"
        },
        "confidence": 0.5238095238095238,
        "votes": [
          {
            "Games played": "count of matches",
            "Goals scored": "count of goals",
            "Goals conceded": "count of goals",
            "Points": "league points"
          },
          {
            "Games played": "count",
            "Wins": "count",
            "Draws": "count",
            "Loses": "count",
            "Goals scored": "count",
            "Goals conceded": "count",
            "Points": "count"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Data appears to contain multiple leagues/competitions mixed together (rows 1-17 show Lithuanian teams, rows 18-20 show Spanish teams)",
          "Different leagues may have different numbers of games in a season"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data appears to contain multiple leagues/competitions mixed together (rows 1-17 show Lithuanian teams, rows 18-20 show Spanish teams)",
            "Different leagues may have different numbers of games in a season"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "'Games played' should be positive integers",
          "'Games played' should be consistent within the same competition",
          "Position numbers should be unique within a competition",
          "Games played should be a positive integer",
          "Games played = Wins + Draws + Loses",
          "All numeric columns should be non-negative integers",
          "'Games played' column must be converted to numeric type before finding the minimum"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Games played' should be positive integers",
            "'Games played' should be consistent within the same competition",
            "Position numbers should be unique within a competition"
          ],
          [
            "Games played should be a positive integer",
            "Games played = Wins + Draws + Loses",
            "All numeric columns should be non-negative integers"
          ],
          [
            "'Games played' column must be converted to numeric type before finding the minimum"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to specific competition if analyzing within one league",
          "Remove rows with non-numeric 'Games played' values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to specific competition if analyzing within one league",
            "Remove rows with non-numeric 'Games played' values"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for consistency of 'Games played' values within data subsets",
          "Identify outliers in 'Games played' column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for consistency of 'Games played' values within data subsets",
            "Identify outliers in 'Games played' column"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single numeric value",
          "Handle potential ties if multiple clubs have same minimum games played",
          "Return a single integer value representing the minimum number of games played"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single numeric value",
            "Handle potential ties if multiple clubs have same minimum games played"
          ],
          [
            "Return a single integer value representing the minimum number of games played"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5595238095238096
  },
  "dbbench-084": {
    "m_q": {
      "target_metric": {
        "value": "maximum value of the 'Number One(s)' column",
        "confidence": 0.6666666666666666,
        "votes": [
          "maximum value of the 'Number One(s)' column",
          "Maximum value of the 'Number One(s)' column",
          "maximum value of 'Number One(s)'"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the maximum numeric value in the 'Number One(s)' column?",
          "Are there multiple rows with this maximum value?",
          "Which artists/songs have this maximum number of number ones?",
          "What is the maximum value in the 'Number One(s)' column?",
          "Which artist(s) have this highest number of overall number ones?",
          "What is the data type of 'Number One(s)' - is it numeric or needs conversion?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the maximum numeric value in the 'Number One(s)' column?",
            "Are there multiple rows with this maximum value?",
            "Which artists/songs have this maximum number of number ones?"
          ],
          [
            "What is the maximum value in the 'Number One(s)' column?",
            "Which artist(s) have this highest number of overall number ones?",
            "What is the data type of 'Number One(s)' - is it numeric or needs conversion?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Music Chart History"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Music Chart History"
          ],
          [
            "Music Chart History"
          ],
          [
            "Music Chart History"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "number one(s)": "count of number one hits",
          "whole weeks": "total weeks on chart",
          "song(s) \u2014 weeks": "weeks per song"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "Number One(s)": "count of number one hits",
            "Whole Weeks": "total weeks on chart"
          },
          {
            "Number One(s)": "count of number one songs",
            "Whole Weeks": "count of weeks",
            "Song(s) \u2014 Weeks": "weeks per song"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "'Number One(s)' column appears to contain numeric values but is stored as object dtype",
          "'Whole Weeks' column appears to contain numeric values but is stored as object dtype"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Number One(s)' column appears to contain numeric values but is stored as object dtype",
            "'Whole Weeks' column appears to contain numeric values but is stored as object dtype"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "'Number One(s)' values should be positive integers",
          "'Whole Weeks' values should be positive integers",
          "Each row represents a specific song by an artist",
          "'Number One(s)' column must be converted to numeric type for comparison",
          "Each row represents a single song achievement, not aggregated artist totals",
          "'Number One(s)' must be converted to numeric type before finding the maximum"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Number One(s)' values should be positive integers",
            "'Whole Weeks' values should be positive integers",
            "Each row represents a specific song by an artist"
          ],
          [
            "'Number One(s)' column must be converted to numeric type for comparison",
            "Each row represents a single song achievement, not aggregated artist totals"
          ],
          [
            "'Number One(s)' must be converted to numeric type before finding the maximum"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert 'Number One(s)' column to numeric type",
          "Check for non-numeric values in 'Number One(s)' column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert 'Number One(s)' column to numeric type",
            "Check for non-numeric values in 'Number One(s)' column"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of 'Number One(s)' values",
          "Identify maximum value and frequency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of 'Number One(s)' values",
            "Identify maximum value and frequency"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return the maximum numeric value",
          "Consider returning associated artist/song information if needed",
          "Return the maximum numeric value from 'Number One(s)' column",
          "Optionally identify which artist(s) have this maximum value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return the maximum numeric value",
            "Consider returning associated artist/song information if needed"
          ],
          [
            "Return the maximum numeric value from 'Number One(s)' column",
            "Optionally identify which artist(s) have this maximum value"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5611111111111112
  },
  "dbbench-085": {
    "m_q": {
      "target_metric": {
        "value": "Year of the most recent pylon construction in Germany",
        "confidence": 0.3333333333333333,
        "votes": [
          "Year of the most recent pylon construction in Germany",
          "The most recent year in which a pylon was built in Germany",
          "the latest year a pylon was built in Germany"
        ]
      },
      "filters": {
        "value": [
          "Country = 'Germany'",
          "Year built is not null",
          "Country == 'Germany'",
          "Country is Germany"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Country = 'Germany'",
            "Year built is not null"
          ],
          [
            "Country == 'Germany'"
          ],
          [
            "Country is Germany"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the maximum Year built value for German entries?",
          "Are there any incomplete year values that need cleaning?",
          "Do we need to handle year ranges or ambiguous dates like 'Early 2012'?",
          "Which rows have 'Germany' in the Country column?",
          "What are the valid years in the 'Year built' column for German pylons?",
          "What is the maximum/latest year among German pylon construction years?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the maximum Year built value for German entries?",
            "Are there any incomplete year values that need cleaning?",
            "Do we need to handle year ranges or ambiguous dates like 'Early 2012'?"
          ],
          [
            "Which rows have 'Germany' in the Country column?",
            "What are the valid years in the 'Year built' column for German pylons?",
            "What is the maximum/latest year among German pylon construction years?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Aerial Tramway Table"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Aerial Tramway Table"
          ],
          [
            "Aerial Tramway Table"
          ],
          [
            "Aerial Tramway Table"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "height": "meters",
          "year built": "year"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Height": "meters"
          },
          {
            "Year built": "year",
            "Height": "meters (m)"
          },
          {
            "Height": "meters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Year built column contains mixed formats (numeric years, 'Early 2012', '194?')",
          "Height column contains units in string format (e.g., '113.6 m')",
          "Year built column has dtype 'object' instead of numeric, may contain non-numeric values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year built column contains mixed formats (numeric years, 'Early 2012', '194?')",
            "Height column contains units in string format (e.g., '113.6 m')"
          ],
          [
            "Year built column has dtype 'object' instead of numeric, may contain non-numeric values"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year built must be interpretable as a year",
          "Only German entries should be considered",
          "The most recent year among German entries is needed",
          "Country must equal 'Germany'",
          "Year built must be parseable as a valid year",
          "Year built should not be empty or null for valid records",
          "Year built must be a valid year.",
          "Year built may contain ranges or approximate values, which need to be handled."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year built must be interpretable as a year",
            "Only German entries should be considered",
            "The most recent year among German entries is needed"
          ],
          [
            "Country must equal 'Germany'",
            "Year built must be parseable as a valid year",
            "Year built should not be empty or null for valid records"
          ],
          [
            "Year built must be a valid year.",
            "Year built may contain ranges or approximate values, which need to be handled."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Extract numeric year from 'Year built' column",
          "Filter to Country = 'Germany'",
          "Remove rows with ambiguous/unparseable year values",
          "Filter out rows where Year built is empty or cannot be converted to a year",
          "Extract numeric year from 'Year built' column handling various formats",
          "Year built is the maximum year for Germany"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Extract numeric year from 'Year built' column",
            "Filter to Country = 'Germany'",
            "Remove rows with ambiguous/unparseable year values"
          ],
          [
            "Filter out rows where Year built is empty or cannot be converted to a year",
            "Extract numeric year from 'Year built' column handling various formats"
          ],
          [
            "Year built is the maximum year for Germany"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Year built values",
          "Validate that year values are within reasonable range (e.g., 1800-2024)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Year built values",
            "Validate that year values are within reasonable range (e.g., 1800-2024)"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single year value as integer",
          "Should handle ambiguous dates by taking the year part only",
          "Return a single year value (integer or string format)",
          "Result should be the maximum year among all German pylon construction years",
          "Output must be a single year."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single year value as integer",
            "Should handle ambiguous dates by taking the year part only"
          ],
          [
            "Return a single year value (integer or string format)",
            "Result should be the maximum year among all German pylon construction years"
          ],
          [
            "Output must be a single year."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5666666666666668
  },
  "dbbench-086": {
    "m_q": {
      "target_metric": {
        "value": "Height value for rank 95",
        "confidence": 0.3333333333333333,
        "votes": [
          "Height value for rank 95",
          "Height value for the highest point",
          "maximum 'Highest point' value for rows where 'Rank' is 95"
        ]
      },
      "filters": {
        "value": [
          "Rank == '95'",
          "Rank equals 95",
          "Rank = 95"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rank == '95'"
          ],
          [
            "Rank equals 95"
          ],
          [
            "Rank = 95"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the Height value in the row where Rank equals '95'?",
          "Is the Height column properly formatted for numerical extraction?",
          "Does rank 95 exist in the dataset?",
          "Filter the data to find the row where Rank = 95",
          "Extract the Height value from that row",
          "Parse the Height value to extract the numeric amount of points"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the Height value in the row where Rank equals '95'?",
            "Is the Height column properly formatted for numerical extraction?",
            "Does rank 95 exist in the dataset?"
          ],
          [
            "Filter the data to find the row where Rank = 95",
            "Extract the Height value from that row",
            "Parse the Height value to extract the numeric amount of points"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "highest_points"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "highest_points"
          ],
          [
            "highest_points"
          ],
          [
            "highest_points"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "height": "meters and feet (m (ft))"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Height": "meters and feet (m (ft))"
          },
          {
            "Height": "meters (m) and feet (ft)"
          },
          {
            "Height": "meters (feet)"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Height column contains mixed text and numbers with formatting like '4,421 m (14,505 ft)'",
          "Commas used as thousands separators in Height values",
          "Need to extract numerical value from Height column for rank 95",
          "Height column contains both metric and imperial units in format 'm (ft)'",
          "Some Height values show only 'm (ft)' without numeric values",
          "Height values need parsing to extract numeric component"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Height column contains mixed text and numbers with formatting like '4,421 m (14,505 ft)'",
            "Commas used as thousands separators in Height values",
            "Need to extract numerical value from Height column for rank 95"
          ],
          [
            "Height column contains both metric and imperial units in format 'm (ft)'",
            "Some Height values show only 'm (ft)' without numeric values",
            "Height values need parsing to extract numeric component"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "m (ft)"
        ],
        "confidence": 0.5833333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "m (ft)"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Rank values should be unique",
          "Height should contain valid measurement data for each row",
          "For rank 95, Height should follow the pattern 'X,XXX m (X,XXX ft)'",
          "Rank must equal 95",
          "Height value must be extracted and parsed correctly",
          "The 'Rank' column should be numeric, but it is read as object. Need to convert to numeric.",
          "The 'Height' column contains both meters and feet. Need to extract the meter value or convert all values to a single unit for comparison."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Rank values should be unique",
            "Height should contain valid measurement data for each row",
            "For rank 95, Height should follow the pattern 'X,XXX m (X,XXX ft)'"
          ],
          [
            "Rank must equal 95",
            "Height value must be extracted and parsed correctly"
          ],
          [
            "The 'Rank' column should be numeric, but it is read as object. Need to convert to numeric.",
            "The 'Height' column contains both meters and feet. Need to extract the meter value or convert all values to a single unit for comparison."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter dataset to only row where Rank == '95'",
          "Extract numerical value from Height column for that row",
          "Convert Rank column to numeric type for filtering",
          "Parse Height column to extract numeric values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter dataset to only row where Rank == '95'",
            "Extract numerical value from Height column for that row"
          ],
          [
            "Convert Rank column to numeric type for filtering",
            "Parse Height column to extract numeric values"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if rank 95 exists in the dataset",
          "Verify Height format consistency for rank 95"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if rank 95 exists in the dataset",
            "Verify Height format consistency for rank 95"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return the Height value as a string",
          "Consider extracting just the meter value as a number if needed for comparison",
          "Return the height value as a numeric metric (in meters and/or feet)",
          "Handle case where Height may only contain placeholder 'm (ft)' without numeric value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return the Height value as a string",
            "Consider extracting just the meter value as a number if needed for comparison"
          ],
          [
            "Return the height value as a numeric metric (in meters and/or feet)",
            "Handle case where Height may only contain placeholder 'm (ft)' without numeric value"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5625000000000001
  },
  "dbbench-087": {
    "m_q": {
      "target_metric": {
        "value": "Latest year where Real Colorado Foxes did not qualify for playoffs but reached Open Cup 2nd Round",
        "confidence": 0.6666666666666666,
        "votes": [
          "Latest year where Real Colorado Foxes did not qualify for playoffs but reached Open Cup 2nd Round",
          "Latest year where Real Colorado Foxes did not qualify for playoffs but reached Open Cup 2nd Round",
          "latest year when Real Colorado Foxes did not qualify for playoffs but made it to Open Cup 2nd Round"
        ]
      },
      "filters": {
        "value": [
          "Playoffs = 'Did not qualify'",
          "Open Cup = '2nd Round'",
          "Playoffs == 'Did not qualify'",
          "Open Cup == '2nd Round'"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "Playoffs = 'Did not qualify'",
            "Open Cup = '2nd Round'"
          ],
          [
            "Playoffs == 'Did not qualify'",
            "Open Cup == '2nd Round'"
          ],
          [
            "Playoffs = 'Did not qualify'",
            "Open Cup = '2nd Round'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What years did the team not qualify for playoffs?",
          "What years did the team reach Open Cup 2nd Round?",
          "What is the intersection of these two sets?",
          "What is the maximum year in that intersection?",
          "Which years did the team not qualify for playoffs?",
          "Which years did the team reach Open Cup 2nd Round?",
          "What is the intersection of these two conditions?",
          "What is the maximum/latest year from this intersection?",
          "Identify all years where Playoffs is 'Did not qualify' and Open Cup is '2nd Round'.",
          "From the identified years, find the maximum year."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What years did the team not qualify for playoffs?",
            "What years did the team reach Open Cup 2nd Round?",
            "What is the intersection of these two sets?",
            "What is the maximum year in that intersection?"
          ],
          [
            "Which years did the team not qualify for playoffs?",
            "Which years did the team reach Open Cup 2nd Round?",
            "What is the intersection of these two conditions?",
            "What is the maximum/latest year from this intersection?"
          ],
          [
            "Identify all years where Playoffs is 'Did not qualify' and Open Cup is '2nd Round'.",
            "From the identified years, find the maximum year."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Soccer Team Performance"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Soccer Team Performance"
          ],
          [
            "Soccer Team Performance"
          ],
          [
            "Soccer Team Performance"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "calendar year",
          "division": "league tier number",
          "league": "league name",
          "regular season": "standing with division name",
          "playoffs": "playoff result or qualification status",
          "open cup": "tournament round reached or qualification status"
        },
        "confidence": 0.5555555555555556,
        "votes": [
          {
            "Year": "calendar year"
          },
          {
            "Year": "year (integer or string)",
            "Division": "league tier number",
            "League": "league name",
            "Regular Season": "standing with division name",
            "Playoffs": "playoff result or qualification status",
            "Open Cup": "tournament round reached or qualification status"
          },
          {
            "Year": "year",
            "Division": "division level",
            "League": "league name"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Year column is stored as object/string instead of integer"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column is stored as object/string instead of integer"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "Canceled",
          "Did not qualify"
        ],
        "confidence": 0.6,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "Canceled"
          ],
          [
            "Canceled",
            "Did not qualify"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values should be between 2009-2023",
          "Playoffs column contains specific values: 'Did not qualify', 'Division Final', 'Conference Semifinal', etc.",
          "Open Cup column contains specific values: 'Did not qualify', '1st Round', '2nd Round'",
          "Year must be parseable as integer for comparison",
          "Playoffs value must exactly match 'Did not qualify'",
          "Open Cup value must exactly match '2nd Round'",
          "Result must be a single year value",
          "Year must be a valid year.",
          "Playoffs and Open Cup columns should contain consistent values (e.g., 'Did not qualify', '2nd Round', etc.)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values should be between 2009-2023",
            "Playoffs column contains specific values: 'Did not qualify', 'Division Final', 'Conference Semifinal', etc.",
            "Open Cup column contains specific values: 'Did not qualify', '1st Round', '2nd Round'"
          ],
          [
            "Year must be parseable as integer for comparison",
            "Playoffs value must exactly match 'Did not qualify'",
            "Open Cup value must exactly match '2nd Round'",
            "Result must be a single year value"
          ],
          [
            "Year must be a valid year.",
            "Playoffs and Open Cup columns should contain consistent values (e.g., 'Did not qualify', '2nd Round', etc.)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude Year 2020 where all performance metrics are 'Canceled'",
          "Filter rows where Playoffs column equals 'Did not qualify' AND Open Cup column equals '2nd Round'",
          "From filtered rows, select the maximum Year value"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude Year 2020 where all performance metrics are 'Canceled'"
          ],
          [
            "Filter rows where Playoffs column equals 'Did not qualify' AND Open Cup column equals '2nd Round'",
            "From filtered rows, select the maximum Year value"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate years",
          "Verify chronological order of years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate years",
            "Verify chronological order of years"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Year should be returned as integer",
          "Return the latest (maximum) year as a single value",
          "Year should be returned as integer or year format",
          "Output should be a single year."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year should be returned as integer"
          ],
          [
            "Return the latest (maximum) year as a single value",
            "Year should be returned as integer or year format"
          ],
          [
            "Output should be a single year."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.582777777777778
  },
  "dbbench-088": {
    "m_q": {
      "target_metric": {
        "value": "latest season where the driver had exactly 1 podium",
        "confidence": 0.3333333333333333,
        "votes": [
          "latest season where the driver had exactly 1 podium",
          "Latest season where podiums equals 1",
          "Latest season with at least 1 podium"
        ]
      },
      "filters": {
        "value": [
          "Podiums == '1'",
          "Series contains 'Formula'",
          "Podiums == 1",
          "Podiums >= 1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Podiums == '1'",
            "Series contains 'Formula'"
          ],
          [
            "Podiums == 1"
          ],
          [
            "Podiums >= 1"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Season"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Season"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the maximum season value where Podiums equals 1?",
          "Are there multiple seasons with exactly 1 podium?",
          "Should we consider all series or only specific ones?",
          "Which rows have exactly 1 podium?",
          "What is the maximum/latest season value among those rows?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the maximum season value where Podiums equals 1?",
            "Are there multiple seasons with exactly 1 podium?",
            "Should we consider all series or only specific ones?"
          ],
          [
            "Which rows have exactly 1 podium?",
            "What is the maximum/latest season value among those rows?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Race Statistics"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Race Statistics"
          ],
          [
            "Race Statistics"
          ],
          [
            "Race Statistics"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All columns stored as object dtype despite numeric content (Season, Races, Wins, Poles, F/Laps, Podiums, Points)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns stored as object dtype despite numeric content (Season, Races, Wins, Poles, F/Laps, Podiums, Points)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "season": "year",
          "races": "count",
          "wins": "count",
          "poles": "count",
          "f/laps": "count",
          "podiums": "count",
          "points": "points",
          "position": "rank"
        },
        "confidence": 0.6249999999999999,
        "votes": [
          {
            "Season": "year",
            "Races": "count",
            "Wins": "count",
            "Poles": "count",
            "F/Laps": "count",
            "Podiums": "count",
            "Points": "points"
          },
          {
            "Season": "year",
            "Races": "count",
            "Wins": "count",
            "Poles": "count",
            "F/Laps": "count",
            "Podiums": "count",
            "Points": "score",
            "Position": "rank"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Position column contains ordinal strings (e.g., '20th', '3rd') instead of numeric ranks",
          "Podiums column needs conversion from string to integer for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Position column contains ordinal strings (e.g., '20th', '3rd') instead of numeric ranks",
            "Podiums column needs conversion from string to integer for comparison"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 1.0,
        "votes": [
          10.0,
          10.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Season values should be convertible to integers",
          "Podiums values should be non-negative integers",
          "Races >= Wins + Podiums (logical constraint)",
          "Podiums must be converted to integer for exact comparison",
          "Season must be converted to integer or comparable format for finding latest",
          "Filter for Podiums == 1",
          "Season should be sorted in descending order to find the latest season"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Season values should be convertible to integers",
            "Podiums values should be non-negative integers",
            "Races >= Wins + Podiums (logical constraint)"
          ],
          [
            "Podiums must be converted to integer for exact comparison",
            "Season must be converted to integer or comparable format for finding latest",
            "Filter for Podiums == 1"
          ],
          [
            "Season should be sorted in descending order to find the latest season"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Convert Podiums to integer type",
          "Filter to rows where integer(Podiums) == 1",
          "Sort Season descending",
          "Cast Podiums column to integer",
          "Cast Season column to integer for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Convert Podiums to integer type",
            "Filter to rows where integer(Podiums) == 1",
            "Sort Season descending"
          ],
          [
            "Cast Podiums column to integer",
            "Cast Season column to integer for comparison"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Season values with different podium counts",
          "Validate Season chronological order"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Season values with different podium counts",
            "Validate Season chronological order"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return single season year as string or integer",
          "If multiple seasons with 1 podium, return most recent",
          "Return single season value (year)",
          "Should be the maximum season among rows where Podiums == 1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return single season year as string or integer",
            "If multiple seasons with 1 podium, return most recent"
          ],
          [
            "Return single season value (year)",
            "Should be the maximum season among rows where Podiums == 1"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5979166666666668
  },
  "dbbench-089": {
    "m_q": {
      "target_metric": {
        "value": "The train with the latest arrival time each day",
        "confidence": 0.6666666666666666,
        "votes": [
          "The train with the latest arrival time each day",
          "The train with the latest arrival time each day",
          "last train to arrive each day"
        ]
      },
      "filters": {
        "value": [
          "Day contains 'Daily'",
          "Arrival time is not null or sentinel value",
          "Only trains with valid arrival times (not N/a)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Day contains 'Daily'",
            "Arrival time is not null or sentinel value"
          ],
          [
            "Only trains with valid arrival times (not N/a)"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Day"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Day"
          ],
          [
            "Day"
          ],
          [
            "Day"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "list",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the latest arrival time among daily trains?",
          "Which train(s) have that arrival time?",
          "Are there multiple trains with the same latest arrival time?",
          "What are the unique day categories in the dataset?",
          "How should arrival times be parsed and compared?",
          "Should 'Daily' trains be considered for each specific day?",
          "How to handle trains that run on multiple specific days?",
          "Should trains arriving after midnight (e.g., 01:23) be considered as arriving late at night or early next morning?",
          "For each day, what is the latest arrival time?",
          "Which train corresponds to the latest arrival time for each day?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the latest arrival time among daily trains?",
            "Which train(s) have that arrival time?",
            "Are there multiple trains with the same latest arrival time?"
          ],
          [
            "What are the unique day categories in the dataset?",
            "How should arrival times be parsed and compared?",
            "Should 'Daily' trains be considered for each specific day?",
            "How to handle trains that run on multiple specific days?",
            "Should trains arriving after midnight (e.g., 01:23) be considered as arriving late at night or early next morning?"
          ],
          [
            "For each day, what is the latest arrival time?",
            "Which train corresponds to the latest arrival time for each day?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Train Schedule"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Train Schedule"
          ],
          [
            "Train Schedule"
          ],
          [
            "Train Schedule"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "arrival": "HH:MM time format",
          "departure": "HH:MM time format",
          "day": "day codes (Daily, M=Monday, Tu=Tuesday, W=Wednesday, T=Thursday, F=Friday, St=Saturday, S=Sunday)"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "Arrival": "HH:MM time format",
            "Departure": "HH:MM time format"
          },
          {
            "Arrival": "time in HH:MM format (24-hour)",
            "Departure": "time in HH:MM format (24-hour)",
            "Day": "day codes (Daily, M=Monday, Tu=Tuesday, W=Wednesday, T=Thursday, F=Friday, St=Saturday, S=Sunday)"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Time values are in 24-hour format but need to be parsed for comparison",
          "Day column contains complex strings with multiple day abbreviations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time values are in 24-hour format but need to be parsed for comparison",
            "Day column contains complex strings with multiple day abbreviations"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "N/a",
          "NA",
          ""
        ],
        "confidence": 0.9999999999999999,
        "votes": [
          [
            "N/a",
            "NA",
            "N/A",
            ""
          ],
          [
            "N/a",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Arrival times must be valid 24-hour format",
          "Only trains with 'Daily' in Day column should be considered",
          "N/a in Departure column indicates no departure time",
          "Arrival time must not be 'N/a'",
          "Arrival time must be in valid HH:MM format",
          "Day values must be parsed to handle both single days and comma-separated multiple days",
          "Arrival times must be in a valid time format (HH:MM).",
          "Day values must be valid days of the week or 'Daily'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Arrival times must be valid 24-hour format",
            "Only trains with 'Daily' in Day column should be considered",
            "N/a in Departure column indicates no departure time"
          ],
          [
            "Arrival time must not be 'N/a'",
            "Arrival time must be in valid HH:MM format",
            "Day values must be parsed to handle both single days and comma-separated multiple days"
          ],
          [
            "Arrival times must be in a valid time format (HH:MM).",
            "Day values must be valid days of the week or 'Daily'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows where Day does not contain 'Daily'",
          "Filter out rows where Arrival is null or sentinel value",
          "Expand 'Daily' to all seven days of the week",
          "Parse comma-separated day values into individual day entries",
          "Filter out rows where 'Arrival' is null or invalid.",
          "Create a derived column representing the arrival time as a datetime object for comparison."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows where Day does not contain 'Daily'",
            "Filter out rows where Arrival is null or sentinel value"
          ],
          [
            "Expand 'Daily' to all seven days of the week",
            "Parse comma-separated day values into individual day entries"
          ],
          [
            "Filter out rows where 'Arrival' is null or invalid.",
            "Create a derived column representing the arrival time as a datetime object for comparison."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate Train No.",
          "Validate time format consistency"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Train No.",
            "Validate time format consistency"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Should include Train No., Name of the Train, and Arrival time",
          "If multiple trains have same latest arrival time, list all",
          "Output should show the last arriving train for each day of the week",
          "Include Train No., Name of the Train, and Arrival time",
          "Format as a table with one row per day (Monday through Sunday)",
          "Output should include the day and the name of the train that arrived last.",
          "Output should be sorted by day."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Should include Train No., Name of the Train, and Arrival time",
            "If multiple trains have same latest arrival time, list all"
          ],
          [
            "Output should show the last arriving train for each day of the week",
            "Include Train No., Name of the Train, and Arrival time",
            "Format as a table with one row per day (Monday through Sunday)"
          ],
          [
            "Output should include the day and the name of the train that arrived last.",
            "Output should be sorted by day."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6111111111111112
  },
  "dbbench-090": {
    "m_q": {
      "target_metric": {
        "value": "Maximum venue capacity (spectator count)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Maximum venue capacity (spectator count)",
          "Maximum stadium capacity among venues used in the 2008-2009 Iran Pro League season",
          "maximum capacity of venues used by Iran Pro League teams in the 2008-2009 season"
        ]
      },
      "filters": {
        "value": [
          "Past Season = '2008-2009' or season context implies 2008-2009",
          "2008-2009 season (implicit from Past Season column referring to previous season)",
          "season is 2008-2009",
          "venue is used by a team in the Iran Pro League"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Past Season = '2008-2009' or season context implies 2008-2009"
          ],
          [
            "2008-2009 season (implicit from Past Season column referring to previous season)"
          ],
          [
            "season is 2008-2009",
            "venue is used by a team in the Iran Pro League"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Venue"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Venue"
          ],
          [
            "Venue"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Which teams used each venue in 2008-2009?",
          "Are there multiple venues with the same name in different cities?",
          "Does Capacity column contain numeric values or formatted strings with commas?",
          "What venues were used in the 2008-2009 season?",
          "What is the capacity of each venue?",
          "Which venue has the maximum capacity?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which teams used each venue in 2008-2009?",
            "Are there multiple venues with the same name in different cities?",
            "Does Capacity column contain numeric values or formatted strings with commas?"
          ],
          [
            "What venues were used in the 2008-2009 season?",
            "What is the capacity of each venue?",
            "Which venue has the maximum capacity?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "football_teams"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "football_teams"
          ],
          [
            "football_teams"
          ],
          [
            "football_teams"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Capacity column is object type but appears to contain numeric data with comma formatting"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Capacity column is object type but appears to contain numeric data with comma formatting"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "capacity": "number of spectators"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Capacity": "spectators"
          },
          {
            "Capacity": "number of spectators"
          },
          {
            "Capacity": "number of spectators"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Capacity values contain comma separators (e.g., '35,000') that need conversion to numeric",
          "Capacity values contain commas as thousands separators (e.g., '35,000')",
          "Capacity stored as string/object type but represents numeric values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Capacity values contain comma separators (e.g., '35,000') that need conversion to numeric"
          ],
          [
            "Capacity values contain commas as thousands separators (e.g., '35,000')",
            "Capacity stored as string/object type but represents numeric values"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Capacity must be positive integer",
          "Each venue should have unique capacity value",
          "Venue names should be consistent across teams",
          "All teams in the dataset represent the 2008-2009 Iran Pro League season",
          "Capacity must be converted from string to numeric for comparison",
          "Need to handle duplicate venues when multiple teams share the same stadium",
          "Capacity column needs to be converted to numeric type by removing commas."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Capacity must be positive integer",
            "Each venue should have unique capacity value",
            "Venue names should be consistent across teams"
          ],
          [
            "All teams in the dataset represent the 2008-2009 Iran Pro League season",
            "Capacity must be converted from string to numeric for comparison",
            "Need to handle duplicate venues when multiple teams share the same stadium"
          ],
          [
            "Capacity column needs to be converted to numeric type by removing commas."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows where Capacity is not convertible to numeric",
          "Consider only venues used in 2008-2009 season",
          "Remove duplicate venues to get unique venue-capacity pairs"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows where Capacity is not convertible to numeric",
            "Consider only venues used in 2008-2009 season"
          ],
          [
            "Remove duplicate venues to get unique venue-capacity pairs"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate venue names with different capacities",
          "Validate capacity values against known stadium sizes"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate venue names with different capacities",
            "Validate capacity values against known stadium sizes"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return venue name and capacity",
          "Capacity should be formatted as integer without commas",
          "Return the venue name with the highest capacity",
          "May optionally include the capacity value for context",
          "The output should be the name of the venue with the largest capacity and its capacity."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return venue name and capacity",
            "Capacity should be formatted as integer without commas"
          ],
          [
            "Return the venue name with the highest capacity",
            "May optionally include the capacity value for context"
          ],
          [
            "The output should be the name of the venue with the largest capacity and its capacity."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6333333333333334
  },
  "dbbench-091": {
    "m_q": {
      "target_metric": {
        "value": "maximum engine volume",
        "confidence": 0.6666666666666666,
        "votes": [
          "maximum engine volume",
          "maximum engine volume across all engines",
          "maximum engine volume"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the maximum value in the Volume column?",
          "Are there any missing values in the Volume column that need to be handled?",
          "Do all Volume values follow the same format (e.g., X.XL)?",
          "How should volume values with different units be compared?",
          "What is the Volume column format and unit?",
          "How should volume values be parsed from the 'Volume' column?",
          "Are there any null or missing values in the Volume column?",
          "What is the maximum numeric value in the Volume column?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the maximum value in the Volume column?",
            "Are there any missing values in the Volume column that need to be handled?",
            "Do all Volume values follow the same format (e.g., X.XL)?",
            "How should volume values with different units be compared?"
          ],
          [
            "What is the Volume column format and unit?",
            "How should volume values be parsed from the 'Volume' column?",
            "Are there any null or missing values in the Volume column?",
            "What is the maximum numeric value in the Volume column?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Car_model_info"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Car_model_info"
          ],
          [
            "Car_model_info"
          ],
          [
            "Car_model_info"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "volume": "liters (L)",
          "power/torque": "kW/PS/hp and N\u00b7m/lb\u00b7ft",
          "years produced": "year ranges",
          "produced": "count"
        },
        "confidence": 0.49999999999999994,
        "votes": [
          {
            "Volume": "liters (L)",
            "Power/Torque": "kW/PS/hp and N\u00b7m/lb\u00b7ft",
            "Years produced": "year ranges",
            "Produced": "count"
          },
          {
            "Volume": "liters (L)"
          },
          {
            "Volume": "Liters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Volume column contains values with 'L' suffix that needs to be removed for numerical comparison",
          "Some Volume values may have different units (e.g., cc vs L) but all appear to be in liters in the sample",
          "Volume column is stored as object/string type (e.g., '2.8L', '5.4L') and needs to be parsed to numeric",
          "Volume values include unit suffix 'L' that must be stripped for numeric comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Volume column contains values with 'L' suffix that needs to be removed for numerical comparison",
            "Some Volume values may have different units (e.g., cc vs L) but all appear to be in liters in the sample"
          ],
          [
            "Volume column is stored as object/string type (e.g., '2.8L', '5.4L') and needs to be parsed to numeric",
            "Volume values include unit suffix 'L' that must be stripped for numeric comparison"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Volume should be positive numeric values",
          "Engine names should be unique identifiers",
          "Volume values should be comparable after unit normalization",
          "Volume values must be positive numbers",
          "Volume column must be converted from string to numeric type",
          "Extract numeric value by removing 'L' suffix from Volume column",
          "Volume column must be numeric to find the maximum."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Volume should be positive numeric values",
            "Engine names should be unique identifiers",
            "Volume values should be comparable after unit normalization"
          ],
          [
            "Volume values must be positive numbers",
            "Volume column must be converted from string to numeric type",
            "Extract numeric value by removing 'L' suffix from Volume column"
          ],
          [
            "Volume column must be numeric to find the maximum."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove 'L' suffix from Volume column",
          "Convert Volume to numeric type",
          "Handle any missing or malformed Volume values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove 'L' suffix from Volume column",
            "Convert Volume to numeric type",
            "Handle any missing or malformed Volume values"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Volume values",
          "Verify Volume values follow expected distribution for car engines"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Volume values",
            "Verify Volume values follow expected distribution for car engines"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Volume should be reported with appropriate units (likely liters)",
          "Should include the engine name associated with the maximum volume",
          "Return single numeric value representing maximum volume in liters",
          "Include unit (L) in final output for clarity",
          "The output should be a single numeric value representing the maximum engine volume in Liters."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Volume should be reported with appropriate units (likely liters)",
            "Should include the engine name associated with the maximum volume"
          ],
          [
            "Return single numeric value representing maximum volume in liters",
            "Include unit (L) in final output for clarity"
          ],
          [
            "The output should be a single numeric value representing the maximum engine volume in Liters."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5583333333333335
  },
  "dbbench-092": {
    "m_q": {
      "target_metric": {
        "value": "average number of laps",
        "confidence": 1.0,
        "votes": [
          "average number of laps",
          "average number of laps",
          "Average number of laps"
        ]
      },
      "filters": {
        "value": [
          "Time/Retired contains 'Accident'",
          "Manufacturer = 'Aprilia'",
          "Grid = '27'",
          "Manufacturer equals 'Aprilia'",
          "Grid equals 27",
          "Time/Retired is 'Accident' or 'Retired'",
          "Manufacturer is 'Aprilia'",
          "Grid is 27"
        ],
        "confidence": 0.375,
        "votes": [
          [
            "Time/Retired contains 'Accident'",
            "Manufacturer = 'Aprilia'",
            "Grid = '27'"
          ],
          [
            "Time/Retired contains 'Accident'",
            "Manufacturer equals 'Aprilia'",
            "Grid equals 27"
          ],
          [
            "Time/Retired is 'Accident' or 'Retired'",
            "Manufacturer is 'Aprilia'",
            "Grid is 27"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How many records match all three filter conditions?",
          "What are the Laps values for matching records?",
          "Are Laps values numeric or need conversion?",
          "Which rows have 'Accident' in the Time/Retired column?",
          "Which rows have Manufacturer as 'Aprilia'?",
          "Which rows have Grid equal to 27?",
          "What are the Laps values for rows meeting all three criteria?",
          "What is the mean of the Laps values after filtering?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How many records match all three filter conditions?",
            "What are the Laps values for matching records?",
            "Are Laps values numeric or need conversion?"
          ],
          [
            "Which rows have 'Accident' in the Time/Retired column?",
            "Which rows have Manufacturer as 'Aprilia'?",
            "Which rows have Grid equal to 27?",
            "What are the Laps values for rows meeting all three criteria?",
            "What is the mean of the Laps values after filtering?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Motorcycle Race Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Motorcycle Race Results"
          ],
          [
            "Motorcycle Race Results"
          ],
          [
            "Motorcycle Race Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "laps": "count",
          "time/retired": "time or status string",
          "grid": "starting position"
        },
        "confidence": 0.8888888888888888,
        "votes": [
          {
            "Laps": "count",
            "Time/Retired": "time or status string",
            "Grid": "starting position"
          },
          {
            "Laps": "count",
            "Grid": "position_number",
            "Time/Retired": "time_or_status"
          },
          {
            "Laps": "number of laps",
            "Grid": "starting position"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Laps column is object dtype but appears to contain numeric values",
          "Grid column is object dtype but appears to contain numeric positions",
          "Time/Retired mixes time values (e.g., '+46:22.971') with status strings (e.g., 'Accident')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Laps column is object dtype but appears to contain numeric values",
            "Grid column is object dtype but appears to contain numeric positions",
            "Time/Retired mixes time values (e.g., '+46:22.971') with status strings (e.g., 'Accident')"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Laps should be convertible to numeric values",
          "Grid values should be convertible to numeric positions",
          "Time/Retired values starting with '+' indicate finishing times, others indicate DNF reasons",
          "Laps must be convertible to numeric for averaging",
          "Grid must be convertible to numeric for comparison with 27",
          "Time/Retired must be checked for substring 'Accident' case-insensitively",
          "Grid column should be numeric",
          "Laps column should be numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Laps should be convertible to numeric values",
            "Grid values should be convertible to numeric positions",
            "Time/Retired values starting with '+' indicate finishing times, others indicate DNF reasons"
          ],
          [
            "Laps must be convertible to numeric for averaging",
            "Grid must be convertible to numeric for comparison with 27",
            "Time/Retired must be checked for substring 'Accident' case-insensitively"
          ],
          [
            "Grid column should be numeric",
            "Laps column should be numeric"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter for Time/Retired values containing 'Accident' (case-insensitive)",
          "Filter for Manufacturer exactly 'Aprilia'",
          "Filter for Grid exactly '27'",
          "Filter where Time/Retired status is 'Accident' (not a time value)",
          "Filter where Manufacturer matches 'Aprilia' exactly",
          "Filter where Grid value equals 27 after numeric conversion",
          "Time/Retired is 'Accident' or 'Retired'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter for Time/Retired values containing 'Accident' (case-insensitive)",
            "Filter for Manufacturer exactly 'Aprilia'",
            "Filter for Grid exactly '27'"
          ],
          [
            "Filter where Time/Retired status is 'Accident' (not a time value)",
            "Filter where Manufacturer matches 'Aprilia' exactly",
            "Filter where Grid value equals 27 after numeric conversion"
          ],
          [
            "Time/Retired is 'Accident' or 'Retired'"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if any matching records exist before computing average",
          "Verify Laps distribution for matching records"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if any matching records exist before computing average",
            "Verify Laps distribution for matching records"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Average should be a single numeric value",
          "Handle case where no records match filters",
          "Return a single numeric value representing the average",
          "Handle case where no records match the criteria (return null or 0)",
          "Round to appropriate precision for lap counts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Average should be a single numeric value",
            "Handle case where no records match filters"
          ],
          [
            "Return a single numeric value representing the average",
            "Handle case where no records match the criteria (return null or 0)",
            "Round to appropriate precision for lap counts"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6131944444444446
  },
  "dbbench-093": {
    "m_q": {
      "target_metric": {
        "value": "average number of laps driven per race in Tony Gulotta's Indianapolis 500 career",
        "confidence": 0.3333333333333333,
        "votes": [
          "average number of laps driven per race in Tony Gulotta's Indianapolis 500 career",
          "average number of laps driven in the Indianapolis 500 in Tony Gulotta's career",
          "average number of laps driven by Tony Gulotta"
        ]
      },
      "filters": {
        "value": [
          "Filter to Tony Gulotta's race results only",
          "Exclude races where Laps data is missing or invalid",
          "Tony Gulotta's career races",
          "Indianapolis 500 races only",
          "driver is Tony Gulotta"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to Tony Gulotta's race results only",
            "Exclude races where Laps data is missing or invalid"
          ],
          [
            "Tony Gulotta's career races",
            "Indianapolis 500 races only"
          ],
          [
            "driver is Tony Gulotta"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "How many Indianapolis 500 races did Tony Gulotta participate in?",
          "What is the total number of laps driven across all races?",
          "Are there any races with incomplete lap data that should be excluded?",
          "What is the sum of laps divided by number of races?"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "How many Indianapolis 500 races did Tony Gulotta participate in?",
            "What is the total number of laps driven across all races?",
            "Are there any races with incomplete lap data that should be excluded?"
          ],
          [
            "How many Indianapolis 500 races did Tony Gulotta participate in?",
            "What is the total number of laps driven across all races?",
            "What is the sum of laps divided by number of races?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Race Results"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Race Results"
          ],
          [
            "Race Results"
          ],
          [
            "Race Results"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All columns are stored as object/string type despite containing numeric data (Year, Laps, Led, etc.)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns are stored as object/string type despite containing numeric data (Year, Laps, Led, etc.)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "year",
          "car": "car number",
          "start": "starting position",
          "qual": "qualifying speed (mph)",
          "rank": "qualifying rank",
          "finish": "finishing position",
          "laps": "number of laps completed",
          "led": "number of laps led",
          "retired": "reason for retirement"
        },
        "confidence": 0.48148148148148145,
        "votes": [
          {
            "Year": "year",
            "Car": "car number",
            "Start": "starting position",
            "Qual": "qualifying speed (mph)",
            "Rank": "qualifying rank",
            "Finish": "finishing position",
            "Laps": "number of laps completed",
            "Led": "number of laps led",
            "Retired": "reason for retirement"
          },
          {
            "Laps": "count",
            "Qual": "mph",
            "Led": "count"
          },
          {
            "Laps": "laps"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Laps column contains string values that need conversion to numeric",
          "Year column contains string values that need conversion to numeric",
          "Some Laps values may represent incomplete races (less than 200 laps)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Laps column contains string values that need conversion to numeric",
            "Year column contains string values that need conversion to numeric",
            "Some Laps values may represent incomplete races (less than 200 laps)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Laps should be numeric values between 0 and 200 for Indianapolis 500",
          "Year should be between 1911 and present for Indianapolis 500",
          "Finish position should be numeric or 'DNS'/'DNQ' for non-starters",
          "Laps column must be numeric and non-negative",
          "Only count races where Laps value is valid/non-null",
          "Each row represents one Indianapolis 500 race entry",
          "The 'Laps' column must be numeric to calculate the average."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Laps should be numeric values between 0 and 200 for Indianapolis 500",
            "Year should be between 1911 and present for Indianapolis 500",
            "Finish position should be numeric or 'DNS'/'DNQ' for non-starters"
          ],
          [
            "Laps column must be numeric and non-negative",
            "Only count races where Laps value is valid/non-null",
            "Each row represents one Indianapolis 500 race entry"
          ],
          [
            "The 'Laps' column must be numeric to calculate the average."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to only rows where Laps is a valid numeric value",
          "Exclude rows where Laps is 0 or represents a DNS/DNQ situation",
          "Exclude rows where Laps cannot be converted to numeric",
          "Include all rows as they represent Indianapolis 500 races",
          "Identify rows where the driver is Tony Gulotta. This requires a separate data source or external knowledge, as the provided data does not contain driver names."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to only rows where Laps is a valid numeric value",
            "Exclude rows where Laps is 0 or represents a DNS/DNQ situation"
          ],
          [
            "Exclude rows where Laps cannot be converted to numeric",
            "Include all rows as they represent Indianapolis 500 races"
          ],
          [
            "Identify rows where the driver is Tony Gulotta. This requires a separate data source or external knowledge, as the provided data does not contain driver names."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Laps column (values significantly below 200 may indicate DNFs)",
          "Verify data completeness across years (missing 1936 in sample)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Laps column (values significantly below 200 may indicate DNFs)",
            "Verify data completeness across years (missing 1936 in sample)"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Average should be calculated as mean(Laps) for all Tony Gulotta's races",
          "Result should be a single numeric value with appropriate precision",
          "Result should be a single numeric value",
          "Should represent average laps per race",
          "Round to reasonable precision (e.g., 2 decimal places)",
          "Output should be a single numerical value representing the average number of laps."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Average should be calculated as mean(Laps) for all Tony Gulotta's races",
            "Result should be a single numeric value with appropriate precision"
          ],
          [
            "Result should be a single numeric value",
            "Should represent average laps per race",
            "Round to reasonable precision (e.g., 2 decimal places)"
          ],
          [
            "Output should be a single numerical value representing the average number of laps."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5824074074074075
  },
  "dbbench-094": {
    "m_q": {
      "target_metric": {
        "value": "average of First Elected year",
        "confidence": 0.6666666666666666,
        "votes": [
          "average of First Elected year",
          "average of First Elected year",
          "average of 'First Elected' year"
        ]
      },
      "filters": {
        "value": [
          "District = '11'",
          "Committee contains 'Environmental Matters'",
          "District equals 11",
          "District is 11",
          "Committee is 'Environmental Matters'"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "District = '11'",
            "Committee contains 'Environmental Matters'"
          ],
          [
            "District equals 11",
            "Committee contains 'Environmental Matters'"
          ],
          [
            "District is 11",
            "Committee is 'Environmental Matters'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the exact string match for District 11?",
          "Does 'Environmental Matters' need exact match or partial match?",
          "Should we include variations like 'Environmental Matters (Vice-Chair)'?",
          "Filter records where District is 11",
          "Filter records where Committee is Environmental Matters",
          "Extract First Elected years from filtered records",
          "Calculate average of First Elected years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the exact string match for District 11?",
            "Does 'Environmental Matters' need exact match or partial match?",
            "Should we include variations like 'Environmental Matters (Vice-Chair)'?"
          ],
          [
            "Filter records where District is 11",
            "Filter records where Committee is Environmental Matters",
            "Extract First Elected years from filtered records",
            "Calculate average of First Elected years"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Maryland State Legislature Representatives"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Maryland State Legislature Representatives"
          ],
          [
            "Maryland State Legislature Representatives"
          ],
          [
            "Maryland State Legislature Representatives"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "first elected": "year",
          "district": "district_number"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "First Elected": "year"
          },
          {
            "First Elected": "year",
            "District": "district_number"
          },
          {
            "First Elected": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "First Elected is stored as object/string instead of integer",
          "District column contains mixed formats (decimal notation like '05.2 5B' and integers like '11')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "First Elected is stored as object/string instead of integer"
          ],
          [
            "District column contains mixed formats (decimal notation like '05.2 5B' and integers like '11')"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "First Elected should be convertible to integer",
          "District values should be consistent for filtering",
          "District must exactly match '11' (integer format)",
          "First Elected must be convertible to numeric year",
          "Committee field must contain the exact phrase 'Environmental Matters'",
          "District column should be treated as string to handle values like '05.2 5B' and '12.1 12A'",
          "First Elected column should be converted to numeric type (year)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "First Elected should be convertible to integer",
            "District values should be consistent for filtering"
          ],
          [
            "District must exactly match '11' (integer format)",
            "First Elected must be convertible to numeric year",
            "Committee field must contain the exact phrase 'Environmental Matters'"
          ],
          [
            "District column should be treated as string to handle values like '05.2 5B' and '12.1 12A'",
            "First Elected column should be converted to numeric type (year)"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where District contains '11' as exact match",
          "Filter rows where Committee contains 'Environmental Matters' substring",
          "Committee string matching should handle variations like 'Environmental Matters (Vice-Chair)'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where District contains '11' as exact match",
            "Filter rows where Committee contains 'Environmental Matters' substring"
          ],
          [
            "Committee string matching should handle variations like 'Environmental Matters (Vice-Chair)'"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of First Elected years for filtered subset"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of First Elected years for filtered subset"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Average year should be numeric, possibly with decimal places",
          "Result should be a single numeric value representing the average year",
          "Average should be calculated only from valid year values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Average year should be numeric, possibly with decimal places"
          ],
          [
            "Result should be a single numeric value representing the average year",
            "Average should be calculated only from valid year values"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5866666666666667
  },
  "dbbench-095": {
    "m_q": {
      "target_metric": {
        "value": "average year",
        "confidence": 1.0,
        "votes": [
          "average year",
          "average year",
          "average year"
        ]
      },
      "filters": {
        "value": [
          "Entrant = 'Belanger Motors'",
          "Engine = 'Offenhauser L4'",
          "Entrant is Belanger Motors",
          "Engine is Offenhauser L4"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "Entrant = 'Belanger Motors'",
            "Engine = 'Offenhauser L4'"
          ],
          [
            "Entrant = 'Belanger Motors'",
            "Engine = 'Offenhauser L4'"
          ],
          [
            "Entrant is Belanger Motors",
            "Engine is Offenhauser L4"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What years did Belanger Motors use Offenhauser L4 engines?",
          "How many records match both filters?",
          "Are the Year values numeric for calculation?",
          "Which rows have Entrant as 'Belanger Motors'?",
          "Which rows have Engine as 'Offenhauser L4' or 'Offenhauser l4'?",
          "What are the years that satisfy both conditions?",
          "What is the average of those years?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What years did Belanger Motors use Offenhauser L4 engines?",
            "How many records match both filters?",
            "Are the Year values numeric for calculation?"
          ],
          [
            "Which rows have Entrant as 'Belanger Motors'?",
            "Which rows have Engine as 'Offenhauser L4' or 'Offenhauser l4'?",
            "What are the years that satisfy both conditions?",
            "What is the average of those years?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "racers_database"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "racers_database"
          ],
          [
            "racers_database"
          ],
          [
            "racers_database"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Year column is object dtype but contains numeric years",
          "Points column is object dtype but appears to contain numeric values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column is object dtype but contains numeric years",
            "Points column is object dtype but appears to contain numeric values"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year": "calendar year",
          "points": "racing points (integer)"
        },
        "confidence": 0.8333333333333333,
        "votes": [
          {
            "Year": "calendar year",
            "Points": "racing points (integer)"
          },
          {
            "Year": "calendar year",
            "Points": "racing points"
          },
          {
            "Year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Year values need conversion from object to numeric for averaging"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values need conversion from object to numeric for averaging"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 5.0,
        "confidence": 1.0,
        "votes": [
          5.0,
          5.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Year values must be convertible to numeric for averaging",
          "Only one data source needed for this question",
          "Filtered data must contain at least one record to compute average",
          "Year must be numeric and convertible to integer",
          "Entrant matching must be exact for 'Belanger Motors'",
          "Engine matching should be case-insensitive to handle 'L4' vs 'l4'",
          "Year must be a valid year"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year values must be convertible to numeric for averaging",
            "Only one data source needed for this question",
            "Filtered data must contain at least one record to compute average"
          ],
          [
            "Year must be numeric and convertible to integer",
            "Entrant matching must be exact for 'Belanger Motors'",
            "Engine matching should be case-insensitive to handle 'L4' vs 'l4'"
          ],
          [
            "Year must be a valid year"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Year should be between 1950-1969 based on sample data",
          "Filter rows where Entrant == 'Belanger Motors' AND Engine matches 'Offenhauser L4' (case-insensitive)",
          "Belanger Motors entries with Offenhauser L4 engine"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year should be between 1950-1969 based on sample data"
          ],
          [
            "Filter rows where Entrant == 'Belanger Motors' AND Engine matches 'Offenhauser L4' (case-insensitive)"
          ],
          [
            "Belanger Motors entries with Offenhauser L4 engine"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing Year values in filtered subset",
          "Verify Year values are consistent numeric format"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing Year values in filtered subset",
            "Verify Year values are consistent numeric format"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Average year should be presented as a numeric value",
          "Consider rounding to appropriate precision",
          "Return a single numeric value representing the average year",
          "If no matching records found, handle appropriately (e.g., return null or 0)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Average year should be presented as a numeric value",
            "Consider rounding to appropriate precision"
          ],
          [
            "Return a single numeric value representing the average year",
            "If no matching records found, handle appropriately (e.g., return null or 0)"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6333333333333334
  },
  "dbbench-096": {
    "m_q": {
      "target_metric": {
        "value": "average number of gold medals",
        "confidence": 0.6666666666666666,
        "votes": [
          "average number of gold medals",
          "average number of gold medals won by China, Japan, and North Korea",
          "average number of gold medals"
        ]
      },
      "filters": {
        "value": [
          "Nation in ['China', 'Japan', 'North Korea']",
          "Nation is China",
          "Nation is Japan",
          "Nation is North Korea"
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "Nation in ['China', 'Japan', 'North Korea']"
          ],
          [
            "Nation in ['China', 'Japan', 'North Korea']"
          ],
          [
            "Nation is China",
            "Nation is Japan",
            "Nation is North Korea"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Nation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Nation"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total gold medals for each country?",
          "How many data points exist for each country?",
          "Are there multiple entries for the same country?",
          "What is the number of gold medals won by China?",
          "What is the number of gold medals won by Japan?",
          "What is the number of gold medals won by North Korea?",
          "What is the mean of these three values?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total gold medals for each country?",
            "How many data points exist for each country?",
            "Are there multiple entries for the same country?"
          ],
          [
            "What is the number of gold medals won by China?",
            "What is the number of gold medals won by Japan?",
            "What is the number of gold medals won by North Korea?",
            "What is the mean of these three values?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Olympic Medals"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Olympic Medals"
          ],
          [
            "Olympic Medals"
          ],
          [
            "Olympic Medals"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "gold": "medals",
          "silver": "medals",
          "bronze": "medals",
          "total": "medals",
          "rank": "ordinal"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Gold": "medals",
            "Silver": "medals",
            "Bronze": "medals",
            "Total": "medals"
          },
          {
            "Gold": "count",
            "Silver": "count",
            "Bronze": "count",
            "Total": "count",
            "Rank": "ordinal"
          },
          {
            "Gold": "number of medals"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Gold >= 0",
          "Silver >= 0",
          "Bronze >= 0",
          "Total = Gold + Silver + Bronze",
          "Gold column must be non-negative integer",
          "Nation names must match exactly: 'China', 'Japan', 'North Korea'",
          "All three nations must be present in the dataset",
          "Gold column must be non-negative"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Gold >= 0",
            "Silver >= 0",
            "Bronze >= 0",
            "Total = Gold + Silver + Bronze"
          ],
          [
            "Gold column must be non-negative integer",
            "Nation names must match exactly: 'China', 'Japan', 'North Korea'",
            "All three nations must be present in the dataset"
          ],
          [
            "Gold column must be non-negative"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to only China, Japan, and North Korea rows",
          "Filter rows where Nation equals 'China', 'Japan', or 'North Korea'",
          "Extract Gold values for filtered nations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to only China, Japan, and North Korea rows"
          ],
          [
            "Filter rows where Nation equals 'China', 'Japan', or 'North Korea'",
            "Extract Gold values for filtered nations"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate entries for same country",
          "Verify Gold values are integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate entries for same country",
            "Verify Gold values are integers"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Average should be a single numeric value",
          "Consider whether to average across all entries or per-country then average",
          "Return a single numeric value representing the arithmetic mean",
          "Result should be rounded to appropriate precision (e.g., 2 decimal places)",
          "The output should be a single numerical value representing the average number of gold medals."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Average should be a single numeric value",
            "Consider whether to average across all entries or per-country then average"
          ],
          [
            "Return a single numeric value representing the arithmetic mean",
            "Result should be rounded to appropriate precision (e.g., 2 decimal places)"
          ],
          [
            "The output should be a single numerical value representing the average number of gold medals."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5875000000000001
  },
  "dbbench-097": {
    "m_q": {
      "target_metric": {
        "value": "Avg. Finish",
        "confidence": 0.6666666666666666,
        "votes": [
          "Avg. Finish",
          "Avg. Finish",
          "average of 'Avg. Finish'"
        ]
      },
      "filters": {
        "value": [
          "Position = '70th'",
          "filter rows where 'Position' is equal to '70th'"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "Position = '70th'"
          ],
          [
            "Position = '70th'"
          ],
          [
            "filter rows where 'Position' is equal to '70th'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the average finish position for drivers who finished in 70th place in the championship?",
          "Is there only one record with Position = '70th'?",
          "Does the Avg. Finish column contain numeric values that can be averaged?",
          "What is the value of 'Avg. Finish' column where 'Position' column equals '70th'?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the average finish position for drivers who finished in 70th place in the championship?",
            "Is there only one record with Position = '70th'?",
            "Does the Avg. Finish column contain numeric values that can be averaged?"
          ],
          [
            "What is the value of 'Avg. Finish' column where 'Position' column equals '70th'?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Racing Stats"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Racing Stats"
          ],
          [
            "Racing Stats"
          ],
          [
            "Racing Stats"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "All columns are stored as object dtype, including numeric columns like Avg. Finish, Avg. Start, Winnings, Starts, Wins, Top 5, Top 10, Poles"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All columns are stored as object dtype, including numeric columns like Avg. Finish, Avg. Start, Winnings, Starts, Wins, Top 5, Top 10, Poles"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "avg. start": "position",
          "avg. finish": "position",
          "winnings": "USD",
          "starts": "count",
          "wins": "count",
          "top 5": "count",
          "top 10": "count",
          "poles": "count",
          "year": "calendar year",
          "position": "position"
        },
        "confidence": 0.7333333333333334,
        "votes": [
          {
            "Avg. Start": "position",
            "Avg. Finish": "position",
            "Winnings": "USD",
            "Starts": "count",
            "Wins": "count",
            "Top 5": "count",
            "Top 10": "count",
            "Poles": "count"
          },
          {
            "Avg. Finish": "position number",
            "Avg. Start": "position number",
            "Winnings": "US dollars",
            "Year": "calendar year"
          },
          {
            "Year": "year",
            "Starts": "count",
            "Wins": "count",
            "Top 5": "count",
            "Top 10": "count",
            "Poles": "count",
            "Avg. Start": "position",
            "Avg. Finish": "position",
            "Winnings": "USD",
            "Position": "position"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Winnings column contains currency symbols and commas that need to be removed for numeric operations",
          "Position column contains ordinal suffixes (th, rd, nd) that need to be parsed",
          "Avg. Finish and Avg. Start columns contain decimal numbers stored as strings"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Winnings column contains currency symbols and commas that need to be removed for numeric operations",
            "Position column contains ordinal suffixes (th, rd, nd) that need to be parsed",
            "Avg. Finish and Avg. Start columns contain decimal numbers stored as strings"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Position column should contain ordinal rankings",
          "Avg. Finish should be between 1 and 43 (typical NASCAR field size)",
          "Year should be between 1989 and 2018 based on sample data",
          "Starts should be less than or equal to 36 (typical NASCAR season length)",
          "Position column must match exactly '70th' (case-sensitive with ordinal suffix)",
          "Only one row expected to match Position='70th'",
          "Avg. Finish should be a numeric value",
          "The 'Position' column needs to be cleaned to remove the 'th' suffix and converted to integer for numerical comparisons if needed for other analysis."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Position column should contain ordinal rankings",
            "Avg. Finish should be between 1 and 43 (typical NASCAR field size)",
            "Year should be between 1989 and 2018 based on sample data",
            "Starts should be less than or equal to 36 (typical NASCAR season length)"
          ],
          [
            "Position column must match exactly '70th' (case-sensitive with ordinal suffix)",
            "Only one row expected to match Position='70th'",
            "Avg. Finish should be a numeric value"
          ],
          [
            "The 'Position' column needs to be cleaned to remove the 'th' suffix and converted to integer for numerical comparisons if needed for other analysis."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Position = '70th'",
          "Convert Avg. Finish from string to float",
          "Check for missing values in Avg. Finish column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Position = '70th'",
            "Convert Avg. Finish from string to float",
            "Check for missing values in Avg. Finish column"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if only one record exists for Position = '70th'",
          "Validate numeric conversion of Avg. Finish column",
          "Check for outliers in Avg. Finish values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if only one record exists for Position = '70th'",
            "Validate numeric conversion of Avg. Finish column",
            "Check for outliers in Avg. Finish values"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Return a single numeric value representing the average finish",
          "Handle potential data type conversion from string to numeric",
          "Consider if multiple records exist for Position = '70th'",
          "Return single numeric value for Avg. Finish",
          "Handle potential string-to-numeric conversion for Avg. Finish column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Return a single numeric value representing the average finish",
            "Handle potential data type conversion from string to numeric",
            "Consider if multiple records exist for Position = '70th'"
          ],
          [
            "Return single numeric value for Avg. Finish",
            "Handle potential string-to-numeric conversion for Avg. Finish column"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6116666666666667
  },
  "dbbench-098": {
    "m_q": {
      "target_metric": {
        "value": "average Total score",
        "confidence": 0.3333333333333333,
        "votes": [
          "average Total score",
          "average of the 'Total' column",
          "average of 'Total' for players named 'Hale Irwin' who have a 'Finish' of 'T52'"
        ]
      },
      "filters": {
        "value": [
          "Player = 'Hale Irwin'",
          "Finish contains 'T52'",
          "Finish = 'T52'",
          "Player is 'Hale Irwin'",
          "Finish is 'T52'"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Player = 'Hale Irwin'",
            "Finish contains 'T52'"
          ],
          [
            "Player = 'Hale Irwin'",
            "Finish = 'T52'"
          ],
          [
            "Player is 'Hale Irwin'",
            "Finish is 'T52'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Player"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Player"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the Total scores for Hale Irwin when Finish is T52?",
          "How many T52 finishes does Hale Irwin have?",
          "Are there multiple Total values for Hale Irwin's T52 finishes that need averaging?",
          "Filter rows where Player is 'Hale Irwin'",
          "Further filter where Finish is 'T52'",
          "Extract Total values from filtered rows",
          "Calculate the average of Total values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the Total scores for Hale Irwin when Finish is T52?",
            "How many T52 finishes does Hale Irwin have?",
            "Are there multiple Total values for Hale Irwin's T52 finishes that need averaging?"
          ],
          [
            "Filter rows where Player is 'Hale Irwin'",
            "Further filter where Finish is 'T52'",
            "Extract Total values from filtered rows",
            "Calculate the average of Total values"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Golf Champions"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Golf Champions"
          ],
          [
            "Golf Champions"
          ],
          [
            "Golf Champions"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "total": "golf strokes",
          "to par": "strokes relative to par",
          "year(s) won": "year(s)"
        },
        "confidence": 0.7777777777777778,
        "votes": [
          {
            "Total": "golf strokes",
            "To par": "strokes relative to par"
          },
          {
            "Total": "strokes (golf score)",
            "To par": "strokes relative to par",
            "Year(s) won": "year(s)"
          },
          {
            "Total": "strokes",
            "To par": "strokes"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Total column is stored as object/string instead of numeric",
          "To par column contains mixed formats (e.g., '+8', '-4', 'E')",
          "Year(s) won contains multiple years separated by commas",
          "Total column is stored as 'object' dtype but represents numeric golf scores",
          "To par column contains +/- symbols and is stored as 'object' dtype"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total column is stored as object/string instead of numeric",
            "To par column contains mixed formats (e.g., '+8', '-4', 'E')",
            "Year(s) won contains multiple years separated by commas"
          ],
          [
            "Total column is stored as 'object' dtype but represents numeric golf scores",
            "To par column contains +/- symbols and is stored as 'object' dtype"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 1.0,
        "votes": [
          6.0,
          6.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Total should be convertible to numeric",
          "Finish values starting with 'T' indicate tied positions",
          "Each row represents a player's performance in a specific tournament/year",
          "Player must exactly match 'Hale Irwin'",
          "Finish must exactly match 'T52'",
          "Total column must be converted to numeric type for averaging",
          "Result should handle case where no matching rows exist",
          "The 'Total' column should be numeric.",
          "The 'Finish' column should be a string."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Total should be convertible to numeric",
            "Finish values starting with 'T' indicate tied positions",
            "Each row represents a player's performance in a specific tournament/year"
          ],
          [
            "Player must exactly match 'Hale Irwin'",
            "Finish must exactly match 'T52'",
            "Total column must be converted to numeric type for averaging",
            "Result should handle case where no matching rows exist"
          ],
          [
            "The 'Total' column should be numeric.",
            "The 'Finish' column should be a string."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Need to extract numeric part from Total column",
          "Need to handle Finish values that match 'T52' pattern",
          "Row must satisfy both Player='Hale Irwin' AND Finish='T52' simultaneously"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need to extract numeric part from Total column",
            "Need to handle Finish values that match 'T52' pattern"
          ],
          [
            "Row must satisfy both Player='Hale Irwin' AND Finish='T52' simultaneously"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check if Total values for Hale Irwin's T52 finishes are consistent",
          "Verify there are multiple T52 finishes to calculate average"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check if Total values for Hale Irwin's T52 finishes are consistent",
            "Verify there are multiple T52 finishes to calculate average"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Average should be reported as a numeric value",
          "Should specify if average is based on multiple observations or single value",
          "Return a single numeric value representing the average",
          "If multiple rows match, compute arithmetic mean",
          "If no rows match, return appropriate null/empty indicator"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Average should be reported as a numeric value",
            "Should specify if average is based on multiple observations or single value"
          ],
          [
            "Return a single numeric value representing the average",
            "If multiple rows match, compute arithmetic mean",
            "If no rows match, return appropriate null/empty indicator"
          ],
          []
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5922222222222223
  },
  "dbbench-099": {
    "m_q": {
      "target_metric": {
        "value": "Compare daily mean temperature for July versus daily mean temperature for the year",
        "confidence": 0.3333333333333333,
        "votes": [
          "Compare daily mean temperature for July versus daily mean temperature for the year",
          "Comparison of daily mean temperature for July versus daily mean temperature for the entire year",
          "Compare the daily mean temperature in July to the yearly daily mean temperature."
        ]
      },
      "filters": {
        "value": [
          "Month = 'Daily mean \u00b0C (\u00b0F)'",
          "Extract July and Year columns",
          "July data from 'Jul' column",
          "Year data from 'Year' column",
          "Row containing 'Daily mean' values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Month = 'Daily mean \u00b0C (\u00b0F)'",
            "Extract July and Year columns"
          ],
          [
            "July data from 'Jul' column",
            "Year data from 'Year' column",
            "Row containing 'Daily mean' values"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Month (specifically July)",
          "Year aggregate"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Month (specifically July)",
            "Year aggregate"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the daily mean temperature for July?",
          "What is the daily mean temperature for the year?",
          "Is July's daily mean higher or lower than the yearly daily mean?",
          "Is the July daily mean higher or lower than the yearly daily mean?",
          "Calculate the daily mean temperature for July.",
          "Calculate the daily mean temperature for the entire year.",
          "Compare the two calculated means."
        ],
        "confidence": 0.42857142857142855,
        "votes": [
          [
            "What is the daily mean temperature for July?",
            "What is the daily mean temperature for the year?",
            "Is July's daily mean higher or lower than the yearly daily mean?"
          ],
          [
            "What is the daily mean temperature for July?",
            "What is the daily mean temperature for the Year?",
            "Is the July daily mean higher or lower than the yearly daily mean?"
          ],
          [
            "Calculate the daily mean temperature for July.",
            "Calculate the daily mean temperature for the entire year.",
            "Compare the two calculated means."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Climate Data"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Climate Data"
          ],
          [
            "Climate Data"
          ],
          [
            "Climate Data"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple temperature metrics (Average high, Daily mean, Average low) in same Month column",
          "Data appears to contain multiple locations/climates in same table without location identifier",
          "Multiple data sections appear to be stacked vertically in the same file representing different locations or time periods"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple temperature metrics (Average high, Daily mean, Average low) in same Month column",
            "Data appears to contain multiple locations/climates in same table without location identifier"
          ],
          [
            "Multiple data sections appear to be stacked vertically in the same file representing different locations or time periods"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "jan": "\u00b0C (\u00b0F)",
          "feb": "\u00b0C (\u00b0F)",
          "mar": "\u00b0C (\u00b0F)",
          "apr": "\u00b0C (\u00b0F)",
          "may": "\u00b0C (\u00b0F)",
          "jun": "\u00b0C (\u00b0F)",
          "jul": "\u00b0C (\u00b0F)",
          "aug": "\u00b0C (\u00b0F)",
          "sep": "\u00b0C (\u00b0F)",
          "oct": "\u00b0C (\u00b0F)",
          "nov": "\u00b0C (\u00b0F)",
          "dec": "\u00b0C (\u00b0F)",
          "year": "Int64 (likely annual average)",
          "daily mean \u00b0c (\u00b0f)": "Temperature in Celsius with Fahrenheit in parentheses"
        },
        "confidence": 0.9285714285714286,
        "votes": [
          {
            "Jan": "\u00b0C (\u00b0F)",
            "Feb": "\u00b0C (\u00b0F)",
            "Mar": "\u00b0C (\u00b0F)",
            "Apr": "\u00b0C (\u00b0F)",
            "May": "\u00b0C (\u00b0F)",
            "Jun": "\u00b0C (\u00b0F)",
            "Jul": "\u00b0C (\u00b0F)",
            "Aug": "\u00b0C (\u00b0F)",
            "Sep": "\u00b0C (\u00b0F)",
            "Oct": "\u00b0C (\u00b0F)",
            "Nov": "\u00b0C (\u00b0F)",
            "Dec": "\u00b0C (\u00b0F)",
            "Year": "Int64 (likely annual average)"
          },
          {
            "Jan": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Feb": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Mar": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Apr": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "May": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Jun": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Jul": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Aug": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Sep": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Oct": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Nov": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Dec": "\u00b0C (\u00b0F) or mm or days or hours depending on row",
            "Daily mean \u00b0C (\u00b0F)": "Temperature in Celsius with Fahrenheit in parentheses"
          },
          {
            "Jan": "degrees Celsius",
            "Feb": "degrees Celsius",
            "Mar": "degrees Celsius",
            "Apr": "degrees Celsius",
            "May": "degrees Celsius",
            "Jun": "degrees Celsius",
            "Jul": "degrees Celsius",
            "Aug": "degrees Celsius",
            "Sep": "degrees Celsius",
            "Oct": "degrees Celsius",
            "Nov": "degrees Celsius",
            "Dec": "degrees Celsius",
            "Year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Temperature values contain newline characters and parentheses (e.g., '8\\n(46)')",
          "Year column appears to be integer type while monthly columns are object/string type",
          "Temperature values contain newline characters and multiple units (\u00b0C and \u00b0F)",
          "Values are formatted as compound strings like '-5\\n(23)' requiring parsing"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Temperature values contain newline characters and parentheses (e.g., '8\\n(46)')",
            "Year column appears to be integer type while monthly columns are object/string type"
          ],
          [
            "Temperature values contain newline characters and multiple units (\u00b0C and \u00b0F)",
            "Values are formatted as compound strings like '-5\\n(23)' requiring parsing"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 14.0,
        "confidence": 1.0,
        "votes": [
          14.0,
          14.0,
          14.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Must extract numeric temperature values from strings like '8\\n(46)'",
          "Must filter to only 'Daily mean \u00b0C (\u00b0F)' rows",
          "Must handle multiple datasets in same file",
          "Must identify rows where Month column contains 'Daily mean'",
          "Must extract numeric values from compound string format (e.g., '8\\n(46)' -> 8)",
          "Must handle multiple climate data sections in the file",
          "Year column may be empty for certain metric rows",
          "Need to extract the 'Daily mean \u00b0C (\u00b0F)' row from the 'Climate Data' file.",
          "Need to parse the temperature values from strings to numeric values (Celsius).",
          "Need to handle the newline character and parentheses in the temperature strings.",
          "Need to calculate the average of the 'Daily mean \u00b0C (\u00b0F)' for July.",
          "Need to calculate the average of the 'Daily mean \u00b0C (\u00b0F)' for the entire year."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Must extract numeric temperature values from strings like '8\\n(46)'",
            "Must filter to only 'Daily mean \u00b0C (\u00b0F)' rows",
            "Must handle multiple datasets in same file"
          ],
          [
            "Must identify rows where Month column contains 'Daily mean'",
            "Must extract numeric values from compound string format (e.g., '8\\n(46)' -> 8)",
            "Must handle multiple climate data sections in the file",
            "Year column may be empty for certain metric rows"
          ],
          [
            "Need to extract the 'Daily mean \u00b0C (\u00b0F)' row from the 'Climate Data' file.",
            "Need to parse the temperature values from strings to numeric values (Celsius).",
            "Need to handle the newline character and parentheses in the temperature strings.",
            "Need to calculate the average of the 'Daily mean \u00b0C (\u00b0F)' for July.",
            "Need to calculate the average of the 'Daily mean \u00b0C (\u00b0F)' for the entire year."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter Month column to exactly 'Daily mean \u00b0C (\u00b0F)'",
          "Extract numeric portion before newline for July and Year columns",
          "Filter to rows containing 'Daily mean' in the Month column",
          "Extract Celsius value from July column for Daily mean row",
          "Extract Celsius value from Year column for Daily mean row",
          "Filter the 'Climate Data' to only include the 'Daily mean \u00b0C (\u00b0F)' row."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter Month column to exactly 'Daily mean \u00b0C (\u00b0F)'",
            "Extract numeric portion before newline for July and Year columns"
          ],
          [
            "Filter to rows containing 'Daily mean' in the Month column",
            "Extract Celsius value from July column for Daily mean row",
            "Extract Celsius value from Year column for Daily mean row"
          ],
          [
            "Filter the 'Climate Data' to only include the 'Daily mean \u00b0C (\u00b0F)' row."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Compare mean of July daily mean temperatures across datasets to mean of Year daily mean temperatures"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Compare mean of July daily mean temperatures across datasets to mean of Year daily mean temperatures"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Boolean answer: higher or lower",
          "Supporting numeric values for comparison",
          "Return comparison statement indicating whether July daily mean is higher or lower than yearly daily mean",
          "Include the actual numeric values for context",
          "Output should be a boolean indicating whether the July daily mean is higher than the yearly daily mean."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Boolean answer: higher or lower",
            "Supporting numeric values for comparison"
          ],
          [
            "Return comparison statement indicating whether July daily mean is higher or lower than yearly daily mean",
            "Include the actual numeric values for context"
          ],
          [
            "Output should be a boolean indicating whether the July daily mean is higher than the yearly daily mean."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.617857142857143
  },
  "data-sa-001": {
    "m_q": {
      "target_metric": {
        "value": "p-value for testing null hypothesis that mean number of goals in women's and men's international soccer matches is equal",
        "confidence": 0.3333333333333333,
        "votes": [
          "p-value for testing null hypothesis that mean number of goals in women's and men's international soccer matches is equal",
          "p-value from hypothesis test comparing mean number of goals in women's vs men's international soccer matches, and decision to reject or fail to reject null hypothesis at 10% significance level",
          "p-value for the null hypothesis that the mean number of goals in women's and men's international soccer matches is the same, and a decision to reject or fail to reject the null hypothesis at the 10% significance level."
        ]
      },
      "filters": {
        "value": [
          "All matches from both datasets should be included",
          "No filtering by tournament or date range specified"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All matches from both datasets should be included",
            "No filtering by tournament or date range specified"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "gender (men vs women)",
          "gender (men's vs women's matches)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "gender (men vs women)"
          ],
          [
            "gender (men's vs women's matches)"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "scalar",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate total goals per match for each dataset",
          "Compute mean goals per match for men's matches",
          "Compute mean goals per match for women's matches",
          "Perform two-sample t-test comparing means",
          "Determine if p-value < 0.10 significance level",
          "Format output with columns 'p_val' and 'result'",
          "Calculate total goals per match for men's soccer (home_score + away_score)",
          "Calculate total goals per match for women's soccer (home_score + away_score)",
          "Compute mean goals per match for men's soccer",
          "Compute mean goals per match for women's soccer",
          "Perform two-sample hypothesis test (likely t-test) to compare means",
          "Extract p-value from test",
          "Compare p-value to 0.10 significance level",
          "Determine whether to reject or fail to reject null hypothesis",
          "Calculate the total goals for each match in the men's dataset.",
          "Calculate the total goals for each match in the women's dataset.",
          "Calculate the mean number of goals in men's international soccer matches.",
          "Calculate the mean number of goals in women's international soccer matches.",
          "Perform an independent samples t-test to compare the means of the two groups.",
          "Determine the p-value from the t-test.",
          "Compare the p-value to the significance level (0.10) to decide whether to reject or fail to reject the null hypothesis."
        ],
        "confidence": 0.3333333333333332,
        "votes": [
          [
            "Calculate total goals per match for each dataset",
            "Compute mean goals per match for men's matches",
            "Compute mean goals per match for women's matches",
            "Perform two-sample t-test comparing means",
            "Determine if p-value < 0.10 significance level",
            "Format output with columns 'p_val' and 'result'"
          ],
          [
            "Calculate total goals per match for men's soccer (home_score + away_score)",
            "Calculate total goals per match for women's soccer (home_score + away_score)",
            "Compute mean goals per match for men's soccer",
            "Compute mean goals per match for women's soccer",
            "Perform two-sample hypothesis test (likely t-test) to compare means",
            "Extract p-value from test",
            "Compare p-value to 0.10 significance level",
            "Determine whether to reject or fail to reject null hypothesis"
          ],
          [
            "Calculate the total goals for each match in the men's dataset.",
            "Calculate the total goals for each match in the women's dataset.",
            "Calculate the mean number of goals in men's international soccer matches.",
            "Calculate the mean number of goals in women's international soccer matches.",
            "Perform an independent samples t-test to compare the means of the two groups.",
            "Determine the p-value from the t-test.",
            "Compare the p-value to the significance level (0.10) to decide whether to reject or fail to reject the null hypothesis."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "men_results.csv",
          "women_results.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "men_results.csv",
            "women_results.csv"
          ],
          [
            "men_results.csv",
            "women_results.csv"
          ],
          [
            "men_results.csv",
            "women_results.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Both files have identical column structures: ['Unnamed: 0', 'date', 'home_team', 'away_team', 'home_score', 'away_score', 'tournament']"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Both files have identical column structures: ['Unnamed: 0', 'date', 'home_team', 'away_team', 'home_score', 'away_score', 'tournament']"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "home_score": "goals",
          "away_score": "goals",
          "date": "YYYY-MM-DD",
          "total_goals": "goals per match"
        },
        "confidence": 0.6666666666666667,
        "votes": [
          {
            "home_score": "goals",
            "away_score": "goals",
            "date": "YYYY-MM-DD"
          },
          {
            "home_score": "goals",
            "away_score": "goals",
            "total_goals": "goals per match"
          },
          {
            "home_score": "goals",
            "away_score": "goals"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Men's dataset has 44,353 rows vs women's 4,884 rows - significant size difference",
          "Men's data spans 1872-present, women's 1969-present - different time periods"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Men's dataset has 44,353 rows vs women's 4,884 rows - significant size difference",
            "Men's data spans 1872-present, women's 1969-present - different time periods"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Different historical coverage periods",
          "Potential differences in tournament structures over time",
          "Different sample sizes: men_results.csv has 44353 matches vs women_results.csv has 4884 matches"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Different historical coverage periods",
            "Potential differences in tournament structures over time"
          ],
          [
            "Different sample sizes: men_results.csv has 44353 matches vs women_results.csv has 4884 matches"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "p_val must be float64",
          "result must be exactly 'reject' or 'fail to reject'",
          "Output file must be named 'result.csv'",
          "Significance level is 10% (alpha=0.10)",
          "Null hypothesis: mean goals in women's matches = mean goals in men's matches",
          "Alternative hypothesis: mean goals in women's matches \u2260 mean goals in men's matches",
          "Significance level: 0.10 (10%)",
          "Reject null if p-value < 0.10, otherwise fail to reject",
          "The 'home_score' and 'away_score' columns must contain non-negative integers.",
          "The 'date' column should be parsed as a date object."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "p_val must be float64",
            "result must be exactly 'reject' or 'fail to reject'",
            "Output file must be named 'result.csv'",
            "Significance level is 10% (alpha=0.10)"
          ],
          [
            "Null hypothesis: mean goals in women's matches = mean goals in men's matches",
            "Alternative hypothesis: mean goals in women's matches \u2260 mean goals in men's matches",
            "Significance level: 0.10 (10%)",
            "Reject null if p-value < 0.10, otherwise fail to reject"
          ],
          [
            "The 'home_score' and 'away_score' columns must contain non-negative integers.",
            "The 'date' column should be parsed as a date object."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Calculate total_goals = home_score + away_score for each match",
          "Separate calculations by data source (men vs women)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate total_goals = home_score + away_score for each match",
            "Separate calculations by data source (men vs women)"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Two-sample t-test for equality of means",
          "Assumption: independent samples from men's and women's matches",
          "Need to check variance equality for appropriate t-test variant",
          "Two-sample hypothesis test (likely independent samples t-test or Welch's t-test)",
          "Test compares mean total goals per match between men's and women's datasets",
          "Independent samples t-test"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Two-sample t-test for equality of means",
            "Assumption: independent samples from men's and women's matches",
            "Need to check variance equality for appropriate t-test variant"
          ],
          [
            "Two-sample hypothesis test (likely independent samples t-test or Welch's t-test)",
            "Test compares mean total goals per match between men's and women's datasets"
          ],
          [
            "Independent samples t-test"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file with columns exactly 'p_val' and 'result'",
          "Single row of output",
          "p_val formatted as float",
          "result as string literal",
          "Output file must be named 'result.csv'",
          "Must have exactly two columns: 'p_val' and 'result'",
          "p_val column contains the numeric p-value",
          "result column contains exactly one of: 'reject' or 'fail to reject'",
          "Single row of data (plus header)",
          "The output file should be named 'result.csv'.",
          "The output file should have two columns named 'p_val' and 'result'.",
          "The 'p_val' column should contain the p-value as a float.",
          "The 'result' column should contain either 'reject' or 'fail to reject'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file with columns exactly 'p_val' and 'result'",
            "Single row of output",
            "p_val formatted as float",
            "result as string literal"
          ],
          [
            "Output file must be named 'result.csv'",
            "Must have exactly two columns: 'p_val' and 'result'",
            "p_val column contains the numeric p-value",
            "result column contains exactly one of: 'reject' or 'fail to reject'",
            "Single row of data (plus header)"
          ],
          [
            "The output file should be named 'result.csv'.",
            "The output file should have two columns named 'p_val' and 'result'.",
            "The 'p_val' column should contain the p-value as a float.",
            "The 'result' column should contain either 'reject' or 'fail to reject'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5833333333333334
  },
  "data-sa-004": {
    "m_q": {
      "target_metric": {
        "value": "bike rentals (count) across different weather conditions",
        "confidence": 0.3333333333333333,
        "votes": [
          "bike rentals (count) across different weather conditions",
          "Hypothesis test comparing bike rentals (count) across different weather conditions",
          "Hypothesis test results (test type, p-value, hypothesis decision, and comment) comparing bike rentals ('count') across different weather conditions ('weather')."
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "weather"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "weather"
          ],
          [
            "weather"
          ],
          [
            "weather"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "Perform hypothesis test comparing bike rentals across weather conditions",
          "Save results to weather.csv as single row with Test Type, Hypothesis decision, P-Value, and Comment",
          "What are the distinct weather conditions in the dataset?",
          "What is the distribution of bike rentals (count) for each weather condition?",
          "Is there a statistically significant difference in bike rentals across weather conditions?",
          "What is the appropriate statistical test for comparing multiple groups?",
          "What are the null and alternative hypotheses?",
          "What is the p-value and test statistic from the hypothesis test?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Perform hypothesis test comparing bike rentals across weather conditions",
            "Save results to weather.csv as single row with Test Type, Hypothesis decision, P-Value, and Comment"
          ],
          [
            "What are the distinct weather conditions in the dataset?",
            "What is the distribution of bike rentals (count) for each weather condition?",
            "Is there a statistically significant difference in bike rentals across weather conditions?",
            "What is the appropriate statistical test for comparing multiple groups?",
            "What are the null and alternative hypotheses?",
            "What is the p-value and test statistic from the hypothesis test?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "yulu_bike_sharing_dataset.csv",
          "weather.csv"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "yulu_bike_sharing_dataset.csv"
          ],
          [
            "yulu_bike_sharing_dataset.csv",
            "weather.csv"
          ],
          [
            "yulu_bike_sharing_dataset.csv",
            "weather.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "temp": "Celsius",
          "atemp": "Celsius",
          "humidity": "percentage",
          "windspeed": "km/h",
          "casual": "count",
          "registered": "count",
          "count": "total bike rentals"
        },
        "confidence": 1.0,
        "votes": [
          {
            "temp": "Celsius",
            "atemp": "Celsius",
            "humidity": "percentage",
            "windspeed": "km/h",
            "casual": "count",
            "registered": "count",
            "count": "total bike rentals"
          },
          {
            "temp": "Celsius",
            "atemp": "Celsius (feels like temperature)",
            "humidity": "percentage",
            "windspeed": "km/h or normalized",
            "count": "number of bikes rented",
            "casual": "number of casual users",
            "registered": "number of registered users"
          },
          {
            "temp": "Celsius",
            "atemp": "Celsius",
            "humidity": "%",
            "windspeed": "km/h",
            "casual": "number of rentals",
            "registered": "number of rentals",
            "count": "number of rentals"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "weather column uses integer codes (1-4) representing different conditions",
          "season uses integer codes (1-4)",
          "holiday and workingday are binary indicators"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "weather column uses integer codes (1-4) representing different conditions",
            "season uses integer codes (1-4)",
            "holiday and workingday are binary indicators"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "weather column values should be 1-4",
          "count = casual + registered",
          "temp and atemp should be positive",
          "humidity should be 0-100",
          "weather column contains categorical codes for weather conditions",
          "count column represents total bike rentals (dependent variable)",
          "Multiple weather groups exist requiring non-parametric or ANOVA-type test",
          "Output must be a single row in weather.csv with all required fields",
          "The output must be a single row in weather.csv with columns 'Test Type', 'Kruskal Statistic', 'P-Value', 'Hypothesis', and 'Comment'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "weather column values should be 1-4",
            "count = casual + registered",
            "temp and atemp should be positive",
            "humidity should be 0-100"
          ],
          [
            "weather column contains categorical codes for weather conditions",
            "count column represents total bike rentals (dependent variable)",
            "Multiple weather groups exist requiring non-parametric or ANOVA-type test",
            "Output must be a single row in weather.csv with all required fields"
          ],
          [
            "The output must be a single row in weather.csv with columns 'Test Type', 'Kruskal Statistic', 'P-Value', 'Hypothesis', and 'Comment'."
          ]
        ]
      },
      "derived_filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Kruskal-Wallis test for comparing bike rentals across weather conditions",
          "Kruskal-Wallis H-test for comparing bike rentals across multiple weather conditions (non-parametric alternative to one-way ANOVA)",
          "Null hypothesis: The distribution of bike rentals is the same across all weather conditions",
          "Alternative hypothesis: At least one weather condition has a different distribution of bike rentals",
          "Kruskal-Wallis test to compare 'count' across different 'weather' conditions."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Kruskal-Wallis test for comparing bike rentals across weather conditions"
          ],
          [
            "Kruskal-Wallis H-test for comparing bike rentals across multiple weather conditions (non-parametric alternative to one-way ANOVA)",
            "Null hypothesis: The distribution of bike rentals is the same across all weather conditions",
            "Alternative hypothesis: At least one weather condition has a different distribution of bike rentals"
          ],
          [
            "Kruskal-Wallis test to compare 'count' across different 'weather' conditions."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single row output to weather.csv",
          "Columns: Test Type, Kruskal Statistic, P-Value, Hypothesis, Comment",
          "All fields combined into one row (not multiple rows)",
          "Output must be saved to weather.csv",
          "Must contain exactly one row (not multiple rows or list of dictionaries)",
          "Required columns: Test Type, Kruskal Statistic, P-Value, Hypothesis, Comment",
          "Test Type should specify the statistical test used",
          "Hypothesis should state the decision (reject/fail to reject null hypothesis)",
          "P-Value should be the calculated p-value from the test",
          "Comment should provide interpretation of results",
          "Follow the format specified in tips.md",
          "Output must be a single row CSV string."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single row output to weather.csv",
            "Columns: Test Type, Kruskal Statistic, P-Value, Hypothesis, Comment",
            "All fields combined into one row (not multiple rows)"
          ],
          [
            "Output must be saved to weather.csv",
            "Must contain exactly one row (not multiple rows or list of dictionaries)",
            "Required columns: Test Type, Kruskal Statistic, P-Value, Hypothesis, Comment",
            "Test Type should specify the statistical test used",
            "Hypothesis should state the decision (reject/fail to reject null hypothesis)",
            "P-Value should be the calculated p-value from the test",
            "Comment should provide interpretation of results",
            "Follow the format specified in tips.md"
          ],
          [
            "Output must be a single row CSV string."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5750000000000001
  },
  "data-sa-029": {
    "m_q": {
      "target_metric": {
        "value": "beak length-to-depth ratio (length/depth) for each bird, mean ratio per year, and 99% confidence intervals using bootstrap replicates",
        "confidence": 0.3333333333333333,
        "votes": [
          "beak length-to-depth ratio (length/depth) for each bird, mean ratio per year, and 99% confidence intervals using bootstrap replicates",
          "Mean beak length-to-depth ratio with 99% confidence intervals using bootstrap replicates (n=10,000) for G. scandens birds in 1975 and 2012",
          "Mean beak length-to-depth ratio and 99% confidence interval for G. scandens birds in 1975 and 2012."
        ]
      },
      "filters": {
        "value": [
          "species = G. scandens",
          "location = Daphne Major",
          "years = 1975 and 2012",
          "Species == 'G. scandens'",
          "Location == 'Daphne Major'",
          "Year in [1975, 2012]"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "species = G. scandens",
            "location = Daphne Major",
            "years = 1975 and 2012"
          ],
          [
            "Species == 'G. scandens'",
            "Location == 'Daphne Major'",
            "Year in [1975, 2012]"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Year"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Year"
          ],
          [
            "Year"
          ],
          [
            "Year"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Compute individual bird ratios",
          "Calculate mean ratio per year",
          "Generate 10,000 bootstrap replicates per year",
          "Compute 99% confidence intervals from bootstrap distribution",
          "Output results to result.csv",
          "Compute beak length-to-depth ratio for each individual bird",
          "Calculate mean ratio for 1975",
          "Calculate mean ratio for 2012",
          "Generate 10,000 bootstrap replicates for 1975 data",
          "Generate 10,000 bootstrap replicates for 2012 data",
          "Compute 99% confidence intervals from bootstrap distributions",
          "Format results as specified in sample_result.csv",
          "Calculate the beak length-to-depth ratio for each bird in 1975 and 2012.",
          "Calculate the mean beak length-to-depth ratio for each year (1975 and 2012).",
          "Compute the 99% confidence interval for the mean beak length-to-depth ratio for each year using bootstrap replicates (size = 10,000)."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Compute individual bird ratios",
            "Calculate mean ratio per year",
            "Generate 10,000 bootstrap replicates per year",
            "Compute 99% confidence intervals from bootstrap distribution",
            "Output results to result.csv"
          ],
          [
            "Compute beak length-to-depth ratio for each individual bird",
            "Calculate mean ratio for 1975",
            "Calculate mean ratio for 2012",
            "Generate 10,000 bootstrap replicates for 1975 data",
            "Generate 10,000 bootstrap replicates for 2012 data",
            "Compute 99% confidence intervals from bootstrap distributions",
            "Format results as specified in sample_result.csv"
          ],
          [
            "Calculate the beak length-to-depth ratio for each bird in 1975 and 2012.",
            "Calculate the mean beak length-to-depth ratio for each year (1975 and 2012).",
            "Compute the 99% confidence interval for the mean beak length-to-depth ratio for each year using bootstrap replicates (size = 10,000)."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "beak length data file",
          "beak depth data file",
          "result.csv",
          "1975.csv",
          "2012.csv"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "beak length data file",
            "beak depth data file"
          ],
          [
            "result.csv"
          ],
          [
            "result.csv",
            "1975.csv",
            "2012.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Year column may exist in both files or need to be merged",
          "Missing source data file containing beak measurements (length and depth) for G. scandens birds"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year column may exist in both files or need to be merged"
          ],
          [
            "Missing source data file containing beak measurements (length and depth) for G. scandens birds"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "beak_length": "mm",
          "beak_depth": "mm",
          "ratio": "unitless",
          "mean ratio": "dimensionless ratio",
          "conf": "ratio units (confidence interval width or bound)",
          "year": "year"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "beak_length": "mm",
            "beak_depth": "mm",
            "ratio": "unitless"
          },
          {
            "beak_length": "millimeters (assumed)",
            "beak_depth": "millimeters (assumed)",
            "mean ratio": "dimensionless ratio",
            "conf": "ratio units (confidence interval width or bound)"
          },
          {
            "Year": "year",
            "mean ratio": "beak length / beak depth",
            "conf": "beak length / beak depth"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Need to ensure consistent measurement units across years",
          "Check for measurement precision differences between 1975 and 2012",
          "Need to verify beak length and depth are in same units for valid ratio calculation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need to ensure consistent measurement units across years",
            "Check for measurement precision differences between 1975 and 2012"
          ],
          [
            "Need to verify beak length and depth are in same units for valid ratio calculation"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Potential measurement protocol differences between years",
          "Source data with beak measurements not provided in data files list"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Potential measurement protocol differences between years"
          ],
          [
            "Source data with beak measurements not provided in data files list"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "NaN",
          "null"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.36363636363636365,
        "votes": [
          10.0,
          3.0,
          0.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Bootstrap size = 10,000 replicates",
          "Confidence level = 99%",
          "Years limited to 1975 and 2012",
          "Only G. scandens species",
          "Only include G. scandens species",
          "Only include Daphne Major location",
          "Only include years 1975 and 2012",
          "Bootstrap replicates must equal 10,000",
          "Confidence level must be 99%",
          "Beak depth must be non-zero to compute ratio",
          "Beak length and beak depth must be positive values to calculate the ratio.",
          "Bootstrap replicates should be generated independently for each year."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Bootstrap size = 10,000 replicates",
            "Confidence level = 99%",
            "Years limited to 1975 and 2012",
            "Only G. scandens species"
          ],
          [
            "Only include G. scandens species",
            "Only include Daphne Major location",
            "Only include years 1975 and 2012",
            "Bootstrap replicates must equal 10,000",
            "Confidence level must be 99%",
            "Beak depth must be non-zero to compute ratio"
          ],
          [
            "Beak length and beak depth must be positive values to calculate the ratio.",
            "Bootstrap replicates should be generated independently for each year."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove birds with missing length or depth measurements",
          "Exclude ratio outliers if beyond biological plausibility",
          "Remove records with missing beak length values",
          "Remove records with missing beak depth values",
          "Remove records where beak depth equals zero"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove birds with missing length or depth measurements",
            "Exclude ratio outliers if beyond biological plausibility"
          ],
          [
            "Remove records with missing beak length values",
            "Remove records with missing beak depth values",
            "Remove records where beak depth equals zero"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Bootstrap confidence interval calculation",
          "Mean comparison between years",
          "Bootstrap resampling with replacement (10,000 iterations)",
          "99% confidence interval calculation (percentile method: 0.5th and 99.5th percentiles)",
          "Bootstrap confidence interval"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Bootstrap confidence interval calculation",
            "Mean comparison between years"
          ],
          [
            "Bootstrap resampling with replacement (10,000 iterations)",
            "99% confidence interval calculation (percentile method: 0.5th and 99.5th percentiles)"
          ],
          [
            "Bootstrap confidence interval"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file with columns: Year, mean ratio, conf",
          "Two rows (1975 and 2012)",
          "conf column contains confidence interval value",
          "Format matching sample_result.csv",
          "Output must be CSV format",
          "Must contain exactly 3 columns: Year, mean ratio, conf",
          "Must contain exactly 2 data rows (1975 and 2012)",
          "Year column must be integer type",
          "mean ratio column must be float type",
          "conf column must be float type",
          "Follow exact format shown in sample_result.csv",
          "The result.csv file should contain the columns 'Year', 'mean ratio', and 'conf'.",
          "The 'Year' column should contain the years 1975 and 2012.",
          "The 'mean ratio' column should contain the calculated mean beak length-to-depth ratio for each year.",
          "The 'conf' column should contain the width of the 99% confidence interval for the mean beak length-to-depth ratio for each year (e.g., half the interval width)."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "CSV file with columns: Year, mean ratio, conf",
            "Two rows (1975 and 2012)",
            "conf column contains confidence interval value",
            "Format matching sample_result.csv"
          ],
          [
            "Output must be CSV format",
            "Must contain exactly 3 columns: Year, mean ratio, conf",
            "Must contain exactly 2 data rows (1975 and 2012)",
            "Year column must be integer type",
            "mean ratio column must be float type",
            "conf column must be float type",
            "Follow exact format shown in sample_result.csv"
          ],
          [
            "The result.csv file should contain the columns 'Year', 'mean ratio', and 'conf'.",
            "The 'Year' column should contain the years 1975 and 2012.",
            "The 'mean ratio' column should contain the calculated mean beak length-to-depth ratio for each year.",
            "The 'conf' column should contain the width of the 99% confidence interval for the mean beak length-to-depth ratio for each year (e.g., half the interval width)."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5692929292929294
  },
  "data-sa-031": {
    "m_q": {
      "target_metric": {
        "value": "95% confidence interval for the reduction in deaths due to handwashing",
        "confidence": 0.3333333333333333,
        "votes": [
          "95% confidence interval for the reduction in deaths due to handwashing",
          "95% confidence interval (lower bound and upper bound) for the reduction in deaths due to handwashing using Bootstrap analysis",
          "95% confidence interval for the reduction in deaths due to handwashing using Bootstrap analysis"
        ]
      },
      "filters": {
        "value": [
          "handwashing intervention periods",
          "relevant death records"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "handwashing intervention periods",
            "relevant death records"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "handwashing status (before/after intervention)",
          "period (before/after handwashing)",
          "treatment group"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "handwashing status (before/after intervention)"
          ],
          [
            "period (before/after handwashing)",
            "treatment group"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "scalar",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the mean reduction in deaths?",
          "What is the bootstrap distribution of death reduction?",
          "What are the 2.5th and 97.5th percentiles of the bootstrap distribution?",
          "What is the death rate before handwashing was introduced?",
          "What is the death rate after handwashing was introduced?",
          "What is the absolute reduction in death rate?",
          "What is the 95% confidence interval for this reduction using Bootstrap resampling?",
          "What are the lower and upper bounds of the confidence interval?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the mean reduction in deaths?",
            "What is the bootstrap distribution of death reduction?",
            "What are the 2.5th and 97.5th percentiles of the bootstrap distribution?"
          ],
          [
            "What is the death rate before handwashing was introduced?",
            "What is the death rate after handwashing was introduced?",
            "What is the absolute reduction in death rate?",
            "What is the 95% confidence interval for this reduction using Bootstrap resampling?",
            "What are the lower and upper bounds of the confidence interval?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "result.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "result.csv"
          ],
          [
            "result.csv"
          ],
          [
            "result.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Missing actual data files containing death records and handwashing intervention data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Missing actual data files containing death records and handwashing intervention data"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "lower bound": "number of deaths reduced",
          "upper bound": "number of deaths reduced"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Lower Bound": "number of deaths reduced",
            "Upper Bound": "number of deaths reduced"
          },
          {
            "Lower Bound": "proportion or percentage reduction in deaths",
            "Upper Bound": "proportion or percentage reduction in deaths"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Unknown scale of death counts (absolute numbers or rates?)",
          "Need to determine if bounds should be expressed as proportions (0-1) or percentages (0-100)",
          "Need to clarify if reduction is absolute difference or relative reduction"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Unknown scale of death counts (absolute numbers or rates?)"
          ],
          [
            "Need to determine if bounds should be expressed as proportions (0-1) or percentages (0-100)",
            "Need to clarify if reduction is absolute difference or relative reduction"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Only result.csv provided - missing source data for bootstrap analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only result.csv provided - missing source data for bootstrap analysis"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 2.0,
        "confidence": 1.0,
        "votes": [
          2.0,
          2.0,
          2.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Need death data before and after handwashing intervention",
          "Need sufficient sample size for bootstrap resampling",
          "Confidence interval must be 95%",
          "Bootstrap analysis required (resampling with replacement)",
          "Confidence level must be 95%",
          "Lower Bound must be less than Upper Bound",
          "Both bounds should represent reduction in deaths (positive values if deaths decreased)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need death data before and after handwashing intervention",
            "Need sufficient sample size for bootstrap resampling",
            "Confidence interval must be 95%"
          ],
          [
            "Bootstrap analysis required (resampling with replacement)",
            "Confidence level must be 95%",
            "Lower Bound must be less than Upper Bound",
            "Both bounds should represent reduction in deaths (positive values if deaths decreased)"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude missing death records",
          "Match time periods for comparison"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude missing death records",
            "Match time periods for comparison"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Bootstrap resampling (typically 1000+ iterations)",
          "Percentile method for confidence interval",
          "Bootstrap resampling method to estimate sampling distribution",
          "Percentile method or BCa method for calculating confidence interval bounds",
          "Bootstrap analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Bootstrap resampling (typically 1000+ iterations)",
            "Percentile method for confidence interval"
          ],
          [
            "Bootstrap resampling method to estimate sampling distribution",
            "Percentile method or BCa method for calculating confidence interval bounds"
          ],
          [
            "Bootstrap analysis"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named result.csv",
          "Two columns: Lower Bound, Upper Bound",
          "Single row with confidence interval bounds",
          "Output must be saved to result.csv",
          "Must contain exactly 2 columns: 'Lower Bound' and 'Upper Bound'",
          "Must contain exactly 1 data row with the confidence interval bounds",
          "Values should be numeric (not object dtype)",
          "Follow the exact column naming convention provided",
          "Output must be a CSV file named `result.csv` with columns 'Lower Bound' and 'Upper Bound'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file named result.csv",
            "Two columns: Lower Bound, Upper Bound",
            "Single row with confidence interval bounds"
          ],
          [
            "Output must be saved to result.csv",
            "Must contain exactly 2 columns: 'Lower Bound' and 'Upper Bound'",
            "Must contain exactly 1 data row with the confidence interval bounds",
            "Values should be numeric (not object dtype)",
            "Follow the exact column naming convention provided"
          ],
          [
            "Output must be a CSV file named `result.csv` with columns 'Lower Bound' and 'Upper Bound'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6000000000000001
  },
  "data-sa-043": {
    "m_q": {
      "target_metric": {
        "value": "Sum of squared residuals (SSR) from regression analysis predicting quarterly minimum portfolio returns based on mortgage delinquencies",
        "confidence": 0.3333333333333333,
        "votes": [
          "Sum of squared residuals (SSR) from regression analysis predicting quarterly minimum portfolio returns based on mortgage delinquencies",
          "Sum of squared residuals (SSR) from regression predicting quarterly minimum portfolio returns using mortgage delinquencies",
          "Sum of squared residuals (SSR) from a regression analysis predicting quarterly minimum portfolio returns based on mortgage delinquencies"
        ]
      },
      "filters": {
        "value": [
          "Time period: 2005 to 2010",
          "Quarterly frequency",
          "Data from 2005 to 2010",
          "Years from 2005 to 2010"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time period: 2005 to 2010",
            "Quarterly frequency"
          ],
          [
            "Data from 2005 to 2010"
          ],
          [
            "Years from 2005 to 2010"
          ]
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the quarterly minimum portfolio returns?",
          "What are the mortgage delinquency rates?",
          "What regression model should be used?",
          "How to calculate residuals from the regression?",
          "How to compute sum of squared residuals?",
          "Identify column containing quarterly minimum portfolio returns (dependent variable)",
          "Identify column containing mortgage delinquencies (independent variable)",
          "Filter data to include only records from 2005 to 2010",
          "Perform linear regression with mortgage delinquencies as predictor",
          "Calculate residuals (actual - predicted values)",
          "Square each residual and sum them to get SSR",
          "What are the relevant data files containing mortgage delinquencies and portfolio returns?",
          "How are the quarterly minimum portfolio returns calculated?",
          "What is the independent variable (mortgage delinquencies) and dependent variable (quarterly minimum portfolio returns) in the regression?",
          "How to perform the regression analysis?",
          "How to calculate the sum of squared residuals from the regression results?"
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "What are the quarterly minimum portfolio returns?",
            "What are the mortgage delinquency rates?",
            "What regression model should be used?",
            "How to calculate residuals from the regression?",
            "How to compute sum of squared residuals?"
          ],
          [
            "Identify column containing quarterly minimum portfolio returns (dependent variable)",
            "Identify column containing mortgage delinquencies (independent variable)",
            "Filter data to include only records from 2005 to 2010",
            "Perform linear regression with mortgage delinquencies as predictor",
            "Calculate residuals (actual - predicted values)",
            "Square each residual and sum them to get SSR"
          ],
          [
            "What are the relevant data files containing mortgage delinquencies and portfolio returns?",
            "How are the quarterly minimum portfolio returns calculated?",
            "What is the independent variable (mortgage delinquencies) and dependent variable (quarterly minimum portfolio returns) in the regression?",
            "How to perform the regression analysis?",
            "How to calculate the sum of squared residuals from the regression results?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "portfolio_returns.csv",
          "mortgage_delinquencies.csv",
          "sample_result.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "portfolio_returns.csv",
            "mortgage_delinquencies.csv"
          ],
          [
            "sample_result.csv"
          ],
          []
        ]
      },
      "schema_conflicts": {
        "value": [
          "Missing actual data files - only sample_result.csv provided",
          "Column names and structure unknown for source files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Missing actual data files - only sample_result.csv provided",
            "Column names and structure unknown for source files"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "sum-of-squared residuals": "unitless (squared return units)",
          "quarterly minimum portfolio returns": "percentage or decimal",
          "mortgage delinquencies": "percentage or count"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "Sum-of-squared residuals": "unitless (squared return units)",
            "quarterly minimum portfolio returns": "percentage or decimal",
            "mortgage delinquencies": "percentage or count"
          },
          {
            "Sum-of-squared residuals": "squared units of portfolio returns"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Time scale: quarterly vs annual vs monthly",
          "Return scale: percentage vs decimal",
          "Delinquency scale: rate vs count"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time scale: quarterly vs annual vs monthly",
            "Return scale: percentage vs decimal",
            "Delinquency scale: rate vs count"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Time alignment: quarters must match between portfolio returns and delinquency data",
          "Missing values handling for incomplete quarters"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time alignment: quarters must match between portfolio returns and delinquency data",
            "Missing values handling for incomplete quarters"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "NaN",
          "null"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "NaN",
            "null"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 1.0,
        "confidence": 0.33,
        "votes": [
          3.0,
          1.0,
          1.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Time range constraint: 2005-01-01 to 2010-12-31",
          "Quarterly aggregation required",
          "Regression residuals must be calculated before SSR",
          "SSR must be non-negative",
          "Time period must be from 2005 to 2010 inclusive",
          "Regression must use mortgage delinquencies as independent variable",
          "Dependent variable must be quarterly minimum portfolio returns",
          "Output must be saved to result.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Time range constraint: 2005-01-01 to 2010-12-31",
            "Quarterly aggregation required",
            "Regression residuals must be calculated before SSR",
            "SSR must be non-negative"
          ],
          [
            "Time period must be from 2005 to 2010 inclusive",
            "Regression must use mortgage delinquencies as independent variable",
            "Dependent variable must be quarterly minimum portfolio returns",
            "Output must be saved to result.csv"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Filter to quarterly frequency",
          "Exclude quarters outside 2005-2010 range",
          "Handle missing values in either returns or delinquency data",
          "Extract year from date column and filter for year >= 2005 and year <= 2010",
          "Year >= 2005",
          "Year <= 2010"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter to quarterly frequency",
            "Exclude quarters outside 2005-2010 range",
            "Handle missing values in either returns or delinquency data"
          ],
          [
            "Extract year from date column and filter for year >= 2005 and year <= 2010"
          ],
          [
            "Year >= 2005",
            "Year <= 2010"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Regression significance test",
          "Residual normality test",
          "Homoscedasticity test",
          "Autocorrelation test for time series",
          "Linear regression analysis",
          "Calculate SSR = sum((y_actual - y_predicted)^2)",
          "Regression analysis",
          "SSR calculation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Regression significance test",
            "Residual normality test",
            "Homoscedasticity test",
            "Autocorrelation test for time series"
          ],
          [
            "Linear regression analysis",
            "Calculate SSR = sum((y_actual - y_predicted)^2)"
          ],
          [
            "Regression analysis",
            "SSR calculation"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Single CSV file named result.csv",
          "Single column: 'Sum-of-squared residuals'",
          "Single row with SSR value",
          "Float64 data type",
          "Save result to result.csv",
          "Single column named 'Sum-of-squared residuals'",
          "Single row containing the SSR value",
          "Save the SSR value to result.csv with column name 'Sum-of-squared residuals'"
        ],
        "confidence": 0.375,
        "votes": [
          [
            "Single CSV file named result.csv",
            "Single column: 'Sum-of-squared residuals'",
            "Single row with SSR value",
            "Float64 data type"
          ],
          [
            "Save result to result.csv",
            "Single column named 'Sum-of-squared residuals'",
            "Single row containing the SSR value",
            "Float64 data type"
          ],
          [
            "Save the SSR value to result.csv with column name 'Sum-of-squared residuals'"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5108055555555556
  },
  "di-text-001": {
    "m_q": {
      "target_metric": {
        "value": "Countries with highest and lowest population densities after imputing missing values with mean",
        "confidence": 0.3333333333333333,
        "votes": [
          "Countries with highest and lowest population densities after imputing missing values with mean",
          "Countries with highest and lowest population densities after filling missing density values with mean",
          "Identify countries with highest and lowest population density after imputing missing values with the mean."
        ]
      },
      "filters": {
        "value": [
          "No explicit filters mentioned in question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No explicit filters mentioned in question"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Country"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Country"
          ],
          [
            "Country"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate mean of Density\\n(P/Km2) column",
          "Impute missing values in Density\\n(P/Km2) with mean",
          "Identify countries with maximum density value",
          "Identify countries with minimum density value",
          "What are the missing values in the Density column?",
          "What is the mean of the Density column (excluding missing values)?",
          "After imputing missing values with mean, which country has the highest population density?",
          "After imputing missing values with mean, which country has the lowest population density?",
          "Calculate the mean of 'Density\\n(P/Km2)' after converting it to numeric.",
          "Impute missing values in 'Density\\n(P/Km2)' with the calculated mean.",
          "Identify the country with the highest 'Density\\n(P/Km2)'.",
          "Identify the country with the lowest 'Density\\n(P/Km2)'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate mean of Density\\n(P/Km2) column",
            "Impute missing values in Density\\n(P/Km2) with mean",
            "Identify countries with maximum density value",
            "Identify countries with minimum density value"
          ],
          [
            "What are the missing values in the Density column?",
            "What is the mean of the Density column (excluding missing values)?",
            "After imputing missing values with mean, which country has the highest population density?",
            "After imputing missing values with mean, which country has the lowest population density?"
          ],
          [
            "Calculate the mean of 'Density\\n(P/Km2)' after converting it to numeric.",
            "Impute missing values in 'Density\\n(P/Km2)' with the calculated mean.",
            "Identify the country with the highest 'Density\\n(P/Km2)'.",
            "Identify the country with the lowest 'Density\\n(P/Km2)'."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "world-data-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "world-data-2023.csv"
          ],
          [
            "world-data-2023.csv"
          ],
          [
            "world-data-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Density\\n(P/Km2) column has object dtype but contains numeric data with commas",
          "Multiple columns have object dtype but contain numeric data (e.g., Population, GDP, Land Area(Km2))"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Density\\n(P/Km2) column has object dtype but contains numeric data with commas",
            "Multiple columns have object dtype but contain numeric data (e.g., Population, GDP, Land Area(Km2))"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "density\\n(p/km2)": "persons per square kilometer",
          "land area(km2)": "square kilometers",
          "population": "persons",
          "gdp": "currency units",
          "density\n(p/km2)": "persons per square kilometer"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Density\\n(P/Km2)": "persons per square kilometer",
            "Land Area(Km2)": "square kilometers",
            "Population": "persons",
            "GDP": "currency units"
          },
          {
            "Density\n(P/Km2)": "persons per square kilometer",
            "Land Area(Km2)": "square kilometers",
            "Population": "count"
          },
          {
            "Density\n(P/Km2)": "people/km^2",
            "Land Area(Km2)": "km^2",
            "Population": "people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Density values contain commas (e.g., '2,239' for Bahrain)",
          "Land Area values contain commas and quotes",
          "Population values contain commas",
          "Some density values may be strings with commas that need conversion",
          "Density column contains comma-separated thousands (e.g., '2,239')",
          "Population may contain comma-separated values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Density values contain commas (e.g., '2,239' for Bahrain)",
            "Land Area values contain commas and quotes",
            "Population values contain commas",
            "Some density values may be strings with commas that need conversion"
          ],
          [
            "Density column contains comma-separated thousands (e.g., '2,239')",
            "Population may contain comma-separated values"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 35.0,
        "confidence": 1.0,
        "votes": [
          35.0,
          35.0,
          35.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Density\\n(P/Km2) must be non-negative",
          "Population density should be calculated as Population/Land Area",
          "Mean imputation assumes missing values are missing at random",
          "Density values must be non-negative",
          "Country names must be unique",
          "Mean calculation should exclude non-numeric and missing values before imputation",
          "The 'Density\\n(P/Km2)' column needs to be converted to a numeric type before calculating the mean and identifying highest/lowest values.",
          "Missing values in 'Density\\n(P/Km2)' should be imputed with the mean of the non-missing values.",
          "The 'Density\\n(P/Km2)' column contains commas and needs to be cleaned before converting to numeric."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Density\\n(P/Km2) must be non-negative",
            "Population density should be calculated as Population/Land Area",
            "Mean imputation assumes missing values are missing at random"
          ],
          [
            "Density values must be non-negative",
            "Country names must be unique",
            "Mean calculation should exclude non-numeric and missing values before imputation"
          ],
          [
            "The 'Density\\n(P/Km2)' column needs to be converted to a numeric type before calculating the mean and identifying highest/lowest values.",
            "Missing values in 'Density\\n(P/Km2)' should be imputed with the mean of the non-missing values.",
            "The 'Density\\n(P/Km2)' column contains commas and needs to be cleaned before converting to numeric."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove non-numeric characters from Density\\n(P/Km2) before calculation",
          "Handle empty/missing values in Density\\n(P/Km2) column",
          "Convert Density column from object to numeric, handling commas",
          "Identify rows with missing or invalid Density values",
          "Calculate mean only from valid numeric density values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove non-numeric characters from Density\\n(P/Km2) before calculation",
            "Handle empty/missing values in Density\\n(P/Km2) column"
          ],
          [
            "Convert Density column from object to numeric, handling commas",
            "Identify rows with missing or invalid Density values",
            "Calculate mean only from valid numeric density values"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in density values",
          "Verify distribution of density values before imputation",
          "Validate that mean density is a positive number",
          "Confirm all missing values are filled after imputation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in density values",
            "Verify distribution of density values before imputation"
          ],
          [
            "Validate that mean density is a positive number",
            "Confirm all missing values are filled after imputation"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "JSON with 'highest country' and 'lowest country' arrays",
          "Country names should match original data",
          "Output must be valid JSON",
          "JSON structure: {\"highest country\": [...], \"lowest country\": [...]}",
          "Country names should be returned as list values",
          "Lists should contain country names as strings",
          "Output should be a JSON object with 'highest country' and 'lowest country' keys, each containing a list of countries."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "JSON with 'highest country' and 'lowest country' arrays",
            "Country names should match original data"
          ],
          [
            "Output must be valid JSON",
            "JSON structure: {\"highest country\": [...], \"lowest country\": [...]}",
            "Country names should be returned as list values",
            "Lists should contain country names as strings"
          ],
          [
            "Output should be a JSON object with 'highest country' and 'lowest country' keys, each containing a list of countries."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6083333333333335
  },
  "di-text-002": {
    "m_q": {
      "target_metric": {
        "value": "Country with highest Agricultural Land (%) after filling missing values with mean",
        "confidence": 0.3333333333333333,
        "votes": [
          "Country with highest Agricultural Land (%) after filling missing values with mean",
          "Country with highest percentage of agricultural land",
          "Country with the highest percentage of agricultural land after imputing missing values with the mean."
        ]
      },
      "filters": {
        "value": [
          "Remove rows where 'Agricultural Land( %)' is completely missing or invalid"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows where 'Agricultural Land( %)' is completely missing or invalid"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Country"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Country"
          ],
          [
            "Country"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 0.6666666666666666,
        "votes": [
          "scalar",
          "scalar",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate mean of 'Agricultural Land( %)' column",
          "Fill missing values with calculated mean",
          "Find maximum value of 'Agricultural Land( %)'",
          "Identify country with maximum value",
          "What is the mean value of Agricultural Land( %) for non-missing values?",
          "Which values in Agricultural Land( %) are missing?",
          "After filling missing values with mean, which country has the maximum Agricultural Land( %)?",
          "What is the Agricultural Land( %) value for the country with highest percentage?",
          "Calculate the mean of 'Agricultural Land( %)'",
          "Impute missing values in 'Agricultural Land( %)' with the calculated mean",
          "Find the maximum value in the imputed 'Agricultural Land( %)' column",
          "Identify the country corresponding to the maximum 'Agricultural Land( %)'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate mean of 'Agricultural Land( %)' column",
            "Fill missing values with calculated mean",
            "Find maximum value of 'Agricultural Land( %)'",
            "Identify country with maximum value"
          ],
          [
            "What is the mean value of Agricultural Land( %) for non-missing values?",
            "Which values in Agricultural Land( %) are missing?",
            "After filling missing values with mean, which country has the maximum Agricultural Land( %)?",
            "What is the Agricultural Land( %) value for the country with highest percentage?"
          ],
          [
            "Calculate the mean of 'Agricultural Land( %)'",
            "Impute missing values in 'Agricultural Land( %)' with the calculated mean",
            "Find the maximum value in the imputed 'Agricultural Land( %)' column",
            "Identify the country corresponding to the maximum 'Agricultural Land( %)'"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "world-data-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "world-data-2023.csv"
          ],
          [
            "world-data-2023.csv"
          ],
          [
            "world-data-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "'Agricultural Land( %)' column has dtype 'object' but contains percentage values with '%' symbol",
          "Multiple numeric columns stored as 'object' dtype (e.g., 'Density\\n(P/Km2)', 'Land Area(Km2)', 'GDP')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "'Agricultural Land( %)' column has dtype 'object' but contains percentage values with '%' symbol",
            "Multiple numeric columns stored as 'object' dtype (e.g., 'Density\\n(P/Km2)', 'Land Area(Km2)', 'GDP')"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "agricultural land( %)": "percentage",
          "land area(km2)": "square kilometers",
          "density\\n(p/km2)": "persons per square kilometer",
          "gdp": "USD",
          "co2-emissions": "metric tons",
          "density\n(p/km2)": "persons_per_square_kilometer"
        },
        "confidence": 0.5555555555555557,
        "votes": [
          {
            "Agricultural Land( %)": "percentage",
            "Land Area(Km2)": "square kilometers",
            "Density\\n(P/Km2)": "persons per square kilometer",
            "GDP": "USD",
            "Co2-Emissions": "metric tons"
          },
          {
            "Agricultural Land( %)": "percentage",
            "Land Area(Km2)": "square_kilometers",
            "Density\n(P/Km2)": "persons_per_square_kilometer"
          },
          {
            "Agricultural Land( %)": "percentage",
            "Land Area(Km2)": "square kilometers"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Percentage values contain '%' symbol in data",
          "Numeric values contain commas and currency symbols",
          "Some values have trailing spaces (e.g., '$0.70 ')",
          "Agricultural Land( %) appears to be stored as string with % symbol, needs conversion to numeric",
          "Land Area(Km2) stored as string with comma separators, needs conversion to numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Percentage values contain '%' symbol in data",
            "Numeric values contain commas and currency symbols",
            "Some values have trailing spaces (e.g., '$0.70 ')"
          ],
          [
            "Agricultural Land( %) appears to be stored as string with % symbol, needs conversion to numeric",
            "Land Area(Km2) stored as string with comma separators, needs conversion to numeric"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "null",
          "nan"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "null"
          ],
          [
            "",
            "NA",
            "N/A",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 35.0,
        "confidence": 1.0,
        "votes": [
          35.0,
          35.0,
          35.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Agricultural Land( %) must be between 0 and 100",
          "Country names must be unique",
          "Percentage values must be cleaned before calculation",
          "Agricultural Land( %) should be between 0 and 100",
          "Each country should appear only once in the dataset",
          "Missing values must be filled with mean before identifying maximum",
          "The 'Agricultural Land( %)' column may contain missing values that need to be imputed.",
          "The 'Agricultural Land( %)' column is stored as text and needs to be converted to a numeric type for calculations.",
          "The 'Land Area(Km2)' column is stored as text and needs to be converted to a numeric type for calculations.",
          "The 'Population' column is stored as text and needs to be converted to a numeric type for calculations."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Agricultural Land( %) must be between 0 and 100",
            "Country names must be unique",
            "Percentage values must be cleaned before calculation"
          ],
          [
            "Agricultural Land( %) should be between 0 and 100",
            "Each country should appear only once in the dataset",
            "Missing values must be filled with mean before identifying maximum"
          ],
          [
            "The 'Agricultural Land( %)' column may contain missing values that need to be imputed.",
            "The 'Agricultural Land( %)' column is stored as text and needs to be converted to a numeric type for calculations.",
            "The 'Land Area(Km2)' column is stored as text and needs to be converted to a numeric type for calculations.",
            "The 'Population' column is stored as text and needs to be converted to a numeric type for calculations."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove '%' symbol from 'Agricultural Land( %)' values",
          "Convert cleaned values to numeric type",
          "Exclude non-numeric values after cleaning",
          "Convert Agricultural Land( %) from string to numeric by removing % symbol",
          "Handle missing/null values in Agricultural Land( %) column",
          "Calculate mean of non-missing Agricultural Land( %) values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove '%' symbol from 'Agricultural Land( %)' values",
            "Convert cleaned values to numeric type",
            "Exclude non-numeric values after cleaning"
          ],
          [
            "Convert Agricultural Land( %) from string to numeric by removing % symbol",
            "Handle missing/null values in Agricultural Land( %) column",
            "Calculate mean of non-missing Agricultural Land( %) values"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Agricultural Land( %) distribution",
          "Verify mean calculation excludes missing values before imputation",
          "Verify mean calculation excludes missing values",
          "Confirm maximum value after imputation is valid"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Agricultural Land( %) distribution",
            "Verify mean calculation excludes missing values before imputation"
          ],
          [
            "Verify mean calculation excludes missing values",
            "Confirm maximum value after imputation is valid"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output must be in specified JSON format with 'highest country' and 'Agricultural Land %' keys",
          "Agricultural Land % should be formatted as percentage",
          "Output must be in exact JSON format with keys 'highest country' and 'Agricultural Land %'",
          "Values should be provided as lists even for single values",
          "Agricultural Land % should be numeric value (without % symbol in output)",
          "Output the country with the highest agricultural land percentage and the corresponding percentage."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Output must be in specified JSON format with 'highest country' and 'Agricultural Land %' keys",
            "Agricultural Land % should be formatted as percentage"
          ],
          [
            "Output must be in exact JSON format with keys 'highest country' and 'Agricultural Land %'",
            "Values should be provided as lists even for single values",
            "Agricultural Land % should be numeric value (without % symbol in output)"
          ],
          [
            "Output the country with the highest agricultural land percentage and the corresponding percentage."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5811111111111111
  },
  "di-text-003": {
    "m_q": {
      "target_metric": {
        "value": "Birth Rate",
        "confidence": 0.3333333333333333,
        "votes": [
          "Birth Rate",
          "Top 5 countries with highest birth rates and top 5 countries with lowest birth rates",
          "Top 5 countries with highest and lowest birth rates after imputing missing values with the mean."
        ]
      },
      "filters": {
        "value": [
          "No explicit filters mentioned"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No explicit filters mentioned"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Country"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Country"
          ],
          [
            "Country"
          ],
          [
            "Country"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate mean of Birth Rate column",
          "Fill missing Birth Rate values with mean",
          "Sort countries by Birth Rate descending",
          "Extract top 5 highest birth rate countries",
          "Extract top 5 lowest birth rate countries",
          "What is the mean birth rate across all countries?",
          "Which countries have missing birth rate values?",
          "Fill missing birth rate values with the calculated mean",
          "Sort countries by birth rate in descending order",
          "Identify top 5 countries with highest birth rates",
          "Identify top 5 countries with lowest birth rates",
          "Calculate the mean of 'Birth Rate'.",
          "Impute missing values in 'Birth Rate' with the calculated mean.",
          "Identify the top 5 countries with the highest 'Birth Rate'.",
          "Identify the top 5 countries with the lowest 'Birth Rate'.",
          "Sort the top 5 highest and lowest birth rate countries from highest to lowest."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Calculate mean of Birth Rate column",
            "Fill missing Birth Rate values with mean",
            "Sort countries by Birth Rate descending",
            "Extract top 5 highest birth rate countries",
            "Extract top 5 lowest birth rate countries"
          ],
          [
            "What is the mean birth rate across all countries?",
            "Which countries have missing birth rate values?",
            "Fill missing birth rate values with the calculated mean",
            "Sort countries by birth rate in descending order",
            "Identify top 5 countries with highest birth rates",
            "Identify top 5 countries with lowest birth rates"
          ],
          [
            "Calculate the mean of 'Birth Rate'.",
            "Impute missing values in 'Birth Rate' with the calculated mean.",
            "Identify the top 5 countries with the highest 'Birth Rate'.",
            "Identify the top 5 countries with the lowest 'Birth Rate'.",
            "Sort the top 5 highest and lowest birth rate countries from highest to lowest."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "world-data-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "world-data-2023.csv"
          ],
          [
            "world-data-2023.csv"
          ],
          [
            "world-data-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple columns have object dtype but contain numeric data (e.g., Density, Land Area, GDP, Population)",
          "Birth Rate column is float64 but may have missing values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple columns have object dtype but contain numeric data (e.g., Density, Land Area, GDP, Population)",
            "Birth Rate column is float64 but may have missing values"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "birth rate": "births per 1,000 population per year",
          "density": "persons per square kilometer",
          "agricultural_land_percent": "percentage",
          "land_area": "square kilometers",
          "co2_emissions": "metric tons",
          "cpi_change_percent": "percentage",
          "fertility_rate": "births per woman",
          "forested_area_percent": "percentage",
          "gasoline_price": "USD per liter",
          "gdp": "USD",
          "gross_primary_education_enrollment_percent": "percentage",
          "gross_tertiary_education_enrollment_percent": "percentage",
          "infant_mortality": "deaths per 1,000 live births",
          "life_expectancy": "years",
          "maternal_mortality_ratio": "deaths per 100,000 live births",
          "minimum_wage": "USD per hour",
          "out_of_pocket_health_expenditure": "percentage",
          "physicians_per_thousand": "physicians per 1,000 people",
          "population": "persons",
          "labor_force_participation_percent": "percentage",
          "tax_revenue_percent": "percentage",
          "total_tax_rate": "percentage",
          "unemployment_rate": "percentage",
          "urban_population": "persons"
        },
        "confidence": 0.361111111111111,
        "votes": [
          {
            "Birth Rate": "births per 1,000 population per year",
            "Density": "persons per square kilometer",
            "Agricultural_Land_percent": "percentage",
            "Land_Area": "square kilometers",
            "Co2_Emissions": "metric tons",
            "CPI_Change_percent": "percentage",
            "Fertility_Rate": "births per woman",
            "Forested_Area_percent": "percentage",
            "Gasoline_Price": "USD per liter",
            "GDP": "USD",
            "Gross_primary_education_enrollment_percent": "percentage",
            "Gross_tertiary_education_enrollment_percent": "percentage",
            "Infant_mortality": "deaths per 1,000 live births",
            "Life_expectancy": "years",
            "Maternal_mortality_ratio": "deaths per 100,000 live births",
            "Minimum_wage": "USD per hour",
            "Out_of_pocket_health_expenditure": "percentage",
            "Physicians_per_thousand": "physicians per 1,000 people",
            "Population": "persons",
            "Labor_force_participation_percent": "percentage",
            "Tax_revenue_percent": "percentage",
            "Total_tax_rate": "percentage",
            "Unemployment_rate": "percentage",
            "Urban_population": "persons"
          },
          {
            "Birth Rate": "births per 1000 population (assumed)"
          },
          {
            "Birth Rate": "births per 1,000 population"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Some numeric columns stored as objects with commas and currency symbols (e.g., GDP: '$19,101,353,833')",
          "Some percentage columns have '%' symbol in values",
          "Some columns have mixed numeric and text data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Some numeric columns stored as objects with commas and currency symbols (e.g., GDP: '$19,101,353,833')",
            "Some percentage columns have '%' symbol in values",
            "Some columns have mixed numeric and text data"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Single source file, no cross-source conflicts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single source file, no cross-source conflicts"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.8,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null",
            "NaN"
          ],
          [
            "",
            "NA",
            "N/A",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 35.0,
        "confidence": 1.0,
        "votes": [
          35.0,
          35.0,
          35.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Birth Rate must be numeric or missing",
          "Country must be unique identifier",
          "195 rows expected based on shape information",
          "Mean calculation should exclude missing values before imputation",
          "Birth Rate must be numeric (float64)",
          "Country names must be non-empty strings",
          "Must handle missing Birth Rate values before ranking",
          "The 'Birth Rate' column may contain missing values that need to be handled.",
          "The 'Birth Rate' column needs to be numeric for calculating the mean and sorting."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Birth Rate must be numeric or missing",
            "Country must be unique identifier",
            "195 rows expected based on shape information",
            "Mean calculation should exclude missing values before imputation"
          ],
          [
            "Birth Rate must be numeric (float64)",
            "Country names must be non-empty strings",
            "Must handle missing Birth Rate values before ranking"
          ],
          [
            "The 'Birth Rate' column may contain missing values that need to be handled.",
            "The 'Birth Rate' column needs to be numeric for calculating the mean and sorting."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows where Country is missing",
          "Consider only rows with valid Birth Rate values for initial mean calculation",
          "Exclude or impute missing Birth Rate values before computing rankings"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows where Country is missing",
            "Consider only rows with valid Birth Rate values for initial mean calculation"
          ],
          [
            "Exclude or impute missing Birth Rate values before computing rankings"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Birth Rate distribution",
          "Verify mean imputation doesn't distort distribution significantly",
          "Calculate mean of Birth Rate column excluding missing values",
          "Impute missing Birth Rate values with computed mean"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Birth Rate distribution",
            "Verify mean imputation doesn't distort distribution significantly"
          ],
          [
            "Calculate mean of Birth Rate column excluding missing values",
            "Impute missing Birth Rate values with computed mean"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "JSON format with two arrays: 'Top 5 countries with High Birth Rate' and 'Top 5 countries with Lowest Birth Rate'",
          "Countries should be sorted from highest to lowest birth rate within each array",
          "Output must be valid JSON format",
          "Must contain exactly two keys: 'Top 5 countries with High Birth Rate' and 'Top 5 countries with Lowest Birth Rate'",
          "Each key maps to a list of 5 country names",
          "Top 5 high birth rate countries must be sorted from highest to lowest",
          "Top 5 low birth rate countries must be sorted from highest to lowest (meaning the lowest overall appears last)",
          "Output should be a JSON object with two keys: 'Top 5 countries with High Birth Rate' and 'Top 5 countries with Lowest Birth Rate'.",
          "Each key should have a list of country names sorted from highest to lowest birth rate."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "JSON format with two arrays: 'Top 5 countries with High Birth Rate' and 'Top 5 countries with Lowest Birth Rate'",
            "Countries should be sorted from highest to lowest birth rate within each array"
          ],
          [
            "Output must be valid JSON format",
            "Must contain exactly two keys: 'Top 5 countries with High Birth Rate' and 'Top 5 countries with Lowest Birth Rate'",
            "Each key maps to a list of 5 country names",
            "Top 5 high birth rate countries must be sorted from highest to lowest",
            "Top 5 low birth rate countries must be sorted from highest to lowest (meaning the lowest overall appears last)"
          ],
          [
            "Output should be a JSON object with two keys: 'Top 5 countries with High Birth Rate' and 'Top 5 countries with Lowest Birth Rate'.",
            "Each key should have a list of country names sorted from highest to lowest birth rate."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6247222222222224
  },
  "di-text-004": {
    "m_q": {
      "target_metric": {
        "value": "most frequent experience level and its corresponding ratio as a decimal",
        "confidence": 0.3333333333333333,
        "votes": [
          "most frequent experience level and its corresponding ratio as a decimal",
          "Most frequent experience level (using label mapping from tips.md) and its ratio (frequency / total count)",
          "Most frequent transformed experience level and its ratio"
        ]
      },
      "filters": {
        "value": [
          "employment_type = 'FT' (implied from sample data)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "employment_type = 'FT' (implied from sample data)"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "experience_level",
          "experience_level_transformed"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "experience_level"
          ],
          [
            "experience_level"
          ],
          [
            "experience_level_transformed"
          ]
        ]
      },
      "output_cardinality": {
        "value": "scalar",
        "confidence": 1.0,
        "votes": [
          "scalar",
          "scalar",
          "scalar"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the label mapping from tips.md?",
          "How to transform experience_level labels using the mapping?",
          "What is the frequency distribution of transformed experience levels?",
          "Which experience level has the highest frequency?",
          "What is the ratio of that level's count to total count?",
          "What is the label mapping for experience_level in tips.md?",
          "What are the value counts for each experience_level code in ds_salaries.csv?",
          "Which experience_level code appears most frequently?",
          "What is the transformed/mapped label for the most frequent experience_level?",
          "What is the ratio of the most frequent experience_level (count / total records)?",
          "Transform the 'experience_level' column using the provided mapping.",
          "Calculate the frequency of each transformed experience level.",
          "Identify the most frequent transformed experience level.",
          "Calculate the ratio of the most frequent experience level to the total number of entries."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the label mapping from tips.md?",
            "How to transform experience_level labels using the mapping?",
            "What is the frequency distribution of transformed experience levels?",
            "Which experience level has the highest frequency?",
            "What is the ratio of that level's count to total count?"
          ],
          [
            "What is the label mapping for experience_level in tips.md?",
            "What are the value counts for each experience_level code in ds_salaries.csv?",
            "Which experience_level code appears most frequently?",
            "What is the transformed/mapped label for the most frequent experience_level?",
            "What is the ratio of the most frequent experience_level (count / total records)?"
          ],
          [
            "Transform the 'experience_level' column using the provided mapping.",
            "Calculate the frequency of each transformed experience level.",
            "Identify the most frequent transformed experience level.",
            "Calculate the ratio of the most frequent experience level to the total number of entries."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "ds_salaries.csv",
          "tips.md"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "ds_salaries.csv",
            "tips.md"
          ],
          [
            "ds_salaries.csv",
            "tips.md"
          ],
          [
            "ds_salaries.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "tips.md file not provided in data files list"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "tips.md file not provided in data files list"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "salary": "currency units vary by salary_currency",
          "salary_in_usd": "USD",
          "remote_ratio": "percentage (0-100)",
          "ratio": "decimal (proportion of total records)",
          "experience_level": "categorical code requiring mapping"
        },
        "confidence": 0.4666666666666666,
        "votes": [
          {
            "salary": "currency units vary by salary_currency",
            "salary_in_usd": "USD",
            "remote_ratio": "percentage (0-100)"
          },
          {
            "ratio": "decimal (proportion of total records)",
            "experience_level": "categorical code requiring mapping"
          },
          {
            "salary": "currency",
            "salary_in_usd": "USD"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "salary column has extreme values (e.g., 11000000 HUF) that need currency conversion",
          "salary_in_usd provides normalized values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "salary column has extreme values (e.g., 11000000 HUF) that need currency conversion",
            "salary_in_usd provides normalized values"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "tips.md content unknown - needed for label mapping",
          "experience_level codes (EN, MI, SE, EX) in ds_salaries.csv need to be mapped to full labels from tips.md"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "tips.md content unknown - needed for label mapping"
          ],
          [
            "experience_level codes (EN, MI, SE, EX) in ds_salaries.csv need to be mapped to full labels from tips.md"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "experience_level should be transformed using tips.md mapping",
          "ratio should be calculated as decimal (0-1)",
          "Only FT employment_type considered based on sample",
          "Must use label mapping from tips.md to transform experience_level codes",
          "Ratio must be calculated as decimal (not percentage)",
          "Must identify the single most frequent experience level"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "experience_level should be transformed using tips.md mapping",
            "ratio should be calculated as decimal (0-1)",
            "Only FT employment_type considered based on sample"
          ],
          [
            "Must use label mapping from tips.md to transform experience_level codes",
            "Ratio must be calculated as decimal (not percentage)",
            "Must identify the single most frequent experience level"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "employment_type = 'FT' (full-time)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "employment_type = 'FT' (full-time)"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Frequency count of transformed experience_level",
          "Maximum frequency identification"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Frequency count of transformed experience_level",
            "Maximum frequency identification"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "JSON format with 'The most frequent experience level' and 'ratio' keys",
          "ratio as decimal",
          "Output must be in exact JSON format with keys 'The most frequent experience level' and 'ratio'",
          "The most frequent experience level should be the mapped/transformed label (not the code)",
          "Ratio should be a decimal value (frequency divided by total count of 607)",
          "The most frequent experience level should be a string.",
          "The ratio should be a decimal."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "JSON format with 'The most frequent experience level' and 'ratio' keys",
            "ratio as decimal"
          ],
          [
            "Output must be in exact JSON format with keys 'The most frequent experience level' and 'ratio'",
            "The most frequent experience level should be the mapped/transformed label (not the code)",
            "Ratio should be a decimal value (frequency divided by total count of 607)"
          ],
          [
            "The most frequent experience level should be a string.",
            "The ratio should be a decimal."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5900000000000001
  },
  "dm-csv-001": {
    "m_q": {
      "target_metric": {
        "value": "Count of undefeated fighters per weight class and identification of top fighters in each weight class",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of undefeated fighters per weight class and identification of top fighters in each weight class",
          "Count of undefeated fighters (losses = 0) in each weight class",
          "Identify the top fighters in each weight class and count how many remain undefeated, writing the results to undefeated.csv"
        ]
      },
      "filters": {
        "value": [
          "losses == 0",
          "draws == 0",
          "losses = 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "losses == 0",
            "draws == 0"
          ],
          [
            "losses = 0"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "weight_in_kg",
          "weight_class"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "weight_in_kg"
          ],
          [
            "weight_class"
          ],
          [
            "weight_in_kg"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "How many fighters remain undefeated in each weight class?",
          "Which fighters have the best records in each weight class?",
          "What is the distribution of undefeated fighters across weight classes?",
          "How to determine weight class from weight_in_kg column?",
          "What defines a 'top fighter' in each weight class?",
          "How many fighters have losses = 0?",
          "What is the mapping of weight_in_kg to UFC weight classes?",
          "What format is required for the undefeated.csv output file?",
          "Determine weight classes based on weight_in_kg.",
          "Identify top fighters within each weight class based on wins.",
          "Filter for fighters with losses = 0 and draws = 0 to identify undefeated fighters.",
          "Count the number of undefeated fighters in each weight class.",
          "Format the output according to the undefeated.csv schema."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "How many fighters remain undefeated in each weight class?",
            "Which fighters have the best records in each weight class?",
            "What is the distribution of undefeated fighters across weight classes?"
          ],
          [
            "How to determine weight class from weight_in_kg column?",
            "What defines a 'top fighter' in each weight class?",
            "How many fighters have losses = 0?",
            "What is the mapping of weight_in_kg to UFC weight classes?",
            "What format is required for the undefeated.csv output file?"
          ],
          [
            "Determine weight classes based on weight_in_kg.",
            "Identify top fighters within each weight class based on wins.",
            "Filter for fighters with losses = 0 and draws = 0 to identify undefeated fighters.",
            "Count the number of undefeated fighters in each weight class.",
            "Format the output according to the undefeated.csv schema."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "ufc-fighters-statistics.csv",
          "undefeated.csv"
        ],
        "confidence": 0.8333333333333333,
        "votes": [
          [
            "ufc-fighters-statistics.csv"
          ],
          [
            "ufc-fighters-statistics.csv",
            "undefeated.csv"
          ],
          [
            "ufc-fighters-statistics.csv",
            "undefeated.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Weight class not explicitly provided in ufc-fighters-statistics.csv, must be derived from weight_in_kg",
          "Unclear what 'level' represents in undefeated.csv (likely weight class)",
          "Unclear what 'fighter_num' represents in undefeated.csv (likely count of undefeated fighters)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Weight class not explicitly provided in ufc-fighters-statistics.csv, must be derived from weight_in_kg",
            "Unclear what 'level' represents in undefeated.csv (likely weight class)",
            "Unclear what 'fighter_num' represents in undefeated.csv (likely count of undefeated fighters)"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "height_cm": "centimeters",
          "weight_in_kg": "kilograms",
          "reach_in_cm": "centimeters",
          "significant_strikes_landed_per_minute": "strikes per minute",
          "significant_striking_accuracy": "percentage",
          "significant_strikes_absorbed_per_minute": "strikes per minute",
          "significant_strike_defence": "percentage",
          "average_takedowns_landed_per_15_minutes": "takedowns per 15 minutes",
          "takedown_accuracy": "percentage",
          "takedown_defense": "percentage",
          "average_submissions_attempted_per_15_minutes": "submissions per 15 minutes"
        },
        "confidence": 0.7575757575757577,
        "votes": [
          {
            "height_cm": "centimeters",
            "weight_in_kg": "kilograms",
            "reach_in_cm": "centimeters",
            "significant_strikes_landed_per_minute": "strikes per minute",
            "significant_striking_accuracy": "percentage",
            "significant_strikes_absorbed_per_minute": "strikes per minute",
            "significant_strike_defence": "percentage",
            "average_takedowns_landed_per_15_minutes": "takedowns per 15 minutes",
            "takedown_accuracy": "percentage",
            "takedown_defense": "percentage",
            "average_submissions_attempted_per_15_minutes": "submissions per 15 minutes"
          },
          {
            "height_cm": "centimeters",
            "weight_in_kg": "kilograms",
            "reach_in_cm": "centimeters",
            "significant_strikes_landed_per_minute": "strikes/minute",
            "significant_striking_accuracy": "percentage",
            "significant_strikes_absorbed_per_minute": "strikes/minute",
            "significant_strike_defence": "percentage",
            "average_takedowns_landed_per_15_minutes": "takedowns/15min",
            "takedown_accuracy": "percentage",
            "takedown_defense": "percentage",
            "average_submissions_attempted_per_15_minutes": "submissions/15min"
          },
          {
            "height_cm": "centimeters",
            "weight_in_kg": "kilograms",
            "reach_in_cm": "centimeters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Some percentage columns may exceed 100% (e.g., takedown_accuracy shows 100.0 for multiple fighters)",
          "Weight classes in UFC are typically defined in pounds but data provides weight_in_kg",
          "Need to map weight_in_kg to standard UFC weight divisions (Flyweight, Bantamweight, Featherweight, Lightweight, Welterweight, Middleweight, Light Heavyweight, Heavyweight)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Some percentage columns may exceed 100% (e.g., takedown_accuracy shows 100.0 for multiple fighters)"
          ],
          [
            "Weight classes in UFC are typically defined in pounds but data provides weight_in_kg",
            "Need to map weight_in_kg to standard UFC weight divisions (Flyweight, Bantamweight, Featherweight, Lightweight, Welterweight, Middleweight, Light Heavyweight, Heavyweight)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 18.0,
        "confidence": 1.0,
        "votes": [
          18.0,
          18.0,
          18.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "wins >= 0",
          "losses >= 0",
          "draws >= 0",
          "weight_in_kg > 0 for active fighters",
          "height_cm > 0 for valid measurements",
          "losses = 0 to identify undefeated fighters",
          "Must derive weight class from weight_in_kg column using UFC weight class boundaries",
          "Output must be written to undefeated.csv with columns 'level' and 'fighter_num'",
          "Need to handle fighters with missing weight_in_kg values",
          "Weight classes are not explicitly defined and need to be inferred.",
          "Top fighter definition is based on wins; other metrics could be considered.",
          "The undefeated.csv file must be written to with 'level' representing the weight class and 'fighter_num' representing the number of undefeated fighters in that weight class."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "wins >= 0",
            "losses >= 0",
            "draws >= 0",
            "weight_in_kg > 0 for active fighters",
            "height_cm > 0 for valid measurements"
          ],
          [
            "losses = 0 to identify undefeated fighters",
            "Must derive weight class from weight_in_kg column using UFC weight class boundaries",
            "Output must be written to undefeated.csv with columns 'level' and 'fighter_num'",
            "Need to handle fighters with missing weight_in_kg values"
          ],
          [
            "Weight classes are not explicitly defined and need to be inferred.",
            "Top fighter definition is based on wins; other metrics could be considered.",
            "The undefeated.csv file must be written to with 'level' representing the weight class and 'fighter_num' representing the number of undefeated fighters in that weight class."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Undefeated fighters: losses == 0 AND draws == 0",
          "Active fighters: wins + losses + draws > 0",
          "Filter for losses == 0 to identify undefeated fighters",
          "Exclude fighters with null/missing weight_in_kg values",
          "Group by derived weight_class column",
          "losses = 0",
          "draws = 0"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Undefeated fighters: losses == 0 AND draws == 0",
            "Active fighters: wins + losses + draws > 0"
          ],
          [
            "Filter for losses == 0 to identify undefeated fighters",
            "Exclude fighters with null/missing weight_in_kg values",
            "Group by derived weight_class column"
          ],
          [
            "losses = 0",
            "draws = 0"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in weight_in_kg",
          "Verify percentage columns are within 0-100 range",
          "Validate date_of_birth format"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in weight_in_kg",
            "Verify percentage columns are within 0-100 range",
            "Validate date_of_birth format"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output must be written to undefeated.csv with columns: level, fighter_num",
          "Weight classes need to be standardized (e.g., Lightweight, Welterweight, etc.) from weight_in_kg values",
          "fighter_num likely represents count of undefeated fighters per weight class",
          "Output file must be named undefeated.csv",
          "Must have exactly 2 columns: level, fighter_num",
          "level should contain weight class names",
          "fighter_num should contain count of undefeated fighters in that weight class",
          "CSV format with header row",
          "One row per weight class",
          "The output must be written to undefeated.csv with columns 'level' and 'fighter_num'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Output must be written to undefeated.csv with columns: level, fighter_num",
            "Weight classes need to be standardized (e.g., Lightweight, Welterweight, etc.) from weight_in_kg values",
            "fighter_num likely represents count of undefeated fighters per weight class"
          ],
          [
            "Output file must be named undefeated.csv",
            "Must have exactly 2 columns: level, fighter_num",
            "level should contain weight class names",
            "fighter_num should contain count of undefeated fighters in that weight class",
            "CSV format with header row",
            "One row per weight class"
          ],
          [
            "The output must be written to undefeated.csv with columns 'level' and 'fighter_num'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6045454545454547
  },
  "dm-csv-007": {
    "m_q": {
      "target_metric": {
        "value": "top 10 most popular movies using weighted rating formula from wrFormula.tex, considering only movies in top 15% by vote_count",
        "confidence": 0.3333333333333333,
        "votes": [
          "top 10 most popular movies using weighted rating formula from wrFormula.tex, considering only movies in top 15% by vote_count",
          "Top 10 most popular movies calculated using a weighted rating formula from wrFormula.tex, considering only movies whose vote_count is in the top 15%",
          "Top 10 most popular movies based on a weighted rating formula, considering only movies in the top 15% of vote counts."
        ]
      },
      "filters": {
        "value": [
          "vote_count must be in top 15% of all movies",
          "only movies with sufficient votes considered",
          "vote_count >= 85th percentile threshold",
          "Movies whose vote_count is within the top 15% of all vote_counts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "vote_count must be in top 15% of all movies",
            "only movies with sufficient votes considered"
          ],
          [
            "vote_count >= 85th percentile threshold"
          ],
          [
            "Movies whose vote_count is within the top 15% of all vote_counts"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "title",
          "movie_id/id"
        ],
        "confidence": 0.5,
        "votes": [
          [
            "title",
            "movie_id/id"
          ],
          [],
          [
            "title"
          ]
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 1.0,
        "votes": [
          "list",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the weighted rating formula in wrFormula.tex?",
          "How to calculate top 15% threshold for vote_count?",
          "How to join movie data from both files?",
          "How to sort by calculated weighted rating?",
          "What is the 85th percentile threshold for vote_count?",
          "What is the weighted rating formula specified in wrFormula.tex?",
          "Which movies have vote_count values in the top 15%?",
          "How to calculate the weighted rating for each filtered movie?",
          "What are the top 10 movies by weighted rating after applying the formula?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the weighted rating formula in wrFormula.tex?",
            "How to calculate top 15% threshold for vote_count?",
            "How to join movie data from both files?",
            "How to sort by calculated weighted rating?"
          ],
          [
            "What is the 85th percentile threshold for vote_count?",
            "What is the weighted rating formula specified in wrFormula.tex?",
            "Which movies have vote_count values in the top 15%?",
            "How to calculate the weighted rating for each filtered movie?",
            "What are the top 10 movies by weighted rating after applying the formula?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "tmdb_5000_movies.csv",
          "tmdb_5000_credits.csv",
          "wrFormula.tex"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "tmdb_5000_movies.csv",
            "tmdb_5000_credits.csv",
            "wrFormula.tex"
          ],
          [
            "tmdb_5000_movies.csv",
            "tmdb_5000_credits.csv"
          ],
          [
            "tmdb_5000_movies.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "tmdb_5000_movies.csv has 'id' column while tmdb_5000_credits.csv has 'movie_id' for same identifier",
          "Both files have 'title' column but may have inconsistencies",
          "title column exists in both tmdb_5000_movies.csv and tmdb_5000_credits.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "tmdb_5000_movies.csv has 'id' column while tmdb_5000_credits.csv has 'movie_id' for same identifier",
            "Both files have 'title' column but may have inconsistencies"
          ],
          [
            "title column exists in both tmdb_5000_movies.csv and tmdb_5000_credits.csv"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "budget": "USD",
          "revenue": "USD",
          "runtime": "minutes",
          "vote_average": "scale 0-10",
          "vote_count": "count",
          "popularity": "TMDB proprietary score"
        },
        "confidence": 0.8333333333333334,
        "votes": [
          {
            "budget": "USD",
            "revenue": "USD",
            "runtime": "minutes",
            "vote_average": "scale 0-10",
            "vote_count": "count",
            "popularity": "TMDB proprietary score"
          },
          {
            "vote_count": "count",
            "vote_average": "rating_score_0_10",
            "popularity": "popularity_score",
            "budget": "dollars",
            "revenue": "dollars",
            "runtime": "minutes"
          },
          {
            "budget": "USD",
            "revenue": "USD",
            "runtime": "minutes"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "vote_average scale unknown (likely 0-10)",
          "popularity metric scale unclear",
          "budget/revenue may have currency inconsistencies",
          "vote_average appears to be on a 0-10 scale",
          "popularity is a computed score with unknown scale"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "vote_average scale unknown (likely 0-10)",
            "popularity metric scale unclear",
            "budget/revenue may have currency inconsistencies"
          ],
          [
            "vote_average appears to be on a 0-10 scale",
            "popularity is a computed score with unknown scale"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "vote_count in movies.csv vs no vote data in credits.csv",
          "title field may have different formatting between files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "vote_count in movies.csv vs no vote data in credits.csv",
            "title field may have different formatting between files"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null",
            "NaN"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 20.0,
        "confidence": 1.0,
        "votes": [
          20.0,
          20.0,
          20.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "vote_count > 0",
          "vote_average between 0 and 10",
          "runtime > 0",
          "release_date valid date",
          "top 15% vote_count threshold must be calculated from data",
          "Only include movies with vote_count in top 15% (>=85th percentile)",
          "Calculate weighted rating using formula from wrFormula.tex",
          "Select top 10 movies by calculated weighted rating",
          "Output must include movie title/name only",
          "Calculate the 15th percentile of vote_count.",
          "Filter movies based on vote_count being greater than or equal to the 15th percentile.",
          "Apply the weighted rating formula to the filtered movies.",
          "Sort movies by the weighted rating in descending order.",
          "Select the top 10 movies based on the sorted weighted rating."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "vote_count > 0",
            "vote_average between 0 and 10",
            "runtime > 0",
            "release_date valid date",
            "top 15% vote_count threshold must be calculated from data"
          ],
          [
            "Only include movies with vote_count in top 15% (>=85th percentile)",
            "Calculate weighted rating using formula from wrFormula.tex",
            "Select top 10 movies by calculated weighted rating",
            "Output must include movie title/name only"
          ],
          [
            "Calculate the 15th percentile of vote_count.",
            "Filter movies based on vote_count being greater than or equal to the 15th percentile.",
            "Apply the weighted rating formula to the filtered movies.",
            "Sort movies by the weighted rating in descending order.",
            "Select the top 10 movies based on the sorted weighted rating."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "vote_count >= percentile_85(vote_count)",
          "movies must have valid title",
          "movies must have both vote_average and vote_count",
          "Calculate 85th percentile of vote_count across all movies",
          "Filter movies where vote_count >= calculated threshold",
          "vote_count >= 15th percentile of vote_count"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "vote_count >= percentile_85(vote_count)",
            "movies must have valid title",
            "movies must have both vote_average and vote_count"
          ],
          [
            "Calculate 85th percentile of vote_count across all movies",
            "Filter movies where vote_count >= calculated threshold"
          ],
          [
            "vote_count >= 15th percentile of vote_count"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Calculate 85th percentile of vote_count",
          "Check distribution of vote_average",
          "Verify no duplicate movie_ids",
          "Percentile calculation for vote_count to determine top 15% threshold",
          "Percentile calculation for vote_count"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate 85th percentile of vote_count",
            "Check distribution of vote_average",
            "Verify no duplicate movie_ids"
          ],
          [
            "Percentile calculation for vote_count to determine top 15% threshold"
          ],
          [
            "Percentile calculation for vote_count"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file with single 'Movie' column",
          "Exactly 10 rows",
          "Sorted by calculated weighted rating descending",
          "Follow sample_result.csv format",
          "CSV format with single column 'Movie'",
          "One movie name per row",
          "Exactly 10 rows (top 10 movies)",
          "Header row must be 'Movie'",
          "Follow format specified in sample_result.csv",
          "Write the 'title' of the top 10 movies to result.csv in a column named 'Movie'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "CSV file with single 'Movie' column",
            "Exactly 10 rows",
            "Sorted by calculated weighted rating descending",
            "Follow sample_result.csv format"
          ],
          [
            "CSV format with single column 'Movie'",
            "One movie name per row",
            "Exactly 10 rows (top 10 movies)",
            "Header row must be 'Movie'",
            "Follow format specified in sample_result.csv"
          ],
          [
            "Write the 'title' of the top 10 movies to result.csv in a column named 'Movie'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6033333333333334
  },
  "dm-csv-009": {
    "m_q": {
      "target_metric": {
        "value": "Identify top 10 authors by three criteria: highest average rating, highest average price, and highest total number of ratings",
        "confidence": 0.3333333333333333,
        "votes": [
          "Identify top 10 authors by three criteria: highest average rating, highest average price, and highest total number of ratings",
          "Identify top 10 best-rated authors, most expensive authors, and authors with most ratings",
          "Top 10 best-rated authors, most expensive authors, and authors with the most ratings"
        ]
      },
      "filters": {
        "value": [
          "None specified - use all available data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "None specified - use all available data"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Author"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Author"
          ],
          [
            "Author"
          ],
          [
            "Author"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table with 10 rows and 4 columns (id, Best-Rated Author, Most Expensive Author, Most Rated Author)",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate average rating per author",
          "Calculate average price per author",
          "Calculate total 'No. of People rated' per author",
          "Rank authors for each metric",
          "Select top 10 for each metric",
          "Combine results into single table matching sample_result.csv format",
          "What are the top 10 authors with highest average Rating?",
          "What are the top 10 authors with highest average Price?",
          "What are the top 10 authors with highest total 'No. of People rated'?",
          "Identify the top 10 authors with the highest average rating.",
          "Identify the top 10 authors with the highest average price.",
          "Identify the top 10 authors with the highest total number of ratings."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate average rating per author",
            "Calculate average price per author",
            "Calculate total 'No. of People rated' per author",
            "Rank authors for each metric",
            "Select top 10 for each metric",
            "Combine results into single table matching sample_result.csv format"
          ],
          [
            "What are the top 10 authors with highest average Rating?",
            "What are the top 10 authors with highest average Price?",
            "What are the top 10 authors with highest total 'No. of People rated'?"
          ],
          [
            "Identify the top 10 authors with the highest average rating.",
            "Identify the top 10 authors with the highest average price.",
            "Identify the top 10 authors with the highest total number of ratings."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Books_df.csv",
          "Genre_df.csv",
          "Sub_Genre_df.csv"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "Books_df.csv",
            "Genre_df.csv",
            "Sub_Genre_df.csv"
          ],
          [
            "Books_df.csv"
          ],
          [
            "Books_df.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Books_df.csv has 'Main Genre' column while Genre_df.csv has 'Title' for genre names",
          "Books_df.csv has 'Sub Genre' column while Sub_Genre_df.csv has 'Title' for sub-genre names",
          "Genre and Sub_Genre dataframes appear to be metadata not needed for author analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Books_df.csv has 'Main Genre' column while Genre_df.csv has 'Title' for genre names",
            "Books_df.csv has 'Sub Genre' column while Sub_Genre_df.csv has 'Title' for sub-genre names",
            "Genre and Sub_Genre dataframes appear to be metadata not needed for author analysis"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "price": "Indian Rupees (\u20b9)",
          "rating": "Stars (0-5 scale)",
          "no. of people rated": "Count"
        },
        "confidence": 1.0,
        "votes": [
          {
            "Price": "Indian Rupees (\u20b9)",
            "Rating": "Stars (0-5 scale)",
            "No. of People rated": "Count"
          },
          {
            "Price": "Indian Rupees (\u20b9)",
            "Rating": "scale 0-5",
            "No. of People rated": "count"
          },
          {
            "Price": "\u20b9",
            "Rating": "out of 5",
            "No. of People rated": "number of people"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Price column contains currency symbol and commas (e.g., '\u20b92,132.78') - needs cleaning for numerical operations",
          "Rating is float64 but may have different scales (some ratings show 4.4, 4.5, 4.6 suggesting 0-5 scale)",
          "Price column contains currency symbol '\u20b9' and comma separators that need to be removed for numerical operations",
          "Price values need conversion from string to numeric"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Price column contains currency symbol and commas (e.g., '\u20b92,132.78') - needs cleaning for numerical operations",
            "Rating is float64 but may have different scales (some ratings show 4.4, 4.5, 4.6 suggesting 0-5 scale)"
          ],
          [
            "Price column contains currency symbol '\u20b9' and comma separators that need to be removed for numerical operations",
            "Price values need conversion from string to numeric"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Genre_df.csv and Sub_Genre_df.csv appear to be reference tables but not directly needed for author analysis based on question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Genre_df.csv and Sub_Genre_df.csv appear to be reference tables but not directly needed for author analysis based on question"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 1.0,
        "votes": [
          10.0,
          10.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Rating must be between 0 and 5",
          "Price must be non-negative",
          "No. of People rated must be non-negative integer",
          "Author names should be non-empty strings",
          "Output must have exactly 10 rows with id column from 0 to 9",
          "Each author ranking list must contain top 10 authors",
          "Rating values must be valid floats between 0 and 5",
          "Price must be positive numeric values after cleaning",
          "No. of People rated must be non-negative",
          "The 'Price' column contains currency symbols and needs to be converted to a numerical type.",
          "The 'No. of People rated' column should be treated as an integer."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Rating must be between 0 and 5",
            "Price must be non-negative",
            "No. of People rated must be non-negative integer",
            "Author names should be non-empty strings"
          ],
          [
            "Output must have exactly 10 rows with id column from 0 to 9",
            "Each author ranking list must contain top 10 authors",
            "Rating values must be valid floats between 0 and 5",
            "Price must be positive numeric values after cleaning",
            "No. of People rated must be non-negative"
          ],
          [
            "The 'Price' column contains currency symbols and needs to be converted to a numerical type.",
            "The 'No. of People rated' column should be treated as an integer."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows with missing Author",
          "Remove rows with Price = 0 or missing",
          "Consider only authors with sufficient ratings (e.g., > 0 ratings)",
          "Filter out rows where Rating is null or invalid",
          "Filter out rows where Price cannot be parsed to numeric",
          "Filter out rows where No. of People rated is null or zero for most rated calculation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with missing Author",
            "Remove rows with Price = 0 or missing",
            "Consider only authors with sufficient ratings (e.g., > 0 ratings)"
          ],
          [
            "Filter out rows where Rating is null or invalid",
            "Filter out rows where Price cannot be parsed to numeric",
            "Filter out rows where No. of People rated is null or zero for most rated calculation"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate books by same author",
          "Verify rating distribution per author",
          "Identify outliers in price data",
          "Check for authors with single vs multiple books"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate books by same author",
            "Verify rating distribution per author",
            "Identify outliers in price data",
            "Check for authors with single vs multiple books"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Must match sample_result.csv structure: 10 rows, 4 columns",
          "Columns: id (0-9), Best-Rated Author, Most Expensive Author, Most Rated Author",
          "Author names should be strings (not floats as shown in sample dtypes)",
          "Save as author.csv",
          "Output file must be named author.csv",
          "Output must have columns: id, Best-Rated Author, Most Expensive Author, Most Rated Author",
          "Output must have exactly 10 rows with id values 0-9",
          "Each cell should contain author name or be empty if no data",
          "Format must match sample_result.csv structure",
          "The output should be a CSV file named 'author.csv'.",
          "The output CSV should contain columns: 'Best-Rated Author', 'Most Expensive Author', and 'Most Rated Author'.",
          "The output should contain the top 10 authors for each category."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Must match sample_result.csv structure: 10 rows, 4 columns",
            "Columns: id (0-9), Best-Rated Author, Most Expensive Author, Most Rated Author",
            "Author names should be strings (not floats as shown in sample dtypes)",
            "Save as author.csv"
          ],
          [
            "Output file must be named author.csv",
            "Output must have columns: id, Best-Rated Author, Most Expensive Author, Most Rated Author",
            "Output must have exactly 10 rows with id values 0-9",
            "Each cell should contain author name or be empty if no data",
            "Format must match sample_result.csv structure"
          ],
          [
            "The output should be a CSV file named 'author.csv'.",
            "The output CSV should contain columns: 'Best-Rated Author', 'Most Expensive Author', and 'Most Rated Author'.",
            "The output should contain the top 10 authors for each category."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6144444444444446
  },
  "dm-csv-010": {
    "m_q": {
      "target_metric": {
        "value": "Average monthly sales volume (units sold) for each bike category",
        "confidence": 0.6666666666666666,
        "votes": [
          "Average monthly sales volume (units sold) for each bike category",
          "Average monthly sales volume (units sold) for each bike category",
          "Average monthly sales volume (number of units sold) for each bike category"
        ]
      },
      "filters": {
        "value": [
          "Only orders with order_status=4 (completed orders) should be considered",
          "Only include months where sales occurred for each category"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only orders with order_status=4 (completed orders) should be considered",
            "Only include months where sales occurred for each category"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "category_name",
          "year-month (extracted from order_date)",
          "year-month"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "category_name",
            "year-month (extracted from order_date)"
          ],
          [
            "category_name",
            "year-month"
          ],
          [
            "category_name"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the monthly total quantity sold per category?",
          "What is the mean of monthly totals across all months for each category?",
          "How should months with zero sales be handled in the average calculation?",
          "What is the total quantity of products sold for each order?",
          "What category does each product belong to?",
          "What is the order date for each order to extract year-month?",
          "What is the monthly total sales volume (sum of quantities) for each category?",
          "What is the average of monthly totals for each category across all months?",
          "Calculate the total sales volume (number of units sold) for each bike category per month.",
          "Calculate the average of the monthly sales volumes for each bike category across all months."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the monthly total quantity sold per category?",
            "What is the mean of monthly totals across all months for each category?",
            "How should months with zero sales be handled in the average calculation?"
          ],
          [
            "What is the total quantity of products sold for each order?",
            "What category does each product belong to?",
            "What is the order date for each order to extract year-month?",
            "What is the monthly total sales volume (sum of quantities) for each category?",
            "What is the average of monthly totals for each category across all months?"
          ],
          [
            "Calculate the total sales volume (number of units sold) for each bike category per month.",
            "Calculate the average of the monthly sales volumes for each bike category across all months."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "order_items.csv",
          "orders.csv",
          "products.csv",
          "categories.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "order_items.csv",
            "orders.csv",
            "products.csv",
            "categories.csv"
          ],
          [
            "order_items.csv",
            "orders.csv",
            "products.csv",
            "categories.csv"
          ],
          [
            "orders.csv",
            "order_items.csv",
            "products.csv",
            "categories.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "order_date in orders.csv is object type but should be datetime for month extraction",
          "sample_result.csv has empty values in Average Units Sold column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "order_date in orders.csv is object type but should be datetime for month extraction",
            "sample_result.csv has empty values in Average Units Sold column"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "quantity": "units",
          "list_price": "currency",
          "discount": "percentage (0-1)",
          "order_date": "date",
          "model_year": "year",
          "average units sold": "units per month"
        },
        "confidence": 0.5555555555555556,
        "votes": [
          {
            "quantity": "units",
            "list_price": "currency",
            "discount": "percentage (0-1)",
            "order_date": "date",
            "model_year": "year"
          },
          {
            "quantity": "units",
            "Average Units Sold": "units per month",
            "order_date": "date (YYYY-MM-DD)"
          },
          {
            "quantity": "units",
            "list_price": "USD"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "discount values range 0-1 (e.g., 0.2 = 20%)",
          "list_price has varying scales from 269.99 to 3999.99"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "discount values range 0-1 (e.g., 0.2 = 20%)",
            "list_price has varying scales from 269.99 to 3999.99"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "product_id exists in order_items, products, and stocks - need to ensure consistent product definitions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "product_id exists in order_items, products, and stocks - need to ensure consistent product definitions"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "null"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 0.33,
        "votes": [
          10.0,
          2.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "quantity in order_items must be positive integer",
          "order_status=4 indicates completed orders",
          "discount must be between 0 and 1 inclusive",
          "list_price must be positive",
          "Only include completed orders (order_status = 4) or all orders depending on business logic",
          "Average must be calculated as mean of monthly totals, not overall mean",
          "All 7 categories from categories.csv should be present in output",
          "Categories with no sales should have appropriate handling (0 or null)",
          "order_status in orders.csv should be a valid status (e.g., 4 for completed)",
          "quantity in order_items.csv should be a positive integer"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "quantity in order_items must be positive integer",
            "order_status=4 indicates completed orders",
            "discount must be between 0 and 1 inclusive",
            "list_price must be positive"
          ],
          [
            "Only include completed orders (order_status = 4) or all orders depending on business logic",
            "Average must be calculated as mean of monthly totals, not overall mean",
            "All 7 categories from categories.csv should be present in output",
            "Categories with no sales should have appropriate handling (0 or null)"
          ],
          [
            "order_status in orders.csv should be a valid status (e.g., 4 for completed)",
            "quantity in order_items.csv should be a positive integer"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter orders where order_status = 4",
          "Extract year-month from order_date for grouping",
          "Sum quantity per category per month",
          "Extract year-month from order_date field",
          "Group by category_name and year-month to get monthly totals",
          "Then compute mean across all months for each category"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter orders where order_status = 4",
            "Extract year-month from order_date for grouping",
            "Sum quantity per category per month"
          ],
          [
            "Extract year-month from order_date field",
            "Group by category_name and year-month to get monthly totals",
            "Then compute mean across all months for each category"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing order dates",
          "Verify no negative quantities",
          "Validate category_id references exist in categories.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing order dates",
            "Verify no negative quantities",
            "Validate category_id references exist in categories.csv"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named avg_units_sold.csv",
          "Two columns: Category and Average Units Sold",
          "7 rows (one per bike category)",
          "Average Units Sold as float with appropriate precision",
          "Match ordering/layout of sample_result.csv",
          "Output file must be named avg_units_sold.csv",
          "Must match format of sample_result.csv with columns: Category, Average Units Sold",
          "Category names must match exactly with category_name from categories.csv",
          "Average Units Sold should be numeric with appropriate decimal precision",
          "All 7 categories must be present in output: Children Bicycles, Comfort Bicycles, Cruisers Bicycles, Cyclocross Bicycles, Electric Bikes, Mountain Bikes, Road Bikes",
          "Output should be a CSV file named avg_units_sold.csv",
          "Output CSV should have columns 'Category' and 'Average Units Sold'",
          "The 'Average Units Sold' column should contain the mean of monthly sales volume for each category"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file named avg_units_sold.csv",
            "Two columns: Category and Average Units Sold",
            "7 rows (one per bike category)",
            "Average Units Sold as float with appropriate precision",
            "Match ordering/layout of sample_result.csv"
          ],
          [
            "Output file must be named avg_units_sold.csv",
            "Must match format of sample_result.csv with columns: Category, Average Units Sold",
            "Category names must match exactly with category_name from categories.csv",
            "Average Units Sold should be numeric with appropriate decimal precision",
            "All 7 categories must be present in output: Children Bicycles, Comfort Bicycles, Cruisers Bicycles, Cyclocross Bicycles, Electric Bikes, Mountain Bikes, Road Bikes"
          ],
          [
            "Output should be a CSV file named avg_units_sold.csv",
            "Output CSV should have columns 'Category' and 'Average Units Sold'",
            "The 'Average Units Sold' column should contain the mean of monthly sales volume for each category"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5970555555555557
  },
  "dm-csv-011": {
    "m_q": {
      "target_metric": {
        "value": "total quantity sold and total sales revenue for each bike category from 2016 to 2018, accounting for discounts",
        "confidence": 0.3333333333333333,
        "votes": [
          "total quantity sold and total sales revenue for each bike category from 2016 to 2018, accounting for discounts",
          "Total quantity sold and total sales revenue (after discounts) for each bike category",
          "Total quantity sold and total sales revenue"
        ]
      },
      "filters": {
        "value": [
          "order_date between 2016-01-01 and 2018-12-31",
          "order_status = 4 (completed orders based on sample data)",
          "Years from 2016 to 2018"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "order_date between 2016-01-01 and 2018-12-31",
            "order_status = 4 (completed orders based on sample data)"
          ],
          [
            "order_date between 2016-01-01 and 2018-12-31"
          ],
          [
            "Years from 2016 to 2018"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "category_name"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "category_name"
          ],
          [
            "category_name"
          ],
          [
            "category_name"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total quantity sold per category?",
          "What is the total revenue (list_price * quantity * (1 - discount)) per category?",
          "How should we handle orders with multiple items across different categories?",
          "Do we need to filter by model_year or order_date for the 2016-2018 timeframe?",
          "What is the total quantity of items sold for each bike category?",
          "What is the total sales revenue after applying discounts for each bike category?",
          "How to calculate revenue with discount: quantity * list_price * (1 - discount)",
          "Which orders fall within the 2016-2018 date range?",
          "How to map products to their categories?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is the total quantity sold per category?",
            "What is the total revenue (list_price * quantity * (1 - discount)) per category?",
            "How should we handle orders with multiple items across different categories?",
            "Do we need to filter by model_year or order_date for the 2016-2018 timeframe?"
          ],
          [
            "What is the total quantity of items sold for each bike category?",
            "What is the total sales revenue after applying discounts for each bike category?",
            "How to calculate revenue with discount: quantity * list_price * (1 - discount)",
            "Which orders fall within the 2016-2018 date range?",
            "How to map products to their categories?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "order_items.csv",
          "orders.csv",
          "products.csv",
          "categories.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "order_items.csv",
            "orders.csv",
            "products.csv",
            "categories.csv"
          ],
          [
            "orders.csv",
            "order_items.csv",
            "products.csv",
            "categories.csv"
          ],
          [
            "orders.csv",
            "order_items.csv",
            "products.csv",
            "categories.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "order_date in orders.csv is object type but should be datetime for filtering",
          "sample_result.csv has empty values but expects float64 for total_quantity and total_price"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "order_date in orders.csv is object type but should be datetime for filtering",
            "sample_result.csv has empty values but expects float64 for total_quantity and total_price"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "list_price": "USD",
          "discount": "decimal (0-1)",
          "quantity": "count",
          "total_price": "USD",
          "total_quantity": "count"
        },
        "confidence": 0.9333333333333333,
        "votes": [
          {
            "list_price": "USD",
            "discount": "decimal (0-1)",
            "quantity": "count",
            "total_price": "USD",
            "total_quantity": "count"
          },
          {
            "quantity": "count",
            "list_price": "USD",
            "discount": "decimal (0-1 representing percentage)",
            "total_price": "USD",
            "total_quantity": "count"
          },
          {
            "quantity": "number of items",
            "list_price": "USD",
            "discount": "percentage",
            "total_price": "USD"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "discount values range from 0.0 to 0.2 in sample data - need to verify full range",
          "list_price has varying precision (e.g., 599.99 vs 1549.0)",
          "discount is in decimal format (0.2 = 20% discount), not percentage",
          "revenue calculation must apply discount: quantity * list_price * (1 - discount)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "discount values range from 0.0 to 0.2 in sample data - need to verify full range",
            "list_price has varying precision (e.g., 599.99 vs 1549.0)"
          ],
          [
            "discount is in decimal format (0.2 = 20% discount), not percentage",
            "revenue calculation must apply discount: quantity * list_price * (1 - discount)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "products.csv has model_year column but orders.csv has order_date - which determines timeframe?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "products.csv has model_year column but orders.csv has order_date - which determines timeframe?"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 3.0,
        "confidence": 0.33,
        "votes": [
          3.0,
          3.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "discount >= 0 and discount <= 1",
          "quantity > 0",
          "list_price > 0",
          "order_date must be valid dates between 2016-2018",
          "category_name must match categories.csv",
          "Only include orders with order_date >= 2016-01-01 and order_date <= 2018-12-31",
          "Revenue must account for discounts: quantity * list_price * (1 - discount)",
          "All 7 bike categories from categories.csv must appear in output"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "discount >= 0 and discount <= 1",
            "quantity > 0",
            "list_price > 0",
            "order_date must be valid dates between 2016-2018",
            "category_name must match categories.csv"
          ],
          [
            "Only include orders with order_date >= 2016-01-01 and order_date <= 2018-12-31",
            "Revenue must account for discounts: quantity * list_price * (1 - discount)",
            "All 7 bike categories from categories.csv must appear in output"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "EXTRACT(YEAR FROM order_date) BETWEEN 2016 AND 2018",
          "order_status = 4 (assuming 4 means completed)",
          "Extract year from order_date and filter for years 2016, 2017, 2018",
          "order_date between 2016-01-01 and 2018-12-31"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "EXTRACT(YEAR FROM order_date) BETWEEN 2016 AND 2018",
            "order_status = 4 (assuming 4 means completed)"
          ],
          [
            "Extract year from order_date and filter for years 2016, 2017, 2018"
          ],
          [
            "order_date between 2016-01-01 and 2018-12-31"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for negative revenue values",
          "Verify discount distribution",
          "Validate date range coverage"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for negative revenue values",
            "Verify discount distribution",
            "Validate date range coverage"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Columns: total_quantity, total_price, category_name",
          "Sorted by category_name matching sample_result.csv order",
          "Numeric columns as float64",
          "No index column",
          "CSV format with header",
          "Output file must be named result.csv",
          "Column order: total_quantity, total_price, category_name",
          "Format must match sample_result.csv structure exactly",
          "Include all 7 categories even if they have zero sales",
          "Numeric values should be calculated (not empty as in sample)",
          "CSV format with header row",
          "Output to result.csv",
          "Match sample_result.csv structure and formatting"
        ],
        "confidence": 0.3589743589743589,
        "votes": [
          [
            "Columns: total_quantity, total_price, category_name",
            "Sorted by category_name matching sample_result.csv order",
            "Numeric columns as float64",
            "No index column",
            "CSV format with header"
          ],
          [
            "Output file must be named result.csv",
            "Column order: total_quantity, total_price, category_name",
            "Format must match sample_result.csv structure exactly",
            "Include all 7 categories even if they have zero sales",
            "Numeric values should be calculated (not empty as in sample)",
            "CSV format with header row"
          ],
          [
            "Output to result.csv",
            "Columns: total_quantity, total_price, category_name",
            "Match sample_result.csv structure and formatting"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6311153846153847
  },
  "dm-csv-015": {
    "m_q": {
      "target_metric": {
        "value": "Identify top player for each of four categories: games played, runs scored, hits, and home runs",
        "confidence": 0.3333333333333333,
        "votes": [
          "Identify top player for each of four categories: games played, runs scored, hits, and home runs",
          "Identify top players in four categories: most games played, most runs scored, most hits, and most home runs",
          "Identify the player with the most games played, runs, hits, and home runs."
        ]
      },
      "filters": {
        "value": [
          "Regular season data only (exclude postseason)",
          "All players considered"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Regular season data only (exclude postseason)",
            "All players considered"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "player_id"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "player_id"
          ],
          [
            "player_id"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Which player has highest total games played?",
          "Which player has highest total runs scored?",
          "Which player has highest total hits?",
          "Which player has highest total home runs?",
          "Which player has the most games played (sum of 'g' column in batting.csv)?",
          "Which player has the most runs scored (sum of 'r' column in batting.csv)?",
          "Which player has the most hits (sum of 'h' column in batting.csv)?",
          "Which player has the most home runs (sum of 'hr' column in batting.csv)?",
          "What are the full names of these players from player.csv?",
          "How to format the output with columns 'job title', 'Player_Name', and 'Batting_Table_Topper' as shown in sample_result.csv?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Which player has highest total games played?",
            "Which player has highest total runs scored?",
            "Which player has highest total hits?",
            "Which player has highest total home runs?"
          ],
          [
            "Which player has the most games played (sum of 'g' column in batting.csv)?",
            "Which player has the most runs scored (sum of 'r' column in batting.csv)?",
            "Which player has the most hits (sum of 'h' column in batting.csv)?",
            "Which player has the most home runs (sum of 'hr' column in batting.csv)?",
            "What are the full names of these players from player.csv?",
            "How to format the output with columns 'job title', 'Player_Name', and 'Batting_Table_Topper' as shown in sample_result.csv?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "batting.csv",
          "player.csv",
          "appearances.csv"
        ],
        "confidence": 0.7777777777777778,
        "votes": [
          [
            "batting.csv",
            "player.csv"
          ],
          [
            "batting.csv",
            "player.csv"
          ],
          [
            "batting.csv",
            "appearances.csv",
            "player.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "batting.csv has 'g' for games, appearances.csv has 'g_all' for games",
          "batting.csv has 'r' for runs, 'h' for hits, 'hr' for home runs"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "batting.csv has 'g' for games, appearances.csv has 'g_all' for games",
            "batting.csv has 'r' for runs, 'h' for hits, 'hr' for home runs"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "g": "games",
          "r": "runs",
          "h": "hits",
          "hr": "home runs",
          "ab": "at bats",
          "year": "season year"
        },
        "confidence": 0.7777777777777777,
        "votes": [
          {
            "g": "games",
            "r": "runs",
            "h": "hits",
            "hr": "home runs",
            "ab": "at bats",
            "year": "season year"
          },
          {
            "g": "count of games",
            "r": "count of runs",
            "h": "count of hits",
            "hr": "count of home runs"
          },
          {
            "g": "games",
            "r": "runs",
            "h": "hits",
            "hr": "home runs"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "batting.csv spans multiple years (1871+), need to aggregate across seasons",
          "Some players have multiple stints per year (stint column)",
          "Player statistics are spread across multiple years and stints, requiring aggregation by player_id"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "batting.csv spans multiple years (1871+), need to aggregate across seasons",
            "Some players have multiple stints per year (stint column)"
          ],
          [
            "Player statistics are spread across multiple years and stints, requiring aggregation by player_id"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "batting.csv has 'g' (games) but appearances.csv has 'g_all' - need to verify which represents games played for batting statistics"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "batting.csv has 'g' (games) but appearances.csv has 'g_all' - need to verify which represents games played for batting statistics"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.75,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 22.0,
        "confidence": 1.0,
        "votes": [
          22.0,
          22.0,
          22.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Sum statistics across all years for each player",
          "Use batting.csv 'g' column for games played (not appearances.csv)",
          "Exclude postseason data (batting_postseason.csv not needed)",
          "Player must have at least 1 game played",
          "Must aggregate statistics across all years and stints for each player",
          "Must find the maximum value for each of the four categories",
          "Must output exactly 4 rows in result.csv",
          "Need to aggregate 'g', 'r', 'h', and 'hr' across all years for each player.",
          "Need to find the maximum value for each of the aggregated metrics.",
          "Need to retrieve the player_id associated with each maximum value.",
          "Need to retrieve the player's name (name_first, name_last) from the player.csv file using player_id."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Sum statistics across all years for each player",
            "Use batting.csv 'g' column for games played (not appearances.csv)",
            "Exclude postseason data (batting_postseason.csv not needed)",
            "Player must have at least 1 game played"
          ],
          [
            "Must aggregate statistics across all years and stints for each player",
            "Must find the maximum value for each of the four categories",
            "Must output exactly 4 rows in result.csv"
          ],
          [
            "Need to aggregate 'g', 'r', 'h', and 'hr' across all years for each player.",
            "Need to find the maximum value for each of the aggregated metrics.",
            "Need to retrieve the player_id associated with each maximum value.",
            "Need to retrieve the player's name (name_first, name_last) from the player.csv file using player_id."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out players with null/zero games played",
          "Aggregate across multiple stints within same year",
          "Combine first and last name from player.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out players with null/zero games played",
            "Aggregate across multiple stints within same year",
            "Combine first and last name from player.csv"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in aggregated totals",
          "Verify data consistency across years",
          "Validate that sum of yearly stats equals career totals"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in aggregated totals",
            "Verify data consistency across years",
            "Validate that sum of yearly stats equals career totals"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named result.csv",
          "Columns: 'job title', 'Player_Name', 'Batting_Table_Topper'",
          "Four rows: 'Most Games Played', 'Most Runs', 'Most Hits', 'Most Home Runs'",
          "Player names should be full names (first + last)",
          "Output file must be named result.csv",
          "Must have three columns: 'job title', 'Player_Name', 'Batting_Table_Topper'",
          "job title column must contain: 'Most Games Played', 'Most Runs', 'Most Hits', 'Most Home Runs'",
          "Player_Name column should contain the full name of the top player (name_first + name_last)",
          "Batting_Table_Topper column should contain the numeric value of the statistic",
          "Must follow the exact format shown in sample_result.csv",
          "Output must be in CSV format.",
          "Output CSV must contain columns 'job title', 'Player_Name', and 'Batting_Table_Topper'.",
          "The 'job title' column should contain the category (e.g., 'Most Games Played').",
          "The 'Player_Name' column should contain the name of the player with the most in that category.",
          "The 'Batting_Table_Topper' column should contain the value of the metric for that player."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "CSV file named result.csv",
            "Columns: 'job title', 'Player_Name', 'Batting_Table_Topper'",
            "Four rows: 'Most Games Played', 'Most Runs', 'Most Hits', 'Most Home Runs'",
            "Player names should be full names (first + last)"
          ],
          [
            "Output file must be named result.csv",
            "Must have three columns: 'job title', 'Player_Name', 'Batting_Table_Topper'",
            "job title column must contain: 'Most Games Played', 'Most Runs', 'Most Hits', 'Most Home Runs'",
            "Player_Name column should contain the full name of the top player (name_first + name_last)",
            "Batting_Table_Topper column should contain the numeric value of the statistic",
            "Must follow the exact format shown in sample_result.csv"
          ],
          [
            "Output must be in CSV format.",
            "Output CSV must contain columns 'job title', 'Player_Name', and 'Batting_Table_Topper'.",
            "The 'job title' column should contain the category (e.g., 'Most Games Played').",
            "The 'Player_Name' column should contain the name of the player with the most in that category.",
            "The 'Batting_Table_Topper' column should contain the value of the metric for that player."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6152777777777778
  },
  "dm-csv-043": {
    "m_q": {
      "target_metric": {
        "value": "Retention rate for each cohort, defined as (number of unique customers who made a purchase in subsequent months) / (initial cohort size)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Retention rate for each cohort, defined as (number of unique customers who made a purchase in subsequent months) / (initial cohort size)",
          "Retention rate for each cohort by month, calculated as the ratio of unique customers who made purchases in subsequent months to the initial cohort size",
          "Retention rate for each cohort by month"
        ]
      },
      "filters": {
        "value": [
          "Valid CustomerID values (non-null)",
          "Valid InvoiceDate values (non-null)",
          "Positive Quantity values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Valid CustomerID values (non-null)",
            "Valid InvoiceDate values (non-null)",
            "Positive Quantity values"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "CohortMonth",
          "MonthOffset (1-13)",
          "MonthNumber"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "CohortMonth",
            "MonthOffset (1-13)"
          ],
          [
            "CohortMonth",
            "MonthNumber"
          ],
          [
            "CohortMonth"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the initial cohort size for each CohortMonth?",
          "How many unique customers from each cohort made purchases in each subsequent month?",
          "How to handle missing values in cohort_counts.csv (empty cells)?",
          "Should retention be calculated as percentage or ratio?",
          "What is the expected format of retension.csv output?",
          "How many unique customers returned in each subsequent month (1-13) for each cohort?",
          "What is the retention rate formula: (customers in month N / initial cohort size) * 100?",
          "How should the retention data be formatted to match the template structure?",
          "Identify unique customers for each cohort in the first month.",
          "Determine which of these customers made purchases in subsequent months.",
          "Calculate the retention rate by dividing the number of retained customers by the initial cohort size for each month."
        ],
        "confidence": 0.36363636363636365,
        "votes": [
          [
            "What is the initial cohort size for each CohortMonth?",
            "How many unique customers from each cohort made purchases in each subsequent month?",
            "How to handle missing values in cohort_counts.csv (empty cells)?",
            "Should retention be calculated as percentage or ratio?",
            "What is the expected format of retension.csv output?"
          ],
          [
            "What is the initial cohort size for each CohortMonth?",
            "How many unique customers returned in each subsequent month (1-13) for each cohort?",
            "What is the retention rate formula: (customers in month N / initial cohort size) * 100?",
            "How should the retention data be formatted to match the template structure?"
          ],
          [
            "Identify unique customers for each cohort in the first month.",
            "Determine which of these customers made purchases in subsequent months.",
            "Calculate the retention rate by dividing the number of retained customers by the initial cohort size for each month."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "online.csv",
          "cohort_counts.csv",
          "average_quantity.csv"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "online.csv",
            "cohort_counts.csv",
            "average_quantity.csv"
          ],
          [
            "cohort_counts.csv",
            "online.csv"
          ],
          [
            "online.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "cohort_counts.csv and average_quantity.csv have identical column structures but different data types (float64 vs object for CohortMonth)",
          "online.csv has InvoiceDate as object type that needs parsing",
          "cohort_counts.csv has empty cells that appear as NaN/None values",
          "cohort_counts.csv already contains aggregated cohort data with months as columns ('1'-'13'), while online.csv contains raw transaction data",
          "CohortMonth in cohort_counts.csv is a date string, while online.csv would require deriving cohort membership from InvoiceDate and CustomerID"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "cohort_counts.csv and average_quantity.csv have identical column structures but different data types (float64 vs object for CohortMonth)",
            "online.csv has InvoiceDate as object type that needs parsing",
            "cohort_counts.csv has empty cells that appear as NaN/None values"
          ],
          [
            "cohort_counts.csv already contains aggregated cohort data with months as columns ('1'-'13'), while online.csv contains raw transaction data",
            "CohortMonth in cohort_counts.csv is a date string, while online.csv would require deriving cohort membership from InvoiceDate and CustomerID"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "quantity": "count of items",
          "unitprice": "currency units",
          "cohort_counts.csv columns (1-13)": "count of unique customers",
          "average_quantity.csv columns (1-13)": "average quantity per customer",
          "cohortmonth": "date (YYYY-MM-DD format)",
          "cohort_counts columns 1-13": "count of unique customers",
          "retention_rate": "percentage (0-100)"
        },
        "confidence": 0.42857142857142855,
        "votes": [
          {
            "Quantity": "count of items",
            "UnitPrice": "currency units",
            "cohort_counts.csv columns (1-13)": "count of unique customers",
            "average_quantity.csv columns (1-13)": "average quantity per customer"
          },
          {
            "CohortMonth": "date (YYYY-MM-DD format)",
            "cohort_counts columns 1-13": "count of unique customers",
            "retention_rate": "percentage (0-100)"
          },
          {
            "Quantity": "number of items",
            "UnitPrice": "monetary unit"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "cohort_counts.csv values are floats but represent counts (should be integers)",
          "Missing values in cohort_counts.csv represented as empty strings in sample",
          "InvoiceDate needs timezone consideration (all UK in sample)",
          "Retention rates should be expressed as percentages, not decimals"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "cohort_counts.csv values are floats but represent counts (should be integers)",
            "Missing values in cohort_counts.csv represented as empty strings in sample",
            "InvoiceDate needs timezone consideration (all UK in sample)"
          ],
          [
            "Retention rates should be expressed as percentages, not decimals"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "cohort_counts.csv provides aggregated data while online.csv provides transactional data - need to verify consistency",
          "CohortMonth format differs between files (YYYY-MM-DD vs YYYY-MM-01)",
          "cohort_counts.csv contains pre-aggregated customer counts that should be used for retention calculation rather than re-aggregating from online.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "cohort_counts.csv provides aggregated data while online.csv provides transactional data - need to verify consistency",
            "CohortMonth format differs between files (YYYY-MM-DD vs YYYY-MM-01)"
          ],
          [
            "cohort_counts.csv contains pre-aggregated customer counts that should be used for retention calculation rather than re-aggregating from online.csv"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "null",
          "NaN"
        ],
        "confidence": 0.6,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "null"
          ],
          [
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 14.0,
        "confidence": 0.6666666666666667,
        "votes": [
          14.0,
          14.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "CohortMonth values must be valid dates",
          "Month offset columns (1-13) must be sequential",
          "Initial cohort size (Month1 in cohort_counts.csv) must be positive",
          "Retention rate must be between 0 and 1 (or 0-100%)",
          "CustomerID must be integer type",
          "Initial cohort size (month 1) should always have 100% retention",
          "Retention rates for subsequent months should be <= 100%",
          "Retention rates should be non-negative",
          "CohortMonth should be in YYYY-MM-DD format",
          "Missing values (NaN) should be preserved for months beyond data availability",
          "CustomerID should be unique for each customer.",
          "InvoiceDate should be within a reasonable range."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CohortMonth values must be valid dates",
            "Month offset columns (1-13) must be sequential",
            "Initial cohort size (Month1 in cohort_counts.csv) must be positive",
            "Retention rate must be between 0 and 1 (or 0-100%)",
            "CustomerID must be integer type"
          ],
          [
            "Initial cohort size (month 1) should always have 100% retention",
            "Retention rates for subsequent months should be <= 100%",
            "Retention rates should be non-negative",
            "CohortMonth should be in YYYY-MM-DD format",
            "Missing values (NaN) should be preserved for months beyond data availability"
          ],
          [
            "CustomerID should be unique for each customer.",
            "InvoiceDate should be within a reasonable range."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude transactions with Quantity <= 0",
          "Exclude transactions with UnitPrice <= 0",
          "Exclude rows with missing CustomerID",
          "Consider only first purchase month for cohort assignment",
          "Only calculate retention for months where cohort_counts data exists (not NaN)",
          "Create a CohortMonth column by extracting the year and month from the InvoiceDate for each customer's first purchase.",
          "Create a MonthOffset column representing the number of months since the CohortMonth."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude transactions with Quantity <= 0",
            "Exclude transactions with UnitPrice <= 0",
            "Exclude rows with missing CustomerID",
            "Consider only first purchase month for cohort assignment"
          ],
          [
            "Only calculate retention for months where cohort_counts data exists (not NaN)"
          ],
          [
            "Create a CohortMonth column by extracting the year and month from the InvoiceDate for each customer's first purchase.",
            "Create a MonthOffset column representing the number of months since the CohortMonth."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Verify cohort_counts.csv Month1 matches unique customers from online.csv for each cohort",
          "Check for duplicate InvoiceNo-CustomerID combinations",
          "Validate retention rates are monotonically non-increasing over time"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Verify cohort_counts.csv Month1 matches unique customers from online.csv for each cohort",
            "Check for duplicate InvoiceNo-CustomerID combinations",
            "Validate retention rates are monotonically non-increasing over time"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output file named retension.csv (note spelling)",
          "Match template format (likely similar to cohort_counts.csv structure)",
          "Include CohortMonth column",
          "Include retention rates for months 1-13",
          "Handle missing values appropriately",
          "Output file must be named 'retension.csv' (note: spelling as specified in question)",
          "First column must be 'CohortMonth'",
          "Subsequent columns should be '1', '2', '3', ... '13' representing retention percentages",
          "Retention rates should be calculated as (customers_in_month_N / customers_in_month_1) * 100",
          "Format should exactly match the template structure with 14 columns total",
          "Preserve NaN values where data is not available for newer cohorts",
          "The output CSV (retension.csv) should have 'CohortMonth' as the first column, followed by columns representing the retention rate for each subsequent month (1, 2, 3, ...).",
          "Retention rates should be represented as percentages."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Output file named retension.csv (note spelling)",
            "Match template format (likely similar to cohort_counts.csv structure)",
            "Include CohortMonth column",
            "Include retention rates for months 1-13",
            "Handle missing values appropriately"
          ],
          [
            "Output file must be named 'retension.csv' (note: spelling as specified in question)",
            "First column must be 'CohortMonth'",
            "Subsequent columns should be '1', '2', '3', ... '13' representing retention percentages",
            "Retention rates should be calculated as (customers_in_month_N / customers_in_month_1) * 100",
            "Format should exactly match the template structure with 14 columns total",
            "Preserve NaN values where data is not available for newer cohorts"
          ],
          [
            "The output CSV (retension.csv) should have 'CohortMonth' as the first column, followed by columns representing the retention rate for each subsequent month (1, 2, 3, ...).",
            "Retention rates should be represented as percentages."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5640548340548341
  },
  "dm-csv-044": {
    "m_q": {
      "target_metric": {
        "value": "average unit price paid for items by customer cohorts defined by month of first purchase, tracked over 13 months",
        "confidence": 0.3333333333333333,
        "votes": [
          "average unit price paid for items by customer cohorts defined by month of first purchase, tracked over 13 months",
          "Average unit price paid for items by customer cohorts over time",
          "Average unit price paid for items by customer groups (cohorts) defined by the month of their first purchase, tracked over time."
        ]
      },
      "filters": {
        "value": [
          "CustomerID must not be null",
          "UnitPrice > 0",
          "Quantity > 0",
          "Exclude transactions with missing CustomerID",
          "Exclude transactions with negative or zero UnitPrice",
          "Exclude transactions with negative Quantity"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CustomerID must not be null",
            "UnitPrice > 0",
            "Quantity > 0"
          ],
          [
            "Exclude transactions with missing CustomerID",
            "Exclude transactions with negative or zero UnitPrice",
            "Exclude transactions with negative Quantity"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "CohortMonth (month of first purchase)",
          "MonthSinceFirstPurchase (1-13)",
          "CohortMonth (month of customer's first purchase)",
          "Purchase month relative to cohort month (1-13)",
          "CohortMonth"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CohortMonth (month of first purchase)",
            "MonthSinceFirstPurchase (1-13)"
          ],
          [
            "CohortMonth (month of customer's first purchase)",
            "Purchase month relative to cohort month (1-13)"
          ],
          [
            "CohortMonth"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is each customer's first purchase month?",
          "For each cohort, what is the average unit price in each subsequent month?",
          "How does pricing behavior evolve over time for different cohorts?",
          "What is the first purchase date for each CustomerID?",
          "What is the cohort month (YYYY-MM-01) for each customer?",
          "For each transaction, what is the month offset from the customer's cohort month?",
          "What is the average UnitPrice for each cohort-month offset combination?",
          "How should the results be formatted to match the template structure?",
          "Determine the first purchase month for each customer (CohortMonth).",
          "Calculate the unit price for each transaction.",
          "Group transactions by CohortMonth and the month of the transaction.",
          "Calculate the average unit price for each cohort in each subsequent month.",
          "Format the output to match the provided average_price.csv template."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What is each customer's first purchase month?",
            "For each cohort, what is the average unit price in each subsequent month?",
            "How does pricing behavior evolve over time for different cohorts?"
          ],
          [
            "What is the first purchase date for each CustomerID?",
            "What is the cohort month (YYYY-MM-01) for each customer?",
            "For each transaction, what is the month offset from the customer's cohort month?",
            "What is the average UnitPrice for each cohort-month offset combination?",
            "How should the results be formatted to match the template structure?"
          ],
          [
            "Determine the first purchase month for each customer (CohortMonth).",
            "Calculate the unit price for each transaction.",
            "Group transactions by CohortMonth and the month of the transaction.",
            "Calculate the average unit price for each cohort in each subsequent month.",
            "Format the output to match the provided average_price.csv template."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "online.csv",
          "average_price.csv",
          "average_quantity.csv",
          "cohort_counts.csv"
        ],
        "confidence": 0.5833333333333333,
        "votes": [
          [
            "online.csv",
            "average_price.csv",
            "average_quantity.csv",
            "cohort_counts.csv"
          ],
          [
            "online.csv",
            "average_price.csv"
          ],
          [
            "online.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "CohortMonth column appears in three files but average_price.csv has empty values while others have data",
          "online.csv has InvoiceDate as object/string while cohort files use date format",
          "average_price.csv contains only template structure with null values that need to be populated",
          "online.csv contains raw transactional data that needs aggregation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CohortMonth column appears in three files but average_price.csv has empty values while others have data",
            "online.csv has InvoiceDate as object/string while cohort files use date format"
          ],
          [
            "average_price.csv contains only template structure with null values that need to be populated",
            "online.csv contains raw transactional data that needs aggregation"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "unitprice": "currency (likely GBP)",
          "quantity": "count of items",
          "cohortmonth": "YYYY-MM-01 format",
          "invoicedate": "YYYY-MM-DD HH:MM:SS"
        },
        "confidence": 0.8333333333333333,
        "votes": [
          {
            "UnitPrice": "currency (likely GBP)",
            "Quantity": "count of items",
            "CohortMonth": "YYYY-MM-01 format",
            "InvoiceDate": "YYYY-MM-DD HH:MM:SS"
          },
          {
            "UnitPrice": "currency (presumably GBP based on Country column)",
            "Quantity": "units",
            "InvoiceDate": "datetime string",
            "CohortMonth": "date string YYYY-MM-DD"
          },
          {
            "UnitPrice": "currency unit",
            "Quantity": "number of items"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "UnitPrice values range from 0.42 to 7.9 in sample, need to check full range",
          "Quantity has large values (24 in sample), may need outlier handling",
          "InvoiceDate is stored as string and needs conversion to datetime",
          "CohortMonth needs to be extracted as first day of month from customer's first purchase"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "UnitPrice values range from 0.42 to 7.9 in sample, need to check full range",
            "Quantity has large values (24 in sample), may need outlier handling"
          ],
          [
            "InvoiceDate is stored as string and needs conversion to datetime",
            "CohortMonth needs to be extracted as first day of month from customer's first purchase"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "average_price.csv template is empty while analytical question requires filling it",
          "cohort_counts.csv shows decreasing customer counts over months (attrition)",
          "average_price.csv uses numeric column headers (1-13) representing month offsets",
          "online.csv contains raw dates that need to be converted to relative month offsets"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "average_price.csv template is empty while analytical question requires filling it",
            "cohort_counts.csv shows decreasing customer counts over months (attrition)"
          ],
          [
            "average_price.csv uses numeric column headers (1-13) representing month offsets",
            "online.csv contains raw dates that need to be converted to relative month offsets"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "null"
          ],
          [
            "",
            "NA",
            "N/A",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each customer belongs to exactly one cohort (first purchase month)",
          "Month columns 1-13 represent sequential months after cohort month",
          "CohortMonth values must be first day of month (YYYY-MM-01)",
          "Only include customers with valid CustomerID (not null)",
          "Only include transactions with positive UnitPrice",
          "Only include transactions with positive Quantity",
          "CohortMonth must be the first day of the month of customer's first purchase",
          "Month offset columns (1-13) represent months since cohort month",
          "Average price calculation should use UnitPrice from transactions",
          "CustomerID must be present for each transaction to determine the cohort.",
          "InvoiceDate must be a valid date to determine the cohort month and transaction month.",
          "UnitPrice must be greater than zero.",
          "Quantity must be greater than zero."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each customer belongs to exactly one cohort (first purchase month)",
            "Month columns 1-13 represent sequential months after cohort month",
            "CohortMonth values must be first day of month (YYYY-MM-01)"
          ],
          [
            "Only include customers with valid CustomerID (not null)",
            "Only include transactions with positive UnitPrice",
            "Only include transactions with positive Quantity",
            "CohortMonth must be the first day of the month of customer's first purchase",
            "Month offset columns (1-13) represent months since cohort month",
            "Average price calculation should use UnitPrice from transactions"
          ],
          [
            "CustomerID must be present for each transaction to determine the cohort.",
            "InvoiceDate must be a valid date to determine the cohort month and transaction month.",
            "UnitPrice must be greater than zero.",
            "Quantity must be greater than zero."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude returns (negative Quantity)",
          "Exclude zero or negative UnitPrice",
          "Exclude customers with missing CustomerID",
          "Customers must have at least one purchase to be assigned to a cohort",
          "Only transactions occurring within 13 months of cohort month should be included",
          "Each customer should be assigned to exactly one cohort based on first purchase month",
          "Filter out transactions with missing CustomerID.",
          "Filter out transactions with invalid InvoiceDate.",
          "Filter out transactions with UnitPrice <= 0.",
          "Filter out transactions with Quantity <= 0."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Exclude returns (negative Quantity)",
            "Exclude zero or negative UnitPrice",
            "Exclude customers with missing CustomerID"
          ],
          [
            "Customers must have at least one purchase to be assigned to a cohort",
            "Only transactions occurring within 13 months of cohort month should be included",
            "Each customer should be assigned to exactly one cohort based on first purchase month"
          ],
          [
            "Filter out transactions with missing CustomerID.",
            "Filter out transactions with invalid InvoiceDate.",
            "Filter out transactions with UnitPrice <= 0.",
            "Filter out transactions with Quantity <= 0."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in UnitPrice",
          "Verify cohort size consistency across files",
          "Test correlation between average_price and average_quantity",
          "Verify that cohort month assignments are consistent",
          "Validate that average prices are within reasonable ranges"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in UnitPrice",
            "Verify cohort size consistency across files",
            "Test correlation between average_price and average_quantity"
          ],
          [
            "Verify that cohort month assignments are consistent",
            "Validate that average prices are within reasonable ranges"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Must match average_price.csv template format",
          "13 rows (cohorts from 2010-12 to 2011-12)",
          "14 columns (CohortMonth + months 1-13)",
          "Float values for average unit prices",
          "Empty cells for months beyond data availability",
          "Output file must be named average_price.csv",
          "First column must be CohortMonth in YYYY-MM-DD format",
          "Subsequent columns must be labeled 1 through 13",
          "CohortMonth values should match those in template: 2010-12-01 through 2011-12-01",
          "Numeric values should be float64 type",
          "Empty cells should remain as NaN for cohort-month combinations with no data",
          "File structure must have 13 rows (one per cohort) and 14 columns total",
          "Output must be a CSV file named average_price.csv.",
          "The CSV file must have columns: CohortMonth, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13.",
          "CohortMonth column must be in 'YYYY-MM-01' format.",
          "The values in columns 1-13 represent the average unit price for each cohort in the subsequent months after their first purchase.",
          "Missing values should be represented as empty strings or NaN."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Must match average_price.csv template format",
            "13 rows (cohorts from 2010-12 to 2011-12)",
            "14 columns (CohortMonth + months 1-13)",
            "Float values for average unit prices",
            "Empty cells for months beyond data availability"
          ],
          [
            "Output file must be named average_price.csv",
            "First column must be CohortMonth in YYYY-MM-DD format",
            "Subsequent columns must be labeled 1 through 13",
            "CohortMonth values should match those in template: 2010-12-01 through 2011-12-01",
            "Numeric values should be float64 type",
            "Empty cells should remain as NaN for cohort-month combinations with no data",
            "File structure must have 13 rows (one per cohort) and 14 columns total"
          ],
          [
            "Output must be a CSV file named average_price.csv.",
            "The CSV file must have columns: CohortMonth, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13.",
            "CohortMonth column must be in 'YYYY-MM-01' format.",
            "The values in columns 1-13 represent the average unit price for each cohort in the subsequent months after their first purchase.",
            "Missing values should be represented as empty strings or NaN."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5908333333333335
  },
  "dm-csv-050": {
    "m_q": {
      "target_metric": {
        "value": "cumulative returns for three portfolio strategies: default portfolio, equal-weight portfolio, and market value-weighted portfolio",
        "confidence": 0.6666666666666666,
        "votes": [
          "cumulative returns for three portfolio strategies: default portfolio, equal-weight portfolio, and market value-weighted portfolio",
          "Cumulative returns for three portfolio strategies: default portfolio, equal-weight portfolio, and market value-weighted portfolio",
          "Cumulative returns for default, equal-weight, and market value-weighted portfolios based on 2017 stock returns of the 9 biggest companies"
        ]
      },
      "filters": {
        "value": [
          "Date range: 2017 only",
          "Companies: 9 biggest companies (AAPL, MSFT, XOM, JNJ, JPM, AMZN, GE, FB, T)",
          "Year 2017 data only"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date range: 2017 only",
            "Companies: 9 biggest companies (AAPL, MSFT, XOM, JNJ, JPM, AMZN, GE, FB, T)"
          ],
          [
            "Year 2017 data only"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Date",
          "Portfolio Strategy Type"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Date"
          ],
          [
            "Date",
            "Portfolio Strategy Type"
          ],
          [
            "Date"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the daily returns for each company in 2017?",
          "What weights should be used for each portfolio strategy?",
          "How to calculate cumulative returns from daily returns?",
          "What is the format requirement for result.csv?",
          "What is the default portfolio strategy and its weights?",
          "How to calculate equal-weight portfolio returns (1/9 weight per stock)?",
          "What are the market capitalization weights for the 9 companies?",
          "How to compute cumulative returns from daily returns?",
          "What is the required output format for result.csv?",
          "Calculate daily returns for each portfolio strategy.",
          "Calculate cumulative returns from daily returns for each strategy.",
          "Format the output as specified in sample_result.csv."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the daily returns for each company in 2017?",
            "What weights should be used for each portfolio strategy?",
            "How to calculate cumulative returns from daily returns?",
            "What is the format requirement for result.csv?"
          ],
          [
            "What is the default portfolio strategy and its weights?",
            "How to calculate equal-weight portfolio returns (1/9 weight per stock)?",
            "What are the market capitalization weights for the 9 companies?",
            "How to compute cumulative returns from daily returns?",
            "What is the required output format for result.csv?"
          ],
          [
            "Calculate daily returns for each portfolio strategy.",
            "Calculate cumulative returns from daily returns for each strategy.",
            "Format the output as specified in sample_result.csv."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Big9Returns2017.csv",
          "EfficientFrontierPortfoliosSlim.csv",
          "FamaFrenchFactors.csv",
          "MSFTPrices.csv",
          "USO.csv",
          "sample_result.csv"
        ],
        "confidence": 0.5555555555555557,
        "votes": [
          [
            "Big9Returns2017.csv",
            "EfficientFrontierPortfoliosSlim.csv",
            "FamaFrenchFactors.csv",
            "MSFTPrices.csv",
            "USO.csv",
            "sample_result.csv"
          ],
          [
            "Big9Returns2017.csv",
            "EfficientFrontierPortfoliosSlim.csv"
          ],
          [
            "Big9Returns2017.csv",
            "EfficientFrontierPortfoliosSlim.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Date format consistency across files needs verification",
          "FamaFrenchFactors.csv starts from 2013, while returns data is 2017 only",
          "MSFTPrices.csv has much longer date range (2000+) than needed",
          "EfficientFrontierPortfoliosSlim.csv has weight columns with ' weight' suffix while Big9Returns2017.csv has ticker symbols as column names",
          "Date column only exists in Big9Returns2017.csv, not in EfficientFrontierPortfoliosSlim.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date format consistency across files needs verification",
            "FamaFrenchFactors.csv starts from 2013, while returns data is 2017 only",
            "MSFTPrices.csv has much longer date range (2000+) than needed"
          ],
          [
            "EfficientFrontierPortfoliosSlim.csv has weight columns with ' weight' suffix while Big9Returns2017.csv has ticker symbols as column names",
            "Date column only exists in Big9Returns2017.csv, not in EfficientFrontierPortfoliosSlim.csv"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "aapl": "daily return (decimal)",
          "msft": "daily return (decimal)",
          "xom": "daily return (decimal)",
          "jnj": "daily return (decimal)",
          "jpm": "daily return (decimal)",
          "amzn": "daily return (decimal)",
          "ge": "daily return (decimal)",
          "fb": "daily return (decimal)",
          "t": "daily return (decimal)",
          "returns": "portfolio return (decimal)",
          "volatility": "portfolio volatility (decimal)",
          "market_excess": "excess return (decimal)",
          "smb": "factor return (decimal)",
          "hml": "factor return (decimal)",
          "rmw": "factor return (decimal)",
          "cma": "factor return (decimal)",
          "rf": "risk-free rate (decimal)",
          "uso": "daily return (decimal)",
          "open": "price",
          "high": "price",
          "low": "price",
          "close": "price",
          "adjusted": "adjusted price",
          "aapl weight": "portfolio weight (decimal)",
          "msft weight": "portfolio weight (decimal)",
          "xom weight": "portfolio weight (decimal)",
          "jnj weight": "portfolio weight (decimal)",
          "jpm weight": "portfolio weight (decimal)",
          "amzn weight": "portfolio weight (decimal)",
          "ge weight": "portfolio weight (decimal)",
          "fb weight": "portfolio weight (decimal)",
          "t weight": "portfolio weight (decimal)",
          "cumulative": "cumulative return (decimal)",
          "cumulative_ew": "cumulative return equal-weight (decimal)",
          "cumulative_mcap": "cumulative return market-cap-weighted (decimal)"
        },
        "confidence": 0.5047619047619049,
        "votes": [
          {
            "AAPL": "daily return (decimal)",
            "MSFT": "daily return (decimal)",
            "XOM": "daily return (decimal)",
            "JNJ": "daily return (decimal)",
            "JPM": "daily return (decimal)",
            "AMZN": "daily return (decimal)",
            "GE": "daily return (decimal)",
            "FB": "daily return (decimal)",
            "T": "daily return (decimal)",
            "Returns": "portfolio return (decimal)",
            "Volatility": "portfolio volatility (decimal)",
            "Market_Excess": "excess return (decimal)",
            "SMB": "factor return (decimal)",
            "HML": "factor return (decimal)",
            "RMW": "factor return (decimal)",
            "CMA": "factor return (decimal)",
            "RF": "risk-free rate (decimal)",
            "USO": "daily return (decimal)",
            "Open": "price",
            "High": "price",
            "Low": "price",
            "Close": "price",
            "Adjusted": "adjusted price"
          },
          {
            "AAPL": "daily return (decimal)",
            "MSFT": "daily return (decimal)",
            "XOM": "daily return (decimal)",
            "JNJ": "daily return (decimal)",
            "JPM": "daily return (decimal)",
            "AMZN": "daily return (decimal)",
            "GE": "daily return (decimal)",
            "FB": "daily return (decimal)",
            "T": "daily return (decimal)",
            "AAPL weight": "portfolio weight (decimal)",
            "MSFT weight": "portfolio weight (decimal)",
            "XOM weight": "portfolio weight (decimal)",
            "JNJ weight": "portfolio weight (decimal)",
            "JPM weight": "portfolio weight (decimal)",
            "AMZN weight": "portfolio weight (decimal)",
            "GE weight": "portfolio weight (decimal)",
            "FB weight": "portfolio weight (decimal)",
            "T weight": "portfolio weight (decimal)",
            "Cumulative": "cumulative return (decimal)",
            "Cumulative_EW": "cumulative return equal-weight (decimal)",
            "Cumulative_MCap": "cumulative return market-cap-weighted (decimal)"
          },
          {
            "AAPL": "return",
            "MSFT": "return",
            "XOM": "return",
            "JNJ": "return",
            "JPM": "return",
            "AMZN": "return",
            "GE": "return",
            "FB": "return",
            "T": "return"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "All return columns appear to be in decimal format (e.g., 0.0028 = 0.28%)",
          "Weights in EfficientFrontierPortfoliosSlim.csv sum to approximately 1",
          "sample_result.csv shows all zeros - likely placeholder data",
          "Returns in Big9Returns2017.csv are in decimal format (e.g., 0.0028 = 0.28%)",
          "Weights in EfficientFrontierPortfoliosSlim.csv sum to 1.0 per portfolio"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All return columns appear to be in decimal format (e.g., 0.0028 = 0.28%)",
            "Weights in EfficientFrontierPortfoliosSlim.csv sum to approximately 1",
            "sample_result.csv shows all zeros - likely placeholder data"
          ],
          [
            "Returns in Big9Returns2017.csv are in decimal format (e.g., 0.0028 = 0.28%)",
            "Weights in EfficientFrontierPortfoliosSlim.csv sum to 1.0 per portfolio"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "FamaFrenchFactors.csv has Portfolio column that may overlap with calculated portfolio returns",
          "EfficientFrontierPortfoliosSlim.csv has multiple portfolio weight sets - need to identify which is 'default'",
          "Need to determine which row in EfficientFrontierPortfoliosSlim.csv represents the 'default' portfolio"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "FamaFrenchFactors.csv has Portfolio column that may overlap with calculated portfolio returns",
            "EfficientFrontierPortfoliosSlim.csv has multiple portfolio weight sets - need to identify which is 'default'"
          ],
          [
            "Need to determine which row in EfficientFrontierPortfoliosSlim.csv represents the 'default' portfolio"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.4545454545454546,
        "votes": [
          10.0,
          4.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Weights must sum to 1 for each portfolio",
          "Cumulative returns should start at 0 on first date",
          "Only 2017 data should be used",
          "Output must match sample_result.csv format with Date, Cumulative, Cumulative_EW, Cumulative_MCap columns",
          "Must use 2017 data only from Big9Returns2017.csv",
          "Must calculate three distinct portfolio strategies",
          "Equal-weight portfolio assigns 1/9 weight to each of 9 stocks",
          "Cumulative returns start at 0.0 on first date",
          "Output must match sample_result.csv format with 251 rows",
          "Portfolio weights must sum to 1.0",
          "The 'Date' column in Big9Returns2017.csv represents the trading day.",
          "The returns in Big9Returns2017.csv are daily returns.",
          "The weights in EfficientFrontierPortfoliosSlim.csv can be used to calculate portfolio returns."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Weights must sum to 1 for each portfolio",
            "Cumulative returns should start at 0 on first date",
            "Only 2017 data should be used",
            "Output must match sample_result.csv format with Date, Cumulative, Cumulative_EW, Cumulative_MCap columns"
          ],
          [
            "Must use 2017 data only from Big9Returns2017.csv",
            "Must calculate three distinct portfolio strategies",
            "Equal-weight portfolio assigns 1/9 weight to each of 9 stocks",
            "Cumulative returns start at 0.0 on first date",
            "Output must match sample_result.csv format with 251 rows",
            "Portfolio weights must sum to 1.0"
          ],
          [
            "The 'Date' column in Big9Returns2017.csv represents the trading day.",
            "The returns in Big9Returns2017.csv are daily returns.",
            "The weights in EfficientFrontierPortfoliosSlim.csv can be used to calculate portfolio returns."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter FamaFrenchFactors.csv to 2017 dates only",
          "Filter MSFTPrices.csv to 2017 dates only",
          "Filter USO.csv to 2017 dates only",
          "Filter Big9Returns2017.csv to ensure Date column contains only 2017 dates",
          "Select appropriate default portfolio from EfficientFrontierPortfoliosSlim.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter FamaFrenchFactors.csv to 2017 dates only",
            "Filter MSFTPrices.csv to 2017 dates only",
            "Filter USO.csv to 2017 dates only"
          ],
          [
            "Filter Big9Returns2017.csv to ensure Date column contains only 2017 dates",
            "Select appropriate default portfolio from EfficientFrontierPortfoliosSlim.csv"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing values in returns data",
          "Verify weight columns sum to approximately 1",
          "Validate that cumulative returns are monotonically increasing/decreasing",
          "Verify portfolio weights sum to 1.0",
          "Validate cumulative return calculations are monotonic or consistent with return patterns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in returns data",
            "Verify weight columns sum to approximately 1",
            "Validate that cumulative returns are monotonically increasing/decreasing"
          ],
          [
            "Verify portfolio weights sum to 1.0",
            "Validate cumulative return calculations are monotonic or consistent with return patterns"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named result.csv",
          "251 rows (matching Big9Returns2017.csv)",
          "4 columns: Date, Cumulative, Cumulative_EW, Cumulative_MCap",
          "Date column in YYYY-MM-DD format",
          "Cumulative returns as float values",
          "Output file name must be 'result.csv'",
          "Columns must be: Date, Cumulative, Cumulative_EW, Cumulative_MCap",
          "Must have exactly 251 rows matching dates in Big9Returns2017.csv",
          "Date column must preserve YYYY-MM-DD format",
          "Numeric columns should be float64 type",
          "No missing values allowed in output",
          "Output should be a CSV file named 'result.csv'.",
          "The output CSV should contain 'Date', 'Cumulative', 'Cumulative_EW', and 'Cumulative_MCap' columns.",
          "The 'Cumulative' column represents the cumulative return of the default portfolio (equal weights).",
          "The 'Cumulative_EW' column represents the cumulative return of the equal-weight portfolio.",
          "The 'Cumulative_MCap' column represents the cumulative return of the market value-weighted portfolio."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "CSV file named result.csv",
            "251 rows (matching Big9Returns2017.csv)",
            "4 columns: Date, Cumulative, Cumulative_EW, Cumulative_MCap",
            "Date column in YYYY-MM-DD format",
            "Cumulative returns as float values"
          ],
          [
            "Output file name must be 'result.csv'",
            "Columns must be: Date, Cumulative, Cumulative_EW, Cumulative_MCap",
            "Must have exactly 251 rows matching dates in Big9Returns2017.csv",
            "Date column must preserve YYYY-MM-DD format",
            "Numeric columns should be float64 type",
            "No missing values allowed in output"
          ],
          [
            "Output should be a CSV file named 'result.csv'.",
            "The output CSV should contain 'Date', 'Cumulative', 'Cumulative_EW', and 'Cumulative_MCap' columns.",
            "The 'Cumulative' column represents the cumulative return of the default portfolio (equal weights).",
            "The 'Cumulative_EW' column represents the cumulative return of the equal-weight portfolio.",
            "The 'Cumulative_MCap' column represents the cumulative return of the market value-weighted portfolio."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5924098124098125
  },
  "dm-csv-052": {
    "m_q": {
      "target_metric": {
        "value": "RFM score (Recency, Frequency, Monetary) for each customer, segmentation based on RFM scores, and customer level assignment",
        "confidence": 0.3333333333333333,
        "votes": [
          "RFM score (Recency, Frequency, Monetary) for each customer, segmentation based on RFM scores, and customer level assignment",
          "RFM (Recency, Frequency, Monetary) scores for each customer with segmentation and levels",
          "RFM score and customer segmentation level for each customer"
        ]
      },
      "filters": {
        "value": [
          "No explicit filters mentioned in question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No explicit filters mentioned in question"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "CustomerID"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "CustomerID"
          ],
          [
            "CustomerID"
          ],
          [
            "CustomerID"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Calculate Recency score (R)",
          "Calculate Frequency score (F)",
          "Calculate Monetary score (M)",
          "Combine R, F, M into RFM_Segment",
          "Calculate RFM_Score",
          "Assign RFM_Level based on segmentation",
          "How is Recency calculated for each customer?",
          "How is Frequency calculated for each customer?",
          "How is Monetary Value calculated for each customer?",
          "How are R, F, M scores derived from Recency, Frequency, MonetaryValue?",
          "How is RFM_Segment computed from R, F, M scores?",
          "How is RFM_Score computed from R, F, M scores?",
          "How are customer levels (RFM_Level) assigned based on RFM scores?",
          "What columns should be included in result.csv?",
          "Calculate Recency, Frequency, and Monetary values for each customer.",
          "Calculate RFM scores based on Recency, Frequency, and Monetary values.",
          "Segment customers based on RFM scores.",
          "Assign a level (e.g., Top, Middle, Low) to each customer based on their segment."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Calculate Recency score (R)",
            "Calculate Frequency score (F)",
            "Calculate Monetary score (M)",
            "Combine R, F, M into RFM_Segment",
            "Calculate RFM_Score",
            "Assign RFM_Level based on segmentation"
          ],
          [
            "How is Recency calculated for each customer?",
            "How is Frequency calculated for each customer?",
            "How is Monetary Value calculated for each customer?",
            "How are R, F, M scores derived from Recency, Frequency, MonetaryValue?",
            "How is RFM_Segment computed from R, F, M scores?",
            "How is RFM_Score computed from R, F, M scores?",
            "How are customer levels (RFM_Level) assigned based on RFM scores?",
            "What columns should be included in result.csv?"
          ],
          [
            "Calculate Recency, Frequency, and Monetary values for each customer.",
            "Calculate RFM scores based on Recency, Frequency, and Monetary values.",
            "Segment customers based on RFM scores.",
            "Assign a level (e.g., Top, Middle, Low) to each customer based on their segment."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "datamart.csv",
          "datamart_rfm.csv",
          "datamart_rfm_and_scores.csv",
          "datamart_rfm_scores_named_segment.csv",
          "online12M.csv",
          "sample_result.csv"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "datamart.csv",
            "datamart_rfm.csv",
            "datamart_rfm_and_scores.csv",
            "datamart_rfm_scores_named_segment.csv",
            "online12M.csv",
            "sample_result.csv"
          ],
          [
            "datamart_rfm_scores_named_segment.csv"
          ],
          [
            "online12M.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "datamart.csv has CustomerID as object type while others have int64",
          "datamart.csv has shape [0,4] indicating empty data",
          "datamart.csv has MonetaryValue as object while others have float64",
          "online12M.csv has CustomerID as int64 but different structure with transactional data",
          "datamart.csv has shape [0, 4] indicating it is empty",
          "sample_result.csv only contains CustomerID and RFM_Level, while other files contain full RFM details"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "datamart.csv has CustomerID as object type while others have int64",
            "datamart.csv has shape [0,4] indicating empty data",
            "datamart.csv has MonetaryValue as object while others have float64",
            "online12M.csv has CustomerID as int64 but different structure with transactional data"
          ],
          [
            "datamart.csv has shape [0, 4] indicating it is empty",
            "sample_result.csv only contains CustomerID and RFM_Level, while other files contain full RFM details"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "recency": "days since last purchase",
          "frequency": "count of transactions",
          "monetaryvalue": "currency amount (likely GBP)",
          "unitprice": "currency per unit",
          "quantity": "count of items",
          "r": "recency score (1-4 scale)",
          "f": "frequency score (1-4 scale)",
          "m": "monetary score (1-4 scale)",
          "rfm_segment": "concatenated RFM scores (111-444)",
          "rfm_score": "sum of R+F+M scores (3-12 scale)"
        },
        "confidence": 0.5999999999999999,
        "votes": [
          {
            "Recency": "days since last purchase",
            "Frequency": "count of transactions",
            "MonetaryValue": "currency amount (likely GBP)",
            "UnitPrice": "currency per unit",
            "Quantity": "count of items"
          },
          {
            "Recency": "days since last purchase",
            "Frequency": "count of transactions/purchases",
            "MonetaryValue": "currency amount (unit not specified)",
            "R": "recency score (1-4 scale)",
            "F": "frequency score (1-4 scale)",
            "M": "monetary score (1-4 scale)",
            "RFM_Segment": "concatenated RFM scores (111-444)",
            "RFM_Score": "sum of R+F+M scores (3-12 scale)"
          },
          {
            "Quantity": "number of items",
            "UnitPrice": "monetary unit",
            "Recency": "days",
            "Frequency": "number of transactions",
            "MonetaryValue": "monetary unit"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "MonetaryValue in datamart_rfm.csv shows decimal precision issues (e.g., 7046.159999999979)",
          "Frequency values vary widely (1 to 888)",
          "Recency values vary widely (1 to 337 days)",
          "RFM scores use 1-4 scale (quartile-based scoring)",
          "RFM_Score ranges from 3 to 12 based on summation"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "MonetaryValue in datamart_rfm.csv shows decimal precision issues (e.g., 7046.159999999979)",
            "Frequency values vary widely (1 to 888)",
            "Recency values vary widely (1 to 337 days)"
          ],
          [
            "RFM scores use 1-4 scale (quartile-based scoring)",
            "RFM_Score ranges from 3 to 12 based on summation"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "datamart.csv is empty but expected to contain source data",
          "online12M.csv contains transactional data while others contain aggregated RFM data",
          "sample_result.csv shows all customers as 'Top' level but datamart_rfm_scores_named_segment.csv shows mixed levels",
          "datamart.csv is empty while datamart_rfm.csv contains 3643 records",
          "sample_result.csv shows all customers with 'Top' level while datamart_rfm_scores_named_segment.csv shows varied levels (Top/Middle/Low)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "datamart.csv is empty but expected to contain source data",
            "online12M.csv contains transactional data while others contain aggregated RFM data",
            "sample_result.csv shows all customers as 'Top' level but datamart_rfm_scores_named_segment.csv shows mixed levels"
          ],
          [
            "datamart.csv is empty while datamart_rfm.csv contains 3643 records",
            "sample_result.csv shows all customers with 'Top' level while datamart_rfm_scores_named_segment.csv shows varied levels (Top/Middle/Low)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.9090909090909091,
        "votes": [
          10.0,
          10.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "CustomerID must be unique in output",
          "RFM_Level must be one of: Top, Middle, Low",
          "RFM_Segment is 3-digit combination of R, F, M scores",
          "RFM_Score is sum of R, F, M scores",
          "R, F, M scores are integers between 1-4",
          "R, F, M scores must be integers between 1 and 4",
          "RFM_Segment must be a 3-digit integer formed by concatenating R, F, M",
          "RFM_Score must equal R + F + M",
          "All customers in source data must be included in output",
          "CustomerID should be unique.",
          "Recency, Frequency, and Monetary values should be non-negative."
        ],
        "confidence": 0.3939393939393939,
        "votes": [
          [
            "CustomerID must be unique in output",
            "RFM_Level must be one of: Top, Middle, Low",
            "RFM_Segment is 3-digit combination of R, F, M scores",
            "RFM_Score is sum of R, F, M scores",
            "R, F, M scores are integers between 1-4"
          ],
          [
            "CustomerID must be unique in output",
            "R, F, M scores must be integers between 1 and 4",
            "RFM_Segment must be a 3-digit integer formed by concatenating R, F, M",
            "RFM_Score must equal R + F + M",
            "RFM_Level must be one of: Top, Middle, Low",
            "All customers in source data must be included in output"
          ],
          [
            "CustomerID should be unique.",
            "Recency, Frequency, and Monetary values should be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude customers with negative MonetaryValue",
          "Exclude customers with Frequency = 0",
          "Consider only valid CustomerID values (non-null)",
          "RFM_Level assignment likely based on RFM_Score thresholds",
          "Top level appears to include RFM_Score >= 10",
          "Middle level appears to include RFM_Score 6-9",
          "Low level appears to include RFM_Score <= 5"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude customers with negative MonetaryValue",
            "Exclude customers with Frequency = 0",
            "Consider only valid CustomerID values (non-null)"
          ],
          [
            "RFM_Level assignment likely based on RFM_Score thresholds",
            "Top level appears to include RFM_Score >= 10",
            "Middle level appears to include RFM_Score 6-9",
            "Low level appears to include RFM_Score <= 5"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check distribution of R, F, M scores",
          "Validate RFM_Score = R + F + M",
          "Check consistency between RFM_Segment and RFM_Level assignments"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check distribution of R, F, M scores",
            "Validate RFM_Score = R + F + M",
            "Check consistency between RFM_Segment and RFM_Level assignments"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Save as result.csv",
          "Include CustomerID and RFM_Level columns at minimum",
          "Should match structure of sample_result.csv",
          "Output file must be named result.csv",
          "Must include CustomerID and customer segmentation/levels",
          "Based on sample_result.csv, minimum columns are CustomerID and RFM_Level",
          "May optionally include additional RFM metrics (Recency, Frequency, MonetaryValue, R, F, M, RFM_Segment, RFM_Score)",
          "CSV format with header row",
          "No index column in output",
          "Output should be a CSV file named result.csv.",
          "The result.csv file should contain CustomerID and RFM_Level columns."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Save as result.csv",
            "Include CustomerID and RFM_Level columns at minimum",
            "Should match structure of sample_result.csv"
          ],
          [
            "Output file must be named result.csv",
            "Must include CustomerID and customer segmentation/levels",
            "Based on sample_result.csv, minimum columns are CustomerID and RFM_Level",
            "May optionally include additional RFM metrics (Recency, Frequency, MonetaryValue, R, F, M, RFM_Segment, RFM_Score)",
            "CSV format with header row",
            "No index column in output"
          ],
          [
            "Output should be a CSV file named result.csv.",
            "The result.csv file should contain CustomerID and RFM_Level columns."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6173737373737375
  },
  "ml-binary-009": {
    "m_q": {
      "target_metric": {
        "value": "Passenger satisfaction prediction (binary classification: 'satisfied' vs 'neutral or dissatisfied')",
        "confidence": 0.3333333333333333,
        "votes": [
          "Passenger satisfaction prediction (binary classification: 'satisfied' vs 'neutral or dissatisfied')",
          "Predict passenger satisfaction (binary classification) for test.csv and save predictions to result.csv with column name 'satisfaction'",
          "Passenger satisfaction (satisfied or neutral/dissatisfied)"
        ]
      },
      "filters": {
        "value": [
          "No explicit filters mentioned in analytical question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No explicit filters mentioned in analytical question"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features are most predictive of satisfaction?",
          "How should missing values in Arrival Delay in Minutes be handled?",
          "Should the 'Unnamed: 0' and 'id' columns be used as features or excluded?",
          "What is the distribution of satisfaction classes in training data?",
          "What are the unique values of 'satisfaction' in train.csv?",
          "What features are most predictive of passenger satisfaction?",
          "Are there missing values in predictive features that need handling?",
          "What is the distribution of satisfaction in the training data?",
          "Should categorical variables (Gender, Customer Type, Type of Travel, Class) be encoded?",
          "What is the appropriate model type for this binary classification task?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What features are most predictive of satisfaction?",
            "How should missing values in Arrival Delay in Minutes be handled?",
            "Should the 'Unnamed: 0' and 'id' columns be used as features or excluded?",
            "What is the distribution of satisfaction classes in training data?"
          ],
          [
            "What are the unique values of 'satisfaction' in train.csv?",
            "What features are most predictive of passenger satisfaction?",
            "Are there missing values in predictive features that need handling?",
            "What is the distribution of satisfaction in the training data?",
            "Should categorical variables (Gender, Customer Type, Type of Travel, Class) be encoded?",
            "What is the appropriate model type for this binary classification task?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "train.csv",
          "test.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "train.csv",
            "test.csv"
          ],
          [
            "train.csv",
            "test.csv"
          ],
          [
            "test.csv",
            "train.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "test.csv lacks 'satisfaction' column (target variable)",
          "test.csv has 24 columns vs train.csv has 25 columns",
          "Arrival Delay in Minutes dtype differs: float64 in test.csv vs float64 in train.csv (consistent)",
          "train.csv has 'satisfaction' column (target variable) which is absent in test.csv",
          "Both files have 'Unnamed: 0' and 'id' columns which appear to be index columns",
          "The 'satisfaction' column is only present in train.csv.",
          "The 'Unnamed: 0' column is present in both datasets and may represent an index.",
          "The 'id' column is present in both datasets and may represent a unique identifier."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv lacks 'satisfaction' column (target variable)",
            "test.csv has 24 columns vs train.csv has 25 columns",
            "Arrival Delay in Minutes dtype differs: float64 in test.csv vs float64 in train.csv (consistent)"
          ],
          [
            "train.csv has 'satisfaction' column (target variable) which is absent in test.csv",
            "Both files have 'Unnamed: 0' and 'id' columns which appear to be index columns"
          ],
          [
            "The 'satisfaction' column is only present in train.csv.",
            "The 'Unnamed: 0' column is present in both datasets and may represent an index.",
            "The 'id' column is present in both datasets and may represent a unique identifier."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "age": "years",
          "flight distance": "miles or kilometers (unspecified)",
          "departure delay in minutes": "minutes",
          "arrival delay in minutes": "minutes",
          "inflight wifi service": "Likert scale 0-5",
          "departure/arrival time convenient": "Likert scale 0-5",
          "ease of online booking": "Likert scale 0-5",
          "gate location": "Likert scale 0-5",
          "food and drink": "Likert scale 0-5",
          "online boarding": "Likert scale 0-5",
          "seat comfort": "Likert scale 0-5",
          "inflight entertainment": "Likert scale 0-5",
          "on-board service": "Likert scale 0-5",
          "leg room service": "Likert scale 0-5",
          "baggage handling": "Likert scale 0-5",
          "checkin service": "Likert scale 0-5",
          "inflight service": "Likert scale 0-5",
          "cleanliness": "Likert scale 0-5"
        },
        "confidence": 0.7407407407407406,
        "votes": [
          {
            "Age": "years",
            "Flight Distance": "miles or kilometers (unspecified)",
            "Departure Delay in Minutes": "minutes",
            "Arrival Delay in Minutes": "minutes",
            "Inflight wifi service": "Likert scale 0-5",
            "Departure/Arrival time convenient": "Likert scale 0-5",
            "Ease of Online booking": "Likert scale 0-5",
            "Gate location": "Likert scale 0-5",
            "Food and drink": "Likert scale 0-5",
            "Online boarding": "Likert scale 0-5",
            "Seat comfort": "Likert scale 0-5",
            "Inflight entertainment": "Likert scale 0-5",
            "On-board service": "Likert scale 0-5",
            "Leg room service": "Likert scale 0-5",
            "Baggage handling": "Likert scale 0-5",
            "Checkin service": "Likert scale 0-5",
            "Inflight service": "Likert scale 0-5",
            "Cleanliness": "Likert scale 0-5"
          },
          {
            "Age": "years",
            "Flight Distance": "miles or kilometers (unspecified)",
            "Departure Delay in Minutes": "minutes",
            "Arrival Delay in Minutes": "minutes",
            "Inflight wifi service": "rating scale (likely 0-5)",
            "Departure/Arrival time convenient": "rating scale (likely 0-5)",
            "Ease of Online booking": "rating scale (likely 0-5)",
            "Gate location": "rating scale (likely 0-5)",
            "Food and drink": "rating scale (likely 0-5)",
            "Online boarding": "rating scale (likely 0-5)",
            "Seat comfort": "rating scale (likely 0-5)",
            "Inflight entertainment": "rating scale (likely 0-5)",
            "On-board service": "rating scale (likely 0-5)",
            "Leg room service": "rating scale (likely 0-5)",
            "Baggage handling": "rating scale (likely 0-5)",
            "Checkin service": "rating scale (likely 0-5)",
            "Inflight service": "rating scale (likely 0-5)",
            "Cleanliness": "rating scale (likely 0-5)"
          },
          {
            "Age": "years",
            "Flight Distance": "miles",
            "Departure Delay in Minutes": "minutes",
            "Arrival Delay in Minutes": "minutes"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Service rating columns use 0-5 scale but sample shows values 0-5",
          "Flight Distance range appears large (83 to 3987 in sample)",
          "Delay minutes have wide range (0-117 in sample)",
          "Service rating columns appear to use integer scale, need to verify range (0-5 based on sample data)",
          "Arrival Delay in Minutes is float64 while Departure Delay in Minutes is int64"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Service rating columns use 0-5 scale but sample shows values 0-5",
            "Flight Distance range appears large (83 to 3987 in sample)",
            "Delay minutes have wide range (0-117 in sample)"
          ],
          [
            "Service rating columns appear to use integer scale, need to verify range (0-5 based on sample data)",
            "Arrival Delay in Minutes is float64 while Departure Delay in Minutes is int64"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "All service rating columns should have same scale (0-5) across both files",
          "Need to verify that rating scales are consistent between train.csv and test.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All service rating columns should have same scale (0-5) across both files"
          ],
          [
            "Need to verify that rating scales are consistent between train.csv and test.csv"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 25.0,
        "confidence": 0.9615384615384616,
        "votes": [
          25.0,
          25.0,
          24.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "satisfaction column must be predicted for all 25976 test rows",
          "Output file must be named 'result.csv' with single 'satisfaction' column",
          "satisfaction values must be either 'satisfied' or 'neutral or dissatisfied'",
          "All service ratings should be between 0-5 inclusive",
          "Age should be positive integer",
          "Flight Distance should be non-negative",
          "Delay minutes should be non-negative",
          "Predictions must be made for all 25976 rows in test.csv",
          "Output must be saved to result.csv with column name 'satisfaction'",
          "Satisfaction values should match training data format: 'satisfied' or 'neutral or dissatisfied'",
          "Age should be non-negative integer",
          "Service ratings should be within valid range (0-5 based on sample)",
          "Delay values should be non-negative",
          "'Arrival Delay in Minutes' should be greater than or equal to 0.",
          "'Departure Delay in Minutes' should be greater than or equal to 0.",
          "Age should be a positive integer."
        ],
        "confidence": 0.3541666666666666,
        "votes": [
          [
            "satisfaction column must be predicted for all 25976 test rows",
            "Output file must be named 'result.csv' with single 'satisfaction' column",
            "satisfaction values must be either 'satisfied' or 'neutral or dissatisfied'",
            "All service ratings should be between 0-5 inclusive",
            "Age should be positive integer",
            "Flight Distance should be non-negative",
            "Delay minutes should be non-negative"
          ],
          [
            "Predictions must be made for all 25976 rows in test.csv",
            "Output must be saved to result.csv with column name 'satisfaction'",
            "Satisfaction values should match training data format: 'satisfied' or 'neutral or dissatisfied'",
            "Age should be non-negative integer",
            "Flight Distance should be non-negative",
            "Service ratings should be within valid range (0-5 based on sample)",
            "Delay values should be non-negative"
          ],
          [
            "'Arrival Delay in Minutes' should be greater than or equal to 0.",
            "'Departure Delay in Minutes' should be greater than or equal to 0.",
            "Age should be a positive integer."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Check for outliers in Flight Distance",
          "Validate service ratings are within 0-5 range",
          "Ensure Age values are realistic (e.g., >0 and <120)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Flight Distance",
            "Validate service ratings are within 0-5 range",
            "Ensure Age values are realistic (e.g., >0 and <120)"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Class imbalance check in training data",
          "Correlation analysis between service ratings and satisfaction",
          "Check for multicollinearity among service features",
          "Check for class imbalance in training satisfaction labels",
          "Validate feature distributions between train and test sets",
          "Check for multicollinearity among service rating features",
          "Assess missing value patterns in Arrival Delay in Minutes"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Class imbalance check in training data",
            "Correlation analysis between service ratings and satisfaction",
            "Check for multicollinearity among service features"
          ],
          [
            "Check for class imbalance in training satisfaction labels",
            "Validate feature distributions between train and test sets",
            "Check for multicollinearity among service rating features",
            "Assess missing value patterns in Arrival Delay in Minutes"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file with 'satisfaction' column only",
          "Same row order as test.csv",
          "No index column in output",
          "result.csv must contain column named 'satisfaction'",
          "result.csv should have 25976 rows matching test.csv",
          "satisfaction values must be either 'satisfied' or 'neutral or dissatisfied'",
          "Output file format: CSV with header",
          "The output file should be named 'result.csv'.",
          "The output file should contain a column named 'satisfaction'.",
          "The 'satisfaction' column should contain the predicted satisfaction for each passenger in test.csv."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "CSV file with 'satisfaction' column only",
            "Same row order as test.csv",
            "No index column in output"
          ],
          [
            "result.csv must contain column named 'satisfaction'",
            "result.csv should have 25976 rows matching test.csv",
            "satisfaction values must be either 'satisfied' or 'neutral or dissatisfied'",
            "Output file format: CSV with header"
          ],
          [
            "The output file should be named 'result.csv'.",
            "The output file should contain a column named 'satisfaction'.",
            "The 'satisfaction' column should contain the predicted satisfaction for each passenger in test.csv."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5728222934472934
  },
  "ml-binary-013": {
    "m_q": {
      "target_metric": {
        "value": "Binary classification prediction (0 or 1) for turbine failure",
        "confidence": 0.3333333333333333,
        "votes": [
          "Binary classification prediction (0 or 1) for turbine failure",
          "Binary classification prediction of turbine failure for each test instance",
          "Probability of turbine failure for each turbine in Test.csv"
        ]
      },
      "filters": {
        "value": [
          "No explicit filters mentioned in question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No explicit filters mentioned in question"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "None - predictions are per turbine/row"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "None - predictions are per turbine/row"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What patterns in V1-V40 predict turbine failure?",
          "How to handle missing values in test data?",
          "What model should be trained on Train.csv?",
          "How to format predictions to match sample_target.csv?",
          "What are the characteristics of the training data for turbine failure prediction?",
          "What is the distribution of the Target variable in the training set?",
          "Which features (V1-V40) are most predictive of turbine failure?",
          "What is the required format for the prediction output?",
          "How many predictions need to be generated for the test set?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What patterns in V1-V40 predict turbine failure?",
            "How to handle missing values in test data?",
            "What model should be trained on Train.csv?",
            "How to format predictions to match sample_target.csv?"
          ],
          [
            "What are the characteristics of the training data for turbine failure prediction?",
            "What is the distribution of the Target variable in the training set?",
            "Which features (V1-V40) are most predictive of turbine failure?",
            "What is the required format for the prediction output?",
            "How many predictions need to be generated for the test set?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Test.csv",
          "Train.csv",
          "sample_target.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Test.csv",
            "Train.csv",
            "sample_target.csv"
          ],
          [
            "Train.csv",
            "Test.csv",
            "sample_target.csv"
          ],
          [
            "Test.csv",
            "Train.csv",
            "sample_target.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Test.csv has 40 columns, Train.csv has 41 columns (includes Target)",
          "sample_target.csv has Target as object dtype while Train.csv has Target as int64",
          "Train.csv has 41 columns (V1-V40 plus Target), Test.csv has 40 columns (V1-V40 only)",
          "sample_target.csv shows Target as object dtype while Train.csv has Target as int64"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Test.csv has 40 columns, Train.csv has 41 columns (includes Target)",
            "sample_target.csv has Target as object dtype while Train.csv has Target as int64"
          ],
          [
            "Train.csv has 41 columns (V1-V40 plus Target), Test.csv has 40 columns (V1-V40 only)",
            "sample_target.csv shows Target as object dtype while Train.csv has Target as int64"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "v1-v40": "Unknown - likely sensor measurements or engineered features",
          "target": "Binary indicator (0=no failure, 1=failure)",
          "v1": "unknown sensor measurement",
          "v2": "unknown sensor measurement",
          "v3": "unknown sensor measurement",
          "v4": "unknown sensor measurement",
          "v5": "unknown sensor measurement",
          "v6": "unknown sensor measurement",
          "v7": "unknown sensor measurement",
          "v8": "unknown sensor measurement",
          "v9": "unknown sensor measurement",
          "v10": "unknown sensor measurement",
          "v11": "unknown sensor measurement",
          "v12": "unknown sensor measurement",
          "v13": "unknown sensor measurement",
          "v14": "unknown sensor measurement",
          "v15": "unknown sensor measurement",
          "v16": "unknown sensor measurement",
          "v17": "unknown sensor measurement",
          "v18": "unknown sensor measurement",
          "v19": "unknown sensor measurement",
          "v20": "unknown sensor measurement",
          "v21": "unknown sensor measurement",
          "v22": "unknown sensor measurement",
          "v23": "unknown sensor measurement",
          "v24": "unknown sensor measurement",
          "v25": "unknown sensor measurement",
          "v26": "unknown sensor measurement",
          "v27": "unknown sensor measurement",
          "v28": "unknown sensor measurement",
          "v29": "unknown sensor measurement",
          "v30": "unknown sensor measurement",
          "v31": "unknown sensor measurement",
          "v32": "unknown sensor measurement",
          "v33": "unknown sensor measurement",
          "v34": "unknown sensor measurement",
          "v35": "unknown sensor measurement",
          "v36": "unknown sensor measurement",
          "v37": "unknown sensor measurement",
          "v38": "unknown sensor measurement",
          "v39": "unknown sensor measurement",
          "v40": "unknown sensor measurement"
        },
        "confidence": 0.34126984126984145,
        "votes": [
          {
            "V1-V40": "Unknown - likely sensor measurements or engineered features",
            "Target": "Binary indicator (0=no failure, 1=failure)"
          },
          {
            "V1": "unknown sensor measurement",
            "V2": "unknown sensor measurement",
            "V3": "unknown sensor measurement",
            "V4": "unknown sensor measurement",
            "V5": "unknown sensor measurement",
            "V6": "unknown sensor measurement",
            "V7": "unknown sensor measurement",
            "V8": "unknown sensor measurement",
            "V9": "unknown sensor measurement",
            "V10": "unknown sensor measurement",
            "V11": "unknown sensor measurement",
            "V12": "unknown sensor measurement",
            "V13": "unknown sensor measurement",
            "V14": "unknown sensor measurement",
            "V15": "unknown sensor measurement",
            "V16": "unknown sensor measurement",
            "V17": "unknown sensor measurement",
            "V18": "unknown sensor measurement",
            "V19": "unknown sensor measurement",
            "V20": "unknown sensor measurement",
            "V21": "unknown sensor measurement",
            "V22": "unknown sensor measurement",
            "V23": "unknown sensor measurement",
            "V24": "unknown sensor measurement",
            "V25": "unknown sensor measurement",
            "V26": "unknown sensor measurement",
            "V27": "unknown sensor measurement",
            "V28": "unknown sensor measurement",
            "V29": "unknown sensor measurement",
            "V30": "unknown sensor measurement",
            "V31": "unknown sensor measurement",
            "V32": "unknown sensor measurement",
            "V33": "unknown sensor measurement",
            "V34": "unknown sensor measurement",
            "V35": "unknown sensor measurement",
            "V36": "unknown sensor measurement",
            "V37": "unknown sensor measurement",
            "V38": "unknown sensor measurement",
            "V39": "unknown sensor measurement",
            "V40": "unknown sensor measurement",
            "Target": "binary label (0 or 1) indicating turbine failure"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Features appear to have different scales/ranges based on sample values",
          "Some values have extreme ranges (e.g., -10.5113424 to 9.445586089)",
          "All feature columns (V1-V40) appear to be continuous float64 values with varying ranges",
          "Features may require normalization or standardization for model training"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Features appear to have different scales/ranges based on sample values",
            "Some values have extreme ranges (e.g., -10.5113424 to 9.445586089)"
          ],
          [
            "All feature columns (V1-V40) appear to be continuous float64 values with varying ranges",
            "Features may require normalization or standardization for model training"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Test.csv lacks Target column for prediction",
          "sample_target.csv shows Target as object but likely expects 0/1 integers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Test.csv lacks Target column for prediction",
            "sample_target.csv shows Target as object but likely expects 0/1 integers"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 40.0,
        "confidence": 1.0,
        "votes": [
          40.0,
          40.0,
          40.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Predictions must be saved in target.csv with single 'Target' column",
          "Output format must match sample_target.csv structure",
          "Predictions must be binary (0 or 1)",
          "Test.csv has no Target column - must be predicted from V1-V40",
          "Predictions must be made for exactly 5000 test instances",
          "Output must contain exactly one column named 'Target'",
          "Target values should be binary (0 or 1) based on training data",
          "Output file must be named 'target.csv'",
          "All 40 feature columns (V1-V40) must be used from Test.csv",
          "Target values must be 0 or 1 in target.csv",
          "The target.csv file must contain a column named 'Target'"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Predictions must be saved in target.csv with single 'Target' column",
            "Output format must match sample_target.csv structure",
            "Predictions must be binary (0 or 1)",
            "Test.csv has no Target column - must be predicted from V1-V40"
          ],
          [
            "Predictions must be made for exactly 5000 test instances",
            "Output must contain exactly one column named 'Target'",
            "Target values should be binary (0 or 1) based on training data",
            "Output file must be named 'target.csv'",
            "All 40 feature columns (V1-V40) must be used from Test.csv"
          ],
          [
            "Target values must be 0 or 1 in target.csv",
            "The target.csv file must contain a column named 'Target'"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Need to ensure consistent feature scaling between train and test",
          "Potential outliers in V1-V40 features should be examined"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need to ensure consistent feature scaling between train and test",
            "Potential outliers in V1-V40 features should be examined"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for class imbalance in Train.csv Target column",
          "Test for multicollinearity among V1-V40 features",
          "Validate model performance using cross-validation",
          "Check class balance in training Target variable",
          "Verify feature distributions between train and test sets",
          "Check for multicollinearity among features",
          "Assess feature importance for turbine failure prediction",
          "Check for class imbalance in the 'Target' column of Train.csv",
          "Assess feature importance using a suitable method (e.g., permutation importance)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for class imbalance in Train.csv Target column",
            "Test for multicollinearity among V1-V40 features",
            "Validate model performance using cross-validation"
          ],
          [
            "Check class balance in training Target variable",
            "Verify feature distributions between train and test sets",
            "Check for multicollinearity among features",
            "Assess feature importance for turbine failure prediction"
          ],
          [
            "Check for class imbalance in the 'Target' column of Train.csv",
            "Assess feature importance using a suitable method (e.g., permutation importance)"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named target.csv",
          "Single column labeled 'Target'",
          "5000 rows (one per test observation)",
          "Binary values (0 or 1) as integers or compatible format",
          "Output file name: target.csv",
          "Single column with header 'Target'",
          "5000 rows of predictions corresponding to Test.csv rows",
          "Binary values (0 or 1) indicating predicted turbine failure",
          "CSV format matching sample_target.csv structure",
          "The output file must be named 'target.csv'",
          "The output file must contain a single column named 'Target'",
          "The 'Target' column must contain predicted probabilities or binary predictions (0 or 1) for each turbine in Test.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file named target.csv",
            "Single column labeled 'Target'",
            "5000 rows (one per test observation)",
            "Binary values (0 or 1) as integers or compatible format"
          ],
          [
            "Output file name: target.csv",
            "Single column with header 'Target'",
            "5000 rows of predictions corresponding to Test.csv rows",
            "Binary values (0 or 1) indicating predicted turbine failure",
            "CSV format matching sample_target.csv structure"
          ],
          [
            "The output file must be named 'target.csv'",
            "The output file must contain a single column named 'Target'",
            "The 'Target' column must contain predicted probabilities or binary predictions (0 or 1) for each turbine in Test.csv"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5670634920634922
  },
  "ml-binary-016": {
    "m_q": {
      "target_metric": {
        "value": "Binary prediction (0 or 1) for 'IsDefaulter' column indicating whether each client in test.csv will default next month",
        "confidence": 0.3333333333333333,
        "votes": [
          "Binary prediction (0 or 1) for 'IsDefaulter' column indicating whether each client in test.csv will default next month",
          "Binary classification predictions indicating whether credit card clients will default on payment",
          "Probability of default for each client in test.csv"
        ]
      },
      "filters": {
        "value": [
          "Only test.csv rows need predictions",
          "Training data from UCI_Credit_Card.csv includes all historical records"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only test.csv rows need predictions",
            "Training data from UCI_Credit_Card.csv includes all historical records"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "ID"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "ID"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What patterns in payment history predict default?",
          "How do demographic factors (AGE, SEX, EDUCATION, MARRIAGE) relate to default risk?",
          "What is the relationship between credit limit (LIMIT_BAL) and default probability?",
          "How do bill amounts and payment amounts interact to predict default?",
          "What features best predict credit card default?",
          "How should the training data be preprocessed?",
          "What classification model should be used?",
          "How should model performance be evaluated?",
          "What format should the output predictions take?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What patterns in payment history predict default?",
            "How do demographic factors (AGE, SEX, EDUCATION, MARRIAGE) relate to default risk?",
            "What is the relationship between credit limit (LIMIT_BAL) and default probability?",
            "How do bill amounts and payment amounts interact to predict default?"
          ],
          [
            "What features best predict credit card default?",
            "How should the training data be preprocessed?",
            "What classification model should be used?",
            "How should model performance be evaluated?",
            "What format should the output predictions take?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "UCI_Credit_Card.csv",
          "test.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "UCI_Credit_Card.csv",
            "test.csv"
          ],
          [
            "UCI_Credit_Card.csv",
            "test.csv"
          ],
          [
            "UCI_Credit_Card.csv",
            "test.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "test.csv missing 'default.payment.next.month' column (target variable)",
          "UCI_Credit_Card.csv has 25 columns vs test.csv has 24 columns",
          "UCI_Credit_Card.csv has 25 columns including target variable 'default.payment.next.month', test.csv has 24 columns without target variable"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv missing 'default.payment.next.month' column (target variable)",
            "UCI_Credit_Card.csv has 25 columns vs test.csv has 24 columns"
          ],
          [
            "UCI_Credit_Card.csv has 25 columns including target variable 'default.payment.next.month', test.csv has 24 columns without target variable"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "limit_bal": "New Taiwan Dollar (NT$)",
          "bill_amt1-6": "New Taiwan Dollar (NT$)",
          "pay_amt1-6": "New Taiwan Dollar (NT$)",
          "age": "years",
          "sex": "1=male, 2=female",
          "education": "1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown",
          "marriage": "1=married, 2=single, 3=others, 0=unknown",
          "pay_0-6": "-2=no consumption, -1=paid duly, 0=revolving credit, 1=payment delay 1 month, 2=payment delay 2 months, ..., 9=payment delay 9+ months",
          "bill_amt1": "NT dollars",
          "bill_amt2": "NT dollars",
          "bill_amt3": "NT dollars",
          "bill_amt4": "NT dollars",
          "bill_amt5": "NT dollars",
          "bill_amt6": "NT dollars",
          "pay_amt1": "NT dollars",
          "pay_amt2": "NT dollars",
          "pay_amt3": "NT dollars",
          "pay_amt4": "NT dollars",
          "pay_amt5": "NT dollars",
          "pay_amt6": "NT dollars",
          "pay_0": "repayment status in September",
          "pay_2": "repayment status in August",
          "pay_3": "repayment status in July",
          "pay_4": "repayment status in June",
          "pay_5": "repayment status in May",
          "pay_6": "repayment status in April",
          "default.payment.next.month": "binary (0=no default, 1=default)"
        },
        "confidence": 0.5679012345679012,
        "votes": [
          {
            "LIMIT_BAL": "New Taiwan Dollar (NT$)",
            "BILL_AMT1-6": "New Taiwan Dollar (NT$)",
            "PAY_AMT1-6": "New Taiwan Dollar (NT$)",
            "AGE": "years",
            "SEX": "1=male, 2=female",
            "EDUCATION": "1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown",
            "MARRIAGE": "1=married, 2=single, 3=others, 0=unknown",
            "PAY_0-6": "-2=no consumption, -1=paid duly, 0=revolving credit, 1=payment delay 1 month, 2=payment delay 2 months, ..., 9=payment delay 9+ months"
          },
          {
            "LIMIT_BAL": "NT dollars (credit limit)",
            "AGE": "years",
            "BILL_AMT1": "NT dollars",
            "BILL_AMT2": "NT dollars",
            "BILL_AMT3": "NT dollars",
            "BILL_AMT4": "NT dollars",
            "BILL_AMT5": "NT dollars",
            "BILL_AMT6": "NT dollars",
            "PAY_AMT1": "NT dollars",
            "PAY_AMT2": "NT dollars",
            "PAY_AMT3": "NT dollars",
            "PAY_AMT4": "NT dollars",
            "PAY_AMT5": "NT dollars",
            "PAY_AMT6": "NT dollars",
            "SEX": "categorical (1=male, 2=female)",
            "EDUCATION": "categorical (1=graduate school, 2=university, 3=high school, 4=others)",
            "MARRIAGE": "categorical (1=married, 2=single, 3=others)",
            "PAY_0": "repayment status in September",
            "PAY_2": "repayment status in August",
            "PAY_3": "repayment status in July",
            "PAY_4": "repayment status in June",
            "PAY_5": "repayment status in May",
            "PAY_6": "repayment status in April",
            "default.payment.next.month": "binary (0=no default, 1=default)"
          },
          {
            "LIMIT_BAL": "NT$",
            "AGE": "years",
            "BILL_AMT1": "NT$",
            "BILL_AMT2": "NT$",
            "BILL_AMT3": "NT$",
            "BILL_AMT4": "NT$",
            "BILL_AMT5": "NT$",
            "BILL_AMT6": "NT$",
            "PAY_AMT1": "NT$",
            "PAY_AMT2": "NT$",
            "PAY_AMT3": "NT$",
            "PAY_AMT4": "NT$",
            "PAY_AMT5": "NT$",
            "PAY_AMT6": "NT$"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Negative values in BILL_AMT columns in test.csv (e.g., -2, -3)",
          "Negative values in PAY_AMT columns in test.csv",
          "PAY_X columns have both negative and positive values with different meanings",
          "LIMIT_BAL and BILL_AMT columns are in much larger scale (thousands) compared to categorical features",
          "PAY_AMT columns have wide range requiring normalization"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Negative values in BILL_AMT columns in test.csv (e.g., -2, -3)",
            "Negative values in PAY_AMT columns in test.csv",
            "PAY_X columns have both negative and positive values with different meanings"
          ],
          [
            "LIMIT_BAL and BILL_AMT columns are in much larger scale (thousands) compared to categorical features",
            "PAY_AMT columns have wide range requiring normalization"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "test.csv contains negative bill amounts (BILL_AMT) while UCI_Credit_Card.csv doesn't show this pattern in sample",
          "test.csv has EDUCATION=6 while UCI_Credit_Card.csv sample shows EDUCATION=6 exists",
          "No conflicts - test.csv has identical schema to UCI_Credit_Card.csv except missing target variable"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv contains negative bill amounts (BILL_AMT) while UCI_Credit_Card.csv doesn't show this pattern in sample",
            "test.csv has EDUCATION=6 while UCI_Credit_Card.csv sample shows EDUCATION=6 exists"
          ],
          [
            "No conflicts - test.csv has identical schema to UCI_Credit_Card.csv except missing target variable"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 25.0,
        "confidence": 1.0,
        "votes": [
          25.0,
          25.0,
          25.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "ID should be unique in both datasets",
          "AGE should be positive integer",
          "LIMIT_BAL should be positive",
          "PAY_X values should be between -2 and 9",
          "default.payment.next.month should be 0 or 1",
          "SEX should be 1 or 2",
          "EDUCATION should be 1-6",
          "MARRIAGE should be 0-3",
          "Predictions must be made for all 4500 rows in test.csv",
          "Output must be saved in file named 'defaulter.csv'",
          "Output must contain column named 'IsDefaulter'",
          "Predictions must be binary (0 or 1) indicating default status",
          "ID column from test.csv should be preserved for matching predictions to clients",
          "Values in 'SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', and 'PAY_6' should be checked against their documented ranges.",
          "The 'ID' column in test.csv should be present in the output 'defaulter.csv'."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "ID should be unique in both datasets",
            "AGE should be positive integer",
            "LIMIT_BAL should be positive",
            "PAY_X values should be between -2 and 9",
            "default.payment.next.month should be 0 or 1",
            "SEX should be 1 or 2",
            "EDUCATION should be 1-6",
            "MARRIAGE should be 0-3"
          ],
          [
            "Predictions must be made for all 4500 rows in test.csv",
            "Output must be saved in file named 'defaulter.csv'",
            "Output must contain column named 'IsDefaulter'",
            "Predictions must be binary (0 or 1) indicating default status",
            "ID column from test.csv should be preserved for matching predictions to clients"
          ],
          [
            "Values in 'SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', and 'PAY_6' should be checked against their documented ranges.",
            "The 'ID' column in test.csv should be present in the output 'defaulter.csv'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows with invalid EDUCATION values (0, 5, 6 might need recoding)",
          "Check for inconsistent PAY_X sequences (should show deteriorating or improving payment behavior)",
          "Validate that BILL_AMT >= 0 (except test.csv shows negatives)",
          "Train on UCI_Credit_Card.csv using 'default.payment.next.month' as target",
          "Predict on test.csv using same features (excluding ID and target)",
          "Features used: LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0 through PAY_6, BILL_AMT1 through BILL_AMT6, PAY_AMT1 through PAY_AMT6"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with invalid EDUCATION values (0, 5, 6 might need recoding)",
            "Check for inconsistent PAY_X sequences (should show deteriorating or improving payment behavior)",
            "Validate that BILL_AMT >= 0 (except test.csv shows negatives)"
          ],
          [
            "Train on UCI_Credit_Card.csv using 'default.payment.next.month' as target",
            "Predict on test.csv using same features (excluding ID and target)",
            "Features used: LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0 through PAY_6, BILL_AMT1 through BILL_AMT6, PAY_AMT1 through PAY_AMT6"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check class imbalance in default.payment.next.month",
          "Test correlation between PAY_0 and default",
          "Check multicollinearity among PAY_X variables",
          "Test relationship between LIMIT_BAL and default rate",
          "Evaluate model performance using classification metrics (accuracy, precision, recall, F1-score) on validation set",
          "Check for class imbalance in training data",
          "Assess feature importance to understand prediction drivers"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check class imbalance in default.payment.next.month",
            "Test correlation between PAY_0 and default",
            "Check multicollinearity among PAY_X variables",
            "Test relationship between LIMIT_BAL and default rate"
          ],
          [
            "Evaluate model performance using classification metrics (accuracy, precision, recall, F1-score) on validation set",
            "Check for class imbalance in training data",
            "Assess feature importance to understand prediction drivers"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "defaulter.csv must have exactly 4500 rows matching test.csv",
          "defaulter.csv must have columns: ID, IsDefaulter",
          "IsDefaulter must be 0 or 1 for each row",
          "Predictions must be based on test.csv features",
          "Output file must be CSV format named 'defaulter.csv'",
          "Must include column 'IsDefaulter' with binary predictions",
          "Should include ID column to match predictions to test records",
          "Predictions should be integers 0 or 1",
          "Output file must be named 'defaulter.csv'.",
          "Output file must contain 'ID' and 'IsDefaulter' columns.",
          "Output file must be in CSV format."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "defaulter.csv must have exactly 4500 rows matching test.csv",
            "defaulter.csv must have columns: ID, IsDefaulter",
            "IsDefaulter must be 0 or 1 for each row",
            "Predictions must be based on test.csv features"
          ],
          [
            "Output file must be CSV format named 'defaulter.csv'",
            "Must include column 'IsDefaulter' with binary predictions",
            "Should include ID column to match predictions to test records",
            "Predictions should be integers 0 or 1"
          ],
          [
            "Output file must be named 'defaulter.csv'.",
            "Output file must contain 'ID' and 'IsDefaulter' columns.",
            "Output file must be in CSV format."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5950617283950619
  },
  "ml-cluster-009": {
    "m_q": {
      "target_metric": {
        "value": "Cluster labels for each country based on feature vectors",
        "confidence": 0.3333333333333333,
        "votes": [
          "Cluster labels for each country based on feature vectors",
          "Clustering of countries into 3 clusters based on features",
          "Clustering of countries into 3 clusters based on available features."
        ]
      },
      "filters": {
        "value": [
          "No explicit filters mentioned in question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No explicit filters mentioned in question"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Country"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Country"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Which features should be selected for clustering?",
          "How to handle missing values?",
          "What normalization/scaling is needed?",
          "Which clustering algorithm to use?",
          "How to validate cluster quality?",
          "Which columns should be used as features for clustering?",
          "How should non-numeric columns be handled?",
          "How should missing values be treated before clustering?",
          "What clustering algorithm should be used (e.g., K-Means, Hierarchical)?",
          "Should features be normalized/standardized before clustering?",
          "How should the feature vector be ordered for output columns Feature_1, Feature_2, etc.?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Which features should be selected for clustering?",
            "How to handle missing values?",
            "What normalization/scaling is needed?",
            "Which clustering algorithm to use?",
            "How to validate cluster quality?"
          ],
          [
            "Which columns should be used as features for clustering?",
            "How should non-numeric columns be handled?",
            "How should missing values be treated before clustering?",
            "What clustering algorithm should be used (e.g., K-Means, Hierarchical)?",
            "Should features be normalized/standardized before clustering?",
            "How should the feature vector be ordered for output columns Feature_1, Feature_2, etc.?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "world-data-2023.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "world-data-2023.csv"
          ],
          [
            "world-data-2023.csv"
          ],
          [
            "world-data-2023.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Multiple columns have object dtype but contain numeric data (e.g., Density, GDP, Population)",
          "Some numeric columns have percentage symbols in values (e.g., Agricultural Land( %))",
          "Some columns have currency symbols and commas in numeric values (e.g., GDP, Minimum wage)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Multiple columns have object dtype but contain numeric data (e.g., Density, GDP, Population)",
            "Some numeric columns have percentage symbols in values (e.g., Agricultural Land( %))",
            "Some columns have currency symbols and commas in numeric values (e.g., GDP, Minimum wage)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "density_per_km2": "persons per square kilometer",
          "agricultural_land_percent": "percentage",
          "land_area_km2": "square kilometers",
          "armed forces size": "count",
          "birth rate": "births per 1000 population",
          "co2_emissions": "metric tons",
          "cpi": "index",
          "cpi_change_percent": "percentage",
          "fertility rate": "births per woman",
          "forested_area_percent": "percentage",
          "gasoline_price": "USD per liter",
          "gdp": "USD",
          "gross_primary_education_enrollment_percent": "percentage",
          "gross_tertiary_education_enrollment_percent": "percentage",
          "infant mortality": "deaths per 1000 live births",
          "life expectancy": "years",
          "maternal mortality ratio": "deaths per 100,000 live births",
          "minimum wage": "USD",
          "out of pocket health expenditure": "percentage",
          "physicians per thousand": "doctors per 1000 people",
          "population": "count",
          "labor_force_participation_percent": "percentage",
          "tax_revenue_percent": "percentage",
          "total_tax_rate": "percentage",
          "unemployment_rate": "percentage",
          "urban_population": "count",
          "latitude": "degrees",
          "longitude": "degrees",
          "density\n(p/km2)": "people per square kilometer",
          "agricultural land( %)": "percentage",
          "land area(km2)": "square kilometers",
          "forested area (%)": "percentage",
          "gasoline price": "USD per liter",
          "gross primary education enrollment (%)": "percentage",
          "gross tertiary education enrollment (%)": "percentage",
          "population: labor force participation (%)": "percentage",
          "tax revenue (%)": "percentage",
          "total tax rate": "percentage",
          "unemployment rate": "percentage",
          "co2-emissions": "metric tons",
          "cpi change (%)": "%"
        },
        "confidence": 0.6260162601626019,
        "votes": [
          {
            "Density_Per_Km2": "persons per square kilometer",
            "Agricultural_Land_Percent": "percentage",
            "Land_Area_Km2": "square kilometers",
            "Armed Forces size": "count",
            "Birth Rate": "births per 1000 population",
            "Co2_Emissions": "metric tons",
            "CPI": "index",
            "CPI_Change_Percent": "percentage",
            "Fertility Rate": "births per woman",
            "Forested_Area_Percent": "percentage",
            "Gasoline_Price": "USD per liter",
            "GDP": "USD",
            "Gross_primary_education_enrollment_Percent": "percentage",
            "Gross_tertiary_education_enrollment_Percent": "percentage",
            "Infant mortality": "deaths per 1000 live births",
            "Life expectancy": "years",
            "Maternal mortality ratio": "deaths per 100,000 live births",
            "Minimum wage": "USD per hour",
            "Out of pocket health expenditure": "percentage",
            "Physicians per thousand": "doctors per 1000 people",
            "Population": "count",
            "Labor_force_participation_Percent": "percentage",
            "Tax_revenue_Percent": "percentage",
            "Total_tax_rate": "percentage",
            "Unemployment_rate": "percentage",
            "Urban_population": "count",
            "Latitude": "degrees",
            "Longitude": "degrees"
          },
          {
            "Density\n(P/Km2)": "people per square kilometer",
            "Agricultural Land( %)": "percentage",
            "Land Area(Km2)": "square kilometers",
            "Birth Rate": "per 1000 population",
            "Fertility Rate": "births per woman",
            "Forested Area (%)": "percentage",
            "Gasoline Price": "USD per liter",
            "GDP": "USD",
            "Gross primary education enrollment (%)": "percentage",
            "Gross tertiary education enrollment (%)": "percentage",
            "Infant mortality": "per 1000 live births",
            "Life expectancy": "years",
            "Maternal mortality ratio": "per 100000 live births",
            "Minimum wage": "USD",
            "Out of pocket health expenditure": "percentage",
            "Physicians per thousand": "per 1000 population",
            "Population: Labor force participation (%)": "percentage",
            "Tax revenue (%)": "percentage",
            "Total tax rate": "percentage",
            "Unemployment rate": "percentage",
            "Latitude": "degrees",
            "Longitude": "degrees"
          },
          {
            "Density\n(P/Km2)": "people/km^2",
            "Agricultural Land( %)": "%",
            "Land Area(Km2)": "km^2",
            "Armed Forces size": "number of personnel",
            "Birth Rate": "births per 1000 population",
            "Co2-Emissions": "metric tons",
            "CPI Change (%)": "%",
            "Fertility Rate": "births per woman",
            "Forested Area (%)": "%",
            "Gasoline Price": "USD",
            "GDP": "USD",
            "Gross primary education enrollment (%)": "%",
            "Gross tertiary education enrollment (%)": "%",
            "Infant mortality": "deaths per 1000 live births",
            "Life expectancy": "years",
            "Maternal mortality ratio": "deaths per 100,000 live births",
            "Minimum wage": "USD",
            "Out of pocket health expenditure": "%",
            "Physicians per thousand": "physicians per 1000 population",
            "Population": "number of people",
            "Population: Labor force participation (%)": "%",
            "Tax revenue (%)": "% of GDP",
            "Total tax rate": "%",
            "Unemployment rate": "%",
            "Urban_population": "number of people",
            "Latitude": "degrees",
            "Longitude": "degrees"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "GDP values have commas and dollar signs",
          "Population values have commas",
          "Co2-Emissions have commas",
          "Land Area values have commas",
          "Some percentage columns have % symbols",
          "Currency symbols in Gasoline Price and Minimum wage",
          "GDP values are very large compared to percentages and rates",
          "Population and Land Area have much larger scales than other numeric features",
          "Multiple percentage columns with different ranges",
          "Armed Forces size has large absolute values",
          "GDP and Population are represented as strings with inconsistent formatting (e.g., commas).",
          "Density is a string and needs to be converted to numeric.",
          "Several percentage columns need to have the '%' character removed and be converted to numeric."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "GDP values have commas and dollar signs",
            "Population values have commas",
            "Co2-Emissions have commas",
            "Land Area values have commas",
            "Some percentage columns have % symbols",
            "Currency symbols in Gasoline Price and Minimum wage"
          ],
          [
            "GDP values are very large compared to percentages and rates",
            "Population and Land Area have much larger scales than other numeric features",
            "Multiple percentage columns with different ranges",
            "Armed Forces size has large absolute values"
          ],
          [
            "GDP and Population are represented as strings with inconsistent formatting (e.g., commas).",
            "Density is a string and needs to be converted to numeric.",
            "Several percentage columns need to have the '%' character removed and be converted to numeric."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Single source file, no cross-source conflicts"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single source file, no cross-source conflicts"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "NaN"
        ],
        "confidence": 0.9999999999999999,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " ",
            "NaN",
            "nan"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 35.0,
        "confidence": 1.0,
        "votes": [
          35.0,
          35.0,
          35.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Must produce exactly 3 clusters",
          "Output file must be named cluster.csv",
          "Output columns must be named Feature_i (for features) and Cluster (for labels)",
          "195 countries should be clustered",
          "Feature vectors must be created from available data",
          "Output must be saved as cluster.csv",
          "Output must contain columns named 'Feature_i' where i is the index of features",
          "Output must contain a 'Cluster' column with cluster labels",
          "Number of clusters must be exactly 3",
          "Each row in output corresponds to a country from input data",
          "The output file 'cluster.csv' must contain columns named 'Feature_i' and 'Cluster'.",
          "The number of clusters must be 3."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Must produce exactly 3 clusters",
            "Output file must be named cluster.csv",
            "Output columns must be named Feature_i (for features) and Cluster (for labels)",
            "195 countries should be clustered",
            "Feature vectors must be created from available data"
          ],
          [
            "Output must be saved as cluster.csv",
            "Output must contain columns named 'Feature_i' where i is the index of features",
            "Output must contain a 'Cluster' column with cluster labels",
            "Number of clusters must be exactly 3",
            "Each row in output corresponds to a country from input data"
          ],
          [
            "The output file 'cluster.csv' must contain columns named 'Feature_i' and 'Cluster'.",
            "The number of clusters must be 3."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove non-numeric columns for clustering",
          "Handle missing values before clustering",
          "Normalize features to comparable scales",
          "Consider removing columns with high missingness",
          "Exclude rows with excessive missing values in key numeric columns",
          "Exclude non-numeric columns from feature vector (Country, Abbreviation, Capital/Major City, Currency-Code, Official language, Largest city)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove non-numeric columns for clustering",
            "Handle missing values before clustering",
            "Normalize features to comparable scales",
            "Consider removing columns with high missingness"
          ],
          [
            "Exclude rows with excessive missing values in key numeric columns",
            "Exclude non-numeric columns from feature vector (Country, Abbreviation, Capital/Major City, Currency-Code, Official language, Largest city)"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for multicollinearity among features",
          "Assess feature distributions for normalization needs",
          "Evaluate cluster separation metrics (silhouette score, Davies-Bouldin index)",
          "Check for outliers that may affect clustering",
          "Appropriateness of chosen clustering algorithm (e.g., silhouette score for K-means)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for multicollinearity among features",
            "Assess feature distributions for normalization needs",
            "Evaluate cluster separation metrics (silhouette score, Davies-Bouldin index)",
            "Check for outliers that may affect clustering"
          ],
          [],
          [
            "Appropriateness of chosen clustering algorithm (e.g., silhouette score for K-means)."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV format",
          "Column names: Feature_0, Feature_1, ..., Feature_n, Cluster",
          "One row per country",
          "Cluster column contains integer labels 0, 1, or 2",
          "CSV file named 'cluster.csv'",
          "Column headers: Feature_1, Feature_2, ..., Feature_n, Cluster",
          "Cluster labels should be integers (0, 1, 2) or (1, 2, 3)",
          "Number of rows should match number of countries after preprocessing",
          "All Feature_i columns should contain numeric values from the feature vector",
          "No country names in output, only feature values and cluster assignments",
          "The output file must be in CSV format.",
          "The 'Cluster' column must contain the cluster labels (0, 1, or 2)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV format",
            "Column names: Feature_0, Feature_1, ..., Feature_n, Cluster",
            "One row per country",
            "Cluster column contains integer labels 0, 1, or 2"
          ],
          [
            "CSV file named 'cluster.csv'",
            "Column headers: Feature_1, Feature_2, ..., Feature_n, Cluster",
            "Cluster labels should be integers (0, 1, 2) or (1, 2, 3)",
            "Number of rows should match number of countries after preprocessing",
            "All Feature_i columns should contain numeric values from the feature vector",
            "No country names in output, only feature values and cluster assignments"
          ],
          [
            "The output file must be in CSV format.",
            "The 'Cluster' column must contain the cluster labels (0, 1, or 2)."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6146341463414635
  },
  "ml-cluster-010": {
    "m_q": {
      "target_metric": {
        "value": "Clustering labels for each data point",
        "confidence": 0.3333333333333333,
        "votes": [
          "Clustering labels for each data point",
          "Cluster assignments for each observation in the Forest Cover Type Dataset",
          "Clustering of the Forest Cover Type Dataset into an appropriate number of clusters and saving the results to cluster.csv."
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Determine appropriate number of clusters",
          "Select clustering algorithm",
          "Scale/normalize features appropriately",
          "Handle categorical one-hot encoded variables (Wilderness_Area and Soil_Type)",
          "Exclude Cover_Type column from clustering features",
          "What features should be used for clustering (exclude Cover_Type as it is the label)?",
          "What is the appropriate number of clusters for this dataset?",
          "Should feature scaling/normalization be applied before clustering?",
          "Which clustering algorithm is most appropriate (K-means, hierarchical, DBSCAN, etc.)?",
          "How should the feature vector be represented in the output (all 54 features as Feature_1 to Feature_54)?",
          "What is the optimal number of clusters for the dataset?",
          "Which clustering algorithm is most suitable for this dataset?",
          "How should the features be preprocessed (e.g., scaled, normalized) before clustering?",
          "How can the quality of the clustering be evaluated?",
          "How should the cluster assignments be saved to the cluster.csv file?"
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Determine appropriate number of clusters",
            "Select clustering algorithm",
            "Scale/normalize features appropriately",
            "Handle categorical one-hot encoded variables (Wilderness_Area and Soil_Type)",
            "Exclude Cover_Type column from clustering features"
          ],
          [
            "What features should be used for clustering (exclude Cover_Type as it is the label)?",
            "What is the appropriate number of clusters for this dataset?",
            "Should feature scaling/normalization be applied before clustering?",
            "Which clustering algorithm is most appropriate (K-means, hierarchical, DBSCAN, etc.)?",
            "How should the feature vector be represented in the output (all 54 features as Feature_1 to Feature_54)?"
          ],
          [
            "What is the optimal number of clusters for the dataset?",
            "Which clustering algorithm is most suitable for this dataset?",
            "How should the features be preprocessed (e.g., scaled, normalized) before clustering?",
            "How can the quality of the clustering be evaluated?",
            "How should the cluster assignments be saved to the cluster.csv file?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "covtype.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "covtype.csv"
          ],
          [
            "covtype.csv"
          ],
          [
            "covtype.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "elevation": "meters",
          "aspect": "degrees azimuth",
          "slope": "degrees",
          "horizontal_distance_to_hydrology": "meters",
          "vertical_distance_to_hydrology": "meters",
          "horizontal_distance_to_roadways": "meters",
          "hillshade_9am": "index (0-255)",
          "hillshade_noon": "index (0-255)",
          "hillshade_3pm": "index (0-255)",
          "horizontal_distance_to_fire_points": "meters",
          "wilderness_area1": "binary indicator",
          "wilderness_area2": "binary indicator",
          "wilderness_area3": "binary indicator",
          "wilderness_area4": "binary indicator",
          "soil_type1": "binary indicator",
          "soil_type2": "binary indicator",
          "soil_type3": "binary indicator",
          "soil_type4": "binary indicator",
          "soil_type5": "binary indicator",
          "soil_type6": "binary indicator",
          "soil_type7": "binary indicator",
          "soil_type8": "binary indicator",
          "soil_type9": "binary indicator",
          "soil_type10": "binary indicator",
          "soil_type11": "binary indicator",
          "soil_type12": "binary indicator",
          "soil_type13": "binary indicator",
          "soil_type14": "binary indicator",
          "soil_type15": "binary indicator",
          "soil_type16": "binary indicator",
          "soil_type17": "binary indicator",
          "soil_type18": "binary indicator",
          "soil_type19": "binary indicator",
          "soil_type20": "binary indicator",
          "soil_type21": "binary indicator",
          "soil_type22": "binary indicator",
          "soil_type23": "binary indicator",
          "soil_type24": "binary indicator",
          "soil_type25": "binary indicator",
          "soil_type26": "binary indicator",
          "soil_type27": "binary indicator",
          "soil_type28": "binary indicator",
          "soil_type29": "binary indicator",
          "soil_type30": "binary indicator",
          "soil_type31": "binary indicator",
          "soil_type32": "binary indicator",
          "soil_type33": "binary indicator",
          "soil_type34": "binary indicator",
          "soil_type35": "binary indicator",
          "soil_type36": "binary indicator",
          "soil_type37": "binary indicator",
          "soil_type38": "binary indicator",
          "soil_type39": "binary indicator",
          "soil_type40": "binary indicator",
          "cover_type": "categorical label 1-7"
        },
        "confidence": 0.43636363636363606,
        "votes": [
          {
            "Elevation": "feet",
            "Aspect": "degrees azimuth",
            "Slope": "degrees",
            "Horizontal_Distance_To_Hydrology": "meters",
            "Vertical_Distance_To_Hydrology": "meters",
            "Horizontal_Distance_To_Roadways": "meters",
            "Hillshade_9am": "index (0-255)",
            "Hillshade_Noon": "index (0-255)",
            "Hillshade_3pm": "index (0-255)",
            "Horizontal_Distance_To_Fire_Points": "meters"
          },
          {
            "Elevation": "meters",
            "Aspect": "degrees azimuth",
            "Slope": "degrees",
            "Horizontal_Distance_To_Hydrology": "meters",
            "Vertical_Distance_To_Hydrology": "meters",
            "Horizontal_Distance_To_Roadways": "meters",
            "Hillshade_9am": "index 0-255",
            "Hillshade_Noon": "index 0-255",
            "Hillshade_3pm": "index 0-255",
            "Horizontal_Distance_To_Fire_Points": "meters",
            "Wilderness_Area1": "binary indicator",
            "Wilderness_Area2": "binary indicator",
            "Wilderness_Area3": "binary indicator",
            "Wilderness_Area4": "binary indicator",
            "Soil_Type1": "binary indicator",
            "Soil_Type2": "binary indicator",
            "Soil_Type3": "binary indicator",
            "Soil_Type4": "binary indicator",
            "Soil_Type5": "binary indicator",
            "Soil_Type6": "binary indicator",
            "Soil_Type7": "binary indicator",
            "Soil_Type8": "binary indicator",
            "Soil_Type9": "binary indicator",
            "Soil_Type10": "binary indicator",
            "Soil_Type11": "binary indicator",
            "Soil_Type12": "binary indicator",
            "Soil_Type13": "binary indicator",
            "Soil_Type14": "binary indicator",
            "Soil_Type15": "binary indicator",
            "Soil_Type16": "binary indicator",
            "Soil_Type17": "binary indicator",
            "Soil_Type18": "binary indicator",
            "Soil_Type19": "binary indicator",
            "Soil_Type20": "binary indicator",
            "Soil_Type21": "binary indicator",
            "Soil_Type22": "binary indicator",
            "Soil_Type23": "binary indicator",
            "Soil_Type24": "binary indicator",
            "Soil_Type25": "binary indicator",
            "Soil_Type26": "binary indicator",
            "Soil_Type27": "binary indicator",
            "Soil_Type28": "binary indicator",
            "Soil_Type29": "binary indicator",
            "Soil_Type30": "binary indicator",
            "Soil_Type31": "binary indicator",
            "Soil_Type32": "binary indicator",
            "Soil_Type33": "binary indicator",
            "Soil_Type34": "binary indicator",
            "Soil_Type35": "binary indicator",
            "Soil_Type36": "binary indicator",
            "Soil_Type37": "binary indicator",
            "Soil_Type38": "binary indicator",
            "Soil_Type39": "binary indicator",
            "Soil_Type40": "binary indicator",
            "Cover_Type": "categorical label 1-7"
          },
          {
            "Elevation": "meters",
            "Aspect": "degrees",
            "Slope": "degrees",
            "Horizontal_Distance_To_Hydrology": "meters",
            "Vertical_Distance_To_Hydrology": "meters",
            "Horizontal_Distance_To_Roadways": "meters",
            "Horizontal_Distance_To_Fire_Points": "meters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Elevation range (2000-4000 ft) vs other distance metrics (0-7000 m) - different units and scales",
          "Hillshade indices (0-255) vs distance measurements",
          "One-hot encoded variables (0/1) vs continuous measurements",
          "Continuous features (Elevation, distances, hillshade) have different scales than binary features (Wilderness_Area, Soil_Type)",
          "Distance features range from 0 to thousands while hillshade ranges from 0-255",
          "Aspect is circular (0 and 360 degrees are the same)",
          "Features have different scales, which may affect clustering performance. Scaling or normalization may be required."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Elevation range (2000-4000 ft) vs other distance metrics (0-7000 m) - different units and scales",
            "Hillshade indices (0-255) vs distance measurements",
            "One-hot encoded variables (0/1) vs continuous measurements"
          ],
          [
            "Continuous features (Elevation, distances, hillshade) have different scales than binary features (Wilderness_Area, Soil_Type)",
            "Distance features range from 0 to thousands while hillshade ranges from 0-255",
            "Aspect is circular (0 and 360 degrees are the same)"
          ],
          [
            "Features have different scales, which may affect clustering performance. Scaling or normalization may be required."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 55.0,
        "confidence": 1.0,
        "votes": [
          55.0,
          55.0,
          55.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each row has exactly one Wilderness_Area = 1",
          "Each row has exactly one Soil_Type = 1",
          "Cover_Type values range from 1-7",
          "Hillshade values should be between 0-255",
          "Distances should be non-negative",
          "Cover_Type column should not be used as a clustering feature (it is the label)",
          "All 54 feature columns (excluding Cover_Type) should be used for clustering",
          "Output file must be named cluster.csv",
          "Output must have columns named Feature_1, Feature_2, ..., Feature_54, and Cluster",
          "Number of output rows must match input rows (5000)",
          "Cluster labels should be integer values starting from 0 or 1",
          "The 'Cover_Type' column is not used for clustering, but may be used for validation.",
          "The 'Wilderness_Area' and 'Soil_Type' columns are one-hot encoded."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Each row has exactly one Wilderness_Area = 1",
            "Each row has exactly one Soil_Type = 1",
            "Cover_Type values range from 1-7",
            "Hillshade values should be between 0-255",
            "Distances should be non-negative"
          ],
          [
            "Cover_Type column should not be used as a clustering feature (it is the label)",
            "All 54 feature columns (excluding Cover_Type) should be used for clustering",
            "Output file must be named cluster.csv",
            "Output must have columns named Feature_1, Feature_2, ..., Feature_54, and Cluster",
            "Number of output rows must match input rows (5000)",
            "Cluster labels should be integer values starting from 0 or 1"
          ],
          [
            "The 'Cover_Type' column is not used for clustering, but may be used for validation.",
            "The 'Wilderness_Area' and 'Soil_Type' columns are one-hot encoded."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude Cover_Type column from feature set",
          "Consider feature scaling/normalization before clustering",
          "Check for missing values in all columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude Cover_Type column from feature set",
            "Consider feature scaling/normalization before clustering",
            "Check for missing values in all columns"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for multicollinearity among continuous features",
          "Assess feature distributions for scaling needs",
          "Determine optimal number of clusters using elbow method/silhouette score",
          "Elbow method to determine optimal number of clusters",
          "Silhouette score to evaluate clustering quality",
          "Davies-Bouldin index for cluster separation assessment",
          "Silhouette score to evaluate cluster quality.",
          "Elbow method to determine the optimal number of clusters."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for multicollinearity among continuous features",
            "Assess feature distributions for scaling needs",
            "Determine optimal number of clusters using elbow method/silhouette score"
          ],
          [
            "Elbow method to determine optimal number of clusters",
            "Silhouette score to evaluate clustering quality",
            "Davies-Bouldin index for cluster separation assessment"
          ],
          [
            "Silhouette score to evaluate cluster quality.",
            "Elbow method to determine the optimal number of clusters."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output file: cluster.csv",
          "Columns: Feature_1 through Feature_54 (for 54 features after excluding Cover_Type)",
          "Additional column: Cluster (cluster labels)",
          "No header row required as per instructions",
          "Columns: Feature_1, Feature_2, ..., Feature_54, Cluster",
          "Feature_i columns contain the original feature values from the 54 input features (excluding Cover_Type)",
          "Cluster column contains integer cluster labels assigned by the clustering algorithm",
          "CSV format with header row",
          "Same number of rows as input data (5000)",
          "The output file 'cluster.csv' must contain columns named 'Feature_i' (where i is the index of the feature) and 'Cluster'.",
          "The 'Cluster' column should contain the cluster assignment for each data point."
        ],
        "confidence": 0.36363636363636365,
        "votes": [
          [
            "Output file: cluster.csv",
            "Columns: Feature_1 through Feature_54 (for 54 features after excluding Cover_Type)",
            "Additional column: Cluster (cluster labels)",
            "No header row required as per instructions"
          ],
          [
            "Output file: cluster.csv",
            "Columns: Feature_1, Feature_2, ..., Feature_54, Cluster",
            "Feature_i columns contain the original feature values from the 54 input features (excluding Cover_Type)",
            "Cluster column contains integer cluster labels assigned by the clustering algorithm",
            "CSV format with header row",
            "Same number of rows as input data (5000)"
          ],
          [
            "The output file 'cluster.csv' must contain columns named 'Feature_i' (where i is the index of the feature) and 'Cluster'.",
            "The 'Cluster' column should contain the cluster assignment for each data point."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5233333333333333
  },
  "ml-cluster-013": {
    "m_q": {
      "target_metric": {
        "value": "Cluster labels for each country based on socioeconomic and health factors",
        "confidence": 0.3333333333333333,
        "votes": [
          "Cluster labels for each country based on socioeconomic and health factors",
          "Cluster assignments for countries based on socioeconomic and health factors",
          "Clustering of countries based on socioeconomic and health factors and saving the cluster assignments."
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Cluster"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [
            "Cluster"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the appropriate number of clusters for this dataset?",
          "Which features should be used for clustering?",
          "How should features be scaled/normalized?",
          "What clustering algorithm is most appropriate?",
          "How should cluster quality be evaluated?",
          "What are the relevant features to use for clustering (all numeric columns excluding country)?",
          "What is the optimal number of clusters for this dataset?",
          "Should the features be normalized/standardized before clustering?",
          "Which clustering algorithm is most appropriate (K-means, hierarchical, DBSCAN)?",
          "How should the features be represented in the output (original values or transformed values)?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the appropriate number of clusters for this dataset?",
            "Which features should be used for clustering?",
            "How should features be scaled/normalized?",
            "What clustering algorithm is most appropriate?",
            "How should cluster quality be evaluated?"
          ],
          [
            "What are the relevant features to use for clustering (all numeric columns excluding country)?",
            "What is the optimal number of clusters for this dataset?",
            "Should the features be normalized/standardized before clustering?",
            "Which clustering algorithm is most appropriate (K-means, hierarchical, DBSCAN)?",
            "How should the features be represented in the output (original values or transformed values)?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Country-data.csv",
          "data-dictionary.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Country-data.csv",
            "data-dictionary.csv"
          ],
          [
            "Country-data.csv",
            "data-dictionary.csv"
          ],
          [
            "Country-data.csv",
            "data-dictionary.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column name case mismatch: 'Income' vs 'income', 'Inflation' vs 'inflation' in data dictionary vs data file",
          "data-dictionary.csv has 'Income' capitalized but Country-data.csv has 'income' lowercase"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Column name case mismatch: 'Income' vs 'income', 'Inflation' vs 'inflation' in data dictionary vs data file"
          ],
          [
            "data-dictionary.csv has 'Income' capitalized but Country-data.csv has 'income' lowercase"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "child_mort": "deaths per 1000 live births",
          "exports": "percentage of GDP",
          "health": "percentage of GDP",
          "imports": "percentage of GDP",
          "income": "net income per person (currency units)",
          "inflation": "annual growth rate percentage",
          "life_expec": "years",
          "total_fer": "children per woman",
          "gdpp": "GDP per capita (currency units)"
        },
        "confidence": 1.0,
        "votes": [
          {
            "child_mort": "deaths per 1000 live births",
            "exports": "percentage of GDP",
            "health": "percentage of GDP",
            "imports": "percentage of GDP",
            "income": "net income per person (currency units)",
            "inflation": "annual growth rate percentage",
            "life_expec": "years",
            "total_fer": "children per woman",
            "gdpp": "GDP per capita (currency units)"
          },
          {
            "child_mort": "deaths per 1000 live births",
            "exports": "percentage of GDP",
            "health": "percentage of GDP",
            "imports": "percentage of GDP",
            "income": "net income per person (currency unspecified)",
            "inflation": "annual growth rate percentage",
            "life_expec": "years",
            "total_fer": "children per woman",
            "gdpp": "GDP per capita (currency unspecified)"
          },
          {
            "child_mort": "deaths per 1000 live births",
            "exports": "% of GDP",
            "health": "% of GDP",
            "imports": "% of GDP",
            "income": "Net income per person",
            "inflation": "annual growth rate of GDP",
            "life_expec": "years",
            "total_fer": "children per woman",
            "gdpp": "GDP per capita"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Features have different scales and units (percentages, counts, currency values)",
          "Income and gdpp are in different currency units but both measure economic output per capita",
          "Some percentages (exports, health, imports) exceed 100% in sample data",
          "Features have vastly different scales (e.g., gdpp ranges to 50000+ while inflation is typically <30)",
          "income and gdpp are in absolute currency values while exports, health, imports are percentages",
          "Standardization/normalization required before clustering",
          "Variables have different scales, requiring standardization or normalization before clustering."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Features have different scales and units (percentages, counts, currency values)",
            "Income and gdpp are in different currency units but both measure economic output per capita",
            "Some percentages (exports, health, imports) exceed 100% in sample data"
          ],
          [
            "Features have vastly different scales (e.g., gdpp ranges to 50000+ while inflation is typically <30)",
            "income and gdpp are in absolute currency values while exports, health, imports are percentages",
            "Standardization/normalization required before clustering"
          ],
          [
            "Variables have different scales, requiring standardization or normalization before clustering."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "nan"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            "",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 1.0,
        "votes": [
          10.0,
          10.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Country column should be excluded from clustering features",
          "All numeric columns except 'country' should be considered for clustering",
          "Features need scaling/normalization before clustering",
          "Output file must be named 'cluster.csv'",
          "Output must have columns 'Feature_i' (feature values) and 'Cluster' (cluster label)",
          "All numeric features (child_mort, exports, health, imports, income, inflation, life_expec, total_fer, gdpp) must be included in clustering",
          "country column should be excluded from clustering features but retained for reference",
          "Output must be saved as cluster.csv",
          "Output must contain Feature_i columns (where i represents each feature dimension) and Cluster column",
          "The number of clusters needs to be determined using methods like the elbow method or silhouette analysis.",
          "The clustering algorithm needs to be chosen (e.g., K-means, hierarchical clustering).",
          "The 'country' column should not be used directly in the clustering algorithm."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Country column should be excluded from clustering features",
            "All numeric columns except 'country' should be considered for clustering",
            "Features need scaling/normalization before clustering",
            "Output file must be named 'cluster.csv'",
            "Output must have columns 'Feature_i' (feature values) and 'Cluster' (cluster label)"
          ],
          [
            "All numeric features (child_mort, exports, health, imports, income, inflation, life_expec, total_fer, gdpp) must be included in clustering",
            "country column should be excluded from clustering features but retained for reference",
            "Output must be saved as cluster.csv",
            "Output must contain Feature_i columns (where i represents each feature dimension) and Cluster column"
          ],
          [
            "The number of clusters needs to be determined using methods like the elbow method or silhouette analysis.",
            "The clustering algorithm needs to be chosen (e.g., K-means, hierarchical clustering).",
            "The 'country' column should not be used directly in the clustering algorithm."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Check for missing values in numeric columns",
          "Identify outliers in features like child_mort, income, gdpp",
          "Verify percentage ranges for exports, health, imports",
          "Remove or impute any rows with missing values in numeric columns before clustering"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing values in numeric columns",
            "Identify outliers in features like child_mort, income, gdpp",
            "Verify percentage ranges for exports, health, imports"
          ],
          [
            "Remove or impute any rows with missing values in numeric columns before clustering"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Correlation analysis between features",
          "Multicollinearity check (e.g., between income and gdpp)",
          "Elbow method or silhouette analysis for optimal cluster count",
          "Feature importance/variance analysis",
          "Elbow method to determine optimal number of clusters",
          "Silhouette score to validate cluster quality",
          "Within-cluster sum of squares (WCSS) analysis"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Correlation analysis between features",
            "Multicollinearity check (e.g., between income and gdpp)",
            "Elbow method or silhouette analysis for optimal cluster count",
            "Feature importance/variance analysis"
          ],
          [
            "Elbow method to determine optimal number of clusters",
            "Silhouette score to validate cluster quality",
            "Within-cluster sum of squares (WCSS) analysis"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV format",
          "Column names: 'Feature_i' for feature vector values, 'Cluster' for cluster labels",
          "One row per country",
          "Feature_i columns should contain the scaled/normalized feature values used for clustering",
          "Output file named cluster.csv",
          "Column names must be Feature_1, Feature_2, ..., Feature_9, Cluster (9 features total)",
          "Feature columns should contain the feature values (likely standardized)",
          "Cluster column should contain integer cluster labels",
          "One row per country (167 rows expected)",
          "CSV format with header row",
          "The output file should be named 'cluster.csv'.",
          "The output file should have columns named 'Feature_i' where i is the index of the feature vector, and a 'Cluster' column.",
          "The 'Cluster' column should contain the cluster assignment for each country."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV format",
            "Column names: 'Feature_i' for feature vector values, 'Cluster' for cluster labels",
            "One row per country",
            "Feature_i columns should contain the scaled/normalized feature values used for clustering"
          ],
          [
            "Output file named cluster.csv",
            "Column names must be Feature_1, Feature_2, ..., Feature_9, Cluster (9 features total)",
            "Feature columns should contain the feature values (likely standardized)",
            "Cluster column should contain integer cluster labels",
            "One row per country (167 rows expected)",
            "CSV format with header row"
          ],
          [
            "The output file should be named 'cluster.csv'.",
            "The output file should have columns named 'Feature_i' where i is the index of the feature vector, and a 'Cluster' column.",
            "The 'Cluster' column should contain the cluster assignment for each country."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5916666666666668
  },
  "ml-cluster-014": {
    "m_q": {
      "target_metric": {
        "value": "Cluster labels for each customer based on personal and purchase behavior features",
        "confidence": 0.3333333333333333,
        "votes": [
          "Cluster labels for each customer based on personal and purchase behavior features",
          "Customer clustering labels and feature vectors",
          "Clusters of customers based on personal and purchase data"
        ]
      },
      "filters": {
        "value": [
          "Remove rows with missing Income values",
          "Consider only relevant features for clustering"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with missing Income values",
            "Consider only relevant features for clustering"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Which features should be selected for clustering?",
          "What preprocessing is needed for categorical variables?",
          "How to handle missing values?",
          "What clustering algorithm and parameters to use?",
          "How many clusters are appropriate?",
          "How to interpret the resulting clusters?",
          "What features should be extracted from personal data (Year_Birth, Education, Marital_Status, Income, Kidhome, Teenhome)?",
          "What features should be extracted from purchase data (MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds)?",
          "What features should be extracted from behavior data (NumDealsPurchases, NumWebPurchases, NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth, Recency)?",
          "Should campaign acceptance features (AcceptedCmp1-5, Response) be included?",
          "What is the optimal number of clusters?",
          "What clustering algorithm should be applied (K-means, hierarchical, DBSCAN)?",
          "Should features be normalized/standardized before clustering?",
          "How should categorical variables (Education, Marital_Status) be encoded?",
          "How should missing values in Income be handled?"
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "Which features should be selected for clustering?",
            "What preprocessing is needed for categorical variables?",
            "How to handle missing values?",
            "What clustering algorithm and parameters to use?",
            "How many clusters are appropriate?",
            "How to interpret the resulting clusters?"
          ],
          [
            "What features should be extracted from personal data (Year_Birth, Education, Marital_Status, Income, Kidhome, Teenhome)?",
            "What features should be extracted from purchase data (MntWines, MntFruits, MntMeatProducts, MntFishProducts, MntSweetProducts, MntGoldProds)?",
            "What features should be extracted from behavior data (NumDealsPurchases, NumWebPurchases, NumCatalogPurchases, NumStorePurchases, NumWebVisitsMonth, Recency)?",
            "Should campaign acceptance features (AcceptedCmp1-5, Response) be included?",
            "What is the optimal number of clusters?",
            "What clustering algorithm should be applied (K-means, hierarchical, DBSCAN)?",
            "Should features be normalized/standardized before clustering?",
            "How should categorical variables (Education, Marital_Status) be encoded?",
            "How should missing values in Income be handled?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "marketing_campaign.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "marketing_campaign.csv"
          ],
          [
            "marketing_campaign.csv"
          ],
          [
            "marketing_campaign.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Single column with tab-separated values instead of proper CSV structure",
          "Column names appear as single string with tabs",
          "All columns are combined in a single tab-separated string column instead of being properly delimited",
          "Column headers indicate tab separation but data appears to be in one column"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Single column with tab-separated values instead of proper CSV structure",
            "Column names appear as single string with tabs"
          ],
          [
            "All columns are combined in a single tab-separated string column instead of being properly delimited",
            "Column headers indicate tab separation but data appears to be in one column"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "year_birth": "year",
          "income": "currency",
          "recency": "days",
          "mntwines": "currency spent",
          "mntfruits": "currency spent",
          "mntmeatproducts": "currency spent",
          "mntfishproducts": "currency spent",
          "mntsweetproducts": "currency spent",
          "mntgoldprods": "currency spent",
          "numdealspurchases": "count",
          "numwebpurchases": "count",
          "numcatalogpurchases": "count",
          "numstorepurchases": "count",
          "numwebvisitsmonth": "count",
          "z_costcontact": "cost units",
          "z_revenue": "revenue units",
          "kidhome": "count",
          "teenhome": "count",
          "dt_customer": "date"
        },
        "confidence": 0.824561403508772,
        "votes": [
          {
            "Year_Birth": "year",
            "Income": "currency",
            "Recency": "days since last purchase",
            "MntWines": "currency spent",
            "MntFruits": "currency spent",
            "MntMeatProducts": "currency spent",
            "MntFishProducts": "currency spent",
            "MntSweetProducts": "currency spent",
            "MntGoldProds": "currency spent",
            "NumDealsPurchases": "count",
            "NumWebPurchases": "count",
            "NumCatalogPurchases": "count",
            "NumStorePurchases": "count",
            "NumWebVisitsMonth": "count per month",
            "Z_CostContact": "cost units",
            "Z_Revenue": "revenue units"
          },
          {
            "Year_Birth": "year",
            "Income": "currency",
            "Kidhome": "count",
            "Teenhome": "count",
            "Dt_Customer": "date",
            "Recency": "days",
            "MntWines": "currency_amount",
            "MntFruits": "currency_amount",
            "MntMeatProducts": "currency_amount",
            "MntFishProducts": "currency_amount",
            "MntSweetProducts": "currency_amount",
            "MntGoldProds": "currency_amount",
            "NumDealsPurchases": "count",
            "NumWebPurchases": "count",
            "NumCatalogPurchases": "count",
            "NumStorePurchases": "count",
            "NumWebVisitsMonth": "count"
          },
          {
            "Year_Birth": "year",
            "Income": "currency",
            "Recency": "days",
            "MntWines": "currency",
            "MntFruits": "currency",
            "MntMeatProducts": "currency",
            "MntFishProducts": "currency",
            "MntSweetProducts": "currency",
            "MntGoldProds": "currency",
            "NumDealsPurchases": "count",
            "NumWebPurchases": "count",
            "NumCatalogPurchases": "count",
            "NumStorePurchases": "count",
            "NumWebVisitsMonth": "count"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Income has missing values (empty strings)",
          "Monetary amounts (Mnt*) have different scales",
          "Count variables have different ranges",
          "Z_CostContact and Z_Revenue appear constant in sample",
          "Year_Birth is in years (1900s) while other numeric features are smaller counts/amounts",
          "Income values are much larger scale than purchase amounts",
          "Age should be derived from Year_Birth for meaningful clustering",
          "Customer tenure could be derived from Dt_Customer"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Income has missing values (empty strings)",
            "Monetary amounts (Mnt*) have different scales",
            "Count variables have different ranges",
            "Z_CostContact and Z_Revenue appear constant in sample"
          ],
          [
            "Year_Birth is in years (1900s) while other numeric features are smaller counts/amounts",
            "Income values are much larger scale than purchase amounts",
            "Age should be derived from Year_Birth for meaningful clustering",
            "Customer tenure could be derived from Dt_Customer"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": "\t",
        "confidence": 0.6666666666666666,
        "votes": [
          "\\t",
          "\t",
          "\t"
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "NaN"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 29.0,
        "confidence": 1.0,
        "votes": [
          29.0,
          29.0,
          29.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 0.6666666666666666,
        "votes": [
          "csv with tab delimiter",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "ID should be unique",
          "Year_Birth should be between 1900-2024",
          "Income should be non-negative when present",
          "Kidhome and Teenhome should be non-negative integers",
          "Purchase amounts should be non-negative",
          "Count variables should be non-negative integers",
          "Binary variables (AcceptedCmp*, Complain, Response) should be 0 or 1",
          "Output file must be named cluster.csv",
          "Output must contain Feature_i columns where i represents feature index",
          "Output must contain Cluster column with cluster labels",
          "Each row represents one customer",
          "Cluster labels should be integer values starting from 0 or 1",
          "The 'Year_Birth' column should be within a reasonable range (e.g., 1900 to current year).",
          "Income should be a non-negative value."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "ID should be unique",
            "Year_Birth should be between 1900-2024",
            "Income should be non-negative when present",
            "Kidhome and Teenhome should be non-negative integers",
            "Purchase amounts should be non-negative",
            "Count variables should be non-negative integers",
            "Binary variables (AcceptedCmp*, Complain, Response) should be 0 or 1"
          ],
          [
            "Output file must be named cluster.csv",
            "Output must contain Feature_i columns where i represents feature index",
            "Output must contain Cluster column with cluster labels",
            "Each row represents one customer",
            "Cluster labels should be integer values starting from 0 or 1"
          ],
          [
            "The 'Year_Birth' column should be within a reasonable range (e.g., 1900 to current year).",
            "Income should be a non-negative value."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Calculate age from Year_Birth",
          "Calculate customer tenure from Dt_Customer",
          "Create total spend variable from Mnt* columns",
          "Create total purchases variable from Num*Purchases columns",
          "Create family size from Marital_Status, Kidhome, Teenhome",
          "Remove or impute rows with missing Income values",
          "Exclude constant columns (Z_CostContact, Z_Revenue) from clustering features",
          "Consider excluding ID from feature vector as it's an identifier"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Calculate age from Year_Birth",
            "Calculate customer tenure from Dt_Customer",
            "Create total spend variable from Mnt* columns",
            "Create total purchases variable from Num*Purchases columns",
            "Create family size from Marital_Status, Kidhome, Teenhome"
          ],
          [
            "Remove or impute rows with missing Income values",
            "Exclude constant columns (Z_CostContact, Z_Revenue) from clustering features",
            "Consider excluding ID from feature vector as it's an identifier"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for multicollinearity among features",
          "Test normality of continuous variables",
          "Check for outliers in Income and spend variables",
          "Assess cluster separation metrics (silhouette score, Davies-Bouldin index)",
          "Determine optimal number of clusters using elbow method or silhouette score",
          "Validate cluster quality using silhouette coefficient",
          "Check for cluster separation using Davies-Bouldin index",
          "Check for outliers in numerical features like 'Income', 'MntWines', etc.",
          "Determine the optimal number of clusters using methods like the elbow method or silhouette analysis."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for multicollinearity among features",
            "Test normality of continuous variables",
            "Check for outliers in Income and spend variables",
            "Assess cluster separation metrics (silhouette score, Davies-Bouldin index)"
          ],
          [
            "Determine optimal number of clusters using elbow method or silhouette score",
            "Validate cluster quality using silhouette coefficient",
            "Check for cluster separation using Davies-Bouldin index"
          ],
          [
            "Check for outliers in numerical features like 'Income', 'MntWines', etc.",
            "Determine the optimal number of clusters using methods like the elbow method or silhouette analysis."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output file: cluster.csv",
          "Columns: Feature_1 through Feature_n (where n is feature vector dimension)",
          "Column: Cluster (cluster label)",
          "One row per customer",
          "Feature_i columns should contain the feature vector used for clustering",
          "CSV format with comma delimiter",
          "Column names: Feature_1, Feature_2, ..., Feature_n, Cluster",
          "One row per customer (2240 rows expected)",
          "Feature columns contain normalized/standardized feature values used for clustering",
          "Cluster column contains integer cluster assignment",
          "The output file 'cluster.csv' should contain all features used for clustering.",
          "The 'Cluster' column should contain the cluster labels assigned to each customer."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Output file: cluster.csv",
            "Columns: Feature_1 through Feature_n (where n is feature vector dimension)",
            "Column: Cluster (cluster label)",
            "One row per customer",
            "Feature_i columns should contain the feature vector used for clustering"
          ],
          [
            "CSV format with comma delimiter",
            "Column names: Feature_1, Feature_2, ..., Feature_n, Cluster",
            "One row per customer (2240 rows expected)",
            "Feature columns contain normalized/standardized feature values used for clustering",
            "Cluster column contains integer cluster assignment"
          ],
          [
            "The output file 'cluster.csv' should contain all features used for clustering.",
            "The 'Cluster' column should contain the cluster labels assigned to each customer."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.549561403508772
  },
  "ml-cluster-016": {
    "m_q": {
      "target_metric": {
        "value": "Customer segmentation clusters based on purchasing behavior features",
        "confidence": 0.3333333333333333,
        "votes": [
          "Customer segmentation clusters based on purchasing behavior features",
          "Customer segmentation using clustering algorithm with feature vectors and cluster labels",
          "Customer segments based on purchasing behavior"
        ]
      },
      "filters": {
        "value": [
          "Remove rows with missing Customer ID",
          "Remove negative or zero Quantity values",
          "Remove zero or negative Price values",
          "Remove records with missing Customer ID",
          "Remove records with invalid Quantity or Price values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with missing Customer ID",
            "Remove negative or zero Quantity values",
            "Remove zero or negative Price values"
          ],
          [
            "Remove records with missing Customer ID",
            "Remove records with invalid Quantity or Price values"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Customer ID"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Customer ID"
          ],
          [
            "Customer ID"
          ],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features should be engineered for customer segmentation?",
          "What is the appropriate number of clusters?",
          "How to handle customers with only one transaction?",
          "Should we include country as a feature?",
          "How to normalize features for clustering?",
          "What features should be engineered from transactional data to represent customer behavior?",
          "How many customers have sufficient transaction history for clustering?",
          "What is the optimal number of clusters for customer segmentation?",
          "What clustering algorithm should be used (K-means, hierarchical, DBSCAN)?",
          "How should the feature vector be normalized/standardized?",
          "What are the characteristics of each customer cluster?",
          "What are the appropriate features to use for clustering?",
          "How many clusters are optimal for this dataset?",
          "What are the characteristics of each cluster?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What features should be engineered for customer segmentation?",
            "What is the appropriate number of clusters?",
            "How to handle customers with only one transaction?",
            "Should we include country as a feature?",
            "How to normalize features for clustering?"
          ],
          [
            "What features should be engineered from transactional data to represent customer behavior?",
            "How many customers have sufficient transaction history for clustering?",
            "What is the optimal number of clusters for customer segmentation?",
            "What clustering algorithm should be used (K-means, hierarchical, DBSCAN)?",
            "How should the feature vector be normalized/standardized?",
            "What are the characteristics of each customer cluster?"
          ],
          [
            "What are the appropriate features to use for clustering?",
            "How many clusters are optimal for this dataset?",
            "What are the characteristics of each cluster?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "online_retail_II.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "online_retail_II.csv"
          ],
          [
            "online_retail_II.csv"
          ],
          [
            "online_retail_II.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "InvoiceDate stored as object instead of datetime",
          "Customer ID stored as float64 but represents integer IDs"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "InvoiceDate stored as object instead of datetime",
            "Customer ID stored as float64 but represents integer IDs"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "quantity": "number of items",
          "price": "currency (likely GBP)",
          "invoicedate": "datetime",
          "customer id": "identifier"
        },
        "confidence": 0.75,
        "votes": [
          {
            "Quantity": "count",
            "Price": "currency (likely GBP)",
            "InvoiceDate": "datetime"
          },
          {
            "Quantity": "number of items",
            "Price": "currency (likely GBP)",
            "InvoiceDate": "timestamp",
            "Customer ID": "identifier"
          },
          {
            "Quantity": "number of items",
            "Price": "currency unit"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Quantity values may have large variance",
          "Price values may have different scales",
          "Need to normalize features before clustering",
          "Price values may vary significantly across products",
          "Quantity can include negative values (returns)",
          "Customer ID stored as float64 instead of integer"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Quantity values may have large variance",
            "Price values may have different scales",
            "Need to normalize features before clustering"
          ],
          [
            "Price values may vary significantly across products",
            "Quantity can include negative values (returns)",
            "Customer ID stored as float64 instead of integer"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            null
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 8.0,
        "confidence": 1.0,
        "votes": [
          8.0,
          8.0,
          8.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Each row represents one transaction item",
          "Multiple items per invoice",
          "Customer ID is required for clustering",
          "Quantity must be positive for valid transactions",
          "Price must be positive",
          "Customer ID must not be null for clustering analysis",
          "Must aggregate transactional data to customer level before clustering",
          "Feature vectors must have consistent dimensionality across all customers",
          "Cluster labels must be integer values starting from 0 or 1",
          "Customer ID should be unique for each customer.",
          "Quantity and Price should be non-negative."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Each row represents one transaction item",
            "Multiple items per invoice",
            "Customer ID is required for clustering",
            "Quantity must be positive for valid transactions",
            "Price must be positive"
          ],
          [
            "Customer ID must not be null for clustering analysis",
            "Must aggregate transactional data to customer level before clustering",
            "Feature vectors must have consistent dimensionality across all customers",
            "Cluster labels must be integer values starting from 0 or 1"
          ],
          [
            "Customer ID should be unique for each customer.",
            "Quantity and Price should be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out cancelled transactions (negative Quantity)",
          "Filter out transactions with missing Customer ID",
          "Consider time period constraints if needed",
          "Exclude transactions with missing Customer ID",
          "Consider filtering out cancelled orders (Invoice starting with 'C')",
          "Remove customers with insufficient transaction history",
          "Filter outliers in monetary values and quantities",
          "Remove rows with missing Customer ID.",
          "Handle or remove cancelled orders (negative quantities)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out cancelled transactions (negative Quantity)",
            "Filter out transactions with missing Customer ID",
            "Consider time period constraints if needed"
          ],
          [
            "Exclude transactions with missing Customer ID",
            "Consider filtering out cancelled orders (Invoice starting with 'C')",
            "Remove customers with insufficient transaction history",
            "Filter outliers in monetary values and quantities"
          ],
          [
            "Remove rows with missing Customer ID.",
            "Handle or remove cancelled orders (negative quantities)."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Quantity and Price",
          "Test correlation between engineered features",
          "Determine optimal number of clusters using elbow method or silhouette score",
          "Elbow method or silhouette score to determine optimal number of clusters",
          "Check for feature multicollinearity before clustering",
          "Validate cluster separation using silhouette analysis",
          "Test cluster stability using multiple random initializations",
          "Determine the optimal number of clusters using the Elbow method or Silhouette analysis."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Quantity and Price",
            "Test correlation between engineered features",
            "Determine optimal number of clusters using elbow method or silhouette score"
          ],
          [
            "Elbow method or silhouette score to determine optimal number of clusters",
            "Check for feature multicollinearity before clustering",
            "Validate cluster separation using silhouette analysis",
            "Test cluster stability using multiple random initializations"
          ],
          [
            "Determine the optimal number of clusters using the Elbow method or Silhouette analysis."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "File named cluster.csv",
          "Columns named Feature_i for feature values",
          "Column named Cluster for cluster labels",
          "One row per customer",
          "Output file must be named 'cluster.csv'",
          "Must contain columns: Feature_1, Feature_2, ..., Feature_n, Cluster",
          "Column names for features must follow pattern 'Feature_i' where i is 1-indexed",
          "Cluster column contains integer cluster labels",
          "One row per customer in the output",
          "CSV format with header row",
          "Output file must be named 'cluster.csv'.",
          "Output file must contain columns 'Feature_i' for each feature used in clustering and a 'Cluster' column representing the cluster assignment.",
          "The 'Cluster' column should contain integer values representing the cluster labels."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "File named cluster.csv",
            "Columns named Feature_i for feature values",
            "Column named Cluster for cluster labels",
            "One row per customer"
          ],
          [
            "Output file must be named 'cluster.csv'",
            "Must contain columns: Feature_1, Feature_2, ..., Feature_n, Cluster",
            "Column names for features must follow pattern 'Feature_i' where i is 1-indexed",
            "Cluster column contains integer cluster labels",
            "One row per customer in the output",
            "CSV format with header row"
          ],
          [
            "Output file must be named 'cluster.csv'.",
            "Output file must contain columns 'Feature_i' for each feature used in clustering and a 'Cluster' column representing the cluster assignment.",
            "The 'Cluster' column should contain integer values representing the cluster labels."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6208333333333333
  },
  "ml-cluster-019": {
    "m_q": {
      "target_metric": {
        "value": "Cluster labels for customer groups based on behavioral features",
        "confidence": 0.3333333333333333,
        "votes": [
          "Cluster labels for customer groups based on behavioral features",
          "Customer segmentation clusters with feature vectors and cluster labels",
          "Customer clusters based on purchasing behavior"
        ]
      },
      "filters": {
        "value": [
          "Data from 2009-2010 only",
          "Valid Customer ID not null",
          "Positive Quantity and Price",
          "Valid Customer ID (non-null)",
          "Data from 2009-2010"
        ],
        "confidence": 0.39999999999999997,
        "votes": [
          [
            "Data from 2009-2010 only",
            "Valid Customer ID not null",
            "Positive Quantity and Price"
          ],
          [
            "Data from 2009-2010 only",
            "Valid Customer ID (non-null)"
          ],
          [
            "Data from 2009-2010"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Customer ID"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Customer ID"
          ],
          [
            "Customer ID"
          ],
          [
            "Customer ID"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features should represent customer behavior?",
          "How many clusters are appropriate?",
          "Which clustering algorithm to use?",
          "How to handle customers with limited transactions?",
          "What customer-level features should be engineered for clustering (e.g., total spend, frequency, recency, average order value)?",
          "How many customers have valid Customer ID in the 2009-2010 dataset?",
          "What is the appropriate number of clusters for customer segmentation?",
          "How to handle missing Customer IDs and negative quantities (returns)?",
          "What feature scaling/normalization is needed before clustering?",
          "Which clustering algorithm should be used (K-means, hierarchical, DBSCAN)?",
          "How to validate cluster quality and determine optimal cluster count?",
          "What features can be engineered from the available data to represent customer purchasing behavior?",
          "How many clusters are appropriate for this dataset?",
          "What are the characteristics of each cluster?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What features should represent customer behavior?",
            "How many clusters are appropriate?",
            "Which clustering algorithm to use?",
            "How to handle customers with limited transactions?"
          ],
          [
            "What customer-level features should be engineered for clustering (e.g., total spend, frequency, recency, average order value)?",
            "How many customers have valid Customer ID in the 2009-2010 dataset?",
            "What is the appropriate number of clusters for customer segmentation?",
            "How to handle missing Customer IDs and negative quantities (returns)?",
            "What feature scaling/normalization is needed before clustering?",
            "Which clustering algorithm should be used (K-means, hierarchical, DBSCAN)?",
            "How to validate cluster quality and determine optimal cluster count?"
          ],
          [
            "What features can be engineered from the available data to represent customer purchasing behavior?",
            "How many clusters are appropriate for this dataset?",
            "What are the characteristics of each cluster?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Year 2009-2010.csv",
          "Year 2010-2011.csv",
          "online_retail_II.xlsx"
        ],
        "confidence": 0.7777777777777777,
        "votes": [
          [
            "Year 2009-2010.csv",
            "Year 2010-2011.csv",
            "online_retail_II.xlsx"
          ],
          [
            "Year 2009-2010.csv"
          ],
          [
            "Year 2009-2010.csv",
            "Year 2010-2011.csv",
            "online_retail_II.xlsx"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "InvoiceDate dtype differs: object in CSV vs datetime64[ns] in Excel",
          "Year 2010-2011.csv contains data from 2010-2011, not just 2009-2010",
          "Customer ID has float64 dtype with NaN values, should be treated as integer identifier",
          "InvoiceDate is object type in CSV files but datetime64 in Excel file",
          "InvoiceDate column has different data types across files (object in CSVs, datetime64[ns] in XLSX).",
          "Column names are consistent across files."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "InvoiceDate dtype differs: object in CSV vs datetime64[ns] in Excel",
            "Year 2010-2011.csv contains data from 2010-2011, not just 2009-2010"
          ],
          [
            "Customer ID has float64 dtype with NaN values, should be treated as integer identifier",
            "InvoiceDate is object type in CSV files but datetime64 in Excel file"
          ],
          [
            "InvoiceDate column has different data types across files (object in CSVs, datetime64[ns] in XLSX).",
            "Column names are consistent across files."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "quantity": "units",
          "price": "currency units (likely GBP)",
          "invoicedate": "datetime"
        },
        "confidence": 0.8888888888888888,
        "votes": [
          {
            "Quantity": "units",
            "Price": "currency units (likely GBP)",
            "InvoiceDate": "datetime"
          },
          {
            "Price": "currency (GBP implied from UK-based retailer)",
            "Quantity": "count of items",
            "InvoiceDate": "timestamp"
          },
          {
            "Quantity": "number of items",
            "Price": "monetary unit"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Quantity may have negative values (returns)",
          "Price may have extreme values",
          "Customer ID is float but should be categorical",
          "Price values vary significantly across products",
          "Quantity can be negative (indicating returns/cancellations)",
          "Customer purchase frequencies and amounts likely span multiple orders of magnitude"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Quantity may have negative values (returns)",
            "Price may have extreme values",
            "Customer ID is float but should be categorical"
          ],
          [
            "Price values vary significantly across products",
            "Quantity can be negative (indicating returns/cancellations)",
            "Customer purchase frequencies and amounts likely span multiple orders of magnitude"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Year 2010-2011.csv includes 2011 data which should be excluded per question",
          "online_retail_II.xlsx appears to duplicate Year 2009-2010.csv data",
          "Year 2010-2011.csv contains data outside the required 2009-2010 range",
          "Need to ensure only 2009-2010 data is used from Year 2009-2010.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Year 2010-2011.csv includes 2011 data which should be excluded per question",
            "online_retail_II.xlsx appears to duplicate Year 2009-2010.csv data"
          ],
          [
            "Year 2010-2011.csv contains data outside the required 2009-2010 range",
            "Need to ensure only 2009-2010 data is used from Year 2009-2010.csv"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 8.0,
        "confidence": 1.0,
        "votes": [
          8.0,
          8.0,
          8.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Only use data from 2009-2010",
          "Customer ID must not be null",
          "Quantity > 0 for purchases (exclude returns)",
          "Price > 0",
          "Output file must be named cluster.csv",
          "Output columns: Feature_i (for feature values) and Cluster (cluster label)",
          "Only use data from 2009-2010 period",
          "Customer ID must not be null for clustering",
          "Quantity and Price should be validated for data quality",
          "Each customer should be represented by a single feature vector",
          "Customer ID should be treated as a categorical variable.",
          "Quantity and Price should be non-negative."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only use data from 2009-2010",
            "Customer ID must not be null",
            "Quantity > 0 for purchases (exclude returns)",
            "Price > 0",
            "Output file must be named cluster.csv",
            "Output columns: Feature_i (for feature values) and Cluster (cluster label)"
          ],
          [
            "Only use data from 2009-2010 period",
            "Customer ID must not be null for clustering",
            "Quantity and Price should be validated for data quality",
            "Each customer should be represented by a single feature vector"
          ],
          [
            "Customer ID should be treated as a categorical variable.",
            "Quantity and Price should be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "InvoiceDate between 2009-01-01 and 2010-12-31",
          "Quantity > 0",
          "Price > 0",
          "Customer ID not null",
          "Remove records with missing Customer ID",
          "Filter InvoiceDate to ensure 2009-2010 range only",
          "Consider filtering cancelled orders (invoices starting with 'C')",
          "Remove or handle negative quantities appropriately",
          "Remove rows with missing Customer ID.",
          "Handle or remove cancelled orders (invoices starting with 'C')."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "InvoiceDate between 2009-01-01 and 2010-12-31",
            "Quantity > 0",
            "Price > 0",
            "Customer ID not null"
          ],
          [
            "Remove records with missing Customer ID",
            "Filter InvoiceDate to ensure 2009-2010 range only",
            "Consider filtering cancelled orders (invoices starting with 'C')",
            "Remove or handle negative quantities appropriately"
          ],
          [
            "Remove rows with missing Customer ID.",
            "Handle or remove cancelled orders (invoices starting with 'C')."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in Quantity and Price",
          "Assess customer transaction frequency distribution",
          "Evaluate feature correlation for multicollinearity",
          "Determine optimal number of clusters using elbow method/silhouette score",
          "Elbow method to determine optimal number of clusters",
          "Silhouette score for cluster quality evaluation",
          "Davies-Bouldin index for cluster separation",
          "Feature correlation analysis before clustering",
          "Determine the optimal number of clusters using methods like the Elbow method or Silhouette analysis."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in Quantity and Price",
            "Assess customer transaction frequency distribution",
            "Evaluate feature correlation for multicollinearity",
            "Determine optimal number of clusters using elbow method/silhouette score"
          ],
          [
            "Elbow method to determine optimal number of clusters",
            "Silhouette score for cluster quality evaluation",
            "Davies-Bouldin index for cluster separation",
            "Feature correlation analysis before clustering"
          ],
          [
            "Determine the optimal number of clusters using methods like the Elbow method or Silhouette analysis."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV format",
          "Column names: Feature_1, Feature_2, ..., Feature_n, Cluster",
          "One row per customer",
          "Cluster column contains integer labels",
          "Output file must be named 'cluster.csv'",
          "Column names must be 'Feature_1', 'Feature_2', ..., 'Feature_n', 'Cluster'",
          "One row per customer in the feature space",
          "Cluster labels should be integer identifiers",
          "Feature columns should contain normalized/scaled values used for clustering",
          "Output file should be named 'cluster.csv'.",
          "Output file should contain columns 'Feature_i' and 'Cluster'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "CSV format",
            "Column names: Feature_1, Feature_2, ..., Feature_n, Cluster",
            "One row per customer",
            "Cluster column contains integer labels"
          ],
          [
            "Output file must be named 'cluster.csv'",
            "Column names must be 'Feature_1', 'Feature_2', ..., 'Feature_n', 'Cluster'",
            "One row per customer in the feature space",
            "Cluster labels should be integer identifiers",
            "Feature columns should contain normalized/scaled values used for clustering"
          ],
          [
            "Output file should be named 'cluster.csv'.",
            "Output file should contain columns 'Feature_i' and 'Cluster'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.64
  },
  "ml-competition-001": {
    "m_q": {
      "target_metric": {
        "value": "Binary churn prediction (Exited) for test.csv customers",
        "confidence": 0.3333333333333333,
        "votes": [
          "Binary churn prediction (Exited) for test.csv customers",
          "Predict customer churn probability (Exited) for test data and output to submission.csv",
          "Probability of customer churn (Exited) for each customer in test.csv"
        ]
      },
      "filters": {
        "value": [
          "Predict only for rows in test.csv",
          "Follow submission format from sample_submission.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Predict only for rows in test.csv",
            "Follow submission format from sample_submission.csv"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "CustomerId"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CustomerId"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features predict customer churn?",
          "How to handle missing values in training data?",
          "Should Surname be used as a feature?",
          "What model should be trained on train.csv?",
          "How to map test predictions to submission format?",
          "What features predict customer churn based on training data?",
          "What is the expected output format for submission.csv?",
          "Should predictions be binary (0/1) or probability scores?",
          "How to handle missing values in features?",
          "What is the relationship between train.csv and Churn_Modelling.csv?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What features predict customer churn?",
            "How to handle missing values in training data?",
            "Should Surname be used as a feature?",
            "What model should be trained on train.csv?",
            "How to map test predictions to submission format?"
          ],
          [
            "What features predict customer churn based on training data?",
            "What is the expected output format for submission.csv?",
            "Should predictions be binary (0/1) or probability scores?",
            "How to handle missing values in features?",
            "What is the relationship between train.csv and Churn_Modelling.csv?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Churn_Modelling.csv",
          "train.csv",
          "test.csv",
          "sample_submission.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Churn_Modelling.csv",
            "train.csv",
            "test.csv",
            "sample_submission.csv"
          ],
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv",
            "Churn_Modelling.csv"
          ],
          [
            "Churn_Modelling.csv",
            "test.csv",
            "train.csv",
            "sample_submission.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Churn_Modelling.csv has RowNumber column not in other files",
          "train.csv has 'id' column not in test.csv",
          "sample_submission.csv uses 'id' while test.csv uses 'CustomerId'",
          "Exited column type differs: int64 in train.csv vs float64 in sample_submission.csv",
          "train.csv has 'id' and 'Exited' columns not present in test.csv",
          "test.csv missing 'id' and 'Exited' columns present in train.csv",
          "Churn_Modelling.csv has 'RowNumber' column not in train.csv or test.csv",
          "Churn_Modelling.csv missing 'id' column present in train.csv",
          "Churn_Modelling.csv contains 'RowNumber' and 'Exited' which are not present in test.csv. train.csv contains 'id' and 'Exited' which are not present in test.csv. Churn_Modelling.csv contains all columns from test.csv, plus 'RowNumber' and 'Exited'. train.csv contains all columns from test.csv, plus 'id' and 'Exited'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Churn_Modelling.csv has RowNumber column not in other files",
            "train.csv has 'id' column not in test.csv",
            "sample_submission.csv uses 'id' while test.csv uses 'CustomerId'",
            "Exited column type differs: int64 in train.csv vs float64 in sample_submission.csv"
          ],
          [
            "train.csv has 'id' and 'Exited' columns not present in test.csv",
            "test.csv missing 'id' and 'Exited' columns present in train.csv",
            "Churn_Modelling.csv has 'RowNumber' column not in train.csv or test.csv",
            "Churn_Modelling.csv missing 'id' column present in train.csv"
          ],
          [
            "Churn_Modelling.csv contains 'RowNumber' and 'Exited' which are not present in test.csv. train.csv contains 'id' and 'Exited' which are not present in test.csv. Churn_Modelling.csv contains all columns from test.csv, plus 'RowNumber' and 'Exited'. train.csv contains all columns from test.csv, plus 'id' and 'Exited'."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "creditscore": "score points",
          "age": "years",
          "balance": "currency units",
          "estimatedsalary": "currency units",
          "tenure": "years",
          "numofproducts": "count",
          "hascrcard": "binary_flag",
          "isactivemember": "binary_flag",
          "exited": "binary_flag_or_probability"
        },
        "confidence": 0.7037037037037036,
        "votes": [
          {
            "CreditScore": "score points",
            "Age": "years",
            "Balance": "currency units",
            "EstimatedSalary": "currency units",
            "Tenure": "years"
          },
          {
            "CreditScore": "credit_score_points",
            "Age": "years",
            "Tenure": "years",
            "Balance": "currency_units",
            "NumOfProducts": "count",
            "HasCrCard": "binary_flag",
            "IsActiveMember": "binary_flag",
            "EstimatedSalary": "currency_units",
            "Exited": "binary_flag_or_probability"
          },
          {
            "CreditScore": "score",
            "Age": "years",
            "Tenure": "years",
            "Balance": "currency",
            "EstimatedSalary": "currency"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Balance and EstimatedSalary likely in same currency but scale not specified",
          "CreditScore range unknown (typical 300-850)",
          "Age has decimal values (e.g., 45.25) suggesting possible measurement error",
          "Balance and EstimatedSalary are on much larger scales than other numeric features",
          "CreditScore ranges differently from Age and Tenure"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Balance and EstimatedSalary likely in same currency but scale not specified",
            "CreditScore range unknown (typical 300-850)",
            "Age has decimal values (e.g., 45.25) suggesting possible measurement error"
          ],
          [
            "Balance and EstimatedSalary are on much larger scales than other numeric features",
            "CreditScore ranges differently from Age and Tenure"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Churn_Modelling.csv has 10002 rows while train.csv has 140278 rows - different datasets",
          "test.csv missing Exited column (target for prediction)",
          "sample_submission.csv has 110023 rows while test.csv has 24756 rows - mismatch"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Churn_Modelling.csv has 10002 rows while train.csv has 140278 rows - different datasets",
            "test.csv missing Exited column (target for prediction)",
            "sample_submission.csv has 110023 rows while test.csv has 24756 rows - mismatch"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "?",
          "??",
          "NaN"
        ],
        "confidence": 0.6111111111111112,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            " ",
            "?",
            "??"
          ],
          [
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 14.0,
        "confidence": 0.8666666666666667,
        "votes": [
          14.0,
          14.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "CustomerId should be unique across datasets",
          "Exited must be 0 or 1 (or probability between 0-1 for submission)",
          "HasCrCard and IsActiveMember should be 0.0 or 1.0",
          "NumOfProducts should be positive integer",
          "CreditScore should be between 300-850 typically",
          "test.csv has 24756 rows requiring predictions",
          "sample_submission.csv has 110023 rows, suggesting multiple submissions or extended test set",
          "Exited values in sample_submission.csv are float (0.5), indicating probability predictions expected",
          "train.csv Exited column is binary (0 or 1) for training labels",
          "submission.csv must have 'id' and 'Exited' columns matching sample_submission.csv format",
          "Predictions must be made for ids 165034 to 275056 based on sample_submission.csv",
          "The 'id' column in sample_submission.csv corresponds to the index of the rows to be predicted in test.csv, which is implicitly given by the order of rows in test.csv.",
          "The 'Exited' column in sample_submission.csv should contain the predicted probability of churn for each customer in test.csv."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CustomerId should be unique across datasets",
            "Exited must be 0 or 1 (or probability between 0-1 for submission)",
            "HasCrCard and IsActiveMember should be 0.0 or 1.0",
            "NumOfProducts should be positive integer",
            "CreditScore should be between 300-850 typically"
          ],
          [
            "test.csv has 24756 rows requiring predictions",
            "sample_submission.csv has 110023 rows, suggesting multiple submissions or extended test set",
            "Exited values in sample_submission.csv are float (0.5), indicating probability predictions expected",
            "train.csv Exited column is binary (0 or 1) for training labels",
            "submission.csv must have 'id' and 'Exited' columns matching sample_submission.csv format",
            "Predictions must be made for ids 165034 to 275056 based on sample_submission.csv"
          ],
          [
            "The 'id' column in sample_submission.csv corresponds to the index of the rows to be predicted in test.csv, which is implicitly given by the order of rows in test.csv.",
            "The 'Exited' column in sample_submission.csv should contain the predicted probability of churn for each customer in test.csv."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows with missing Geography",
          "Handle missing Age values",
          "Check for duplicate CustomerId in test.csv",
          "Validate binary columns have only 0/1 values",
          "Use only rows from train.csv for model training",
          "Generate predictions only for test.csv data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with missing Geography",
            "Handle missing Age values",
            "Check for duplicate CustomerId in test.csv",
            "Validate binary columns have only 0/1 values"
          ],
          [
            "Use only rows from train.csv for model training",
            "Generate predictions only for test.csv data"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check class imbalance in Exited column",
          "Test correlation between features and Exited",
          "Validate train/test distribution similarity",
          "Check for data leakage between train and test",
          "Check class balance in training data (Exited distribution)",
          "Evaluate missing data patterns across train and test sets",
          "Verify feature distributions match between train and test"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check class imbalance in Exited column",
            "Test correlation between features and Exited",
            "Validate train/test distribution similarity",
            "Check for data leakage between train and test"
          ],
          [
            "Check class balance in training data (Exited distribution)",
            "Evaluate missing data patterns across train and test sets",
            "Verify feature distributions match between train and test"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "submission.csv must have exactly columns: id, Exited",
          "id must match test.csv CustomerId",
          "Exited must be float64 between 0-1",
          "Row order must match sample_submission.csv",
          "submission.csv must have exactly 2 columns: id, Exited",
          "id column must contain integer identifiers matching sample_submission.csv",
          "Exited column must contain probability scores (float) between 0 and 1",
          "Number of rows in submission.csv must match sample_submission.csv (110023 rows)",
          "CSV format with comma delimiter and header row",
          "The submission file must be named 'submission.csv'.",
          "The submission file must contain two columns: 'id' and 'Exited'.",
          "The 'id' column must contain the CustomerId from test.csv.",
          "The 'Exited' column must contain the predicted probability of churn (a float between 0 and 1)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "submission.csv must have exactly columns: id, Exited",
            "id must match test.csv CustomerId",
            "Exited must be float64 between 0-1",
            "Row order must match sample_submission.csv"
          ],
          [
            "submission.csv must have exactly 2 columns: id, Exited",
            "id column must contain integer identifiers matching sample_submission.csv",
            "Exited column must contain probability scores (float) between 0 and 1",
            "Number of rows in submission.csv must match sample_submission.csv (110023 rows)",
            "CSV format with comma delimiter and header row"
          ],
          [
            "The submission file must be named 'submission.csv'.",
            "The submission file must contain two columns: 'id' and 'Exited'.",
            "The 'id' column must contain the CustomerId from test.csv.",
            "The 'Exited' column must contain the predicted probability of churn (a float between 0 and 1)."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5924074074074075
  },
  "ml-competition-003": {
    "m_q": {
      "target_metric": {
        "value": "Predict class probabilities (class_0 and class_1) for test.csv samples",
        "confidence": 0.3333333333333333,
        "votes": [
          "Predict class probabilities (class_0 and class_1) for test.csv samples",
          "Binary classification probabilities (class_0 and class_1) for each test sample",
          "Predict the probability of each class (class_0, class_1) for each Id in test.csv and format the predictions in submission.csv as specified in sample_submission.csv."
        ]
      },
      "filters": {
        "value": [
          "Test samples only (75 rows)",
          "Must follow competition submission format"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Test samples only (75 rows)",
            "Must follow competition submission format"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Id"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Id"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the relationship between greeks.csv metadata and target class?",
          "How do the 56 feature columns (AB-GL) relate to the binary Class?",
          "What patterns exist in the 'EJ' categorical column?",
          "How should missing values in test.csv be handled?",
          "What is the target variable in the training data?",
          "What features are available in train.csv and test.csv?",
          "How should predictions be formatted in submission.csv?",
          "What is the relationship between greeks.csv and the main datasets?",
          "Are there any missing values that need handling?",
          "What is the class distribution in the training data?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What is the relationship between greeks.csv metadata and target class?",
            "How do the 56 feature columns (AB-GL) relate to the binary Class?",
            "What patterns exist in the 'EJ' categorical column?",
            "How should missing values in test.csv be handled?"
          ],
          [
            "What is the target variable in the training data?",
            "What features are available in train.csv and test.csv?",
            "How should predictions be formatted in submission.csv?",
            "What is the relationship between greeks.csv and the main datasets?",
            "Are there any missing values that need handling?",
            "What is the class distribution in the training data?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "train.csv",
          "test.csv",
          "greeks.csv",
          "sample_submission.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "train.csv",
            "test.csv",
            "greeks.csv",
            "sample_submission.csv"
          ],
          [
            "train.csv",
            "test.csv",
            "greeks.csv",
            "sample_submission.csv"
          ],
          [
            "greeks.csv",
            "sample_submission.csv",
            "test.csv",
            "train.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "train.csv has duplicate 'Id' column as 'Id.1'",
          "train.csv has extra 'Class' column not in test.csv",
          "sample_submission.csv has different columns than train/test",
          "train.csv has duplicate 'Id' column ('Id' and 'Id.1')",
          "train.csv has 'Class' target variable not present in test.csv",
          "Columns 'BD ', 'CD ', 'CW ', 'FD ' have trailing spaces in train.csv and test.csv",
          "train.csv contains an 'Id.1' column which is likely a duplicate of 'Id'.",
          "test.csv does not contain the 'Class' column, which is the target variable in train.csv."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "train.csv has duplicate 'Id' column as 'Id.1'",
            "train.csv has extra 'Class' column not in test.csv",
            "sample_submission.csv has different columns than train/test"
          ],
          [
            "train.csv has duplicate 'Id' column ('Id' and 'Id.1')",
            "train.csv has 'Class' target variable not present in test.csv",
            "Columns 'BD ', 'CD ', 'CW ', 'FD ' have trailing spaces in train.csv and test.csv"
          ],
          [
            "train.csv contains an 'Id.1' column which is likely a duplicate of 'Id'.",
            "test.csv does not contain the 'Class' column, which is the target variable in train.csv."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "ab": "unknown continuous",
          "af": "unknown continuous",
          "ah": "unknown continuous",
          "am": "unknown continuous",
          "ar": "unknown continuous",
          "ax": "unknown continuous",
          "ay": "unknown continuous",
          "az": "unknown continuous",
          "bc": "unknown continuous",
          "bd": "unknown continuous",
          "bn": "unknown continuous",
          "bp": "unknown continuous",
          "bq": "unknown continuous",
          "br": "unknown continuous",
          "bz": "unknown continuous",
          "cb": "unknown continuous",
          "cc": "unknown continuous",
          "cd": "unknown continuous",
          "cf": "unknown continuous",
          "ch": "unknown continuous",
          "cl": "unknown continuous",
          "cr": "unknown continuous",
          "cs": "unknown continuous",
          "cu": "unknown continuous",
          "cw": "unknown continuous",
          "da": "unknown continuous",
          "de": "unknown continuous",
          "df": "unknown continuous",
          "dh": "unknown continuous",
          "di": "unknown continuous",
          "dl": "unknown continuous",
          "dn": "unknown continuous",
          "du": "unknown continuous",
          "dv": "unknown continuous",
          "dy": "unknown continuous",
          "eb": "unknown continuous",
          "ee": "unknown continuous",
          "eg": "unknown continuous",
          "eh": "unknown continuous",
          "el": "unknown continuous",
          "ep": "unknown continuous",
          "eu": "unknown continuous",
          "fc": "unknown continuous",
          "fd": "unknown continuous",
          "fe": "unknown continuous",
          "fi": "unknown continuous",
          "fl": "unknown continuous",
          "fr": "unknown continuous",
          "fs": "unknown continuous",
          "gb": "unknown continuous",
          "ge": "unknown continuous",
          "gf": "unknown continuous",
          "gh": "unknown continuous",
          "gi": "unknown continuous",
          "gl": "unknown continuous",
          "ej": "categorical (A/B)",
          "epsilon": "date/unknown",
          "class": "binary (0/1)",
          "alpha": "categorical",
          "beta": "categorical",
          "gamma": "categorical",
          "delta": "categorical",
          "class_0": "probability [0,1]",
          "class_1": "probability [0,1]"
        },
        "confidence": 0.3489583333333333,
        "votes": [
          {
            "AB": "unknown continuous",
            "AF": "unknown continuous",
            "AH": "unknown continuous",
            "AM": "unknown continuous",
            "AR": "unknown continuous",
            "AX": "unknown continuous",
            "AY": "unknown continuous",
            "AZ": "unknown continuous",
            "BC": "unknown continuous",
            "BD": "unknown continuous",
            "BN": "unknown continuous",
            "BP": "unknown continuous",
            "BQ": "unknown continuous",
            "BR": "unknown continuous",
            "BZ": "unknown continuous",
            "CB": "unknown continuous",
            "CC": "unknown continuous",
            "CD": "unknown continuous",
            "CF": "unknown continuous",
            "CH": "unknown continuous",
            "CL": "unknown continuous",
            "CR": "unknown continuous",
            "CS": "unknown continuous",
            "CU": "unknown continuous",
            "CW": "unknown continuous",
            "DA": "unknown continuous",
            "DE": "unknown continuous",
            "DF": "unknown continuous",
            "DH": "unknown continuous",
            "DI": "unknown continuous",
            "DL": "unknown continuous",
            "DN": "unknown continuous",
            "DU": "unknown continuous",
            "DV": "unknown continuous",
            "DY": "unknown continuous",
            "EB": "unknown continuous",
            "EE": "unknown continuous",
            "EG": "unknown continuous",
            "EH": "unknown continuous",
            "EL": "unknown continuous",
            "EP": "unknown continuous",
            "EU": "unknown continuous",
            "FC": "unknown continuous",
            "FD": "unknown continuous",
            "FE": "unknown continuous",
            "FI": "unknown continuous",
            "FL": "unknown continuous",
            "FR": "unknown continuous",
            "FS": "unknown continuous",
            "GB": "unknown continuous",
            "GE": "unknown continuous",
            "GF": "unknown continuous",
            "GH": "unknown continuous",
            "GI": "unknown continuous",
            "GL": "unknown continuous",
            "EJ": "categorical (A/B)",
            "Epsilon": "date/unknown",
            "Class": "binary (0/1)"
          },
          {
            "Alpha": "categorical",
            "Beta": "categorical",
            "Gamma": "categorical",
            "Delta": "categorical",
            "Epsilon": "date or categorical",
            "EJ": "categorical",
            "Class": "binary (0 or 1)",
            "class_0": "probability [0,1]",
            "class_1": "probability [0,1]"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Features have widely varying scales (e.g., AF in thousands, AY near zero)",
          "Some columns contain missing values (empty strings in sample data)",
          "Date column 'Epsilon' has 'Unknown' values",
          "Features have different scales (e.g., AF ranges in thousands, CF in single digits)",
          "Some features may need standardization or normalization for model training"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Features have widely varying scales (e.g., AF in thousands, AY near zero)",
            "Some columns contain missing values (empty strings in sample data)",
            "Date column 'Epsilon' has 'Unknown' values"
          ],
          [
            "Features have different scales (e.g., AF ranges in thousands, CF in single digits)",
            "Some features may need standardization or normalization for model training"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "greeks.csv has 617 rows while train.csv has 542 rows - not all train IDs have metadata",
          "test.csv has 75 rows but sample_submission.csv shows only 5 rows in sample",
          "greeks.csv has all object dtypes despite some columns appearing to be categorical codes",
          "Epsilon column in greeks.csv contains both dates and 'Unknown' values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "greeks.csv has 617 rows while train.csv has 542 rows - not all train IDs have metadata",
            "test.csv has 75 rows but sample_submission.csv shows only 5 rows in sample"
          ],
          [
            "greeks.csv has all object dtypes despite some columns appearing to be categorical codes",
            "Epsilon column in greeks.csv contains both dates and 'Unknown' values"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "Unknown",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "Unknown",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            "",
            "Unknown"
          ],
          [
            "NA",
            "N/A",
            "Unknown",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 59.0,
        "confidence": 1.0,
        "votes": [
          0.0,
          59.0,
          59.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Submission must match sample_submission.csv format exactly",
          "Probabilities must sum to 1 for each row",
          "All 75 test IDs must be predicted",
          "Class probabilities must be between 0 and 1",
          "Predictions must be probabilities between 0 and 1",
          "class_0 + class_1 should equal 1.0 for each prediction",
          "Output must contain exactly 75 rows matching test.csv",
          "Output must have columns: Id, class_0, class_1",
          "Id values in submission must match test.csv Id values",
          "Binary classification problem with classes 0 and 1",
          "The 'Id' column should be unique within each file.",
          "The 'Class' column in train.csv should only contain values 0 and 1.",
          "The 'class_0' and 'class_1' columns in sample_submission.csv should sum to 1 for each row."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Submission must match sample_submission.csv format exactly",
            "Probabilities must sum to 1 for each row",
            "All 75 test IDs must be predicted",
            "Class probabilities must be between 0 and 1"
          ],
          [
            "Predictions must be probabilities between 0 and 1",
            "class_0 + class_1 should equal 1.0 for each prediction",
            "Output must contain exactly 75 rows matching test.csv",
            "Output must have columns: Id, class_0, class_1",
            "Id values in submission must match test.csv Id values",
            "Binary classification problem with classes 0 and 1"
          ],
          [
            "The 'Id' column should be unique within each file.",
            "The 'Class' column in train.csv should only contain values 0 and 1.",
            "The 'class_0' and 'class_1' columns in sample_submission.csv should sum to 1 for each row."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter train.csv where Class = 0/1 for analysis",
          "Filter greeks.csv where Epsilon != 'Unknown' for temporal analysis",
          "Identify complete cases without missing values",
          "Only use samples with valid feature values for training",
          "Handle missing values appropriately before model training",
          "Consider whether greeks.csv features improve prediction performance"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter train.csv where Class = 0/1 for analysis",
            "Filter greeks.csv where Epsilon != 'Unknown' for temporal analysis",
            "Identify complete cases without missing values"
          ],
          [
            "Only use samples with valid feature values for training",
            "Handle missing values appropriately before model training",
            "Consider whether greeks.csv features improve prediction performance"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for class imbalance in train.csv",
          "Test correlation between features and Class",
          "Validate train/test distribution similarity",
          "Check for multicollinearity among 56 features",
          "Check class balance in training data",
          "Verify feature distributions between train and test sets",
          "Identify correlated features",
          "Detect outliers in numerical features"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for class imbalance in train.csv",
            "Test correlation between features and Class",
            "Validate train/test distribution similarity",
            "Check for multicollinearity among 56 features"
          ],
          [
            "Check class balance in training data",
            "Verify feature distributions between train and test sets",
            "Identify correlated features",
            "Detect outliers in numerical features"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named submission.csv",
          "Columns: Id, class_0, class_1",
          "class_0 + class_1 = 1 for each row",
          "Include all 75 test IDs",
          "Save predictions to submission.csv",
          "Format: CSV with header row",
          "Columns: Id (object/string), class_0 (float), class_1 (float)",
          "75 rows corresponding to test set samples",
          "Probabilities should be valid (0-1 range, sum to 1)",
          "The submission.csv file must contain 'Id', 'class_0', and 'class_1' columns.",
          "The 'class_0' and 'class_1' columns in submission.csv must be floating-point numbers representing probabilities.",
          "The 'Id' column in submission.csv must match the 'Id' column in test.csv."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file named submission.csv",
            "Columns: Id, class_0, class_1",
            "class_0 + class_1 = 1 for each row",
            "Include all 75 test IDs"
          ],
          [
            "Save predictions to submission.csv",
            "Format: CSV with header row",
            "Columns: Id (object/string), class_0 (float), class_1 (float)",
            "75 rows corresponding to test set samples",
            "Probabilities should be valid (0-1 range, sum to 1)"
          ],
          [
            "The submission.csv file must contain 'Id', 'class_0', and 'class_1' columns.",
            "The 'class_0' and 'class_1' columns in submission.csv must be floating-point numbers representing probabilities.",
            "The 'Id' column in submission.csv must match the 'Id' column in test.csv."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.60078125
  },
  "ml-competition-005": {
    "m_q": {
      "target_metric": {
        "value": "Probability predictions for three cirrhosis outcome classes (Status_C, Status_CL, Status_D) for test.csv patients",
        "confidence": 0.3333333333333333,
        "votes": [
          "Probability predictions for three cirrhosis outcome classes (Status_C, Status_CL, Status_D) for test.csv patients",
          "Predict probability distribution over three Status outcomes (C, CL, D) for each patient in test.csv",
          "Probability of each 'Status' category (C, CL, D) for each patient in test.csv"
        ]
      },
      "filters": {
        "value": [
          "No filtering needed - predict for all test.csv rows"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No filtering needed - predict for all test.csv rows"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "id"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "id"
          ],
          [],
          [
            "id"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features predict cirrhosis outcomes?",
          "How to handle missing values in clinical measurements?",
          "How to encode categorical variables (Drug, Sex, Ascites, Hepatomegaly, Spiders, Edema)?",
          "What is the relationship between N_Days (follow-up time) and outcomes?",
          "How to handle class imbalance in Status?",
          "What is the distribution of Status categories in the training data?",
          "Which features are most predictive of cirrhosis outcomes?",
          "What is the relationship between N_Days and Status?",
          "How do clinical measurements (Bilirubin, Albumin, Copper, etc.) correlate with Status?",
          "What is the effect of Drug treatment on Status outcomes?",
          "How should missing values in features be handled for prediction?",
          "What is the baseline probability distribution for Status (C, CL, D)?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What features predict cirrhosis outcomes?",
            "How to handle missing values in clinical measurements?",
            "How to encode categorical variables (Drug, Sex, Ascites, Hepatomegaly, Spiders, Edema)?",
            "What is the relationship between N_Days (follow-up time) and outcomes?",
            "How to handle class imbalance in Status?"
          ],
          [
            "What is the distribution of Status categories in the training data?",
            "Which features are most predictive of cirrhosis outcomes?",
            "What is the relationship between N_Days and Status?",
            "How do clinical measurements (Bilirubin, Albumin, Copper, etc.) correlate with Status?",
            "What is the effect of Drug treatment on Status outcomes?",
            "How should missing values in features be handled for prediction?",
            "What is the baseline probability distribution for Status (C, CL, D)?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "cirrhosis.csv",
          "train.csv",
          "test.csv",
          "sample_submission.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "cirrhosis.csv",
            "train.csv",
            "test.csv",
            "sample_submission.csv"
          ],
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv",
            "cirrhosis.csv"
          ],
          [
            "cirrhosis.csv",
            "test.csv",
            "train.csv",
            "sample_submission.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "cirrhosis.csv has 'ID' column while train/test have 'id'",
          "cirrhosis.csv has 'Status' as single column while sample_submission.csv has three separate probability columns",
          "cirrhosis.csv has 418 rows while train.csv has 6719 rows - different datasets",
          "train.csv has 'Status' column (target), test.csv does not",
          "cirrhosis.csv has 'ID' column (uppercase), train.csv and test.csv have 'id' column (lowercase)",
          "cirrhosis.csv contains 418 rows, train.csv contains 6719 rows - different datasets for training",
          "Column 'id' in test.csv and train.csv represents the same entity but 'ID' in cirrhosis.csv might be different.",
          "Column 'Status' is present in train.csv but not in test.csv. The target variable is encoded in sample_submission.csv.",
          "Column 'id' in test.csv and train.csv, 'ID' in cirrhosis.csv are integers, but 'id' in sample_submission.csv is also an integer."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "cirrhosis.csv has 'ID' column while train/test have 'id'",
            "cirrhosis.csv has 'Status' as single column while sample_submission.csv has three separate probability columns",
            "cirrhosis.csv has 418 rows while train.csv has 6719 rows - different datasets"
          ],
          [
            "train.csv has 'Status' column (target), test.csv does not",
            "cirrhosis.csv has 'ID' column (uppercase), train.csv and test.csv have 'id' column (lowercase)",
            "cirrhosis.csv contains 418 rows, train.csv contains 6719 rows - different datasets for training"
          ],
          [
            "Column 'id' in test.csv and train.csv represents the same entity but 'ID' in cirrhosis.csv might be different.",
            "Column 'Status' is present in train.csv but not in test.csv. The target variable is encoded in sample_submission.csv.",
            "Column 'id' in test.csv and train.csv, 'ID' in cirrhosis.csv are integers, but 'id' in sample_submission.csv is also an integer."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "age": "days",
          "n_days": "days",
          "bilirubin": "mg/dL",
          "cholesterol": "mg/dL",
          "albumin": "g/dL",
          "copper": "\u03bcg/dL",
          "alk_phos": "U/L",
          "sgot": "U/L",
          "tryglicerides": "mg/dL",
          "platelets": "10^3/\u03bcL",
          "prothrombin": "seconds",
          "stage": "ordinal (1-4)"
        },
        "confidence": 0.9444444444444445,
        "votes": [
          {
            "Age": "days (appears to be age in days)",
            "N_Days": "days (follow-up time)",
            "Bilirubin": "mg/dL",
            "Cholesterol": "mg/dL",
            "Albumin": "g/dL",
            "Copper": "\u03bcg/dL",
            "Alk_Phos": "U/L",
            "SGOT": "U/L",
            "Tryglicerides": "mg/dL",
            "Platelets": "10^3/\u03bcL",
            "Prothrombin": "seconds"
          },
          {
            "N_Days": "days",
            "Age": "days",
            "Bilirubin": "mg/dL",
            "Cholesterol": "mg/dL",
            "Albumin": "g/dL",
            "Copper": "\u03bcg/day",
            "Alk_Phos": "U/L",
            "SGOT": "U/mL",
            "Tryglicerides": "mg/dL",
            "Platelets": "cubic mL/1000",
            "Prothrombin": "seconds",
            "Stage": "ordinal (1-4)"
          },
          {
            "N_Days": "days",
            "Age": "days",
            "Bilirubin": "mg/dL",
            "Cholesterol": "mg/dL",
            "Albumin": "g/dL",
            "Copper": "mcg/dL",
            "Alk_Phos": "U/L",
            "SGOT": "U/L",
            "Tryglicerides": "mg/dL",
            "Platelets": "cells/mcL",
            "Prothrombin": "seconds"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Age values (e.g., 21464) suggest days not years - need conversion",
          "Alk_Phos has extreme range (516 to 9009.8)",
          "Bilirubin has wide range (0.3 to 14.5 in sample)",
          "Age is measured in days, not years - needs conversion or normalization",
          "Large scale differences between features (e.g., Alk_Phos in thousands vs Albumin in single digits)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age values (e.g., 21464) suggest days not years - need conversion",
            "Alk_Phos has extreme range (516 to 9009.8)",
            "Bilirubin has wide range (0.3 to 14.5 in sample)"
          ],
          [
            "Age is measured in days, not years - needs conversion or normalization",
            "Large scale differences between features (e.g., Alk_Phos in thousands vs Albumin in single digits)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "train.csv and cirrhosis.csv appear to be different datasets with same schema but different IDs",
          "test.csv missing Status column (target for prediction)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "train.csv and cirrhosis.csv appear to be different datasets with same schema but different IDs",
            "test.csv missing Status column (target for prediction)"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "nan"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 20.0,
        "confidence": 1.0,
        "votes": [
          20.0,
          20.0,
          20.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Status probabilities must sum to 1 for each patient",
          "All test.csv IDs must have predictions",
          "Output must match sample_submission.csv format exactly",
          "Stage should be integer 1-4 but stored as float",
          "Predictions must be probabilities summing to 1.0 for each row",
          "All three probability columns (Status_C, Status_CL, Status_D) must be between 0 and 1",
          "Output must contain exactly same number of rows as test.csv (1186 rows)",
          "Output id column must match test.csv id values exactly",
          "No missing values allowed in submission",
          "The sum of 'Status_C', 'Status_CL', and 'Status_D' for each 'id' in submission.csv must equal 1.",
          "The 'id' column in submission.csv must match the 'id' column in test.csv."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Status probabilities must sum to 1 for each patient",
            "All test.csv IDs must have predictions",
            "Output must match sample_submission.csv format exactly",
            "Stage should be integer 1-4 but stored as float"
          ],
          [
            "Predictions must be probabilities summing to 1.0 for each row",
            "All three probability columns (Status_C, Status_CL, Status_D) must be between 0 and 1",
            "Output must contain exactly same number of rows as test.csv (1186 rows)",
            "Output id column must match test.csv id values exactly",
            "No missing values allowed in submission"
          ],
          [
            "The sum of 'Status_C', 'Status_CL', and 'Status_D' for each 'id' in submission.csv must equal 1.",
            "The 'id' column in submission.csv must match the 'id' column in test.csv."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows with excessive missing clinical measurements",
          "Consider filtering extreme outliers in lab values",
          "Check for biologically impossible values (e.g., negative lab results)",
          "Only use patients from train.csv for model training",
          "Apply same preprocessing to both train and test data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with excessive missing clinical measurements",
            "Consider filtering extreme outliers in lab values",
            "Check for biologically impossible values (e.g., negative lab results)"
          ],
          [
            "Only use patients from train.csv for model training",
            "Apply same preprocessing to both train and test data"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for class imbalance in Status",
          "Test correlation between clinical features and outcomes",
          "Assess missing data patterns (MCAR/MAR/MNAR)",
          "Validate feature distributions between train and test sets",
          "Verify probability distributions sum to 1.0",
          "Check for class imbalance in training data Status distribution",
          "Assess multicollinearity among clinical measurements",
          "Check for multicollinearity among numerical features.",
          "Assess the distribution of each feature to determine appropriate imputation strategies for missing values.",
          "Perform statistical tests (e.g., chi-squared test) to assess the relationship between categorical features and the target variable."
        ],
        "confidence": 0.3666666666666667,
        "votes": [
          [
            "Check for class imbalance in Status",
            "Test correlation between clinical features and outcomes",
            "Assess missing data patterns (MCAR/MAR/MNAR)",
            "Validate feature distributions between train and test sets"
          ],
          [
            "Verify probability distributions sum to 1.0",
            "Check for class imbalance in training data Status distribution",
            "Validate feature distributions between train and test sets",
            "Assess multicollinearity among clinical measurements"
          ],
          [
            "Check for multicollinearity among numerical features.",
            "Assess the distribution of each feature to determine appropriate imputation strategies for missing values.",
            "Perform statistical tests (e.g., chi-squared test) to assess the relationship between categorical features and the target variable."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named submission.csv",
          "Columns: id, Status_C, Status_CL, Status_D",
          "Status columns must be float probabilities between 0-1",
          "Sum of three Status columns must equal 1 for each row",
          "Output format must match sample_submission.csv structure exactly",
          "Column order: id, Status_C, Status_CL, Status_D",
          "File name must be submission.csv",
          "CSV format with header row",
          "id values must be integers, probability values must be floats",
          "The submission file must be named 'submission.csv'.",
          "The submission file must contain the columns 'id', 'Status_C', 'Status_CL', and 'Status_D'.",
          "The 'id' column in the submission file must correspond to the 'id' column in test.csv."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file named submission.csv",
            "Columns: id, Status_C, Status_CL, Status_D",
            "Status columns must be float probabilities between 0-1",
            "Sum of three Status columns must equal 1 for each row"
          ],
          [
            "Output format must match sample_submission.csv structure exactly",
            "Column order: id, Status_C, Status_CL, Status_D",
            "File name must be submission.csv",
            "CSV format with header row",
            "id values must be integers, probability values must be floats"
          ],
          [
            "The submission file must be named 'submission.csv'.",
            "The submission file must contain the columns 'id', 'Status_C', 'Status_CL', and 'Status_D'.",
            "The 'id' column in the submission file must correspond to the 'id' column in test.csv."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6322222222222222
  },
  "ml-competition-006": {
    "m_q": {
      "target_metric": {
        "value": "wine quality predictions (integer values)",
        "confidence": 0.3333333333333333,
        "votes": [
          "wine quality predictions (integer values)",
          "wine quality predictions for test set",
          "Predict wine quality for test.csv and generate a submission file in the format of sample_submission.csv."
        ]
      },
      "filters": {
        "value": [
          "none - use all available training data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "none - use all available training data"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "none - individual wine predictions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "none - individual wine predictions"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features are most predictive of wine quality?",
          "Should quality be treated as regression or classification?",
          "What evaluation metric should be optimized?",
          "How to handle potential class imbalance in quality ratings?",
          "What are the feature distributions in the training data?",
          "What is the distribution of quality scores in the training data?",
          "Are there correlations between physicochemical properties and wine quality?",
          "What is the appropriate model type (classification vs regression)?",
          "How should the predictions be formatted for submission?",
          "What features are most important for predicting quality?",
          "What are the distributions of each feature in the training set?",
          "What are the correlations between features and the target variable 'quality'?",
          "Are there any missing values in the datasets?",
          "Are there any outliers in the datasets?",
          "What is the distribution of the target variable 'quality'?",
          "What are the important features for predicting wine quality?",
          "What is the performance of different machine learning models on the training data?",
          "How can the model be optimized to improve performance?",
          "How can the model predictions be formatted into the submission file?"
        ],
        "confidence": 0.3333333333333332,
        "votes": [
          [
            "What features are most predictive of wine quality?",
            "Should quality be treated as regression or classification?",
            "What evaluation metric should be optimized?",
            "How to handle potential class imbalance in quality ratings?"
          ],
          [
            "What are the feature distributions in the training data?",
            "What is the distribution of quality scores in the training data?",
            "Are there correlations between physicochemical properties and wine quality?",
            "What is the appropriate model type (classification vs regression)?",
            "How should the predictions be formatted for submission?",
            "What features are most important for predicting quality?"
          ],
          [
            "What are the distributions of each feature in the training set?",
            "What are the correlations between features and the target variable 'quality'?",
            "Are there any missing values in the datasets?",
            "Are there any outliers in the datasets?",
            "What is the distribution of the target variable 'quality'?",
            "What are the important features for predicting wine quality?",
            "What is the performance of different machine learning models on the training data?",
            "How can the model be optimized to improve performance?",
            "How can the model predictions be formatted into the submission file?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "WineQT.csv",
          "train.csv",
          "test.csv",
          "sample_submission.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "WineQT.csv",
            "train.csv",
            "test.csv",
            "sample_submission.csv"
          ],
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv",
            "WineQT.csv"
          ],
          [
            "WineQT.csv",
            "sample_submission.csv",
            "test.csv",
            "train.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "WineQT.csv has 'quality' column while test.csv does not",
          "train.csv has 1747 rows while WineQT.csv has 1143 rows - possible overlap or different datasets",
          "sample_submission.csv has 1372 rows while test.csv has 309 rows - mismatch in expected submission size"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "WineQT.csv has 'quality' column while test.csv does not",
            "train.csv has 1747 rows while WineQT.csv has 1143 rows - possible overlap or different datasets",
            "sample_submission.csv has 1372 rows while test.csv has 309 rows - mismatch in expected submission size"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "fixed acidity": "g/dm\u00b3",
          "volatile acidity": "g/dm\u00b3",
          "citric acid": "g/dm\u00b3",
          "residual sugar": "g/dm\u00b3",
          "chlorides": "g/dm\u00b3",
          "free sulfur dioxide": "mg/dm\u00b3",
          "total sulfur dioxide": "mg/dm\u00b3",
          "density": "g/cm\u00b3",
          "alcohol": "% vol",
          "ph": "pH scale",
          "sulphates": "g(potassium sulphate)/dm\u00b3",
          "quality": "score (0-10)",
          "id": "identifier"
        },
        "confidence": 0.564102564102564,
        "votes": [
          {
            "fixed acidity": "g/dm\u00b3",
            "volatile acidity": "g/dm\u00b3",
            "citric acid": "g/dm\u00b3",
            "residual sugar": "g/dm\u00b3",
            "chlorides": "g/dm\u00b3",
            "free sulfur dioxide": "mg/dm\u00b3",
            "total sulfur dioxide": "mg/dm\u00b3",
            "density": "g/cm\u00b3",
            "alcohol": "% vol"
          },
          {
            "fixed acidity": "g(tartaric acid)/dm\u00b3",
            "volatile acidity": "g(acetic acid)/dm\u00b3",
            "citric acid": "g/dm\u00b3",
            "residual sugar": "g/dm\u00b3",
            "chlorides": "g(sodium chloride)/dm\u00b3",
            "free sulfur dioxide": "mg/dm\u00b3",
            "total sulfur dioxide": "mg/dm\u00b3",
            "density": "g/cm\u00b3",
            "pH": "pH scale",
            "sulphates": "g(potassium sulphate)/dm\u00b3",
            "alcohol": "% vol",
            "quality": "score (0-10)",
            "Id": "identifier"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Features have different measurement scales requiring normalization",
          "pH is logarithmic scale while others are linear",
          "sulfur dioxide measurements in mg/dm\u00b3 while acids in g/dm\u00b3",
          "Features have different scales requiring normalization/standardization",
          "density values are close to 1.0 with small variance",
          "pH is on logarithmic scale while other features are linear"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Features have different measurement scales requiring normalization",
            "pH is logarithmic scale while others are linear",
            "sulfur dioxide measurements in mg/dm\u00b3 while acids in g/dm\u00b3"
          ],
          [
            "Features have different scales requiring normalization/standardization",
            "density values are close to 1.0 with small variance",
            "pH is on logarithmic scale while other features are linear"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Id ranges differ across files: WineQT.csv (0-1142), train.csv (202-1967), test.csv (8-1917), sample_submission.csv (2056-3427)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Id ranges differ across files: WineQT.csv (0-1142), train.csv (202-1967), test.csv (8-1917), sample_submission.csv (2056-3427)"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 1.0,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 13.0,
        "confidence": 1.0,
        "votes": [
          13.0,
          13.0,
          13.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "quality values are integers between 0-10 (typically 3-9 in wine datasets)",
          "Id must be preserved for submission matching",
          "submission.csv must match sample_submission.csv format exactly",
          "predictions must be integers",
          "quality is an integer value typically ranging from 3-9",
          "All feature columns must be present in same order as training data",
          "Id column must match between test.csv and submission.csv",
          "Predictions must be made for all 309 test samples",
          "No missing values should be present in predictions",
          "The 'Id' column in test.csv should be used to populate the 'Id' column in submission.csv.",
          "The 'quality' column in submission.csv should contain the predicted wine quality for each 'Id' in test.csv."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "quality values are integers between 0-10 (typically 3-9 in wine datasets)",
            "Id must be preserved for submission matching",
            "submission.csv must match sample_submission.csv format exactly",
            "predictions must be integers"
          ],
          [
            "quality is an integer value typically ranging from 3-9",
            "All feature columns must be present in same order as training data",
            "Id column must match between test.csv and submission.csv",
            "Predictions must be made for all 309 test samples",
            "No missing values should be present in predictions"
          ],
          [
            "The 'Id' column in test.csv should be used to populate the 'Id' column in submission.csv.",
            "The 'quality' column in submission.csv should contain the predicted wine quality for each 'Id' in test.csv."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Check for duplicate Id values across datasets",
          "Remove rows with missing target values in training",
          "Validate feature value ranges (e.g., pH 0-14, alcohol >0)",
          "Remove any rows with missing values in training data",
          "Check for outliers in physicochemical properties",
          "Validate that test data features are within training data range"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate Id values across datasets",
            "Remove rows with missing target values in training",
            "Validate feature value ranges (e.g., pH 0-14, alcohol >0)"
          ],
          [
            "Remove any rows with missing values in training data",
            "Check for outliers in physicochemical properties",
            "Validate that test data features are within training data range"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check correlation between features and quality",
          "Test for multicollinearity among features",
          "Assess distribution of quality scores for class imbalance",
          "Validate feature importance rankings",
          "Check distribution of quality scores (imbalanced classes)",
          "Validate feature distributions between train and test sets",
          "Assess normality of feature distributions",
          "Correlation analysis to identify relationships between features and the target variable.",
          "Tests for normality to determine appropriate statistical methods."
        ],
        "confidence": 0.3703703703703704,
        "votes": [
          [
            "Check correlation between features and quality",
            "Test for multicollinearity among features",
            "Assess distribution of quality scores for class imbalance",
            "Validate feature importance rankings"
          ],
          [
            "Check distribution of quality scores (imbalanced classes)",
            "Test for multicollinearity among features",
            "Validate feature distributions between train and test sets",
            "Assess normality of feature distributions"
          ],
          [
            "Correlation analysis to identify relationships between features and the target variable.",
            "Tests for normality to determine appropriate statistical methods."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "submission.csv must have exactly columns: Id, quality",
          "Id column must match test.csv Id values",
          "quality column must contain integer predictions",
          "File must be comma-delimited with header",
          "submission.csv must have exactly 2 columns: Id, quality",
          "Id values must match test.csv Id values",
          "quality predictions must be integers",
          "File must be saved as submission.csv in CSV format",
          "Must include header row with column names",
          "Total rows in submission should be 309 (matching test set size)",
          "The submission.csv file must contain two columns: 'Id' and 'quality'.",
          "The 'Id' column must contain the 'Id' values from the test.csv file.",
          "The 'quality' column must contain the predicted wine quality values.",
          "The submission.csv file must be a comma-separated file."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "submission.csv must have exactly columns: Id, quality",
            "Id column must match test.csv Id values",
            "quality column must contain integer predictions",
            "File must be comma-delimited with header"
          ],
          [
            "submission.csv must have exactly 2 columns: Id, quality",
            "Id values must match test.csv Id values",
            "quality predictions must be integers",
            "File must be saved as submission.csv in CSV format",
            "Must include header row with column names",
            "Total rows in submission should be 309 (matching test set size)"
          ],
          [
            "The submission.csv file must contain two columns: 'Id' and 'quality'.",
            "The 'Id' column must contain the 'Id' values from the test.csv file.",
            "The 'quality' column must contain the predicted wine quality values.",
            "The submission.csv file must be a comma-separated file."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6133903133903135
  },
  "ml-competition-008": {
    "m_q": {
      "target_metric": {
        "value": "FloodProbability (float64)",
        "confidence": 0.3333333333333333,
        "votes": [
          "FloodProbability (float64)",
          "FloodProbability predictions for test set",
          "FloodProbability for each id in test.csv"
        ]
      },
      "filters": {
        "value": [
          "No explicit filters mentioned in analytical question"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No explicit filters mentioned in analytical question"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "id"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "id"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features are most predictive of flood probability?",
          "How should the model handle the integer-valued features?",
          "What validation strategy should be used given the competition format?",
          "Are there any data quality issues that need addressing before modeling?",
          "How should predictions be formatted for submission?",
          "What features are available for training the flood prediction model?",
          "What is the distribution of FloodProbability in the training data?",
          "Are there any patterns or correlations between features and FloodProbability?",
          "What is the optimal model architecture for this regression/classification task?"
        ],
        "confidence": 0.37037037037037046,
        "votes": [
          [
            "What features are most predictive of flood probability?",
            "How should the model handle the integer-valued features?",
            "What validation strategy should be used given the competition format?",
            "Are there any data quality issues that need addressing before modeling?",
            "How should predictions be formatted for submission?"
          ],
          [
            "What features are available for training the flood prediction model?",
            "What is the distribution of FloodProbability in the training data?",
            "Are there any patterns or correlations between features and FloodProbability?",
            "What is the optimal model architecture for this regression/classification task?",
            "How should predictions be formatted for submission?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "train.csv",
          "test.csv",
          "sample_submission.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv"
          ],
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv"
          ],
          [
            "sample_submission.csv",
            "test.csv",
            "train.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "train.csv has FloodProbability column while test.csv does not",
          "sample_submission.csv only has id and FloodProbability columns while test.csv has 21 columns",
          "train.csv has FloodProbability column, test.csv does not",
          "sample_submission.csv has only id and FloodProbability columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "train.csv has FloodProbability column while test.csv does not",
            "sample_submission.csv only has id and FloodProbability columns while test.csv has 21 columns"
          ],
          [
            "train.csv has FloodProbability column, test.csv does not",
            "sample_submission.csv has only id and FloodProbability columns"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "monsoonintensity": "ordinal scale (likely 0-10+)",
          "topographydrainage": "ordinal scale (likely 0-10+)",
          "rivermanagement": "ordinal scale (likely 0-10+)",
          "deforestation": "ordinal scale (likely 0-10+)",
          "urbanization": "ordinal scale (likely 0-10+)",
          "climatechange": "ordinal scale (likely 0-10+)",
          "damsquality": "ordinal scale (likely 0-10+)",
          "siltation": "ordinal scale (likely 0-10+)",
          "agriculturalpractices": "ordinal scale (likely 0-10+)",
          "encroachments": "ordinal scale (likely 0-10+)",
          "ineffectivedisasterpreparedness": "ordinal scale (likely 0-10+)",
          "drainagesystems": "ordinal scale (likely 0-10+)",
          "coastalvulnerability": "ordinal scale (likely 0-10+)",
          "landslides": "ordinal scale (likely 0-10+)",
          "watersheds": "ordinal scale (likely 0-10+)",
          "deterioratinginfrastructure": "ordinal scale (likely 0-10+)",
          "populationscore": "ordinal scale (likely 0-10+)",
          "wetlandloss": "ordinal scale (likely 0-10+)",
          "inadequateplanning": "ordinal scale (likely 0-10+)",
          "politicalfactors": "ordinal scale (likely 0-10+)",
          "floodprobability": "probability"
        },
        "confidence": 1.0,
        "votes": [
          {
            "MonsoonIntensity": "ordinal scale (likely 0-10+)",
            "TopographyDrainage": "ordinal scale (likely 0-10+)",
            "RiverManagement": "ordinal scale (likely 0-10+)",
            "Deforestation": "ordinal scale (likely 0-10+)",
            "Urbanization": "ordinal scale (likely 0-10+)",
            "ClimateChange": "ordinal scale (likely 0-10+)",
            "DamsQuality": "ordinal scale (likely 0-10+)",
            "Siltation": "ordinal scale (likely 0-10+)",
            "AgriculturalPractices": "ordinal scale (likely 0-10+)",
            "Encroachments": "ordinal scale (likely 0-10+)",
            "IneffectiveDisasterPreparedness": "ordinal scale (likely 0-10+)",
            "DrainageSystems": "ordinal scale (likely 0-10+)",
            "CoastalVulnerability": "ordinal scale (likely 0-10+)",
            "Landslides": "ordinal scale (likely 0-10+)",
            "Watersheds": "ordinal scale (likely 0-10+)",
            "DeterioratingInfrastructure": "ordinal scale (likely 0-10+)",
            "PopulationScore": "ordinal scale (likely 0-10+)",
            "WetlandLoss": "ordinal scale (likely 0-10+)",
            "InadequatePlanning": "ordinal scale (likely 0-10+)",
            "PoliticalFactors": "ordinal scale (likely 0-10+)",
            "FloodProbability": "probability (0-1)"
          },
          {
            "MonsoonIntensity": "intensity_score",
            "TopographyDrainage": "drainage_score",
            "RiverManagement": "management_score",
            "Deforestation": "deforestation_score",
            "Urbanization": "urbanization_score",
            "ClimateChange": "climate_score",
            "DamsQuality": "quality_score",
            "Siltation": "siltation_score",
            "AgriculturalPractices": "practices_score",
            "Encroachments": "encroachment_score",
            "IneffectiveDisasterPreparedness": "preparedness_score",
            "DrainageSystems": "drainage_system_score",
            "CoastalVulnerability": "vulnerability_score",
            "Landslides": "landslide_score",
            "Watersheds": "watershed_score",
            "DeterioratingInfrastructure": "infrastructure_score",
            "PopulationScore": "population_score",
            "WetlandLoss": "wetland_loss_score",
            "InadequatePlanning": "planning_score",
            "PoliticalFactors": "political_score",
            "FloodProbability": "probability"
          },
          {
            "MonsoonIntensity": "ordinal scale",
            "TopographyDrainage": "ordinal scale",
            "RiverManagement": "ordinal scale",
            "Deforestation": "ordinal scale",
            "Urbanization": "ordinal scale",
            "ClimateChange": "ordinal scale",
            "DamsQuality": "ordinal scale",
            "Siltation": "ordinal scale",
            "AgriculturalPractices": "ordinal scale",
            "Encroachments": "ordinal scale",
            "IneffectiveDisasterPreparedness": "ordinal scale",
            "DrainageSystems": "ordinal scale",
            "CoastalVulnerability": "ordinal scale",
            "Landslides": "ordinal scale",
            "Watersheds": "ordinal scale",
            "DeterioratingInfrastructure": "ordinal scale",
            "PopulationScore": "ordinal scale",
            "WetlandLoss": "ordinal scale",
            "InadequatePlanning": "ordinal scale",
            "PoliticalFactors": "ordinal scale",
            "FloodProbability": "probability"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "All features appear to be integer ordinal scales but ranges vary (e.g., Urbanization has value 11 in test.csv sample)",
          "Need to verify if all features use consistent scale ranges",
          "FloodProbability is float between 0-1 but training data shows values like 0.375, 0.445, etc.",
          "All feature columns appear to be integer scores with varying ranges",
          "FloodProbability is float64 ranging approximately 0-1",
          "Need to verify min/max ranges for all feature columns to ensure proper scaling"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All features appear to be integer ordinal scales but ranges vary (e.g., Urbanization has value 11 in test.csv sample)",
            "Need to verify if all features use consistent scale ranges",
            "FloodProbability is float between 0-1 but training data shows values like 0.375, 0.445, etc."
          ],
          [
            "All feature columns appear to be integer scores with varying ranges",
            "FloodProbability is float64 ranging approximately 0-1",
            "Need to verify min/max ranges for all feature columns to ensure proper scaling"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "test.csv has 167,694 rows while sample_submission.csv has 745,305 rows - significant mismatch in expected output size",
          "sample_submission.csv shows default FloodProbability of 0.5 for all records",
          "test.csv id values start from 1105809 while sample_submission.csv starts from 1117957, need to verify alignment"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv has 167,694 rows while sample_submission.csv has 745,305 rows - significant mismatch in expected output size"
          ],
          [
            "sample_submission.csv shows default FloodProbability of 0.5 for all records",
            "test.csv id values start from 1105809 while sample_submission.csv starts from 1117957, need to verify alignment"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            ""
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 22.0,
        "confidence": 1.0,
        "votes": [
          22.0,
          22.0,
          22.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "FloodProbability must be between 0 and 1",
          "id values must match between test.csv and submission.csv",
          "All test.csv rows must have predictions in submission.csv",
          "Submission must follow sample_submission.csv format exactly",
          "FloodProbability predictions must be float values",
          "Predictions must be provided for all 167694 test records",
          "Output must contain id column matching test.csv ids",
          "FloodProbability values should be between 0 and 1 based on training data patterns",
          "FloodProbability in sample_submission.csv should be between 0 and 1",
          "FloodProbability in train.csv should be between 0 and 1"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "FloodProbability must be between 0 and 1",
            "id values must match between test.csv and submission.csv",
            "All test.csv rows must have predictions in submission.csv",
            "Submission must follow sample_submission.csv format exactly"
          ],
          [
            "FloodProbability predictions must be float values",
            "Predictions must be provided for all 167694 test records",
            "Output must contain id column matching test.csv ids",
            "FloodProbability values should be between 0 and 1 based on training data patterns"
          ],
          [
            "FloodProbability in sample_submission.csv should be between 0 and 1",
            "FloodProbability in train.csv should be between 0 and 1"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Check for duplicate id values across datasets",
          "Validate feature value ranges (e.g., Urbanization=11 may be valid or error)",
          "Identify outliers in feature distributions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate id values across datasets",
            "Validate feature value ranges (e.g., Urbanization=11 may be valid or error)",
            "Identify outliers in feature distributions"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check correlation between features and FloodProbability",
          "Test for multicollinearity among features",
          "Validate distribution consistency between train and test sets",
          "Check for class imbalance in FloodProbability",
          "Check distribution of FloodProbability in training data",
          "Verify no data leakage between train and test sets using id columns",
          "Assess feature importance and correlations with target variable",
          "Validate prediction ranges match training data distribution",
          "Check for multicollinearity among independent variables in train.csv",
          "Evaluate feature importance using a suitable model"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Check correlation between features and FloodProbability",
            "Test for multicollinearity among features",
            "Validate distribution consistency between train and test sets",
            "Check for class imbalance in FloodProbability"
          ],
          [
            "Check distribution of FloodProbability in training data",
            "Verify no data leakage between train and test sets using id columns",
            "Assess feature importance and correlations with target variable",
            "Validate prediction ranges match training data distribution"
          ],
          [
            "Check for multicollinearity among independent variables in train.csv",
            "Evaluate feature importance using a suitable model"
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Must produce submission.csv with exactly 745,305 rows and 2 columns: id and FloodProbability",
          "FloodProbability must be float values",
          "File must match sample_submission.csv structure",
          "Output file must be named submission.csv",
          "Must have exactly 2 columns: id, FloodProbability",
          "Must have header row with column names",
          "Must contain predictions for all test set ids",
          "FloodProbability must be float64 format",
          "Follow exact format of sample_submission.csv",
          "submission.csv must contain 'id' and 'FloodProbability' columns",
          "submission.csv must have the same number of rows as test.csv",
          "FloodProbability values in submission.csv must be between 0 and 1"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Must produce submission.csv with exactly 745,305 rows and 2 columns: id and FloodProbability",
            "FloodProbability must be float values",
            "File must match sample_submission.csv structure"
          ],
          [
            "Output file must be named submission.csv",
            "Must have exactly 2 columns: id, FloodProbability",
            "Must have header row with column names",
            "Must contain predictions for all test set ids",
            "FloodProbability must be float64 format",
            "Follow exact format of sample_submission.csv"
          ],
          [
            "submission.csv must contain 'id' and 'FloodProbability' columns",
            "submission.csv must have the same number of rows as test.csv",
            "FloodProbability values in submission.csv must be between 0 and 1"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6185185185185186
  },
  "ml-competition-009": {
    "m_q": {
      "target_metric": {
        "value": "Predict the number of Rings for each abalone in test.csv",
        "confidence": 0.3333333333333333,
        "votes": [
          "Predict the number of Rings for each abalone in test.csv",
          "Predict Rings (age) of abalone for test set and generate submission.csv with id and Rings columns",
          "Predict the number of rings for each abalone in the test set."
        ]
      },
      "filters": {
        "value": [
          "No filters needed for prediction task"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No filters needed for prediction task"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "id"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "id"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features best predict Rings?",
          "How should Sex categorical variable be encoded?",
          "What model type is appropriate for regression?",
          "How to handle potential outliers in measurements?",
          "What validation strategy to use?",
          "What is the distribution of Rings in the training data?",
          "What are the relationships between physical measurements (Length, Diameter, Height, weights) and Rings?",
          "How does Sex category affect the Rings prediction?",
          "What machine learning model is most appropriate for this regression task?",
          "Are there outliers or data quality issues in training data?",
          "How should the predictions be formatted to match sample_submission.csv?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What features best predict Rings?",
            "How should Sex categorical variable be encoded?",
            "What model type is appropriate for regression?",
            "How to handle potential outliers in measurements?",
            "What validation strategy to use?"
          ],
          [
            "What is the distribution of Rings in the training data?",
            "What are the relationships between physical measurements (Length, Diameter, Height, weights) and Rings?",
            "How does Sex category affect the Rings prediction?",
            "What machine learning model is most appropriate for this regression task?",
            "Are there outliers or data quality issues in training data?",
            "How should the predictions be formatted to match sample_submission.csv?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "train.csv",
          "test.csv",
          "sample_submission.csv",
          "abalone.data",
          "Index"
        ],
        "confidence": 0.8666666666666666,
        "votes": [
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv",
            "abalone.data",
            "Index"
          ],
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv",
            "abalone.data"
          ],
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv",
            "abalone.data"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "abalone.data has numeric column names (0.455, 0.365, etc.) while train/test have descriptive names",
          "abalone.data first column is 'M' (categorical) while train/test have 'Sex' column",
          "abalone.data has '15' as target column name while train.csv has 'Rings'",
          "Index file appears to be metadata/directory listing, not actual data",
          "abalone.data has no header row, columns are unnamed",
          "abalone.data has duplicate column name 'Whole weight' appears as 'Whole weight', 'Whole weight.1', 'Whole weight.2' in train.csv and test.csv",
          "train.csv and test.csv have proper headers but abalone.data uses first row as data",
          "abalone.data appears to be the original dataset with 4176 rows, while train.csv has 77022 rows (augmented/expanded data)",
          "Column names in abalone.data are generic (e.g., '0.455', '0.365') and need renaming based on abalone.names or train.csv.",
          "train.csv and test.csv have overlapping 'id' ranges, which might cause issues if not handled correctly.",
          "The 'Rings' column is present in train.csv but not in test.csv, as expected."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "abalone.data has numeric column names (0.455, 0.365, etc.) while train/test have descriptive names",
            "abalone.data first column is 'M' (categorical) while train/test have 'Sex' column",
            "abalone.data has '15' as target column name while train.csv has 'Rings'",
            "Index file appears to be metadata/directory listing, not actual data"
          ],
          [
            "abalone.data has no header row, columns are unnamed",
            "abalone.data has duplicate column name 'Whole weight' appears as 'Whole weight', 'Whole weight.1', 'Whole weight.2' in train.csv and test.csv",
            "train.csv and test.csv have proper headers but abalone.data uses first row as data",
            "abalone.data appears to be the original dataset with 4176 rows, while train.csv has 77022 rows (augmented/expanded data)"
          ],
          [
            "Column names in abalone.data are generic (e.g., '0.455', '0.365') and need renaming based on abalone.names or train.csv.",
            "train.csv and test.csv have overlapping 'id' ranges, which might cause issues if not handled correctly.",
            "The 'Rings' column is present in train.csv but not in test.csv, as expected."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "length": "meters",
          "diameter": "meters",
          "height": "meters",
          "whole weight": "grams",
          "whole weight.1": "grams",
          "whole weight.2": "grams",
          "shell weight": "grams",
          "rings": "count",
          "shucked weight": "grams",
          "viscera weight": "grams"
        },
        "confidence": 0.7666666666666667,
        "votes": [
          {
            "Length": "meters",
            "Diameter": "meters",
            "Height": "meters",
            "Whole weight": "grams",
            "Whole weight.1": "grams",
            "Whole weight.2": "grams",
            "Shell weight": "grams",
            "Rings": "count"
          },
          {
            "Length": "meters",
            "Diameter": "meters",
            "Height": "meters",
            "Whole weight": "grams",
            "Whole weight.1": "grams (likely shucked weight)",
            "Whole weight.2": "grams (likely viscera weight)",
            "Shell weight": "grams",
            "Rings": "count (age indicator)"
          },
          {
            "Length": "mm",
            "Diameter": "mm",
            "Height": "mm",
            "Whole weight": "grams",
            "Shucked weight": "grams",
            "Viscera weight": "grams",
            "Shell weight": "grams"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "All weight measurements appear to be in same unit (grams) but represent different components",
          "Length measurements are in meters but values are small (0.1-0.7 range)",
          "Height values are very small relative to Length and Diameter",
          "Physical dimensions (Length, Diameter, Height) are in fractional meters (0-1 range)",
          "Weight measurements are in grams with different scales",
          "Rings values are discrete integers representing age"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "All weight measurements appear to be in same unit (grams) but represent different components",
            "Length measurements are in meters but values are small (0.1-0.7 range)",
            "Height values are very small relative to Length and Diameter"
          ],
          [
            "Physical dimensions (Length, Diameter, Height) are in fractional meters (0-1 range)",
            "Weight measurements are in grams with different scales",
            "Rings values are discrete integers representing age"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "abalone.data has 4176 rows while train.csv has 77022 rows - significant difference in data volume",
          "test.csv has 13593 rows while sample_submission.csv has 60411 rows - mismatch in expected submission size",
          "train.csv has 77022 rows vs abalone.data has 4176 rows - significant data augmentation",
          "Column names in abalone.data are malformed (first data row as headers)",
          "test.csv has 13593 rows but sample_submission.csv has 60411 rows - mismatch in expected output size"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "abalone.data has 4176 rows while train.csv has 77022 rows - significant difference in data volume",
            "test.csv has 13593 rows while sample_submission.csv has 60411 rows - mismatch in expected submission size"
          ],
          [
            "train.csv has 77022 rows vs abalone.data has 4176 rows - significant data augmentation",
            "Column names in abalone.data are malformed (first data row as headers)",
            "test.csv has 13593 rows but sample_submission.csv has 60411 rows - mismatch in expected output size"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            ""
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 1.0,
        "votes": [
          10.0,
          10.0,
          10.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Rings must be integer values",
          "Sex must be one of M/F/I",
          "All measurements must be positive",
          "Height > 0",
          "Shell weight <= Whole weight",
          "id values must be unique in submission",
          "Predictions must be for all ids in sample_submission.csv (60411 rows)",
          "Output must have exactly 2 columns: id and Rings",
          "Sex must be one of: M, F, I",
          "All physical measurements must be non-negative",
          "Height should be less than Diameter which should be less than or equal to Length",
          "The 'Rings' column in train.csv represents the target variable and must be an integer.",
          "The 'Sex' column has categorical values (M, F, I).",
          "The 'id' column in test.csv must be present in the submission file.",
          "The 'Rings' column in sample_submission.csv provides the expected format for the submission file."
        ],
        "confidence": 0.35555555555555546,
        "votes": [
          [
            "Rings must be integer values",
            "Sex must be one of M/F/I",
            "All measurements must be positive",
            "Height > 0",
            "Shell weight <= Whole weight",
            "id values must be unique in submission"
          ],
          [
            "Predictions must be for all ids in sample_submission.csv (60411 rows)",
            "Rings must be integer values",
            "Output must have exactly 2 columns: id and Rings",
            "Sex must be one of: M, F, I",
            "All physical measurements must be non-negative",
            "Height should be less than Diameter which should be less than or equal to Length"
          ],
          [
            "The 'Rings' column in train.csv represents the target variable and must be an integer.",
            "The 'Sex' column has categorical values (M, F, I).",
            "The 'id' column in test.csv must be present in the submission file.",
            "The 'Rings' column in sample_submission.csv provides the expected format for the submission file."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows with Height <= 0",
          "Check for weight consistency: Whole weight >= sum of component weights",
          "Validate Sex values are in {M, F, I}",
          "Remove rows with invalid or negative physical measurements",
          "Check for outliers in Rings distribution",
          "Validate that weight components sum appropriately"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with Height <= 0",
            "Check for weight consistency: Whole weight >= sum of component weights",
            "Validate Sex values are in {M, F, I}"
          ],
          [
            "Remove rows with invalid or negative physical measurements",
            "Check for outliers in Rings distribution",
            "Validate that weight components sum appropriately"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Correlation analysis between physical measurements and Rings",
          "ANOVA test for Sex vs Rings",
          "Check for multicollinearity among weight measurements",
          "Test normality of Rings distribution",
          "Check correlation between physical features and Rings",
          "Test for multicollinearity among weight measurements",
          "Validate distribution of Rings across Sex categories",
          "Assess train-test feature distribution similarity",
          "Check for multicollinearity among the continuous features (Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight, Shell weight).",
          "Check the distribution of the target variable 'Rings' for skewness."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Correlation analysis between physical measurements and Rings",
            "ANOVA test for Sex vs Rings",
            "Check for multicollinearity among weight measurements",
            "Test normality of Rings distribution"
          ],
          [
            "Check correlation between physical features and Rings",
            "Test for multicollinearity among weight measurements",
            "Validate distribution of Rings across Sex categories",
            "Assess train-test feature distribution similarity"
          ],
          [
            "Check for multicollinearity among the continuous features (Length, Diameter, Height, Whole weight, Shucked weight, Viscera weight, Shell weight).",
            "Check the distribution of the target variable 'Rings' for skewness."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "submission.csv must have exactly columns: id, Rings",
          "Rings must be integer predictions",
          "Must include predictions for all 60411 ids in sample_submission.csv",
          "File must be comma-separated with header",
          "submission.csv must have header row: id,Rings",
          "Must contain exactly 60411 predictions matching sample_submission.csv ids",
          "Rings values must be integers",
          "No missing values allowed in submission",
          "File must be CSV format with comma delimiter",
          "The submission file must be named 'submission.csv'.",
          "The submission file must contain two columns: 'id' and 'Rings'.",
          "The 'id' column in the submission file must match the 'id' column in test.csv.",
          "The 'Rings' column in the submission file must contain integer predictions."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "submission.csv must have exactly columns: id, Rings",
            "Rings must be integer predictions",
            "Must include predictions for all 60411 ids in sample_submission.csv",
            "File must be comma-separated with header"
          ],
          [
            "submission.csv must have header row: id,Rings",
            "Must contain exactly 60411 predictions matching sample_submission.csv ids",
            "Rings values must be integers",
            "No missing values allowed in submission",
            "File must be CSV format with comma delimiter"
          ],
          [
            "The submission file must be named 'submission.csv'.",
            "The submission file must contain two columns: 'id' and 'Rings'.",
            "The 'id' column in the submission file must match the 'id' column in test.csv.",
            "The 'Rings' column in the submission file must contain integer predictions."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.607777777777778
  },
  "ml-competition-017": {
    "m_q": {
      "target_metric": {
        "value": "score (integer essay score)",
        "confidence": 0.3333333333333333,
        "votes": [
          "score (integer essay score)",
          "Predicted essay scores for test.csv essays",
          "Predicted essay score for each essay in test.csv"
        ]
      },
      "filters": {
        "value": [
          "test.csv data only"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv data only"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "essay_id"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "essay_id"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 1.0,
        "votes": [
          "table",
          "table",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "Predict scores for 2597 test essays",
          "Match submission format to sample_submission.csv",
          "Ensure essay_id-score mapping is preserved",
          "What features can be extracted from essay text to predict scores?",
          "What is the distribution of scores in the training data?",
          "What is the relationship between essay characteristics (length, vocabulary, grammar) and scores?",
          "What machine learning model should be used for essay scoring?",
          "How should predictions be formatted for submission?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Predict scores for 2597 test essays",
            "Match submission format to sample_submission.csv",
            "Ensure essay_id-score mapping is preserved"
          ],
          [
            "What features can be extracted from essay text to predict scores?",
            "What is the distribution of scores in the training data?",
            "What is the relationship between essay characteristics (length, vocabulary, grammar) and scores?",
            "What machine learning model should be used for essay scoring?",
            "How should predictions be formatted for submission?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "train.csv",
          "test.csv",
          "sample_submission.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv"
          ],
          [
            "train.csv",
            "test.csv",
            "sample_submission.csv"
          ],
          [
            "test.csv",
            "sample_submission.csv",
            "train.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "train.csv has 'score' column, test.csv lacks 'score' column",
          "sample_submission.csv has only 2 columns while train.csv has 3 columns",
          "train.csv has 'score' column while test.csv does not",
          "sample_submission.csv contains only essay_id and score columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "train.csv has 'score' column, test.csv lacks 'score' column",
            "sample_submission.csv has only 2 columns while train.csv has 3 columns"
          ],
          [
            "train.csv has 'score' column while test.csv does not",
            "sample_submission.csv contains only essay_id and score columns"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "score": "integer rating scale (likely 1-6 or similar)",
          "essay_id": "unique identifier string",
          "full_text": "free text essay content"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "score": "integer rating scale (likely 1-6 or similar)"
          },
          {
            "score": "integer scale (observed values: 3, 4 in samples)",
            "essay_id": "unique identifier string",
            "full_text": "free text essay content"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "Score scale range needs verification from train.csv distribution",
          "Need to determine full score range from train.csv (only samples show 3 and 4)",
          "Score distribution and range must be analyzed from complete training data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Score scale range needs verification from train.csv distribution"
          ],
          [
            "Need to determine full score range from train.csv (only samples show 3 and 4)",
            "Score distribution and range must be analyzed from complete training data"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "test.csv lacks target variable 'score' for prediction"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv lacks target variable 'score' for prediction"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.5833333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 3.0,
        "confidence": 0.75,
        "votes": [
          3.0,
          2.0,
          0.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "essay_id must be unique across all files",
          "score must be integer",
          "submission must match sample_submission.csv format exactly",
          "Predictions must be generated for all 2597 essays in test.csv",
          "Output must contain essay_id from test.csv",
          "Scores must be integers within the valid range observed in train.csv",
          "essay_id values in submission must exactly match those in test.csv",
          "The 'essay_id' in submission.csv must match the 'essay_id' in test.csv.",
          "The 'score' in submission.csv must be an integer."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "essay_id must be unique across all files",
            "score must be integer",
            "submission must match sample_submission.csv format exactly"
          ],
          [
            "Predictions must be generated for all 2597 essays in test.csv",
            "Output must contain essay_id from test.csv",
            "Scores must be integers within the valid range observed in train.csv",
            "essay_id values in submission must exactly match those in test.csv"
          ],
          [
            "The 'essay_id' in submission.csv must match the 'essay_id' in test.csv.",
            "The 'score' in submission.csv must be an integer."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove essays with empty full_text",
          "Ensure predicted scores are within training score range"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove essays with empty full_text",
            "Ensure predicted scores are within training score range"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check score distribution in train.csv",
          "Validate essay length distribution across train/test"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check score distribution in train.csv",
            "Validate essay length distribution across train/test"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named submission.csv",
          "Columns: essay_id,score",
          "2597 rows matching test.csv",
          "No header quotes, comma delimiter",
          "Output file must be named submission.csv",
          "Must contain exactly 2 columns: essay_id and score",
          "Must include header row matching sample_submission.csv format",
          "Must contain 2597 rows (excluding header)",
          "essay_id column must be string/object type",
          "score column must be int64 type",
          "The submission file must be named 'submission.csv'.",
          "The submission file must contain two columns: 'essay_id' and 'score'.",
          "The 'essay_id' column in submission.csv should contain the same essay IDs as in test.csv.",
          "The 'score' column in submission.csv should contain the predicted score for each essay."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file named submission.csv",
            "Columns: essay_id,score",
            "2597 rows matching test.csv",
            "No header quotes, comma delimiter"
          ],
          [
            "Output file must be named submission.csv",
            "Must contain exactly 2 columns: essay_id and score",
            "Must include header row matching sample_submission.csv format",
            "Must contain 2597 rows (excluding header)",
            "essay_id column must be string/object type",
            "score column must be int64 type"
          ],
          [
            "The submission file must be named 'submission.csv'.",
            "The submission file must contain two columns: 'essay_id' and 'score'.",
            "The 'essay_id' column in submission.csv should contain the same essay IDs as in test.csv.",
            "The 'score' column in submission.csv should contain the predicted score for each essay."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5722222222222223
  },
  "ml-multi-008": {
    "m_q": {
      "target_metric": {
        "value": "Predict CATEGORY for each ID in test.csv based on text features",
        "confidence": 0.3333333333333333,
        "votes": [
          "Predict CATEGORY for each ID in test.csv based on text features",
          "Predict CATEGORY for each record in test.csv",
          "Predicted category for each news article in test.csv"
        ]
      },
      "filters": {
        "value": [
          "test.csv has no CATEGORY column - prediction required",
          "uci-news-aggregator.csv has CATEGORY for training"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv has no CATEGORY column - prediction required",
            "uci-news-aggregator.csv has CATEGORY for training"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "ID"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "ID"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What text features are most predictive of CATEGORY?",
          "How to handle missing or inconsistent publisher/story information?",
          "What is the distribution of categories in training data?",
          "How to preprocess TITLE text for classification?",
          "What are the possible category values to predict?",
          "What features from test.csv can be used for prediction (TITLE, PUBLISHER, HOSTNAME, etc.)?",
          "What is the distribution of categories in the training data?",
          "How should text features be processed for category classification?",
          "What machine learning model is appropriate for multi-class text classification?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What text features are most predictive of CATEGORY?",
            "How to handle missing or inconsistent publisher/story information?",
            "What is the distribution of categories in training data?",
            "How to preprocess TITLE text for classification?"
          ],
          [
            "What are the possible category values to predict?",
            "What features from test.csv can be used for prediction (TITLE, PUBLISHER, HOSTNAME, etc.)?",
            "What is the distribution of categories in the training data?",
            "How should text features be processed for category classification?",
            "What machine learning model is appropriate for multi-class text classification?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "test.csv",
          "uci-news-aggregator.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "test.csv",
            "uci-news-aggregator.csv"
          ],
          [
            "uci-news-aggregator.csv",
            "test.csv"
          ],
          [
            "test.csv",
            "uci-news-aggregator.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "test.csv missing CATEGORY column (target variable)",
          "test.csv has 7 columns vs uci-news-aggregator.csv has 8 columns",
          "uci-news-aggregator.csv has CATEGORY column, test.csv does not"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv missing CATEGORY column (target variable)",
            "test.csv has 7 columns vs uci-news-aggregator.csv has 8 columns"
          ],
          [
            "uci-news-aggregator.csv has CATEGORY column, test.csv does not"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "timestamp": "milliseconds since epoch",
          "id": "integer identifier",
          "category": "categorical label (b=business, t=technology, e=entertainment, m=health/medical)"
        },
        "confidence": 0.5555555555555555,
        "votes": [
          {
            "TIMESTAMP": "milliseconds since epoch"
          },
          {
            "ID": "integer identifier",
            "TIMESTAMP": "milliseconds since epoch",
            "CATEGORY": "categorical label (b=business, t=technology, e=entertainment, m=health/medical)"
          },
          {
            "TIMESTAMP": "milliseconds since epoch"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "TIMESTAMP values are large integers (milliseconds)",
          "Text columns (TITLE, URL, PUBLISHER, STORY, HOSTNAME) need normalization"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "TIMESTAMP values are large integers (milliseconds)",
            "Text columns (TITLE, URL, PUBLISHER, STORY, HOSTNAME) need normalization"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "STORY column appears to be encoded/hashed values - unclear meaning",
          "URLs contain special characters and query parameters"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "STORY column appears to be encoded/hashed values - unclear meaning",
            "URLs contain special characters and query parameters"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 7.0,
        "confidence": 1.0,
        "votes": [
          7.0,
          7.0,
          7.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Output file must be named category.csv",
          "Output must have column named 'CATEGORY'",
          "Predictions must align with test.csv IDs",
          "Categories likely from set: b, t, e, m (based on training sample)",
          "Output must be saved as category.csv",
          "Must predict categories for all 63363 rows in test.csv",
          "Categories should match those in training data (b, t, e, m)",
          "Predictions should be based on text features: TITLE, PUBLISHER, HOSTNAME",
          "The 'CATEGORY' column in uci-news-aggregator.csv represents the target variable for training the model.",
          "The predicted categories must be saved in a file named 'category.csv' with a column named 'CATEGORY'."
        ],
        "confidence": 0.3666666666666667,
        "votes": [
          [
            "Output file must be named category.csv",
            "Output must have column named 'CATEGORY'",
            "Predictions must align with test.csv IDs",
            "Categories likely from set: b, t, e, m (based on training sample)"
          ],
          [
            "Output must be saved as category.csv",
            "Output must have column named 'CATEGORY'",
            "Must predict categories for all 63363 rows in test.csv",
            "Categories should match those in training data (b, t, e, m)",
            "Predictions should be based on text features: TITLE, PUBLISHER, HOSTNAME"
          ],
          [
            "The 'CATEGORY' column in uci-news-aggregator.csv represents the target variable for training the model.",
            "The predicted categories must be saved in a file named 'category.csv' with a column named 'CATEGORY'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove duplicate IDs if present",
          "Handle missing text in TITLE column",
          "Standardize publisher names if inconsistent"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove duplicate IDs if present",
            "Handle missing text in TITLE column",
            "Standardize publisher names if inconsistent"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for class imbalance in training CATEGORY",
          "Test correlation between PUBLISHER and CATEGORY",
          "Validate text preprocessing removes noise"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for class imbalance in training CATEGORY",
            "Test correlation between PUBLISHER and CATEGORY",
            "Validate text preprocessing removes noise"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file with ID and CATEGORY columns",
          "Same row order as test.csv or explicit ID mapping",
          "No additional columns beyond required",
          "File name: category.csv",
          "Column name: CATEGORY",
          "Format: CSV with header",
          "Number of rows: 63363 (matching test.csv)",
          "Values: Single character category codes (b, t, e, m)",
          "Output file must be named 'category.csv'.",
          "Output file must contain a column named 'CATEGORY'.",
          "The 'CATEGORY' column in the output file should contain the predicted category for each news article in test.csv."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "CSV file with ID and CATEGORY columns",
            "Same row order as test.csv or explicit ID mapping",
            "No additional columns beyond required"
          ],
          [
            "File name: category.csv",
            "Column name: CATEGORY",
            "Format: CSV with header",
            "Number of rows: 63363 (matching test.csv)",
            "Values: Single character category codes (b, t, e, m)"
          ],
          [
            "Output file must be named 'category.csv'.",
            "Output file must contain a column named 'CATEGORY'.",
            "The 'CATEGORY' column in the output file should contain the predicted category for each news article in test.csv."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5905555555555557
  },
  "ml-multi-011": {
    "m_q": {
      "target_metric": {
        "value": "Sentiment classification for each tweet in Corona_NLP_test.csv",
        "confidence": 0.3333333333333333,
        "votes": [
          "Sentiment classification for each tweet in Corona_NLP_test.csv",
          "Sentiment classification predictions for tweets in test dataset",
          "Sentiment"
        ]
      },
      "filters": {
        "value": [
          "Only text data from OriginalTweet column needs sentiment prediction"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only text data from OriginalTweet column needs sentiment prediction"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the sentiment categories in training data?",
          "How is sentiment distributed in training data?",
          "What text features correlate with different sentiments?",
          "What preprocessing is needed for tweet text?",
          "What are the sentiment classes present in the training data?",
          "What features from the tweets should be used for classification (OriginalTweet text)?",
          "Should UserName, ScreenName, Location, and TweetAt be used as features?",
          "What is the distribution of sentiment classes in the training data?",
          "How should text preprocessing be handled (URLs, mentions, hashtags, special characters)?",
          "What classification model should be trained on Corona_NLP_train.csv?",
          "How should predictions be formatted in sentiment.csv?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What are the sentiment categories in training data?",
            "How is sentiment distributed in training data?",
            "What text features correlate with different sentiments?",
            "What preprocessing is needed for tweet text?"
          ],
          [
            "What are the sentiment classes present in the training data?",
            "What features from the tweets should be used for classification (OriginalTweet text)?",
            "Should UserName, ScreenName, Location, and TweetAt be used as features?",
            "What is the distribution of sentiment classes in the training data?",
            "How should text preprocessing be handled (URLs, mentions, hashtags, special characters)?",
            "What classification model should be trained on Corona_NLP_train.csv?",
            "How should predictions be formatted in sentiment.csv?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Corona_NLP_train.csv",
          "Corona_NLP_test.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Corona_NLP_train.csv",
            "Corona_NLP_test.csv"
          ],
          [
            "Corona_NLP_train.csv",
            "Corona_NLP_test.csv"
          ],
          [
            "Corona_NLP_test.csv",
            "Corona_NLP_train.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Corona_NLP_train.csv has Sentiment column (6 columns) while Corona_NLP_test.csv lacks it (5 columns)",
          "Both files have same column structure except for Sentiment column",
          "Corona_NLP_test.csv lacks the Sentiment column which is the target variable present in Corona_NLP_train.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Corona_NLP_train.csv has Sentiment column (6 columns) while Corona_NLP_test.csv lacks it (5 columns)",
            "Both files have same column structure except for Sentiment column"
          ],
          [
            "Corona_NLP_test.csv lacks the Sentiment column which is the target variable present in Corona_NLP_train.csv"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "tweetat": "date in DD-MM-YYYY format",
          "username": "integer identifier",
          "screenname": "integer identifier",
          "location": "free text location string",
          "originaltweet": "raw tweet text with URLs, hashtags, mentions",
          "sentiment": "categorical sentiment label"
        },
        "confidence": 0.38888888888888884,
        "votes": [
          {
            "TweetAt": "date in DD-MM-YYYY format"
          },
          {
            "UserName": "integer identifier",
            "ScreenName": "integer identifier",
            "Location": "free text location string",
            "TweetAt": "date string (DD-MM-YYYY format)",
            "OriginalTweet": "raw tweet text with URLs, hashtags, mentions",
            "Sentiment": "categorical sentiment label"
          },
          {}
        ]
      },
      "scale_issues": {
        "value": [
          "UserName and ScreenName appear to be sequential IDs rather than actual usernames",
          "Location data has inconsistent formatting with some entries containing coordinates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "UserName and ScreenName appear to be sequential IDs rather than actual usernames",
            "Location data has inconsistent formatting with some entries containing coordinates"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Training data has 41,157 rows while test data has 3,798 rows - significant size difference",
          "Training data includes sentiment labels (Positive, Negative, Neutral, Extremely Positive, Extremely Negative) while test data lacks labels",
          "UserName and ScreenName ranges may differ between train and test sets",
          "Date ranges differ: training data shows 16-03-2020 dates, test data shows 02-03-2020 to 05-03-2020 dates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Training data has 41,157 rows while test data has 3,798 rows - significant size difference",
            "Training data includes sentiment labels (Positive, Negative, Neutral, Extremely Positive, Extremely Negative) while test data lacks labels"
          ],
          [
            "UserName and ScreenName ranges may differ between train and test sets",
            "Date ranges differ: training data shows 16-03-2020 dates, test data shows 02-03-2020 to 05-03-2020 dates"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 0.8888888888888888,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6.0,
        "confidence": 0.8571428571428572,
        "votes": [
          6.0,
          6.0,
          5.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Predictions must be written to sentiment.csv with column name 'Sentiment'",
          "Sentiment predictions must align with training data categories: Positive, Negative, Neutral, Extremely Positive, Extremely Negative",
          "Each test tweet must receive exactly one sentiment prediction",
          "Output file must be named sentiment.csv",
          "Output must contain column named 'Sentiment'",
          "Number of predictions must match number of rows in Corona_NLP_test.csv (3798 rows)",
          "Sentiment predictions must use same class labels as in training data",
          "Model must be trained on Corona_NLP_train.csv",
          "Predictions must be made for Corona_NLP_test.csv",
          "The 'Sentiment' column in the output file 'sentiment.csv' must contain predicted sentiment values for each tweet in 'Corona_NLP_test.csv'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Predictions must be written to sentiment.csv with column name 'Sentiment'",
            "Sentiment predictions must align with training data categories: Positive, Negative, Neutral, Extremely Positive, Extremely Negative",
            "Each test tweet must receive exactly one sentiment prediction"
          ],
          [
            "Output file must be named sentiment.csv",
            "Output must contain column named 'Sentiment'",
            "Number of predictions must match number of rows in Corona_NLP_test.csv (3798 rows)",
            "Sentiment predictions must use same class labels as in training data",
            "Model must be trained on Corona_NLP_train.csv",
            "Predictions must be made for Corona_NLP_test.csv"
          ],
          [
            "The 'Sentiment' column in the output file 'sentiment.csv' must contain predicted sentiment values for each tweet in 'Corona_NLP_test.csv'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove or handle URLs from OriginalTweet",
          "Extract hashtags as potential features",
          "Handle missing Location data",
          "Standardize date format for TweetAt"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove or handle URLs from OriginalTweet",
            "Extract hashtags as potential features",
            "Handle missing Location data",
            "Standardize date format for TweetAt"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check class distribution in training data",
          "Validate sentiment label consistency",
          "Test for text length correlation with sentiment",
          "Check for duplicate tweets"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check class distribution in training data",
            "Validate sentiment label consistency",
            "Test for text length correlation with sentiment",
            "Check for duplicate tweets"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file with single 'Sentiment' column",
          "Same row order as Corona_NLP_test.csv",
          "No additional columns or metadata",
          "CSV file with header row",
          "Column name must be exactly 'Sentiment'",
          "3798 predictions required (one per test instance)",
          "Sentiment values must match training data categories",
          "Should maintain order corresponding to Corona_NLP_test.csv rows",
          "Output file must be named 'sentiment.csv'.",
          "Output file must contain a column named 'Sentiment'.",
          "The 'Sentiment' column in the output file should correspond to the predicted sentiment for the 'OriginalTweet' column in 'Corona_NLP_test.csv'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "CSV file with single 'Sentiment' column",
            "Same row order as Corona_NLP_test.csv",
            "No additional columns or metadata"
          ],
          [
            "CSV file with header row",
            "Column name must be exactly 'Sentiment'",
            "3798 predictions required (one per test instance)",
            "Sentiment values must match training data categories",
            "Should maintain order corresponding to Corona_NLP_test.csv rows"
          ],
          [
            "Output file must be named 'sentiment.csv'.",
            "Output file must contain a column named 'Sentiment'.",
            "The 'Sentiment' column in the output file should correspond to the predicted sentiment for the 'OriginalTweet' column in 'Corona_NLP_test.csv'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5567460317460319
  },
  "ml-regression-002": {
    "m_q": {
      "target_metric": {
        "value": "price actual",
        "confidence": 0.3333333333333333,
        "votes": [
          "price actual",
          "Predicted electricity price actual for test.csv records",
          "Electricity prices for each time point in test.csv"
        ]
      },
      "filters": {
        "value": [
          "time range in test.csv",
          "Spanish electricity data only"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "time range in test.csv",
            "Spanish electricity data only"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "time"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "time"
          ],
          [],
          [
            "time"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "table",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the relationship between electricity generation sources and price actual?",
          "How do weather features affect electricity price?",
          "How accurate are day-ahead forecasts compared to actual values?",
          "What temporal patterns exist in electricity prices?",
          "What features in energy_dataset.csv are predictive of 'price actual'?",
          "How can weather_features.csv be incorporated to improve price prediction?",
          "How do generation sources correlate with actual prices?",
          "What is the relationship between load (forecast/actual) and prices?",
          "How do forecast variables (solar, wind) relate to actual prices?"
        ],
        "confidence": 0.3703703703703704,
        "votes": [
          [
            "What is the relationship between electricity generation sources and price actual?",
            "How do weather features affect electricity price?",
            "How accurate are day-ahead forecasts compared to actual values?",
            "What temporal patterns exist in electricity prices?"
          ],
          [
            "What features in energy_dataset.csv are predictive of 'price actual'?",
            "How can weather_features.csv be incorporated to improve price prediction?",
            "What temporal patterns exist in electricity prices?",
            "How do generation sources correlate with actual prices?",
            "What is the relationship between load (forecast/actual) and prices?",
            "How do forecast variables (solar, wind) relate to actual prices?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "energy_dataset.csv",
          "test.csv",
          "weather_features.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "energy_dataset.csv",
            "test.csv",
            "weather_features.csv"
          ],
          [
            "energy_dataset.csv",
            "test.csv",
            "weather_features.csv"
          ],
          [
            "energy_dataset.csv",
            "test.csv",
            "weather_features.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "energy_dataset.csv has 'price actual' column missing in test.csv",
          "test.csv missing 'price actual' column which is the target variable",
          "weather_features.csv has city_name column not present in energy/test datasets",
          "time column names differ: 'time' vs 'dt_iso'",
          "weather_features.csv has different timezone format in sample data (+01:00 vs +02:00)",
          "test.csv missing 'price actual' column (target variable)",
          "weather_features.csv uses 'dt_iso' while energy files use 'time' for timestamps",
          "weather_features.csv has multiple cities, requiring aggregation or selection"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "energy_dataset.csv has 'price actual' column missing in test.csv",
            "test.csv missing 'price actual' column which is the target variable",
            "weather_features.csv has city_name column not present in energy/test datasets",
            "time column names differ: 'time' vs 'dt_iso'",
            "weather_features.csv has different timezone format in sample data (+01:00 vs +02:00)"
          ],
          [
            "test.csv missing 'price actual' column (target variable)",
            "weather_features.csv uses 'dt_iso' while energy files use 'time' for timestamps",
            "weather_features.csv has multiple cities, requiring aggregation or selection"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "generation biomass": "MW",
          "generation fossil gas": "MW",
          "generation nuclear": "MW",
          "generation solar": "MW",
          "generation wind onshore": "MW",
          "total load actual": "MW",
          "price actual": "\u20ac/MWh",
          "price day ahead": "\u20ac/MWh",
          "temp": "Kelvin",
          "pressure": "hPa",
          "humidity": "%",
          "wind_speed": "m/s",
          "rain_1h": "mm",
          "snow_3h": "mm",
          "clouds_all": "%",
          "time": "datetime with timezone",
          "dt_iso": "datetime with timezone",
          "total load forecast": "MW",
          "temp_min": "Kelvin",
          "temp_max": "Kelvin",
          "wind_deg": "degrees",
          "rain_3h": "mm",
          "generation fossil brown coal/lignite": "MW",
          "generation fossil coal-derived gas": "MW",
          "generation fossil hard coal": "MW",
          "generation fossil oil": "MW",
          "generation fossil oil shale": "MW",
          "generation fossil peat": "MW",
          "generation geothermal": "MW",
          "generation hydro pumped storage aggregated": "MW",
          "generation hydro pumped storage consumption": "MW",
          "generation hydro run-of-river and poundage": "MW",
          "generation hydro water reservoir": "MW",
          "generation marine": "MW",
          "generation other": "MW",
          "generation other renewable": "MW",
          "generation waste": "MW",
          "generation wind offshore": "MW",
          "forecast solar day ahead": "MW",
          "forecast wind offshore eday ahead": "MW",
          "forecast wind onshore day ahead": "MW"
        },
        "confidence": 0.6097560975609752,
        "votes": [
          {
            "generation biomass": "MW",
            "generation fossil gas": "MW",
            "generation nuclear": "MW",
            "generation solar": "MW",
            "generation wind onshore": "MW",
            "total load actual": "MW",
            "price actual": "\u20ac/MWh",
            "price day ahead": "\u20ac/MWh",
            "temp": "Kelvin",
            "pressure": "hPa",
            "humidity": "%",
            "wind_speed": "m/s",
            "rain_1h": "mm",
            "snow_3h": "mm",
            "clouds_all": "%"
          },
          {
            "time": "datetime with timezone",
            "dt_iso": "datetime with timezone",
            "generation biomass": "MW",
            "generation fossil gas": "MW",
            "generation nuclear": "MW",
            "generation solar": "MW",
            "generation wind onshore": "MW",
            "total load forecast": "MW",
            "total load actual": "MW",
            "price day ahead": "EUR/MWh",
            "price actual": "EUR/MWh",
            "temp": "Kelvin",
            "temp_min": "Kelvin",
            "temp_max": "Kelvin",
            "pressure": "hPa",
            "humidity": "percentage",
            "wind_speed": "m/s",
            "wind_deg": "degrees",
            "rain_1h": "mm",
            "rain_3h": "mm",
            "snow_3h": "mm"
          },
          {
            "generation biomass": "MW",
            "generation fossil brown coal/lignite": "MW",
            "generation fossil coal-derived gas": "MW",
            "generation fossil gas": "MW",
            "generation fossil hard coal": "MW",
            "generation fossil oil": "MW",
            "generation fossil oil shale": "MW",
            "generation fossil peat": "MW",
            "generation geothermal": "MW",
            "generation hydro pumped storage aggregated": "MW",
            "generation hydro pumped storage consumption": "MW",
            "generation hydro run-of-river and poundage": "MW",
            "generation hydro water reservoir": "MW",
            "generation marine": "MW",
            "generation nuclear": "MW",
            "generation other": "MW",
            "generation other renewable": "MW",
            "generation solar": "MW",
            "generation waste": "MW",
            "generation wind offshore": "MW",
            "generation wind onshore": "MW",
            "forecast solar day ahead": "MW",
            "forecast wind offshore eday ahead": "MW",
            "forecast wind onshore day ahead": "MW",
            "total load forecast": "MW",
            "total load actual": "MW",
            "price day ahead": "EUR",
            "price actual": "EUR",
            "temp": "Kelvin",
            "temp_min": "Kelvin",
            "temp_max": "Kelvin",
            "pressure": "hPa",
            "humidity": "%",
            "wind_speed": "m/s",
            "wind_deg": "degrees",
            "rain_1h": "mm",
            "rain_3h": "mm",
            "snow_3h": "mm",
            "clouds_all": "%"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Temperature in Kelvin instead of Celsius",
          "Multiple generation columns with same unit (MW) but different scales",
          "Weather data includes multiple cities but energy data is aggregated nationally",
          "Temperature in weather_features.csv is in Kelvin (270-285K range), may need conversion to Celsius",
          "Multiple generation columns have different scales and ranges"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Temperature in Kelvin instead of Celsius",
            "Multiple generation columns with same unit (MW) but different scales",
            "Weather data includes multiple cities but energy data is aggregated nationally"
          ],
          [
            "Temperature in weather_features.csv is in Kelvin (270-285K range), may need conversion to Celsius",
            "Multiple generation columns have different scales and ranges"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Timezone offsets vary between +01:00 and +02:00 indicating daylight saving time changes",
          "Weather data includes multiple cities but no clear mapping to national energy data",
          "weather_features.csv contains multiple records per timestamp (one per city)",
          "Time zones present in timestamps (+01:00, +02:00) need alignment"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Timezone offsets vary between +01:00 and +02:00 indicating daylight saving time changes",
            "Weather data includes multiple cities but no clear mapping to national energy data"
          ],
          [
            "weather_features.csv contains multiple records per timestamp (one per city)",
            "Time zones present in timestamps (+01:00, +02:00) need alignment"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 29.0,
        "confidence": 1.0,
        "votes": [
          29.0,
          29.0,
          29.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "price actual must be positive numeric values",
          "generation values cannot be negative",
          "total load actual should equal sum of generation sources minus losses",
          "time must be in chronological order",
          "price day ahead should correlate with price actual",
          "Predictions must be made for all 7013 rows in test.csv",
          "Output must be saved as result.csv with column name 'price actual'",
          "Training data available in energy_dataset.csv contains 28051 historical records with actual prices",
          "All generation columns should be non-negative",
          "Price values should be positive (EUR/MWh)",
          "Timestamps should be chronological and hourly intervals",
          "The 'time' column must be parsed correctly to handle time zones.",
          "The output 'result.csv' must contain a column named 'price actual'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "price actual must be positive numeric values",
            "generation values cannot be negative",
            "total load actual should equal sum of generation sources minus losses",
            "time must be in chronological order",
            "price day ahead should correlate with price actual"
          ],
          [
            "Predictions must be made for all 7013 rows in test.csv",
            "Output must be saved as result.csv with column name 'price actual'",
            "Training data available in energy_dataset.csv contains 28051 historical records with actual prices",
            "All generation columns should be non-negative",
            "Price values should be positive (EUR/MWh)",
            "Timestamps should be chronological and hourly intervals"
          ],
          [
            "The 'time' column must be parsed correctly to handle time zones.",
            "The output 'result.csv' must contain a column named 'price actual'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows with missing price actual in training",
          "Filter weather data to relevant Spanish cities",
          "Exclude extreme outliers in generation values",
          "Handle daylight saving time transitions consistently",
          "Filter weather_features.csv to relevant Spanish cities for aggregation",
          "Align timestamps across all datasets considering timezone differences",
          "Handle missing values in generation and weather columns appropriately"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with missing price actual in training",
            "Filter weather data to relevant Spanish cities",
            "Exclude extreme outliers in generation values",
            "Handle daylight saving time transitions consistently"
          ],
          [
            "Filter weather_features.csv to relevant Spanish cities for aggregation",
            "Align timestamps across all datasets considering timezone differences",
            "Handle missing values in generation and weather columns appropriately"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Test correlation between generation sources and price actual",
          "Test seasonality in electricity prices",
          "Test stationarity of time series",
          "Test multicollinearity among generation features",
          "Test weather feature importance for price prediction",
          "Check correlation between generation sources and price actual",
          "Analyze relationship between total load (forecast/actual) and prices",
          "Test impact of weather variables on electricity prices",
          "Validate price predictions are within reasonable historical ranges",
          "Check for temporal autocorrelation in price series"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Test correlation between generation sources and price actual",
            "Test seasonality in electricity prices",
            "Test stationarity of time series",
            "Test multicollinearity among generation features",
            "Test weather feature importance for price prediction"
          ],
          [
            "Check correlation between generation sources and price actual",
            "Analyze relationship between total load (forecast/actual) and prices",
            "Test impact of weather variables on electricity prices",
            "Validate price predictions are within reasonable historical ranges",
            "Check for temporal autocorrelation in price series"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "result.csv must have same number of rows as test.csv",
          "result.csv must contain column named 'price actual'",
          "Predictions must be numeric values",
          "Time column should match test.csv format",
          "result.csv must contain predictions for all test.csv rows",
          "Column must be named exactly 'price actual'",
          "Predictions should be numeric float values",
          "Result should maintain same row order as test.csv",
          "File format should be CSV",
          "The result.csv file should contain the 'time' column from test.csv and the predicted 'price actual' column."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "result.csv must have same number of rows as test.csv",
            "result.csv must contain column named 'price actual'",
            "Predictions must be numeric values",
            "Time column should match test.csv format"
          ],
          [
            "result.csv must contain predictions for all test.csv rows",
            "Column must be named exactly 'price actual'",
            "Predictions should be numeric float values",
            "Result should maintain same row order as test.csv",
            "File format should be CSV"
          ],
          [
            "The result.csv file should contain the 'time' column from test.csv and the predicted 'price actual' column."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6156729900632341
  },
  "ml-regression-004": {
    "m_q": {
      "target_metric": {
        "value": "Popularity score for each song in test.csv",
        "confidence": 0.3333333333333333,
        "votes": [
          "Popularity score for each song in test.csv",
          "Popularity predictions for songs in test.csv",
          "Predicted popularity of songs in test.csv"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features best predict song popularity?",
          "How should popularity predictions be formatted for output?",
          "What modeling approach should be used for prediction?",
          "What features in the training data correlate with Popularity?",
          "Which columns are available in both training and test datasets for modeling?",
          "What type of prediction model is appropriate (regression/classification)?",
          "How should missing values in predictor columns be handled?",
          "What is the range and distribution of Popularity values in the training set?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What features best predict song popularity?",
            "How should popularity predictions be formatted for output?",
            "What modeling approach should be used for prediction?"
          ],
          [
            "What features in the training data correlate with Popularity?",
            "Which columns are available in both training and test datasets for modeling?",
            "What type of prediction model is appropriate (regression/classification)?",
            "How should missing values in predictor columns be handled?",
            "What is the range and distribution of Popularity values in the training set?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "test.csv",
          "top_10000_1960-now.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "test.csv",
            "top_10000_1960-now.csv"
          ],
          [
            "top_10000_1960-now.csv",
            "test.csv"
          ],
          [
            "test.csv",
            "top_10000_1960-now.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "test.csv has 34 columns while top_10000_1960-now.csv has 35 columns (includes Popularity)",
          "test.csv lacks the target variable 'Popularity'",
          "test.csv lacks 'Popularity' column which is the target variable present in top_10000_1960-now.csv",
          "Both files have identical column names and types for 34 shared columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv has 34 columns while top_10000_1960-now.csv has 35 columns (includes Popularity)",
            "test.csv lacks the target variable 'Popularity'"
          ],
          [
            "test.csv lacks 'Popularity' column which is the target variable present in top_10000_1960-now.csv",
            "Both files have identical column names and types for 34 shared columns"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "track duration (ms)": "milliseconds",
          "loudness": "dB",
          "tempo": "beats per minute",
          "danceability": "scale 0-1",
          "energy": "scale 0-1",
          "speechiness": "scale 0-1",
          "acousticness": "scale 0-1",
          "instrumentalness": "scale 0-1",
          "liveness": "scale 0-1",
          "valence": "scale 0-1",
          "key": "pitch class notation 0-11",
          "mode": "binary 0=minor 1=major",
          "time signature": "beats per measure",
          "popularity": "scale 0-100"
        },
        "confidence": 0.476190476190476,
        "votes": [
          {
            "Track Duration (ms)": "milliseconds",
            "Loudness": "dB",
            "Tempo": "BPM"
          },
          {
            "Track Duration (ms)": "milliseconds",
            "Loudness": "decibels",
            "Tempo": "beats per minute",
            "Danceability": "scale 0-1",
            "Energy": "scale 0-1",
            "Speechiness": "scale 0-1",
            "Acousticness": "scale 0-1",
            "Instrumentalness": "scale 0-1",
            "Liveness": "scale 0-1",
            "Valence": "scale 0-1",
            "Key": "pitch class notation 0-11",
            "Mode": "binary 0=minor 1=major",
            "Time Signature": "beats per measure",
            "Popularity": "scale 0-100"
          },
          {
            "Track Duration (ms)": "milliseconds",
            "Tempo": "beats per minute",
            "Loudness": "dB"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Key, Mode, Time Signature are float64 but represent categorical/discrete values",
          "Album Genres appears as float64 but likely contains text data with missing values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Key, Mode, Time Signature are float64 but represent categorical/discrete values",
            "Album Genres appears as float64 but likely contains text data with missing values"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Both files have identical column structures except for Popularity column in training data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Both files have identical column structures except for Popularity column in training data"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 35.0,
        "confidence": 0.9722222222222222,
        "votes": [
          34.0,
          35.0,
          35.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Popularity values must be integers between 0-100",
          "Output file must be named popularity.csv with single column 'Popularity'",
          "Predictions must correspond to test.csv row order",
          "Output file must be named 'popularity.csv'",
          "Output must contain column named 'Popularity'",
          "Number of predictions must match test.csv rows (1100)",
          "Popularity values should be non-negative integers",
          "Popularity values typically range 0-100 based on training data",
          "The output file should be named 'popularity.csv'.",
          "The output file should contain a column named 'Popularity'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Popularity values must be integers between 0-100",
            "Output file must be named popularity.csv with single column 'Popularity'",
            "Predictions must correspond to test.csv row order"
          ],
          [
            "Output file must be named 'popularity.csv'",
            "Output must contain column named 'Popularity'",
            "Number of predictions must match test.csv rows (1100)",
            "Popularity values should be non-negative integers",
            "Popularity values typically range 0-100 based on training data"
          ],
          [
            "The output file should be named 'popularity.csv'.",
            "The output file should contain a column named 'Popularity'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude non-numeric columns for modeling",
          "Handle missing values in Artist Genres and Album Genres",
          "Convert categorical columns (Key, Mode, Time Signature) appropriately"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude non-numeric columns for modeling",
            "Handle missing values in Artist Genres and Album Genres",
            "Convert categorical columns (Key, Mode, Time Signature) appropriately"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for multicollinearity among audio features",
          "Validate prediction distribution matches training Popularity distribution",
          "Assess feature importance for popularity prediction",
          "Validate prediction distribution matches reasonable bounds",
          "Check for missing predictions",
          "Verify no negative popularity values"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for multicollinearity among audio features",
            "Validate prediction distribution matches training Popularity distribution",
            "Assess feature importance for popularity prediction"
          ],
          [
            "Validate prediction distribution matches reasonable bounds",
            "Check for missing predictions",
            "Verify no negative popularity values"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file with 1100 rows matching test.csv",
          "Single column named 'Popularity'",
          "Integer values representing predicted popularity scores",
          "CSV format with header",
          "Column name must be exactly 'Popularity'",
          "1100 rows of predictions corresponding to test.csv rows",
          "Integer or float values representing popularity scores",
          "The output should be a CSV file.",
          "The output CSV file should contain a 'Popularity' column with predicted popularity values."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file with 1100 rows matching test.csv",
            "Single column named 'Popularity'",
            "Integer values representing predicted popularity scores"
          ],
          [
            "CSV format with header",
            "Column name must be exactly 'Popularity'",
            "1100 rows of predictions corresponding to test.csv rows",
            "Integer or float values representing popularity scores"
          ],
          [
            "The output should be a CSV file.",
            "The output CSV file should contain a 'Popularity' column with predicted popularity values."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.547420634920635
  },
  "ml-regression-008": {
    "m_q": {
      "target_metric": {
        "value": "quantity_sold (predicted sales quantity for shoe products)",
        "confidence": 0.3333333333333333,
        "votes": [
          "quantity_sold (predicted sales quantity for shoe products)",
          "quantity_sold for shoe products",
          "Predicted quantity sold for each shoe product in test.csv"
        ]
      },
      "filters": {
        "value": [
          "category = 'men' (from test.csv sample)",
          "product type is shoes",
          "category='men' (from test.csv)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "category = 'men' (from test.csv sample)",
            "product type is shoes"
          ],
          [
            "category='men' (from test.csv)"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "id",
          "name",
          "brand",
          "category"
        ],
        "confidence": 0.41666666666666663,
        "votes": [
          [
            "id",
            "name",
            "brand",
            "category"
          ],
          [],
          [
            "id"
          ]
        ]
      },
      "output_cardinality": {
        "value": "table (1750 rows matching test.csv shape)",
        "confidence": 0.3333333333333333,
        "votes": [
          "table (1750 rows matching test.csv shape)",
          "list",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features predict shoe sales?",
          "How to transfer learning from other product categories?",
          "What is the relationship between price, reviews, and sales?",
          "How does brand affect sales prediction?",
          "Should we use all available training data or only shoe-specific data?",
          "What features predict quantity_sold for shoe products?",
          "How to build a predictive model using training data from other product categories?",
          "What is the relationship between price, brand, reviews, and sales quantity?",
          "How to handle missing quantity_sold in test.csv?",
          "What columns are most predictive of sales volume?",
          "What features in the training data are most predictive of quantity_sold?",
          "How can the text data in 'name' and 'description' be used to improve prediction?",
          "Are there any interactions between features that are important for predicting quantity_sold?",
          "How should missing values be handled in each feature?",
          "What is the best model to predict quantity_sold based on the available data?"
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "What features predict shoe sales?",
            "How to transfer learning from other product categories?",
            "What is the relationship between price, reviews, and sales?",
            "How does brand affect sales prediction?",
            "Should we use all available training data or only shoe-specific data?"
          ],
          [
            "What features predict quantity_sold for shoe products?",
            "How to build a predictive model using training data from other product categories?",
            "What is the relationship between price, brand, reviews, and sales quantity?",
            "How to handle missing quantity_sold in test.csv?",
            "What columns are most predictive of sales volume?"
          ],
          [
            "What features in the training data are most predictive of quantity_sold?",
            "How can the text data in 'name' and 'description' be used to improve prediction?",
            "Are there any interactions between features that are important for predicting quantity_sold?",
            "How should missing values be handled in each feature?",
            "What is the best model to predict quantity_sold based on the available data?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "test.csv",
          "vietnamese_tiki_products_backpacks_suitcases.csv",
          "vietnamese_tiki_products_fashion_accessories.csv",
          "vietnamese_tiki_products_men_bags.csv",
          "vietnamese_tiki_products_men_shoes.csv",
          "vietnamese_tiki_products_women_bags.csv",
          "vietnamese_tiki_products_women_shoes.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "test.csv",
            "vietnamese_tiki_products_backpacks_suitcases.csv",
            "vietnamese_tiki_products_fashion_accessories.csv",
            "vietnamese_tiki_products_men_bags.csv",
            "vietnamese_tiki_products_men_shoes.csv",
            "vietnamese_tiki_products_women_bags.csv",
            "vietnamese_tiki_products_women_shoes.csv"
          ],
          [
            "test.csv",
            "vietnamese_tiki_products_men_shoes.csv",
            "vietnamese_tiki_products_women_shoes.csv",
            "vietnamese_tiki_products_backpacks_suitcases.csv",
            "vietnamese_tiki_products_fashion_accessories.csv",
            "vietnamese_tiki_products_men_bags.csv",
            "vietnamese_tiki_products_women_bags.csv"
          ],
          [
            "test.csv",
            "vietnamese_tiki_products_backpacks_suitcases.csv",
            "vietnamese_tiki_products_fashion_accessories.csv",
            "vietnamese_tiki_products_men_bags.csv",
            "vietnamese_tiki_products_men_shoes.csv",
            "vietnamese_tiki_products_women_bags.csv",
            "vietnamese_tiki_products_women_shoes.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "test.csv has 18 columns while training files have 19-20 columns",
          "test.csv missing 'quantity_sold' column",
          "vietnamese_tiki_products_men_shoes.csv and vietnamese_tiki_products_women_shoes.csv have duplicate 'Unnamed: 0.1' and 'Unnamed: 0' columns",
          "Column order differs across files",
          "Some training files have 'category' values that don't match test.csv 'category' values",
          "test.csv has 18 columns without quantity_sold",
          "training files have 19-20 columns including quantity_sold",
          "vietnamese_tiki_products_men_shoes.csv and vietnamese_tiki_products_women_shoes.csv have extra 'Unnamed: 0.1' column",
          "test.csv category='men' while training files have various categories",
          "test.csv does not have quantity_sold column, while other files do.",
          "The vietnamese_tiki_products_men_shoes.csv and vietnamese_tiki_products_women_shoes.csv have an extra 'Unnamed: 0.1' column.",
          "Some files have 'category' column, while test.csv does not."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv has 18 columns while training files have 19-20 columns",
            "test.csv missing 'quantity_sold' column",
            "vietnamese_tiki_products_men_shoes.csv and vietnamese_tiki_products_women_shoes.csv have duplicate 'Unnamed: 0.1' and 'Unnamed: 0' columns",
            "Column order differs across files",
            "Some training files have 'category' values that don't match test.csv 'category' values"
          ],
          [
            "test.csv has 18 columns without quantity_sold",
            "training files have 19-20 columns including quantity_sold",
            "vietnamese_tiki_products_men_shoes.csv and vietnamese_tiki_products_women_shoes.csv have extra 'Unnamed: 0.1' column",
            "test.csv category='men' while training files have various categories"
          ],
          [
            "test.csv does not have quantity_sold column, while other files do.",
            "The vietnamese_tiki_products_men_shoes.csv and vietnamese_tiki_products_women_shoes.csv have an extra 'Unnamed: 0.1' column.",
            "Some files have 'category' column, while test.csv does not."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "original_price": "VND",
          "price": "VND",
          "vnd_cashback": "VND",
          "date_created": "timestamp or days since epoch",
          "review_count": "count",
          "rating_average": "scale 0-5",
          "favourite_count": "count",
          "number_of_images": "count",
          "quantity_sold": "units"
        },
        "confidence": 0.8888888888888888,
        "votes": [
          {
            "original_price": "VND",
            "price": "VND",
            "vnd_cashback": "VND",
            "date_created": "timestamp or days since epoch",
            "review_count": "count",
            "rating_average": "scale 0-5",
            "favourite_count": "count",
            "number_of_images": "count",
            "quantity_sold": "units"
          },
          {
            "original_price": "VND (Vietnamese Dong)",
            "price": "VND (Vietnamese Dong)",
            "vnd_cashback": "VND (Vietnamese Dong)",
            "date_created": "unix_timestamp_or_days_since_epoch",
            "rating_average": "scale_0_to_5",
            "review_count": "count",
            "favourite_count": "count",
            "number_of_images": "count",
            "quantity_sold": "count"
          },
          {
            "original_price": "VND",
            "price": "VND",
            "vnd_cashback": "VND",
            "review_count": "count",
            "favourite_count": "count",
            "number_of_images": "count"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "price ranges from 1,350 VND to 1,900,000 VND - extreme range",
          "review_count has many zeros",
          "rating_average has many zeros",
          "date_created values like 616, 775 suggest non-standard timestamp format",
          "favourite_count has extreme values like 2811",
          "price and original_price are in VND currency (large numbers)",
          "date_created appears to be numeric timestamp, needs interpretation",
          "rating_average is on 0-5 scale with float precision"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "price ranges from 1,350 VND to 1,900,000 VND - extreme range",
            "review_count has many zeros",
            "rating_average has many zeros",
            "date_created values like 616, 775 suggest non-standard timestamp format",
            "favourite_count has extreme values like 2811"
          ],
          [
            "price and original_price are in VND currency (large numbers)",
            "date_created appears to be numeric timestamp, needs interpretation",
            "rating_average is on 0-5 scale with float precision"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "test.csv 'category' values ('men') vs training files 'category' values ('Root', 'Balo n\u1eef', 'C\u00e0i \u00c1o', 'T\u00fai \u0111eo ch\u00e9o nam', etc.)",
          "Different product categories in training data (shoes, bags, accessories) vs test data (shoes only)",
          "Brand naming inconsistencies: 'OEM' appears frequently but may represent different manufacturers",
          "test.csv has all category='men' but training data spans multiple categories",
          "test.csv lacks quantity_sold which is the target variable",
          "date_created values range differs across sources"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv 'category' values ('men') vs training files 'category' values ('Root', 'Balo n\u1eef', 'C\u00e0i \u00c1o', 'T\u00fai \u0111eo ch\u00e9o nam', etc.)",
            "Different product categories in training data (shoes, bags, accessories) vs test data (shoes only)",
            "Brand naming inconsistencies: 'OEM' appears frequently but may represent different manufacturers"
          ],
          [
            "test.csv has all category='men' but training data spans multiple categories",
            "test.csv lacks quantity_sold which is the target variable",
            "date_created values range differs across sources"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "0",
          "0.0",
          "NA",
          "N/A",
          "NaN"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "",
            "0",
            "0.0",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 18.0,
        "confidence": 1.0,
        "votes": [
          18.0,
          18.0,
          18.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "test.csv has no quantity_sold column - target variable missing",
          "Must predict for all 1750 rows in test.csv",
          "Output must be saved to quantity.csv with column name 'quantity_sold'",
          "Training data includes non-shoe products that may not be relevant",
          "id should be unique across datasets",
          "price <= original_price (discount constraint)",
          "test.csv contains only shoe products with category='men'",
          "quantity_sold must be predicted (not present in test.csv)",
          "output must be written to quantity.csv with column name 'quantity_sold'",
          "prediction must be made for all 1750 rows in test.csv",
          "training data comes from multiple product categories (shoes, bags, accessories, backpacks)",
          "quantity_sold in training data is integer type (count variable)",
          "must use product features from test.csv for prediction",
          "The 'id' column should be unique within each file.",
          "The 'price' should be less than or equal to 'original_price'."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "test.csv has no quantity_sold column - target variable missing",
            "Must predict for all 1750 rows in test.csv",
            "Output must be saved to quantity.csv with column name 'quantity_sold'",
            "Training data includes non-shoe products that may not be relevant",
            "id should be unique across datasets",
            "price <= original_price (discount constraint)"
          ],
          [
            "test.csv contains only shoe products with category='men'",
            "quantity_sold must be predicted (not present in test.csv)",
            "output must be written to quantity.csv with column name 'quantity_sold'",
            "prediction must be made for all 1750 rows in test.csv",
            "training data comes from multiple product categories (shoes, bags, accessories, backpacks)",
            "quantity_sold in training data is integer type (count variable)",
            "must use product features from test.csv for prediction"
          ],
          [
            "The 'id' column should be unique within each file.",
            "The 'price' should be less than or equal to 'original_price'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows where price = 0",
          "Filter training data to similar product categories",
          "Consider only products with non-zero review_count for reliability",
          "Exclude extreme outlier prices",
          "filter training data to shoe-related products for better model fit",
          "consider category='men' products from training data as most relevant",
          "exclude products with 0 or missing reviews if modeling review impact",
          "filter out extreme outliers in price or quantity_sold during training",
          "Filter out products where 'price' is negative.",
          "Filter out products where 'original_price' is negative."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Remove rows where price = 0",
            "Filter training data to similar product categories",
            "Consider only products with non-zero review_count for reliability",
            "Exclude extreme outlier prices"
          ],
          [
            "filter training data to shoe-related products for better model fit",
            "consider category='men' products from training data as most relevant",
            "exclude products with 0 or missing reviews if modeling review impact",
            "filter out extreme outliers in price or quantity_sold during training"
          ],
          [
            "Filter out products where 'price' is negative.",
            "Filter out products where 'original_price' is negative."
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Correlation analysis between price and quantity_sold",
          "Test significance of review_count on sales",
          "Check for multicollinearity among features",
          "Validate model on holdout shoe data if available",
          "Test distribution differences between training categories",
          "correlation analysis between price, rating_average, review_count and quantity_sold",
          "feature importance ranking for prediction model",
          "distribution analysis of quantity_sold in training data",
          "cross-validation performance metrics (RMSE, MAE) for model evaluation",
          "Correlation analysis between numerical features and 'quantity_sold'.",
          "Test for statistically significant differences in 'quantity_sold' across different 'brand' values.",
          "Check distribution of numerical features for skewness and outliers."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Correlation analysis between price and quantity_sold",
            "Test significance of review_count on sales",
            "Check for multicollinearity among features",
            "Validate model on holdout shoe data if available",
            "Test distribution differences between training categories"
          ],
          [
            "correlation analysis between price, rating_average, review_count and quantity_sold",
            "feature importance ranking for prediction model",
            "distribution analysis of quantity_sold in training data",
            "cross-validation performance metrics (RMSE, MAE) for model evaluation"
          ],
          [
            "Correlation analysis between numerical features and 'quantity_sold'.",
            "Test for statistically significant differences in 'quantity_sold' across different 'brand' values.",
            "Check distribution of numerical features for skewness and outliers."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "CSV file named quantity.csv",
          "Single column 'quantity_sold'",
          "1750 rows matching test.csv order",
          "Integer values (sales quantities)",
          "No header row beyond column name",
          "output file must be named 'quantity.csv'",
          "output must contain column named 'quantity_sold'",
          "output must have 1750 rows matching test.csv",
          "quantity_sold values must be non-negative integers",
          "output should preserve row order from test.csv or include identifier for matching",
          "Output file must be named 'quantity.csv'.",
          "Output file must contain a column named 'quantity_sold'.",
          "Output file must contain 'id' column from test.csv."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "CSV file named quantity.csv",
            "Single column 'quantity_sold'",
            "1750 rows matching test.csv order",
            "Integer values (sales quantities)",
            "No header row beyond column name"
          ],
          [
            "output file must be named 'quantity.csv'",
            "output must contain column named 'quantity_sold'",
            "output must have 1750 rows matching test.csv",
            "quantity_sold values must be non-negative integers",
            "output should preserve row order from test.csv or include identifier for matching"
          ],
          [
            "Output file must be named 'quantity.csv'.",
            "Output file must contain a column named 'quantity_sold'.",
            "Output file must contain 'id' column from test.csv."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5819444444444446
  },
  "ml-regression-011": {
    "m_q": {
      "target_metric": {
        "value": "Biogas Generation Estimate (cu-ft/day) for each entry in test.csv",
        "confidence": 0.6666666666666666,
        "votes": [
          "Biogas Generation Estimate (cu-ft/day)",
          "Biogas Generation Estimate (cu-ft/day) for each entry in test.csv",
          "Biogas Generation Estimate (cu-ft/day) for each entry in test.csv"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "table",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What features best predict biogas generation?",
          "How do animal types and counts affect biogas output?",
          "What is the relationship between waste efficiency and biogas generation?",
          "How does digester type influence biogas production?",
          "What impact do operational years have on biogas estimates?",
          "What features from test.csv are most predictive of Biogas Generation Estimate?",
          "How to handle missing values in predictor columns?",
          "Which machine learning model is appropriate for this regression task?",
          "How to map test.csv entries to predicted biogas_generation_estimate_cuftday values?",
          "What preprocessing steps are needed for categorical and numerical features?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What features best predict biogas generation?",
            "How do animal types and counts affect biogas output?",
            "What is the relationship between waste efficiency and biogas generation?",
            "How does digester type influence biogas production?",
            "What impact do operational years have on biogas estimates?"
          ],
          [
            "What features from test.csv are most predictive of Biogas Generation Estimate?",
            "How to handle missing values in predictor columns?",
            "Which machine learning model is appropriate for this regression task?",
            "How to map test.csv entries to predicted biogas_generation_estimate_cuftday values?",
            "What preprocessing steps are needed for categorical and numerical features?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "test.csv",
          "veri_seti_son_2.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "test.csv",
            "veri_seti_son_2.csv"
          ],
          [
            "test.csv",
            "veri_seti_son_2.csv"
          ],
          [
            "test.csv",
            "veri_seti_son_2.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "test.csv has 28 columns while veri_seti_son_2.csv has 29 columns (missing target column)",
          "test.csv missing 'Biogas Generation Estimate (cu-ft/day)' column needed for training",
          "test.csv has 28 columns while veri_seti_son_2.csv has 29 columns",
          "veri_seti_son_2.csv contains the target variable 'Biogas Generation Estimate (cu-ft/day)' which is missing in test.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv has 28 columns while veri_seti_son_2.csv has 29 columns (missing target column)",
            "test.csv missing 'Biogas Generation Estimate (cu-ft/day)' column needed for training"
          ],
          [
            "test.csv has 28 columns while veri_seti_son_2.csv has 29 columns",
            "veri_seti_son_2.csv contains the target variable 'Biogas Generation Estimate (cu-ft/day)' which is missing in test.csv"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "biogas generation estimate (cu-ft/day)": "cubic feet per day",
          "biogas_per_animal (cu-ft/day)": "cubic feet per day per animal",
          "electricity generated (kwh/yr)": "kilowatt-hours per year",
          "total emission reductions (mtco2e/yr)": "metric tons CO2 equivalent per year",
          "total_waste_kg/day": "kilograms per day",
          "emission_reduction_per_year": "MTCO2e per year per operational year",
          "electricity_to_biogas_ratio": "kWh/yr per cu-ft/day (unitless ratio)",
          "waste_efficiency": "cu-ft/day per kg/day (unitless ratio)",
          "electricity_efficiency": "kWh/yr per kg/day (unitless ratio)",
          "year operational": "year",
          "operational years": "years",
          "total_animals": "count",
          "cattle": "count",
          "dairy": "count",
          "poultry": "count",
          "swine": "count"
        },
        "confidence": 0.5416666666666665,
        "votes": [
          {
            "Biogas Generation Estimate (cu-ft/day)": "cubic feet per day",
            "Biogas_per_Animal (cu-ft/day)": "cubic feet per day per animal",
            "Electricity Generated (kWh/yr)": "kilowatt-hours per year",
            "Total Emission Reductions (MTCO2e/yr)": "metric tons CO2 equivalent per year",
            "Total_Waste_kg/day": "kilograms per day",
            "Emission_Reduction_per_Year": "MTCO2e per year per operational year",
            "Electricity_to_Biogas_Ratio": "kWh/yr per cu-ft/day (unitless ratio)",
            "Waste_Efficiency": "cu-ft/day per kg/day (unitless ratio)",
            "Electricity_Efficiency": "kWh/yr per kg/day (unitless ratio)"
          },
          {
            "Biogas Generation Estimate (cu-ft/day)": "cubic feet per day",
            "Electricity Generated (kWh/yr)": "kilowatt-hours per year",
            "Total Emission Reductions (MTCO2e/yr)": "metric tons CO2 equivalent per year",
            "Biogas_per_Animal (cu-ft/day)": "cubic feet per day per animal",
            "Total_Waste_kg/day": "kilograms per day",
            "Year Operational": "year",
            "Operational Years": "years",
            "Total_Animals": "count",
            "Cattle": "count",
            "Dairy": "count",
            "Poultry": "count",
            "Swine": "count"
          },
          {
            "Electricity Generated (kWh/yr)": "kWh/yr",
            "Total Emission Reductions (MTCO2e/yr)": "MTCO2e/yr",
            "Biogas Generation Estimate (cu-ft/day)": "cu-ft/day",
            "Biogas_per_Animal (cu-ft/day)": "cu-ft/day",
            "Total_Waste_kg/day": "kg/day"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Large range in animal counts (120 to 500,000)",
          "Biogas_per_Animal varies from 2.08 to 2985.18 cu-ft/day",
          "Electricity Generated ranges from 0 to 16,206,000 kWh/yr",
          "Electricity Generated is in kWh/yr while biogas is in cu-ft/day (different time scales)",
          "Total Emission Reductions is yearly but biogas generation is daily",
          "Biogas_per_Animal and total Biogas Generation Estimate may have scaling relationships"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Large range in animal counts (120 to 500,000)",
            "Biogas_per_Animal varies from 2.08 to 2985.18 cu-ft/day",
            "Electricity Generated ranges from 0 to 16,206,000 kWh/yr"
          ],
          [
            "Electricity Generated is in kWh/yr while biogas is in cu-ft/day (different time scales)",
            "Total Emission Reductions is yearly but biogas generation is daily",
            "Biogas_per_Animal and total Biogas Generation Estimate may have scaling relationships"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "test.csv has 74 rows for prediction, veri_seti_son_2.csv has 417 rows for training",
          "Both datasets share 28 common columns with same names and dtypes",
          "Target variable only exists in training data (veri_seti_son_2.csv)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv has 74 rows for prediction, veri_seti_son_2.csv has 417 rows for training"
          ],
          [
            "Both datasets share 28 common columns with same names and dtypes",
            "Target variable only exists in training data (veri_seti_son_2.csv)"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "No-Info",
          "NA",
          "N/A",
          "",
          "0.0"
        ],
        "confidence": 0.8,
        "votes": [
          [
            "No-Info",
            "NA",
            "N/A",
            "",
            "0.0"
          ],
          [
            "NA",
            "N/A",
            "",
            "No-Info"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 28.0,
        "confidence": 1.0,
        "votes": [
          28.0,
          28.0,
          28.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Biogas Generation Estimate must be positive",
          "Total_Animals >= Cattle + Dairy + Poultry + Swine",
          "Operational Years = Current Year - Year Operational",
          "Biogas_per_Animal = Biogas Generation Estimate / Total_Animals (when both available)",
          "Predictions must be generated for all 74 entries in test.csv",
          "Output column must be named exactly 'biogas_generation_estimate_cuftday'",
          "Biogas Generation Estimate values should be non-negative",
          "Result must be saved as result.csv",
          "The output file must be named 'result.csv'.",
          "The output file must contain a column named 'biogas_generation_estimate_cuftday'."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Biogas Generation Estimate must be positive",
            "Total_Animals >= Cattle + Dairy + Poultry + Swine",
            "Operational Years = Current Year - Year Operational",
            "Biogas_per_Animal = Biogas Generation Estimate / Total_Animals (when both available)"
          ],
          [
            "Predictions must be generated for all 74 entries in test.csv",
            "Output column must be named exactly 'biogas_generation_estimate_cuftday'",
            "Biogas Generation Estimate values should be non-negative",
            "Result must be saved as result.csv"
          ],
          [
            "The output file must be named 'result.csv'.",
            "The output file must contain a column named 'biogas_generation_estimate_cuftday'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows with Total_Animals = 0",
          "Consider operational status for prediction relevance",
          "Check for data completeness in key predictor columns",
          "Training data: veri_seti_son_2.csv contains non-null 'Biogas Generation Estimate (cu-ft/day)' values",
          "Test data: test.csv entries where target variable needs to be predicted"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows with Total_Animals = 0",
            "Consider operational status for prediction relevance",
            "Check for data completeness in key predictor columns"
          ],
          [
            "Training data: veri_seti_son_2.csv contains non-null 'Biogas Generation Estimate (cu-ft/day)' values",
            "Test data: test.csv entries where target variable needs to be predicted"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Correlation analysis between biogas generation and animal counts",
          "ANOVA for biogas generation across digester types",
          "Regression analysis for waste efficiency vs biogas output",
          "Check multicollinearity among predictor variables",
          "Validate that predicted values are in reasonable range based on training data distribution",
          "Check for correlations between Total_Animals, Biogas_per_Animal, and target variable",
          "Assess feature importance of animal counts (Cattle, Dairy, Poultry, Swine) on biogas generation",
          "Evaluate impact of Digester Type and Status on predictions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Correlation analysis between biogas generation and animal counts",
            "ANOVA for biogas generation across digester types",
            "Regression analysis for waste efficiency vs biogas output",
            "Check multicollinearity among predictor variables"
          ],
          [
            "Validate that predicted values are in reasonable range based on training data distribution",
            "Check for correlations between Total_Animals, Biogas_per_Animal, and target variable",
            "Assess feature importance of animal counts (Cattle, Dairy, Poultry, Swine) on biogas generation",
            "Evaluate impact of Digester Type and Status on predictions"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Predictions must be saved in result.csv",
          "Column name must be 'biogas_generation_estimate_cuftday'",
          "One prediction per row in test.csv",
          "Numeric values only",
          "Output file name: result.csv",
          "Output must contain column: biogas_generation_estimate_cuftday",
          "Output should have 74 rows matching test.csv entries",
          "Values should be float type representing cu-ft/day",
          "CSV format with header row",
          "CSV file with a column named 'biogas_generation_estimate_cuftday'"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Predictions must be saved in result.csv",
            "Column name must be 'biogas_generation_estimate_cuftday'",
            "One prediction per row in test.csv",
            "Numeric values only"
          ],
          [
            "Output file name: result.csv",
            "Output must contain column: biogas_generation_estimate_cuftday",
            "Output should have 74 rows matching test.csv entries",
            "Values should be float type representing cu-ft/day",
            "CSV format with header row"
          ],
          [
            "CSV file with a column named 'biogas_generation_estimate_cuftday'"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5670833333333334
  },
  "ml-regression-012": {
    "m_q": {
      "target_metric": {
        "value": "Mileage (float64)",
        "confidence": 0.3333333333333333,
        "votes": [
          "Mileage (float64)",
          "Mileage prediction for vehicles in test.csv",
          "Mileage"
        ]
      },
      "filters": {
        "value": [
          "Only vehicles in test.csv need predictions",
          "All rows in test.csv must have Mileage predictions"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Only vehicles in test.csv need predictions",
            "All rows in test.csv must have Mileage predictions"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "brand",
          "Year",
          "Model",
          "new&used",
          "Fuel type",
          "Transmission"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "brand",
            "Year",
            "Model",
            "new&used",
            "Fuel type",
            "Transmission"
          ],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "How does vehicle age affect mileage?",
          "How do different brands/models have different mileage patterns?",
          "How does new vs used status affect mileage?",
          "What features (MPG, Engine, etc.) correlate with mileage?",
          "How to handle missing values in training data?",
          "What features from New_York_cars.csv are most predictive of Mileage?",
          "How can Car_Rates.csv ratings be joined to enhance prediction?",
          "What is the distribution of Mileage in the training data?",
          "Are there missing values in key predictor columns?",
          "What is the relationship between Year, new&used status, and Mileage?",
          "How does brand and model affect Mileage?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "How does vehicle age affect mileage?",
            "How do different brands/models have different mileage patterns?",
            "How does new vs used status affect mileage?",
            "What features (MPG, Engine, etc.) correlate with mileage?",
            "How to handle missing values in training data?"
          ],
          [
            "What features from New_York_cars.csv are most predictive of Mileage?",
            "How can Car_Rates.csv ratings be joined to enhance prediction?",
            "What is the distribution of Mileage in the training data?",
            "Are there missing values in key predictor columns?",
            "What is the relationship between Year, new&used status, and Mileage?",
            "How does brand and model affect Mileage?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Car_Rates.csv",
          "New_York_cars.csv",
          "test.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Car_Rates.csv",
            "New_York_cars.csv",
            "test.csv"
          ],
          [
            "New_York_cars.csv",
            "Car_Rates.csv",
            "test.csv"
          ],
          [
            "New_York_cars.csv",
            "test.csv",
            "Car_Rates.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Car_Rates.csv uses 'Brand' vs New_York_cars.csv uses 'brand'",
          "Car_Rates.csv uses 'Car_name' vs New_York_cars.csv uses 'name'",
          "Car_Rates.csv has rating columns not in other files",
          "test.csv missing 'Mileage' column (target)",
          "Brand column in Car_Rates.csv vs brand column in New_York_cars.csv and test.csv (capitalization difference)",
          "test.csv lacks Mileage column which is present in New_York_cars.csv",
          "Model column present in all three files but may have formatting differences",
          "The 'name' column in New_York_cars.csv and test.csv might contain the year, brand, and model information, which are also present in separate columns. This redundancy needs to be handled.",
          "The 'MPG' column in New_York_cars.csv and test.csv is of type object and contains a range. It needs to be parsed and converted to a numerical representation.",
          "The 'Car_name' column in Car_Rates.csv contains year, brand and model information. This redundancy needs to be handled."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Car_Rates.csv uses 'Brand' vs New_York_cars.csv uses 'brand'",
            "Car_Rates.csv uses 'Car_name' vs New_York_cars.csv uses 'name'",
            "Car_Rates.csv has rating columns not in other files",
            "test.csv missing 'Mileage' column (target)"
          ],
          [
            "Brand column in Car_Rates.csv vs brand column in New_York_cars.csv and test.csv (capitalization difference)",
            "test.csv lacks Mileage column which is present in New_York_cars.csv",
            "Model column present in all three files but may have formatting differences"
          ],
          [
            "The 'name' column in New_York_cars.csv and test.csv might contain the year, brand, and model information, which are also present in separate columns. This redundancy needs to be handled.",
            "The 'MPG' column in New_York_cars.csv and test.csv is of type object and contains a range. It needs to be parsed and converted to a numerical representation.",
            "The 'Car_name' column in Car_Rates.csv contains year, brand and model information. This redundancy needs to be handled."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "money": "currency units (dollars)",
          "mileage": "miles",
          "mpg": "miles per gallon",
          "engine": "liters",
          "year": "year",
          "general_rate": "rating scale",
          "comfort": "rating scale",
          "interior design": "rating scale",
          "performance": "rating scale",
          "value for the money": "rating scale",
          "exterior styling": "rating scale",
          "reliability": "rating scale",
          "num_of_reviews": "count"
        },
        "confidence": 0.5128205128205127,
        "votes": [
          {
            "money": "currency units (dollars)",
            "Mileage": "miles",
            "MPG": "miles per gallon",
            "Engine": "liters",
            "Year": "calendar year"
          },
          {
            "Mileage": "miles",
            "MPG": "miles per gallon (range format)",
            "money": "currency units",
            "Year": "year",
            "General_rate": "rating scale",
            "Comfort": "rating scale",
            "Interior design": "rating scale",
            "Performance": "rating scale",
            "Value for the money": "rating scale",
            "Exterior styling": "rating scale",
            "Reliability": "rating scale",
            "Num_of_reviews": "count"
          },
          {
            "Mileage": "miles",
            "money": "USD",
            "Year": "year"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "MPG column contains range strings (e.g., '26\u201333') not numeric values",
          "Mileage has very wide range (0 to 147179+)",
          "money values vary widely by vehicle type",
          "MPG is stored as range string (e.g., '26\u201333') requiring parsing",
          "Rating columns in Car_Rates.csv contain float values with many missing entries",
          "Mileage values range from 0.0 (new vehicles) to 150000+"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "MPG column contains range strings (e.g., '26\u201333') not numeric values",
            "Mileage has very wide range (0 to 147179+)",
            "money values vary widely by vehicle type"
          ],
          [
            "MPG is stored as range string (e.g., '26\u201333') requiring parsing",
            "Rating columns in Car_Rates.csv contain float values with many missing entries",
            "Mileage values range from 0.0 (new vehicles) to 150000+"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Car_Rates.csv has many missing values (empty cells)",
          "New_York_cars.csv has inconsistent formatting in feature columns (spaces, lists)",
          "test.csv has certification types (e.g., 'Ford Certified') not in training",
          "Brand naming convention differs between Car_Rates.csv and New_York_cars.csv/test.csv",
          "Model names may have trailing periods or formatting differences across sources"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Car_Rates.csv has many missing values (empty cells)",
            "New_York_cars.csv has inconsistent formatting in feature columns (spaces, lists)",
            "test.csv has certification types (e.g., 'Ford Certified') not in training"
          ],
          [
            "Brand naming convention differs between Car_Rates.csv and New_York_cars.csv/test.csv",
            "Model names may have trailing periods or formatting differences across sources"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "None",
          "None Reported"
        ],
        "confidence": 0.8,
        "votes": [
          [
            "",
            "NA",
            "N/A",
            "None",
            " "
          ],
          [
            "",
            "NA",
            "N/A",
            "None Reported"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 24.0,
        "confidence": 1.0,
        "votes": [
          24.0,
          24.0,
          24.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Mileage must be non-negative",
          "New vehicles typically have low mileage (<100)",
          "Year must be reasonable (e.g., 2000-2023)",
          "MPG ranges must be valid positive numbers",
          "Mileage predictions must be written to result.csv",
          "Mileage must be non-negative float values",
          "Predictions required for all 26324 rows in test.csv",
          "Output must be written to result.csv with column name 'Mileage'",
          "New vehicles (new&used='New') typically have Mileage near 0",
          "Used vehicles should have Mileage > 0",
          "Year should be between reasonable bounds (e.g., 2000-2023)",
          "Training data from New_York_cars.csv has 149166 samples",
          "The predicted 'Mileage' values should be non-negative.",
          "The predicted 'Mileage' values should be within a reasonable range based on the 'Year' of the vehicle."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Mileage must be non-negative",
            "New vehicles typically have low mileage (<100)",
            "Year must be reasonable (e.g., 2000-2023)",
            "MPG ranges must be valid positive numbers",
            "Mileage predictions must be written to result.csv"
          ],
          [
            "Mileage must be non-negative float values",
            "Predictions required for all 26324 rows in test.csv",
            "Output must be written to result.csv with column name 'Mileage'",
            "New vehicles (new&used='New') typically have Mileage near 0",
            "Used vehicles should have Mileage > 0",
            "Year should be between reasonable bounds (e.g., 2000-2023)",
            "Training data from New_York_cars.csv has 149166 samples"
          ],
          [
            "The predicted 'Mileage' values should be non-negative.",
            "The predicted 'Mileage' values should be within a reasonable range based on the 'Year' of the vehicle."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude vehicles with Year < 2000",
          "Consider separate models for new vs used vehicles",
          "Filter out extreme outliers in Mileage (>300,000)",
          "Filter out rows with critical missing values in training data if necessary",
          "Identify and handle outliers in Mileage values",
          "Consider vehicle age (2024 - Year) as derived feature"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude vehicles with Year < 2000",
            "Consider separate models for new vs used vehicles",
            "Filter out extreme outliers in Mileage (>300,000)"
          ],
          [
            "Filter out rows with critical missing values in training data if necessary",
            "Identify and handle outliers in Mileage values",
            "Consider vehicle age (2024 - Year) as derived feature"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Correlation between Year and Mileage",
          "ANOVA of Mileage by brand",
          "Regression of Mileage on vehicle features",
          "Test for missing data patterns",
          "Check correlation between Year and Mileage",
          "Analyze Mileage distribution by brand and model",
          "Test relationship between new&used status and Mileage",
          "Evaluate feature importance for prediction model",
          "Validate prediction model using cross-validation on New_York_cars.csv",
          "Correlation analysis between numerical features (Year, money) and Mileage in New_York_cars.csv.",
          "Distribution analysis of Mileage in New_York_cars.csv to identify potential outliers."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Correlation between Year and Mileage",
            "ANOVA of Mileage by brand",
            "Regression of Mileage on vehicle features",
            "Test for missing data patterns"
          ],
          [
            "Check correlation between Year and Mileage",
            "Analyze Mileage distribution by brand and model",
            "Test relationship between new&used status and Mileage",
            "Evaluate feature importance for prediction model",
            "Validate prediction model using cross-validation on New_York_cars.csv"
          ],
          [
            "Correlation analysis between numerical features (Year, money) and Mileage in New_York_cars.csv.",
            "Distribution analysis of Mileage in New_York_cars.csv to identify potential outliers."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "result.csv must have same row count as test.csv",
          "Column name must be 'Mileage'",
          "Predictions must be numeric (float)",
          "Output file must be named result.csv",
          "Must contain column named 'Mileage'",
          "Must have exactly 26324 predictions matching test.csv row count",
          "Mileage values should be numeric (float)",
          "No missing values allowed in output",
          "The output file should be named 'result.csv'.",
          "The output file should contain a column named 'Mileage' with the predicted mileage values.",
          "The output file should have the same number of rows as test.csv."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "result.csv must have same row count as test.csv",
            "Column name must be 'Mileage'",
            "Predictions must be numeric (float)"
          ],
          [
            "Output file must be named result.csv",
            "Must contain column named 'Mileage'",
            "Must have exactly 26324 predictions matching test.csv row count",
            "Mileage values should be numeric (float)",
            "No missing values allowed in output"
          ],
          [
            "The output file should be named 'result.csv'.",
            "The output file should contain a column named 'Mileage' with the predicted mileage values.",
            "The output file should have the same number of rows as test.csv."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5823076923076924
  },
  "ml-regression-014": {
    "m_q": {
      "target_metric": {
        "value": "Predict car prices for test.csv records",
        "confidence": 0.3333333333333333,
        "votes": [
          "Predict car prices for test.csv records",
          "Predicted car prices for test.csv dataset",
          "Predicted car prices for the test dataset"
        ]
      },
      "filters": {
        "value": [
          "No explicit filters in question, but training data may need filtering for valid records"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "No explicit filters in question, but training data may need filtering for valid records"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "list",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "list"
        ]
      },
      "sub_questions": {
        "value": [
          "What features best predict car price?",
          "How to handle missing generation_name values?",
          "Should location (city/province) be used as features?",
          "How to handle extreme mileage values (e.g., 1 km)?",
          "What modeling approach is appropriate for price prediction?",
          "What features from Car_Prices_Poland_Kaggle.csv are most predictive of price?",
          "How should categorical variables (mark, model, generation_name, fuel, city, province) be encoded?",
          "What is the relationship between year, mileage, vol_engine and price?",
          "Are there any missing values or data quality issues in the training data?",
          "What regression model should be used for price prediction?",
          "How should the test.csv data be preprocessed to match training data?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What features best predict car price?",
            "How to handle missing generation_name values?",
            "Should location (city/province) be used as features?",
            "How to handle extreme mileage values (e.g., 1 km)?",
            "What modeling approach is appropriate for price prediction?"
          ],
          [
            "What features from Car_Prices_Poland_Kaggle.csv are most predictive of price?",
            "How should categorical variables (mark, model, generation_name, fuel, city, province) be encoded?",
            "What is the relationship between year, mileage, vol_engine and price?",
            "Are there any missing values or data quality issues in the training data?",
            "What regression model should be used for price prediction?",
            "How should the test.csv data be preprocessed to match training data?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Car_Prices_Poland_Kaggle.csv",
          "test.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "Car_Prices_Poland_Kaggle.csv",
            "test.csv"
          ],
          [
            "Car_Prices_Poland_Kaggle.csv",
            "test.csv"
          ],
          [
            "Car_Prices_Poland_Kaggle.csv",
            "test.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "test.csv missing 'price' column compared to training data",
          "Both files have 'Unnamed: 0' column which may be an index",
          "test.csv lacks the 'price' column which is the target variable in Car_Prices_Poland_Kaggle.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "test.csv missing 'price' column compared to training data",
            "Both files have 'Unnamed: 0' column which may be an index"
          ],
          [
            "test.csv lacks the 'price' column which is the target variable in Car_Prices_Poland_Kaggle.csv"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "mileage": "kilometers",
          "vol_engine": "cubic centimeters (cc)",
          "price": "Polish z\u0142oty (PLN)",
          "year": "calendar year"
        },
        "confidence": 0.9166666666666666,
        "votes": [
          {
            "mileage": "kilometers",
            "vol_engine": "cubic centimeters (cc)",
            "price": "Polish z\u0142oty (PLN)"
          },
          {
            "year": "calendar year",
            "mileage": "kilometers",
            "vol_engine": "cubic centimeters (cc)",
            "price": "Polish Zloty (PLN)"
          },
          {
            "year": "year",
            "mileage": "km",
            "vol_engine": "cm^3",
            "price": "PLN"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Mileage values include extremely low values (1 km) which may indicate new cars or data errors",
          "Year values span many decades (1966-2021)",
          "Price values likely need scaling for modeling",
          "vol_engine appears to be in cc (e.g., 1995, 1968) but some values are very small (0, 998)",
          "mileage has extreme outliers (value of 1 indicates new/demo car)",
          "year ranges from 1966 to 2021, vintage cars may have different price dynamics"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Mileage values include extremely low values (1 km) which may indicate new cars or data errors",
            "Year values span many decades (1966-2021)",
            "Price values likely need scaling for modeling"
          ],
          [
            "vol_engine appears to be in cc (e.g., 1995, 1968) but some values are very small (0, 998)",
            "mileage has extreme outliers (value of 1 indicates new/demo car)",
            "year ranges from 1966 to 2021, vintage cars may have different price dynamics"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Need to verify same categorical value domains for mark, model, fuel, city, province across both files",
          "Both datasets share same column structure except for price column",
          "Need to verify if categorical values (mark, model, fuel, city, province) in test.csv exist in training data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Need to verify same categorical value domains for mark, model, fuel, city, province across both files"
          ],
          [
            "Both datasets share same column structure except for price column",
            "Need to verify if categorical values (mark, model, fuel, city, province) in test.csv exist in training data"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 11.0,
        "confidence": 1.0,
        "votes": [
          11.0,
          11.0,
          11.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "price must be positive integer",
          "year must be reasonable (e.g., 1900-2024)",
          "mileage must be non-negative",
          "vol_engine must be positive (except possibly 0 for electric vehicles)",
          "price must be positive integer values",
          "year must be between 1900 and current year",
          "vol_engine must be positive",
          "Output must contain exactly 17690 predictions matching test.csv rows",
          "The 'price' column in 'Car_Prices_Poland_Kaggle.csv' should only contain positive values.",
          "The 'year' column should be within a reasonable range (e.g., 1900 to current year).",
          "The 'mileage' column should be non-negative.",
          "The 'vol_engine' column should be non-negative."
        ],
        "confidence": 0.3611111111111111,
        "votes": [
          [
            "price must be positive integer",
            "year must be reasonable (e.g., 1900-2024)",
            "mileage must be non-negative",
            "vol_engine must be positive (except possibly 0 for electric vehicles)"
          ],
          [
            "price must be positive integer values",
            "year must be between 1900 and current year",
            "mileage must be non-negative",
            "vol_engine must be positive",
            "Output must contain exactly 17690 predictions matching test.csv rows"
          ],
          [
            "The 'price' column in 'Car_Prices_Poland_Kaggle.csv' should only contain positive values.",
            "The 'year' column should be within a reasonable range (e.g., 1900 to current year).",
            "The 'mileage' column should be non-negative.",
            "The 'vol_engine' column should be non-negative."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove records with price <= 0",
          "Consider filtering unrealistic mileage (e.g., >500,000 km)",
          "Check for duplicate Unnamed: 0 values",
          "Filter out potential data entry errors where vol_engine = 0",
          "Consider treating mileage = 1 as special case (new vehicles)",
          "May need to handle unseen categorical values in test set"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove records with price <= 0",
            "Consider filtering unrealistic mileage (e.g., >500,000 km)",
            "Check for duplicate Unnamed: 0 values"
          ],
          [
            "Filter out potential data entry errors where vol_engine = 0",
            "Consider treating mileage = 1 as special case (new vehicles)",
            "May need to handle unseen categorical values in test set"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for multicollinearity among features",
          "Test normality of price distribution",
          "Check for outliers in price, mileage, year",
          "Check distribution of price in training data for outliers",
          "Verify linearity assumptions between continuous predictors and price",
          "Test for multicollinearity among numeric features",
          "Validate train-test feature distributions are similar"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for multicollinearity among features",
            "Test normality of price distribution",
            "Check for outliers in price, mileage, year"
          ],
          [
            "Check distribution of price in training data for outliers",
            "Verify linearity assumptions between continuous predictors and price",
            "Test for multicollinearity among numeric features",
            "Validate train-test feature distributions are similar"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output file must be named price.csv",
          "Single column named 'price'",
          "Must match row order of test.csv",
          "Price values should be integers representing PLN",
          "Must contain single column named 'price'",
          "Must have 17690 rows corresponding to test.csv",
          "Price values should be numeric (integer or float)",
          "File format should be CSV with header",
          "The output file 'price.csv' should contain a single column named 'price'.",
          "The 'price' column in 'price.csv' should contain the predicted prices for each car in 'test.csv' in the same order."
        ],
        "confidence": 0.3666666666666667,
        "votes": [
          [
            "Output file must be named price.csv",
            "Single column named 'price'",
            "Must match row order of test.csv",
            "Price values should be integers representing PLN"
          ],
          [
            "Output file must be named price.csv",
            "Must contain single column named 'price'",
            "Must have 17690 rows corresponding to test.csv",
            "Price values should be numeric (integer or float)",
            "File format should be CSV with header"
          ],
          [
            "The output file 'price.csv' should contain a single column named 'price'.",
            "The 'price' column in 'price.csv' should contain the predicted prices for each car in 'test.csv' in the same order."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5905555555555556
  },
  "ml-regression-015": {
    "m_q": {
      "target_metric": {
        "value": "Appliances energy consumption prediction values",
        "confidence": 0.3333333333333333,
        "votes": [
          "Appliances energy consumption prediction values",
          "Predict appliance energy consumption for test dataset",
          "Appliance energy consumption"
        ]
      },
      "filters": {
        "value": [
          "Based on test.csv data only (no training data filtering needed)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Based on test.csv data only (no training data filtering needed)"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "table",
        "confidence": 0.6666666666666666,
        "votes": [
          "table",
          "list",
          "table"
        ]
      },
      "sub_questions": {
        "value": [
          "What features are most predictive of appliance energy consumption?",
          "How should the model be trained on KAG_energydata_complete.csv?",
          "What preprocessing is needed for test.csv?",
          "What model type is appropriate for this regression task?",
          "What features from training data best predict Appliances energy consumption?",
          "How to handle temporal patterns in the date column?",
          "What model should be used for regression prediction?",
          "How to evaluate prediction quality on training data before applying to test?",
          "How to map predictions to test.csv rows maintaining order?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What features are most predictive of appliance energy consumption?",
            "How should the model be trained on KAG_energydata_complete.csv?",
            "What preprocessing is needed for test.csv?",
            "What model type is appropriate for this regression task?"
          ],
          [
            "What features from training data best predict Appliances energy consumption?",
            "How to handle temporal patterns in the date column?",
            "What model should be used for regression prediction?",
            "How to evaluate prediction quality on training data before applying to test?",
            "How to map predictions to test.csv rows maintaining order?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "KAG_energydata_complete.csv",
          "test.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "KAG_energydata_complete.csv",
            "test.csv"
          ],
          [
            "KAG_energydata_complete.csv",
            "test.csv"
          ],
          [
            "KAG_energydata_complete.csv",
            "test.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "test.csv missing 'Appliances' column (target variable)",
          "KAG_energydata_complete.csv has 29 columns, test.csv has 28 columns",
          "Both files have 'date' column but join not required - separate train/test split"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "test.csv missing 'Appliances' column (target variable)",
            "KAG_energydata_complete.csv has 29 columns, test.csv has 28 columns"
          ],
          [
            "test.csv missing 'Appliances' column (target variable)",
            "Both files have 'date' column but join not required - separate train/test split"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "t1-t9, t_out, tdewpoint": "degrees Celsius",
          "rh_1-rh_9, rh_out": "percentage",
          "press_mm_hg": "millimeters of mercury",
          "windspeed": "meters per second",
          "visibility": "kilometers",
          "lights": "watt-hours",
          "appliances": "watt-hours",
          "rv1, rv2": "random variables (unitless)",
          "t1": "degrees Celsius",
          "t2": "degrees Celsius",
          "t3": "degrees Celsius",
          "t4": "degrees Celsius",
          "t5": "degrees Celsius",
          "t6": "degrees Celsius",
          "t7": "degrees Celsius",
          "t8": "degrees Celsius",
          "t9": "degrees Celsius",
          "t_out": "degrees Celsius",
          "tdewpoint": "degrees Celsius",
          "rh_1": "percentage",
          "rh_2": "percentage",
          "rh_3": "percentage",
          "rh_4": "percentage",
          "rh_5": "percentage",
          "rh_6": "percentage",
          "rh_7": "percentage",
          "rh_8": "percentage",
          "rh_9": "percentage",
          "rh_out": "percentage",
          "rv1": "random variable (dimensionless)",
          "rv2": "random variable (dimensionless)"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "T1-T9, T_out, Tdewpoint": "degrees Celsius",
            "RH_1-RH_9, RH_out": "percentage",
            "Press_mm_hg": "millimeters of mercury",
            "Windspeed": "meters per second",
            "Visibility": "kilometers",
            "lights": "watt-hours",
            "Appliances": "watt-hours",
            "rv1, rv2": "random variables (unitless)"
          },
          {
            "Appliances": "Wh (Watt-hours)",
            "lights": "Wh (Watt-hours)",
            "T1": "degrees Celsius",
            "T2": "degrees Celsius",
            "T3": "degrees Celsius",
            "T4": "degrees Celsius",
            "T5": "degrees Celsius",
            "T6": "degrees Celsius",
            "T7": "degrees Celsius",
            "T8": "degrees Celsius",
            "T9": "degrees Celsius",
            "T_out": "degrees Celsius",
            "Tdewpoint": "degrees Celsius",
            "RH_1": "percentage",
            "RH_2": "percentage",
            "RH_3": "percentage",
            "RH_4": "percentage",
            "RH_5": "percentage",
            "RH_6": "percentage",
            "RH_7": "percentage",
            "RH_8": "percentage",
            "RH_9": "percentage",
            "RH_out": "percentage",
            "Press_mm_hg": "mmHg (millimeters of mercury)",
            "Windspeed": "m/s (meters per second)",
            "Visibility": "km (kilometers)",
            "rv1": "random variable (dimensionless)",
            "rv2": "random variable (dimensionless)"
          },
          {
            "Appliances": "Wh",
            "lights": "W",
            "T1": "\u00b0C",
            "RH_1": "%",
            "T2": "\u00b0C",
            "RH_2": "%",
            "T3": "\u00b0C",
            "RH_3": "%",
            "T4": "\u00b0C",
            "RH_4": "%",
            "T5": "\u00b0C",
            "RH_5": "%",
            "T6": "\u00b0C",
            "RH_6": "%",
            "T7": "\u00b0C",
            "RH_7": "%",
            "T8": "\u00b0C",
            "RH_8": "%",
            "T9": "\u00b0C",
            "RH_9": "%",
            "T_out": "\u00b0C",
            "Press_mm_hg": "mmHg",
            "RH_out": "%",
            "Windspeed": "m/s",
            "Visibility": "km",
            "Tdewpoint": "\u00b0C"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Temperature values have different scales (indoor vs outdoor)",
          "RH_6 shows extreme values (1.0 to 99.9) suggesting sensor variability",
          "lights column has discrete values (0, 10, 20)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Temperature values have different scales (indoor vs outdoor)",
            "RH_6 shows extreme values (1.0 to 99.9) suggesting sensor variability",
            "lights column has discrete values (0, 10, 20)"
          ],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "date format consistency needs verification",
          "test.csv covers different time periods than training data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "date format consistency needs verification",
            "test.csv covers different time periods than training data"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null"
        ],
        "confidence": 0.5833333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 29.0,
        "confidence": 1.0,
        "votes": [
          29.0,
          29.0,
          29.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Appliances values must be non-negative integers",
          "RH values should be between 0-100%",
          "Temperature values should be within realistic ranges (-50 to 50\u00b0C)",
          "date values should be chronological",
          "Predictions must be non-negative (energy consumption cannot be negative)",
          "Output must contain exactly 2961 predictions matching test.csv row count",
          "Column names in both files match except for missing 'Appliances' in test.csv",
          "Date ranges may differ between train and test sets",
          "The 'date' column needs to be parsed as datetime objects.",
          "The 'Appliances' column is only present in KAG_energydata_complete.csv and needs to be predicted for test.csv.",
          "The 'rv1' and 'rv2' columns are identical in KAG_energydata_complete.csv.",
          "The test.csv file is missing the target variable 'Appliances'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Appliances values must be non-negative integers",
            "RH values should be between 0-100%",
            "Temperature values should be within realistic ranges (-50 to 50\u00b0C)",
            "date values should be chronological"
          ],
          [
            "Predictions must be non-negative (energy consumption cannot be negative)",
            "Output must contain exactly 2961 predictions matching test.csv row count",
            "Column names in both files match except for missing 'Appliances' in test.csv",
            "Date ranges may differ between train and test sets"
          ],
          [
            "The 'date' column needs to be parsed as datetime objects.",
            "The 'Appliances' column is only present in KAG_energydata_complete.csv and needs to be predicted for test.csv.",
            "The 'rv1' and 'rv2' columns are identical in KAG_energydata_complete.csv.",
            "The test.csv file is missing the target variable 'Appliances'."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Remove rows with missing critical features",
          "Check for duplicate timestamps",
          "Validate sensor readings against physical limits"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Remove rows with missing critical features",
            "Check for duplicate timestamps",
            "Validate sensor readings against physical limits"
          ],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for multicollinearity among temperature/RH features",
          "Test stationarity of time series data",
          "Validate distribution of target variable",
          "Assess feature importance for model selection",
          "Check for multicollinearity among temperature and humidity features",
          "Validate prediction distribution matches training data distribution",
          "Check for temporal autocorrelation in residuals"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for multicollinearity among temperature/RH features",
            "Test stationarity of time series data",
            "Validate distribution of target variable",
            "Assess feature importance for model selection"
          ],
          [
            "Check for multicollinearity among temperature and humidity features",
            "Validate prediction distribution matches training data distribution",
            "Check for temporal autocorrelation in residuals"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Output file must be named 'appliance.csv'",
          "Single column named 'Appliances'",
          "2961 rows matching test.csv",
          "Integer predictions",
          "Save predictions to appliance.csv",
          "Output must have column named 'Appliances'",
          "Output should have 2961 rows corresponding to test.csv",
          "Predictions should be numeric values",
          "The output file should be named 'appliance.csv'.",
          "The output file should contain a single column named 'Appliances'.",
          "The 'Appliances' column should contain the predicted appliance energy consumption values for each row in test.csv."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "Output file must be named 'appliance.csv'",
            "Single column named 'Appliances'",
            "2961 rows matching test.csv",
            "Integer predictions"
          ],
          [
            "Save predictions to appliance.csv",
            "Output must have column named 'Appliances'",
            "Output should have 2961 rows corresponding to test.csv",
            "Predictions should be numeric values"
          ],
          [
            "The output file should be named 'appliance.csv'.",
            "The output file should contain a single column named 'Appliances'.",
            "The 'Appliances' column should contain the predicted appliance energy consumption values for each row in test.csv."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5680555555555556
  },
  "plot-bar-004": {
    "m_q": {
      "target_metric": {
        "value": "Enhance analysis.py, correct errors, generate plots and save as 'result.png' following 'plot.yaml' format",
        "confidence": 0.3333333333333333,
        "votes": [
          "Enhance analysis.py, correct errors, generate plots and save as 'result.png' following 'plot.yaml' format",
          "Enhance analysis.py by correcting errors and creating visualizations saved as result.png according to plot.yaml specifications",
          "Survival rate and other relationships between features, visualized and saved as 'result.png' according to 'plot.yaml'."
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_cardinality": {
        "value": "plot",
        "confidence": 1.0,
        "votes": [
          "plot",
          "plot",
          "plot"
        ]
      },
      "sub_questions": {
        "value": [
          "Correct potential errors in analysis.py",
          "Generate visualization plots",
          "Save plots in 'result.png'",
          "Follow formatting from 'plot.yaml'",
          "What are the current errors or issues in analysis.py?",
          "What visualization specifications are defined in plot.yaml?",
          "What data transformations are needed for the visualizations?",
          "How should multiple plots be arranged in result.png?",
          "What are the appropriate chart types for Titanic survival analysis?",
          "What is the distribution of each feature?",
          "What is the relationship between each feature and survival?",
          "Are there any missing values in the data?",
          "How are the plots defined in 'plot.yaml'?",
          "How to correctly implement the plots defined in 'plot.yaml' using the data?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Correct potential errors in analysis.py",
            "Generate visualization plots",
            "Save plots in 'result.png'",
            "Follow formatting from 'plot.yaml'"
          ],
          [
            "What are the current errors or issues in analysis.py?",
            "What visualization specifications are defined in plot.yaml?",
            "What data transformations are needed for the visualizations?",
            "How should multiple plots be arranged in result.png?",
            "What are the appropriate chart types for Titanic survival analysis?"
          ],
          [
            "What is the distribution of each feature?",
            "What is the relationship between each feature and survival?",
            "Are there any missing values in the data?",
            "How are the plots defined in 'plot.yaml'?",
            "How to correctly implement the plots defined in 'plot.yaml' using the data?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "train.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "train.csv"
          ],
          [
            "train.csv"
          ],
          [
            "train.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "age": "years",
          "fare": "currency units",
          "sibsp": "count of siblings/spouses",
          "parch": "count of parents/children",
          "pclass": "ordinal_class",
          "survived": "binary_indicator"
        },
        "confidence": 0.6666666666666666,
        "votes": [
          {
            "Age": "years",
            "Fare": "currency units",
            "SibSp": "count of siblings/spouses",
            "Parch": "count of parents/children"
          },
          {
            "Age": "years",
            "Fare": "British pounds",
            "SibSp": "count",
            "Parch": "count",
            "Pclass": "ordinal_class",
            "Survived": "binary_indicator"
          },
          {
            "Age": "years",
            "Fare": "currency"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Age contains missing values (NaN)",
          "Cabin contains many missing values",
          "Embarked has some missing values",
          "Age has missing values that may need imputation",
          "Fare ranges from near-zero to over 500",
          "Cabin has significant missing data"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age contains missing values (NaN)",
            "Cabin contains many missing values",
            "Embarked has some missing values"
          ],
          [
            "Age has missing values that may need imputation",
            "Fare ranges from near-zero to over 500",
            "Cabin has significant missing data"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A",
          "nan"
        ],
        "confidence": 0.8333333333333334,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 12.0,
        "confidence": 1.0,
        "votes": [
          12.0,
          12.0,
          12.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "PassengerId should be unique",
          "Survived values should be 0 or 1",
          "Pclass values should be 1, 2, or 3",
          "Age should be positive and reasonable (0-100)",
          "Fare should be positive",
          "SibSp and Parch should be non-negative integers",
          "PassengerId must be unique and range from 1 to 891",
          "Survived must be binary (0 or 1)",
          "Pclass must be 1, 2, or 3",
          "Sex must be 'male' or 'female'",
          "Age must be non-negative when present",
          "SibSp and Parch must be non-negative integers",
          "Fare must be non-negative",
          "Embarked must be 'C', 'Q', or 'S' when present",
          "Age should be non-negative.",
          "SibSp and Parch should be non-negative.",
          "Survived should be binary (0 or 1)."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "PassengerId should be unique",
            "Survived values should be 0 or 1",
            "Pclass values should be 1, 2, or 3",
            "Age should be positive and reasonable (0-100)",
            "Fare should be positive",
            "SibSp and Parch should be non-negative integers"
          ],
          [
            "PassengerId must be unique and range from 1 to 891",
            "Survived must be binary (0 or 1)",
            "Pclass must be 1, 2, or 3",
            "Sex must be 'male' or 'female'",
            "Age must be non-negative when present",
            "SibSp and Parch must be non-negative integers",
            "Fare must be non-negative",
            "Embarked must be 'C', 'Q', or 'S' when present"
          ],
          [
            "Age should be non-negative.",
            "SibSp and Parch should be non-negative.",
            "Survived should be binary (0 or 1)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out rows with missing Age if needed for analysis",
          "Consider imputing missing Embarked values",
          "Handle missing Cabin values appropriately",
          "Filter out rows where critical analysis columns have missing values if required",
          "Handle missing Age values through imputation or exclusion based on analysis needs"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter out rows with missing Age if needed for analysis",
            "Consider imputing missing Embarked values",
            "Handle missing Cabin values appropriately"
          ],
          [
            "Filter out rows where critical analysis columns have missing values if required",
            "Handle missing Age values through imputation or exclusion based on analysis needs"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for correlation between Survived and other variables",
          "Test for significant differences in survival rates by Sex and Pclass",
          "Check distribution of Age and Fare",
          "Chi-square test for categorical variables vs Survived",
          "T-test or Mann-Whitney U test for continuous variables vs Survived",
          "Correlation analysis between numerical features",
          "Chi-squared test for categorical features and survival.",
          "T-test or ANOVA for numerical features and survival."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for correlation between Survived and other variables",
            "Test for significant differences in survival rates by Sex and Pclass",
            "Check distribution of Age and Fare"
          ],
          [
            "Chi-square test for categorical variables vs Survived",
            "T-test or Mann-Whitney U test for continuous variables vs Survived",
            "Correlation analysis between numerical features"
          ],
          [
            "Chi-squared test for categorical features and survival.",
            "T-test or ANOVA for numerical features and survival."
          ]
        ]
      },
      "output_format_requirements": {
        "value": [
          "Save plots to 'result.png'",
          "Follow formatting specified in 'plot.yaml'",
          "Ensure plots are properly labeled and readable",
          "Save visualization as result.png",
          "Follow layout specifications from plot.yaml",
          "Ensure plots are properly labeled with titles, axis labels, and legends",
          "Use appropriate color schemes for categorical data",
          "Handle missing data appropriately in visualizations",
          "Ensure image resolution is suitable for analysis",
          "Use matplotlib or seaborn for visualization generation",
          "Plots should be saved as 'result.png'.",
          "Plots should follow the specifications in 'plot.yaml'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Save plots to 'result.png'",
            "Follow formatting specified in 'plot.yaml'",
            "Ensure plots are properly labeled and readable"
          ],
          [
            "Save visualization as result.png",
            "Follow layout specifications from plot.yaml",
            "Ensure plots are properly labeled with titles, axis labels, and legends",
            "Use appropriate color schemes for categorical data",
            "Handle missing data appropriately in visualizations",
            "Ensure image resolution is suitable for analysis",
            "Use matplotlib or seaborn for visualization generation"
          ],
          [
            "Plots should be saved as 'result.png'.",
            "Plots should follow the specifications in 'plot.yaml'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5416666666666667
  },
  "plot-bar-005": {
    "m_q": {
      "target_metric": {
        "value": "Count of people in each age group",
        "confidence": 0.6666666666666666,
        "votes": [
          "Count of people in each age group",
          "Count of people in each age group who filled out the survey",
          "Count of people in each age group"
        ]
      },
      "filters": {
        "value": [
          "Survey respondents only (no header rows)",
          "Valid age responses (non-null, numeric age values)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Survey respondents only (no header rows)",
            "Valid age responses (non-null, numeric age values)"
          ],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Age Group (derived from Q2 using AgeGroup.md method)",
          "age_group",
          "Age Group"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age Group (derived from Q2 using AgeGroup.md method)"
          ],
          [
            "age_group"
          ],
          [
            "Age Group"
          ]
        ]
      },
      "output_cardinality": {
        "value": "plot",
        "confidence": 1.0,
        "votes": [
          "plot",
          "plot",
          "plot"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the age group boundaries defined in AgeGroup.md?",
          "How should age values be converted from Q2 responses?",
          "Should missing/unknown age values be included or excluded?",
          "What column contains age data?",
          "What are the age group definitions from AgeGroup.md?",
          "How to categorize respondents into age groups?",
          "How to count respondents per age group?",
          "How to create a bar chart with specified formatting?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What are the age group boundaries defined in AgeGroup.md?",
            "How should age values be converted from Q2 responses?",
            "Should missing/unknown age values be included or excluded?"
          ],
          [
            "What column contains age data?",
            "What are the age group definitions from AgeGroup.md?",
            "How to categorize respondents into age groups?",
            "How to count respondents per age group?",
            "How to create a bar chart with specified formatting?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "multipleChoiceResponses.csv",
          "SurveySchema.csv",
          "freeFormResponses.csv",
          "AgeGroup.md"
        ],
        "confidence": 0.5833333333333333,
        "votes": [
          [
            "multipleChoiceResponses.csv",
            "SurveySchema.csv",
            "freeFormResponses.csv"
          ],
          [
            "multipleChoiceResponses.csv",
            "SurveySchema.csv",
            "AgeGroup.md"
          ],
          [
            "multipleChoiceResponses.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "SurveySchema.csv has different structure (12 rows, 52 columns) vs multipleChoiceResponses.csv (23860 rows, 395 columns)",
          "freeFormResponses.csv has 35 columns that appear to be text responses corresponding to multipleChoiceResponses.csv columns",
          "SurveySchema.csv appears to be metadata/schema description file, not actual survey responses",
          "Q2 column in SurveySchema.csv is labeled as age question, but actual data is in multipleChoiceResponses.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "SurveySchema.csv has different structure (12 rows, 52 columns) vs multipleChoiceResponses.csv (23860 rows, 395 columns)",
            "freeFormResponses.csv has 35 columns that appear to be text responses corresponding to multipleChoiceResponses.csv columns"
          ],
          [
            "SurveySchema.csv appears to be metadata/schema description file, not actual survey responses",
            "Q2 column in SurveySchema.csv is labeled as age question, but actual data is in multipleChoiceResponses.csv"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "q2": "years",
          "time from start to finish (seconds)": "seconds",
          "age": "years"
        },
        "confidence": 0.4444444444444444,
        "votes": [
          {
            "Q2": "years",
            "Time from Start to Finish (seconds)": "seconds"
          },
          {
            "Q2": "years"
          },
          {
            "age": "years"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Age values in Q2 may be categorical ranges (e.g., '18-21', '22-24') rather than numeric",
          "Need to check actual Q2 values in multipleChoiceResponses.csv to determine format",
          "Age data in Q2 may be categorical ranges rather than numeric values",
          "Need to verify format of age values (e.g., '18-21', '22-24', or numeric)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age values in Q2 may be categorical ranges (e.g., '18-21', '22-24') rather than numeric",
            "Need to check actual Q2 values in multipleChoiceResponses.csv to determine format"
          ],
          [
            "Age data in Q2 may be categorical ranges rather than numeric values",
            "Need to verify format of age values (e.g., '18-21', '22-24', or numeric)"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "SurveySchema.csv appears to contain question text, not response data",
          "freeFormResponses.csv contains text responses that may not be needed for age analysis",
          "SurveySchema.csv has 12 rows while multipleChoiceResponses.csv has 23860 rows - different purposes"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "SurveySchema.csv appears to contain question text, not response data",
            "freeFormResponses.csv contains text responses that may not be needed for age analysis"
          ],
          [
            "SurveySchema.csv has 12 rows while multipleChoiceResponses.csv has 23860 rows - different purposes"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "nan"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            "",
            "nan"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 395.0,
        "confidence": 1.0,
        "votes": [
          395.0,
          395.0,
          395.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Age values must be interpretable according to AgeGroup.md method",
          "Counts must be non-negative integers",
          "All age groups defined in AgeGroup.md must be represented in output",
          "Must follow age group division method from AgeGroup.md",
          "Exclude header/metadata rows if present in data",
          "Handle missing or invalid age values appropriately",
          "Age must be a valid number",
          "Age must be positive"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Age values must be interpretable according to AgeGroup.md method",
            "Counts must be non-negative integers",
            "All age groups defined in AgeGroup.md must be represented in output"
          ],
          [
            "Must follow age group division method from AgeGroup.md",
            "Exclude header/metadata rows if present in data",
            "Handle missing or invalid age values appropriately"
          ],
          [
            "Age must be a valid number",
            "Age must be positive"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude header row from SurveySchema.csv if present in data",
          "Filter to valid age responses based on Q2 format",
          "Filter out rows where Q2 (age) is null or invalid",
          "Apply age group categorization based on AgeGroup.md specifications"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude header row from SurveySchema.csv if present in data",
            "Filter to valid age responses based on Q2 format"
          ],
          [
            "Filter out rows where Q2 (age) is null or invalid",
            "Apply age group categorization based on AgeGroup.md specifications"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for missing age values in Q2 column",
          "Validate age distribution against expected survey demographics"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for missing age values in Q2 column",
            "Validate age distribution against expected survey demographics"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Bar chart saved as 'result.png'",
          "Title: 'Age Group Distribution'",
          "X-axis label: 'Age Group'",
          "Y-axis label: 'Count'",
          "Save output as 'result.png'",
          "Chart title: 'Age Group Distribution'",
          "Chart type: bar chart"
        ],
        "confidence": 0.619047619047619,
        "votes": [
          [
            "Bar chart saved as 'result.png'",
            "Title: 'Age Group Distribution'",
            "X-axis label: 'Age Group'",
            "Y-axis label: 'Count'"
          ],
          [
            "Save output as 'result.png'",
            "Chart title: 'Age Group Distribution'",
            "X-axis label: 'Age Group'",
            "Y-axis label: 'Count'",
            "Chart type: bar chart"
          ],
          [
            "Bar chart saved as 'result.png'",
            "Chart title: 'Age Group Distribution'",
            "X-axis label: 'Age Group'",
            "Y-axis label: 'Count'"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6023412698412699
  },
  "plot-bar-006": {
    "m_q": {
      "target_metric": {
        "value": "Team performance metrics (likely wins/losses/draws or points) over specified period",
        "confidence": 0.3333333333333333,
        "votes": [
          "Team performance metrics (likely wins/losses/draws or points) over specified period",
          "Team performance metrics (likely goals scored, wins, or match counts) over a specified time period",
          "Number of goals scored by each team"
        ]
      },
      "filters": {
        "value": [
          "date range (unspecified in question)",
          "team selection (unspecified in question)",
          "date range (specified period - not explicitly defined in question)",
          "potentially filter by team"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "date range (unspecified in question)",
            "team selection (unspecified in question)"
          ],
          [
            "date range (specified period - not explicitly defined in question)",
            "potentially filter by team"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "team",
          "period (likely date aggregation)"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "team",
            "period (likely date aggregation)"
          ],
          [
            "team"
          ],
          [
            "team"
          ]
        ]
      },
      "output_cardinality": {
        "value": "plot",
        "confidence": 1.0,
        "votes": [
          "plot",
          "plot",
          "plot"
        ]
      },
      "sub_questions": {
        "value": [
          "What constitutes 'team performance'? (wins, goals, points?)",
          "What is the specified period?",
          "Which teams should be included?",
          "What aggregation level for time? (year, month, tournament?)",
          "What constitutes 'team performance' - goals scored, wins, matches played, or win rate?",
          "What is the 'specified period' for filtering dates?",
          "Which teams should be included in the analysis?",
          "What settings are specified in 'plot.yaml' for the chart configuration?",
          "Should performance be aggregated from results.csv, goalscorers.csv, or both?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "What constitutes 'team performance'? (wins, goals, points?)",
            "What is the specified period?",
            "Which teams should be included?",
            "What aggregation level for time? (year, month, tournament?)"
          ],
          [
            "What constitutes 'team performance' - goals scored, wins, matches played, or win rate?",
            "What is the 'specified period' for filtering dates?",
            "Which teams should be included in the analysis?",
            "What settings are specified in 'plot.yaml' for the chart configuration?",
            "Should performance be aggregated from results.csv, goalscorers.csv, or both?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "goalscorers.csv",
          "results.csv",
          "shootouts.csv",
          "plot.yaml"
        ],
        "confidence": 0.5833333333333333,
        "votes": [
          [
            "goalscorers.csv",
            "results.csv",
            "shootouts.csv"
          ],
          [
            "results.csv",
            "goalscorers.csv",
            "plot.yaml"
          ],
          [
            "goalscorers.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "date column is object type (should be datetime)",
          "team name consistency across files needs verification",
          "goalscorers.csv has 'team' column while results.csv has 'home_team'/'away_team'",
          "Team names appear in multiple columns in results.csv (home_team, away_team) but single column in goalscorers.csv (team)",
          "Need to unpivot or union team data from home_team and away_team to get all team performances"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "date column is object type (should be datetime)",
            "team name consistency across files needs verification",
            "goalscorers.csv has 'team' column while results.csv has 'home_team'/'away_team'"
          ],
          [
            "Team names appear in multiple columns in results.csv (home_team, away_team) but single column in goalscorers.csv (team)",
            "Need to unpivot or union team data from home_team and away_team to get all team performances"
          ],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "date": "YYYY-MM-DD",
          "minute": "minutes in match",
          "home_score": "goals",
          "away_score": "goals"
        },
        "confidence": 0.7499999999999999,
        "votes": [
          {
            "date": "YYYY-MM-DD",
            "minute": "minutes in match",
            "home_score": "goals",
            "away_score": "goals"
          },
          {
            "date": "ISO date format (YYYY-MM-DD)",
            "home_score": "count (goals)",
            "away_score": "count (goals)",
            "minute": "minutes into match (0-120+)"
          },
          {
            "minute": "minute of the match"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Data spans 1872 to present - very long time period",
          "Tournament types vary widely",
          "Team names may change over time",
          "Date column is object type, needs conversion to datetime for filtering",
          "Goals scored needs to be aggregated at team level from both home and away perspectives"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Data spans 1872 to present - very long time period",
            "Tournament types vary widely",
            "Team names may change over time"
          ],
          [
            "Date column is object type, needs conversion to datetime for filtering",
            "Goals scored needs to be aggregated at team level from both home and away perspectives"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "goalscorers.csv only has 44110 rows while results.csv has 46442 rows - missing goal data for some matches",
          "shootouts.csv only has 605 rows - rare event",
          "Team performance could be calculated from results.csv (wins/draws/losses) or goalscorers.csv (total goals)",
          "Date ranges may differ between files"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "goalscorers.csv only has 44110 rows while results.csv has 46442 rows - missing goal data for some matches",
            "shootouts.csv only has 605 rows - rare event"
          ],
          [
            "Team performance could be calculated from results.csv (wins/draws/losses) or goalscorers.csv (total goals)",
            "Date ranges may differ between files"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "False",
          "True",
          "NaN"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "False",
            "True"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 8.0,
        "confidence": 0.8888888888888888,
        "votes": [
          8.0,
          9.0,
          8.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "home_score >= 0",
          "away_score >= 0",
          "minute between 0-120 (plus extra time)",
          "date must be valid",
          "team names not null",
          "Output must be saved as 'team.png'",
          "Chart type must be bar chart",
          "Must adhere to settings specified in 'plot.yaml'",
          "Must filter data by specified period (date range)",
          "Each bar represents a team's performance"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "home_score >= 0",
            "away_score >= 0",
            "minute between 0-120 (plus extra time)",
            "date must be valid",
            "team names not null"
          ],
          [
            "Output must be saved as 'team.png'",
            "Chart type must be bar chart",
            "Must adhere to settings specified in 'plot.yaml'",
            "Must filter data by specified period (date range)",
            "Each bar represents a team's performance"
          ],
          []
        ]
      },
      "derived_filters": {
        "value": [
          "Filter by date range (unspecified)",
          "Filter by tournament type (if relevant)",
          "Exclude own goals if calculating team performance",
          "date >= start_date AND date <= end_date (from specified period)",
          "Potentially filter out own_goal = True when counting performance"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter by date range (unspecified)",
            "Filter by tournament type (if relevant)",
            "Exclude own goals if calculating team performance"
          ],
          [
            "date >= start_date AND date <= end_date (from specified period)",
            "Potentially filter out own_goal = True when counting performance"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for duplicate matches",
          "Validate home_score + away_score matches total goals in goalscorers",
          "Check temporal consistency of team performance"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for duplicate matches",
            "Validate home_score + away_score matches total goals in goalscorers",
            "Check temporal consistency of team performance"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Save as 'team.png'",
          "Follow settings in 'plot.yaml'",
          "Bar chart visualization",
          "Clear labeling of teams and performance metric",
          "Output format: PNG image file",
          "Filename: 'team.png'",
          "Chart type: bar chart",
          "X-axis: team names",
          "Y-axis: performance metric (goals/wins/matches)",
          "Must follow plot.yaml configuration for styling, colors, dimensions, labels",
          "Save the resulting chart as 'team.png'",
          "Adhere to the settings specified in 'plot.yaml'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Save as 'team.png'",
            "Follow settings in 'plot.yaml'",
            "Bar chart visualization",
            "Clear labeling of teams and performance metric"
          ],
          [
            "Output format: PNG image file",
            "Filename: 'team.png'",
            "Chart type: bar chart",
            "X-axis: team names",
            "Y-axis: performance metric (goals/wins/matches)",
            "Must follow plot.yaml configuration for styling, colors, dimensions, labels"
          ],
          [
            "Save the resulting chart as 'team.png'",
            "Adhere to the settings specified in 'plot.yaml'"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.5944444444444446
  },
  "plot-bar-015": {
    "m_q": {
      "target_metric": {
        "value": "Total sales amount for each product size, binned into $10,000 increments, sorted by size ('free size' smallest).",
        "confidence": 0.3333333333333333,
        "votes": [
          "",
          "",
          "Total sales amount for each product size, binned into $10,000 increments, sorted by size ('free size' smallest)."
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "Size"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "Size"
          ]
        ]
      },
      "output_cardinality": {
        "value": "plot",
        "confidence": 0.3333333333333333,
        "votes": [
          "",
          "",
          "plot"
        ]
      },
      "sub_questions": {
        "value": [
          "What is the total sales amount for each size?",
          "How to handle missing or invalid size values?",
          "How to bin the sales amount into $10,000 increments?",
          "How to sort the sizes with 'free size' as the smallest?",
          "Which file contains the sales data and size information?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "What is the total sales amount for each size?",
            "How to handle missing or invalid size values?",
            "How to bin the sales amount into $10,000 increments?",
            "How to sort the sizes with 'free size' as the smallest?",
            "Which file contains the sales data and size information?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "Amazon Sale Report.csv",
          "International sale Report.csv"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "Amazon Sale Report.csv",
            "International sale Report.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "The 'Size' column has different data types and formats across the files.",
          "The 'Amount' column in 'Amazon Sale Report.csv' and 'GROSS AMT' in 'International sale Report.csv' represent sales amount but might have different currencies or units."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "The 'Size' column has different data types and formats across the files.",
            "The 'Amount' column in 'Amazon Sale Report.csv' and 'GROSS AMT' in 'International sale Report.csv' represent sales amount but might have different currencies or units."
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "amount": "INR/Other Currencies",
          "qty": "Units"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {},
          {
            "Amount": "INR/Other Currencies",
            "Qty": "Units"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "The 'Amount' column might be in different currencies. Currency conversion might be needed."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "The 'Amount' column might be in different currencies. Currency conversion might be needed."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Different date formats in 'Amazon Sale Report.csv' (Date) and 'International sale Report.csv' (DATE)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "Different date formats in 'Amazon Sale Report.csv' (Date) and 'International sale Report.csv' (DATE)."
          ]
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 24.0,
        "confidence": 0.33,
        "votes": [
          0.0,
          0.0,
          24.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "The 'Size' column should be cleaned and standardized to ensure consistent values.",
          "The 'Amount' column should be converted to a common currency (e.g., USD or INR) before aggregation.",
          "Handle potential errors or missing values in the 'Size' and 'Amount' columns.",
          "Define the binning strategy for sales amounts (e.g., lower bound inclusive, upper bound exclusive)."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "The 'Size' column should be cleaned and standardized to ensure consistent values.",
            "The 'Amount' column should be converted to a common currency (e.g., USD or INR) before aggregation.",
            "Handle potential errors or missing values in the 'Size' and 'Amount' columns.",
            "Define the binning strategy for sales amounts (e.g., lower bound inclusive, upper bound exclusive)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter out cancelled or returned orders based on the 'Status' column in 'Amazon Sale Report.csv'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "Filter out cancelled or returned orders based on the 'Status' column in 'Amazon Sale Report.csv'."
          ]
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "The bar chart should have clear labels for the axes and title.",
          "The x-axis should represent the product sizes in ascending order, with 'free size' as the smallest.",
          "The y-axis should represent the total sales amount in $10,000 increments.",
          "The chart should be saved as 'result.png'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "The bar chart should have clear labels for the axes and title.",
            "The x-axis should represent the product sizes in ascending order, with 'free size' as the smallest.",
            "The y-axis should represent the total sales amount in $10,000 increments.",
            "The chart should be saved as 'result.png'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.4331666666666667
  },
  "plot-line-006": {
    "m_q": {
      "target_metric": {
        "value": "global seasonal average temperature changes over time",
        "confidence": 1.0,
        "votes": [
          "global seasonal average temperature changes over time",
          "global seasonal average temperature changes over time",
          "Global seasonal average temperature changes over time"
        ]
      },
      "filters": {
        "value": [
          "global aggregation (not by country/city/state)",
          "seasonal grouping (quarterly or monthly to seasonal)",
          "global aggregation (across all countries/locations)",
          "seasonal grouping (by season derived from month)",
          "time period as specified in tips.txt"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "global aggregation (not by country/city/state)",
            "seasonal grouping (quarterly or monthly to seasonal)"
          ],
          [
            "global aggregation (across all countries/locations)",
            "seasonal grouping (by season derived from month)",
            "time period as specified in tips.txt"
          ],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "season",
          "year",
          "season (derived from dt month: Winter=Dec-Feb, Spring=Mar-May, Summer=Jun-Aug, Fall=Sep-Nov)",
          "time period (year or year range)"
        ],
        "confidence": 0.49999999999999994,
        "votes": [
          [
            "season",
            "year"
          ],
          [
            "season (derived from dt month: Winter=Dec-Feb, Spring=Mar-May, Summer=Jun-Aug, Fall=Sep-Nov)",
            "time period (year or year range)"
          ],
          [
            "year",
            "season"
          ]
        ]
      },
      "output_cardinality": {
        "value": "plot",
        "confidence": 1.0,
        "votes": [
          "plot",
          "plot",
          "plot"
        ]
      },
      "sub_questions": {
        "value": [
          "How to extract seasonal averages from monthly data?",
          "Which temperature metric to use (LandAverageTemperature vs LandAndOceanAverageTemperature)?",
          "How to handle missing values in historical data?",
          "What time range should be visualized?",
          "Which data file contains global temperature data?",
          "How to extract season from dt column?",
          "What is the baseline period for calculating temperature changes?",
          "What specific formatting requirements are in plot.yaml?",
          "What specific instructions are in tips.txt?",
          "How to aggregate data globally across all locations?",
          "How to calculate seasonal averages?",
          "How to calculate temperature changes relative to baseline?",
          "Calculate the average temperature for each month.",
          "Determine the season for each month.",
          "Calculate the average temperature for each season in each year.",
          "Plot the seasonal average temperatures over time."
        ],
        "confidence": 0.33333333333333326,
        "votes": [
          [
            "How to extract seasonal averages from monthly data?",
            "Which temperature metric to use (LandAverageTemperature vs LandAndOceanAverageTemperature)?",
            "How to handle missing values in historical data?",
            "What time range should be visualized?"
          ],
          [
            "Which data file contains global temperature data?",
            "How to extract season from dt column?",
            "What is the baseline period for calculating temperature changes?",
            "What specific formatting requirements are in plot.yaml?",
            "What specific instructions are in tips.txt?",
            "How to aggregate data globally across all locations?",
            "How to calculate seasonal averages?",
            "How to calculate temperature changes relative to baseline?"
          ],
          [
            "Calculate the average temperature for each month.",
            "Determine the season for each month.",
            "Calculate the average temperature for each season in each year.",
            "Plot the seasonal average temperatures over time."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "GlobalTemperatures.csv",
          "tips.txt",
          "plot.yaml"
        ],
        "confidence": 0.5555555555555555,
        "votes": [
          [
            "GlobalTemperatures.csv"
          ],
          [
            "GlobalTemperatures.csv",
            "tips.txt",
            "plot.yaml"
          ],
          [
            "GlobalTemperatures.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "GlobalLandTemperaturesByCountry.csv has 'AverageTemperature' while GlobalTemperatures.csv has 'LandAverageTemperature' - different naming conventions",
          "Different temporal coverage across files (1743 vs 1750 start dates)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "GlobalLandTemperaturesByCountry.csv has 'AverageTemperature' while GlobalTemperatures.csv has 'LandAverageTemperature' - different naming conventions",
            "Different temporal coverage across files (1743 vs 1750 start dates)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "landaveragetemperature": "degrees Celsius",
          "landandoceanaveragetemperature": "Celsius",
          "averagetemperature": "Celsius",
          "averagetemperatureuncertainty": "Celsius",
          "landaveragetemperatureuncertainty": "degrees Celsius",
          "landmaxtemperature": "degrees Celsius",
          "landmintemperature": "degrees Celsius",
          "dt": "date string YYYY-MM-DD format"
        },
        "confidence": 0.5416666666666666,
        "votes": [
          {
            "LandAverageTemperature": "Celsius",
            "LandAndOceanAverageTemperature": "Celsius",
            "AverageTemperature": "Celsius",
            "AverageTemperatureUncertainty": "Celsius"
          },
          {
            "LandAverageTemperature": "degrees Celsius",
            "LandAverageTemperatureUncertainty": "degrees Celsius",
            "LandMaxTemperature": "degrees Celsius",
            "LandMinTemperature": "degrees Celsius",
            "LandAndOceanAverageTemperature": "degrees Celsius",
            "AverageTemperature": "degrees Celsius",
            "dt": "date string YYYY-MM-DD format"
          },
          {
            "LandAverageTemperature": "degrees Celsius",
            "LandAverageTemperatureUncertainty": "degrees Celsius"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Latitude/Longitude columns in GlobalLandTemperaturesByMajorCity.csv are strings with N/S/E/W suffixes",
          "Temperature uncertainty columns have same units as temperature but represent measurement error",
          "Temperature values are in Celsius, may need conversion if plot.yaml specifies different unit",
          "Date ranges from 1750 onwards with missing values in early years"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Latitude/Longitude columns in GlobalLandTemperaturesByMajorCity.csv are strings with N/S/E/W suffixes",
            "Temperature uncertainty columns have same units as temperature but represent measurement error"
          ],
          [
            "Temperature values are in Celsius, may need conversion if plot.yaml specifies different unit",
            "Date ranges from 1750 onwards with missing values in early years"
          ],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "GlobalTemperatures.csv provides global aggregates while other files provide granular data - different aggregation levels",
          "GlobalTemperatures.csv provides global aggregated data vs country/city/state files provide regional data",
          "LandAverageTemperature vs LandAndOceanAverageTemperature - need to determine which to use for 'global' temperature"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "GlobalTemperatures.csv provides global aggregates while other files provide granular data - different aggregation levels"
          ],
          [
            "GlobalTemperatures.csv provides global aggregated data vs country/city/state files provide regional data",
            "LandAverageTemperature vs LandAndOceanAverageTemperature - need to determine which to use for 'global' temperature"
          ],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "",
          "NA",
          "N/A"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "",
            "NA",
            "N/A"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 9.0,
        "confidence": 1.0,
        "votes": [
          9.0,
          9.0,
          9.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "GlobalTemperatures.csv has 3192 rows representing monthly data from 1750-2015 (266 years)",
          "Missing values in early years (e.g., 1750-11-01 has nulls)",
          "Seasonal calculation requires grouping months (e.g., Dec-Feb, Mar-May, Jun-Aug, Sep-Nov)",
          "Output must be saved as result.png",
          "Plot must be a line graph",
          "Must show seasonal average temperature changes (not absolute temperatures)",
          "Must follow formatting specifications in plot.yaml",
          "Must follow instructions in tips.txt",
          "dt column must be parsed as datetime objects.",
          "Handle missing values in temperature columns appropriately (e.g., imputation or removal)."
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "GlobalTemperatures.csv has 3192 rows representing monthly data from 1750-2015 (266 years)",
            "Missing values in early years (e.g., 1750-11-01 has nulls)",
            "Seasonal calculation requires grouping months (e.g., Dec-Feb, Mar-May, Jun-Aug, Sep-Nov)"
          ],
          [
            "Output must be saved as result.png",
            "Plot must be a line graph",
            "Must show seasonal average temperature changes (not absolute temperatures)",
            "Must follow formatting specifications in plot.yaml",
            "Must follow instructions in tips.txt"
          ],
          [
            "dt column must be parsed as datetime objects.",
            "Handle missing values in temperature columns appropriately (e.g., imputation or removal)."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude rows with null LandAverageTemperature or LandAndOceanAverageTemperature",
          "Consider only complete seasonal data (all 3 months available)",
          "Extract month from dt column to determine season",
          "Filter out rows with missing temperature values",
          "Determine baseline period for calculating temperature changes from tips.txt",
          "Apply time range filtering as specified in tips.txt"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude rows with null LandAverageTemperature or LandAndOceanAverageTemperature",
            "Consider only complete seasonal data (all 3 months available)"
          ],
          [
            "Extract month from dt column to determine season",
            "Filter out rows with missing temperature values",
            "Determine baseline period for calculating temperature changes from tips.txt",
            "Apply time range filtering as specified in tips.txt"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for temporal autocorrelation in temperature series",
          "Test for seasonal patterns using decomposition",
          "Assess missing data patterns by year/season",
          "Calculate seasonal means by grouping months into seasons",
          "Calculate temperature anomalies/changes relative to baseline period",
          "Handle missing data appropriately in aggregations"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for temporal autocorrelation in temperature series",
            "Test for seasonal patterns using decomposition",
            "Assess missing data patterns by year/season"
          ],
          [
            "Calculate seasonal means by grouping months into seasons",
            "Calculate temperature anomalies/changes relative to baseline period",
            "Handle missing data appropriately in aggregations"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Line graph format specified in plot.yaml",
          "Output file must be saved as result.png",
          "Seasonal averages should be clearly labeled",
          "File format: PNG image (result.png)",
          "Chart type: line graph",
          "X-axis: time (year or seasonal period)",
          "Y-axis: temperature change (relative to baseline)",
          "Multiple lines for different seasons",
          "Formatting according to plot.yaml specifications",
          "Instructions from tips.txt must be followed",
          "The plot should be saved as result.png.",
          "The plot should be formatted according to plot.yaml."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Line graph format specified in plot.yaml",
            "Output file must be saved as result.png",
            "Seasonal averages should be clearly labeled"
          ],
          [
            "File format: PNG image (result.png)",
            "Chart type: line graph",
            "X-axis: time (year or seasonal period)",
            "Y-axis: temperature change (relative to baseline)",
            "Multiple lines for different seasons",
            "Formatting according to plot.yaml specifications",
            "Instructions from tips.txt must be followed"
          ],
          [
            "The plot should be saved as result.png.",
            "The plot should be formatted according to plot.yaml."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6298611111111112
  },
  "plot-pie-005": {
    "m_q": {
      "target_metric": {
        "value": "Count of games across four major genres in Mobile Strategy Game dataset",
        "confidence": 0.3333333333333333,
        "votes": [
          "Count of games across four major genres in Mobile Strategy Game dataset",
          "Count of games per genre across four major genres in Mobile Strategy Game dataset",
          "Number of games in each of the four major genres (Strategy, Puzzle, Board, and Action) and their proportions, visualized as a pie chart."
        ]
      },
      "filters": {
        "value": [
          "Primary Genre must be 'Games'",
          "Genres must contain 'Strategy'",
          "Focus on four major genres within Strategy games",
          "Mobile Strategy Game dataset",
          "Four major genres only",
          "Primary Genre is one of 'Strategy', 'Puzzle', 'Board', or 'Action'"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Primary Genre must be 'Games'",
            "Genres must contain 'Strategy'",
            "Focus on four major genres within Strategy games"
          ],
          [
            "Mobile Strategy Game dataset",
            "Four major genres only"
          ],
          [
            "Primary Genre is one of 'Strategy', 'Puzzle', 'Board', or 'Action'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "Primary Genre",
          "Genres"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "Primary Genre",
            "Genres"
          ],
          [
            "Primary Genre"
          ],
          [
            "Primary Genre"
          ]
        ]
      },
      "output_cardinality": {
        "value": "plot",
        "confidence": 1.0,
        "votes": [
          "plot",
          "plot",
          "plot"
        ]
      },
      "sub_questions": {
        "value": [
          "What are the four major genres within Mobile Strategy Games?",
          "How many games belong to each of these four genres?",
          "What proportion does each genre represent in the dataset?",
          "How to visualize these proportions as a pie chart with specific colors and dimensions?",
          "What are the four major genres in the dataset?",
          "How many games belong to each of the four major genres?",
          "What are the proportions of each genre relative to the total?",
          "How to create a pie chart with specified dimensions (12, 8)?",
          "How to apply colors Green, Orange, Blue, Red to pie slices?",
          "How to include genre names in legend and save as result.png?"
        ],
        "confidence": 0.33333333333333337,
        "votes": [
          [
            "What are the four major genres within Mobile Strategy Games?",
            "How many games belong to each of these four genres?",
            "What proportion does each genre represent in the dataset?",
            "How to visualize these proportions as a pie chart with specific colors and dimensions?"
          ],
          [
            "What are the four major genres in the dataset?",
            "How many games belong to each of the four major genres?",
            "What are the proportions of each genre relative to the total?",
            "How to create a pie chart with specified dimensions (12, 8)?",
            "How to apply colors Green, Orange, Blue, Red to pie slices?",
            "How to include genre names in legend and save as result.png?"
          ],
          []
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "appstore_games.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "appstore_games.csv"
          ],
          [
            "appstore_games.csv"
          ],
          [
            "appstore_games.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "average user rating": "stars (0-5)",
          "user rating count": "number of ratings",
          "price": "USD",
          "size": "bytes",
          "original release date": "DD/MM/YYYY",
          "current version release date": "DD/MM/YYYY"
        },
        "confidence": 0.7222222222222222,
        "votes": [
          {
            "Average User Rating": "stars (0-5)",
            "User Rating Count": "number of ratings",
            "Price": "USD",
            "Size": "bytes",
            "Original Release Date": "DD/MM/YYYY",
            "Current Version Release Date": "DD/MM/YYYY"
          },
          {
            "Size": "bytes",
            "Price": "USD",
            "User Rating Count": "count",
            "Average User Rating": "rating_scale_0_5"
          },
          {
            "Size": "bytes",
            "Price": "USD",
            "User Rating Count": "number of ratings"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "Date columns use DD/MM/YYYY format which may need conversion for analysis",
          "Size in bytes may need conversion to MB/GB for readability",
          "Size is in bytes, may need to convert to MB or GB for better readability."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Date columns use DD/MM/YYYY format which may need conversion for analysis",
            "Size in bytes may need conversion to MB/GB for readability"
          ],
          [],
          [
            "Size is in bytes, may need to convert to MB or GB for better readability."
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 18.0,
        "confidence": 1.0,
        "votes": [
          18.0,
          18.0,
          18.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Dataset contains 17007 rows and 18 columns",
          "Primary Genre column exists with 'Games' as a value",
          "Genres column contains comma-separated genre lists",
          "Must identify four major genres from Strategy games subset",
          "Must identify exactly four major genres from the dataset",
          "Must count games for each of the four genres",
          "Pie chart must have size (12, 8)",
          "Colors must be Green, Orange, Blue, Red in that order",
          "Genre names must appear in legend",
          "Output must be saved as result.png",
          "The pie chart should be sized (12, 8).",
          "The pie chart colors should be Green, Orange, Blue, and Red.",
          "The pie chart legend should include genre names.",
          "The pie chart should be saved as result.png."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Dataset contains 17007 rows and 18 columns",
            "Primary Genre column exists with 'Games' as a value",
            "Genres column contains comma-separated genre lists",
            "Must identify four major genres from Strategy games subset"
          ],
          [
            "Must identify exactly four major genres from the dataset",
            "Must count games for each of the four genres",
            "Pie chart must have size (12, 8)",
            "Colors must be Green, Orange, Blue, Red in that order",
            "Genre names must appear in legend",
            "Output must be saved as result.png"
          ],
          [
            "The pie chart should be sized (12, 8).",
            "The pie chart colors should be Green, Orange, Blue, and Red.",
            "The pie chart legend should include genre names.",
            "The pie chart should be saved as result.png."
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Filter rows where Primary Genre = 'Games'",
          "Filter rows where Genres contains 'Strategy'",
          "Extract four major genres from the filtered Strategy games",
          "Filter to include only the four major genres (most frequent in Primary Genre column)",
          "Exclude games that do not belong to the four major genres"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Filter rows where Primary Genre = 'Games'",
            "Filter rows where Genres contains 'Strategy'",
            "Extract four major genres from the filtered Strategy games"
          ],
          [
            "Filter to include only the four major genres (most frequent in Primary Genre column)",
            "Exclude games that do not belong to the four major genres"
          ],
          []
        ]
      },
      "statistical_tests": {
        "value": [
          "Count distribution of games across four major genres",
          "Proportion calculation for pie chart",
          "Check for missing values in genre columns"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Count distribution of games across four major genres",
            "Proportion calculation for pie chart",
            "Check for missing values in genre columns"
          ],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Pie chart with dimensions (12, 8)",
          "Colors: Green, Orange, Blue, Red",
          "Include legend with genre names",
          "Save as result.png",
          "Generate pie chart visualization",
          "Chart size: (12, 8)",
          "Color scheme: ['Green', 'Orange', 'Blue', 'Red']",
          "Save output as 'result.png'",
          "Show proportions/percentages in pie chart",
          "Pie chart with specified size, colors, and legend.",
          "Image saved as result.png"
        ],
        "confidence": 0.36363636363636365,
        "votes": [
          [
            "Pie chart with dimensions (12, 8)",
            "Colors: Green, Orange, Blue, Red",
            "Include legend with genre names",
            "Save as result.png"
          ],
          [
            "Generate pie chart visualization",
            "Chart size: (12, 8)",
            "Color scheme: ['Green', 'Orange', 'Blue', 'Red']",
            "Include legend with genre names",
            "Save output as 'result.png'",
            "Show proportions/percentages in pie chart"
          ],
          [
            "Pie chart with specified size, colors, and legend.",
            "Image saved as result.png"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.590959595959596
  },
  "plot-pie-008": {
    "m_q": {
      "target_metric": {
        "value": "largest average delivery distance for bikers by hub city",
        "confidence": 0.3333333333333333,
        "votes": [
          "largest average delivery distance for bikers by hub city",
          "Hub city with largest biker average delivery distance, and distribution of order deliveries by driver modes in that city",
          "Average delivery distance for biker deliveries, grouped by hub city, and the distribution of order deliveries by driver modes in the hub city with the largest biker average delivery distance."
        ]
      },
      "filters": {
        "value": [
          "driver_modal = 'BIKER'",
          "delivery_status = 'DELIVERED'",
          "driver_id IS NOT NULL",
          "driver_modal == 'BIKER'",
          "delivery_status == 'DELIVERED'",
          "hub_city == <city_with_max_biker_avg_distance>"
        ],
        "confidence": 0.4444444444444444,
        "votes": [
          [
            "driver_modal = 'BIKER'",
            "delivery_status = 'DELIVERED'",
            "driver_id IS NOT NULL"
          ],
          [
            "driver_modal == 'BIKER'",
            "delivery_status == 'DELIVERED'",
            "hub_city == <city_with_max_biker_avg_distance>"
          ],
          [
            "delivery_status = 'DELIVERED'",
            "driver_modal = 'BIKER'"
          ]
        ]
      },
      "grouping_variables": {
        "value": [
          "hub_city",
          "driver_modal"
        ],
        "confidence": 0.6666666666666666,
        "votes": [
          [
            "hub_city"
          ],
          [
            "hub_city",
            "driver_modal"
          ],
          [
            "hub_city"
          ]
        ]
      },
      "output_cardinality": {
        "value": "plot",
        "confidence": 1.0,
        "votes": [
          "plot",
          "plot",
          "plot"
        ]
      },
      "sub_questions": {
        "value": [
          "Which hub city has the highest average delivery distance for bikers?",
          "What is the distribution of order deliveries by driver modes in that city?",
          "What is the average delivery distance for BIKER drivers in each hub city?",
          "Which hub city has the largest average delivery distance for BIKER drivers?",
          "What is the distribution of deliveries by driver_modal (all modes) in that specific hub city?",
          "Generate a pie chart showing the distribution and save to result.jpg",
          "What is the average delivery distance for biker deliveries in each hub city?",
          "Which hub city has the largest average delivery distance for biker deliveries?",
          "What is the distribution of order deliveries by driver modes in the hub city with the largest biker average delivery distance?"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Which hub city has the highest average delivery distance for bikers?",
            "What is the distribution of order deliveries by driver modes in that city?"
          ],
          [
            "What is the average delivery distance for BIKER drivers in each hub city?",
            "Which hub city has the largest average delivery distance for BIKER drivers?",
            "What is the distribution of deliveries by driver_modal (all modes) in that specific hub city?",
            "Generate a pie chart showing the distribution and save to result.jpg"
          ],
          [
            "What is the average delivery distance for biker deliveries in each hub city?",
            "Which hub city has the largest average delivery distance for biker deliveries?",
            "What is the distribution of order deliveries by driver modes in the hub city with the largest biker average delivery distance?"
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "deliveries.csv",
          "drivers.csv",
          "orders.csv",
          "stores.csv",
          "hubs.csv"
        ],
        "confidence": 1.0,
        "votes": [
          [
            "deliveries.csv",
            "drivers.csv",
            "orders.csv",
            "stores.csv",
            "hubs.csv"
          ],
          [
            "deliveries.csv",
            "drivers.csv",
            "orders.csv",
            "stores.csv",
            "hubs.csv"
          ],
          [
            "deliveries.csv",
            "drivers.csv",
            "orders.csv",
            "stores.csv",
            "hubs.csv"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "deliveries.driver_id is float64 while drivers.driver_id is int64",
          "deliveries.delivery_order_id is int64 while orders.delivery_order_id is int64 (consistent)"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "deliveries.driver_id is float64 while drivers.driver_id is int64",
            "deliveries.delivery_order_id is int64 while orders.delivery_order_id is int64 (consistent)"
          ],
          [],
          []
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "delivery_distance_meters": "meters",
          "order_amount": "currency",
          "order_delivery_fee": "currency",
          "order_delivery_cost": "currency",
          "store_plan_price": "currency",
          "hub_latitude": "degrees",
          "hub_longitude": "degrees",
          "store_latitude": "degrees",
          "store_longitude": "degrees"
        },
        "confidence": 0.5555555555555556,
        "votes": [
          {
            "delivery_distance_meters": "meters",
            "order_amount": "currency",
            "order_delivery_fee": "currency",
            "order_delivery_cost": "currency",
            "store_plan_price": "currency",
            "hub_latitude": "degrees",
            "hub_longitude": "degrees",
            "store_latitude": "degrees",
            "store_longitude": "degrees"
          },
          {
            "delivery_distance_meters": "meters",
            "hub_latitude": "degrees",
            "hub_longitude": "degrees",
            "store_latitude": "degrees",
            "store_longitude": "degrees"
          },
          {
            "delivery_distance_meters": "meters"
          }
        ]
      },
      "scale_issues": {
        "value": [
          "delivery_distance_meters has decimal values suggesting precision to 1 meter",
          "order_amount has 2 decimal places suggesting currency",
          "delivery_distance_meters is in meters, may need conversion for visualization",
          "delivery_distance_meters is in meters, might need conversion to km"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "delivery_distance_meters has decimal values suggesting precision to 1 meter",
            "order_amount has 2 decimal places suggesting currency"
          ],
          [
            "delivery_distance_meters is in meters, may need conversion for visualization"
          ],
          [
            "delivery_distance_meters is in meters, might need conversion to km"
          ]
        ]
      },
      "cross_source_conflicts": {
        "value": [
          "Some stores have missing latitude/longitude values while hubs have complete coordinates"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Some stores have missing latitude/longitude values while hubs have complete coordinates"
          ],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          "",
          "null",
          "NaN"
        ],
        "confidence": 0.7333333333333334,
        "votes": [
          [
            "NA",
            "N/A",
            "",
            "null"
          ],
          [
            "NA",
            "N/A",
            "",
            "NaN"
          ],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 10.0,
        "confidence": 0.6363636363636364,
        "votes": [
          10.0,
          0.0,
          6.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "delivery_distance_meters > 0 for delivered orders",
          "driver_id must be integer for valid deliveries",
          "hub_city must not be null",
          "Only include deliveries with delivery_status == 'DELIVERED'",
          "Only include records where driver_id is not null",
          "Filter for driver_modal == 'BIKER' when calculating average delivery distance by city",
          "Include all driver_modal values for the pie chart distribution in the identified city",
          "delivery_distance_meters should be non-negative",
          "driver_id in deliveries.csv can be null"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "delivery_distance_meters > 0 for delivered orders",
            "driver_id must be integer for valid deliveries",
            "hub_city must not be null"
          ],
          [
            "Only include deliveries with delivery_status == 'DELIVERED'",
            "Only include records where driver_id is not null",
            "Filter for driver_modal == 'BIKER' when calculating average delivery distance by city",
            "Include all driver_modal values for the pie chart distribution in the identified city"
          ],
          [
            "delivery_distance_meters should be non-negative",
            "driver_id in deliveries.csv can be null"
          ]
        ]
      },
      "derived_filters": {
        "value": [
          "Exclude cancelled deliveries from distance calculations",
          "Only include biker deliveries for the main metric",
          "Filter to completed/delivered orders only",
          "Identify hub_city with maximum average(delivery_distance_meters) where driver_modal == 'BIKER'",
          "Filter all deliveries to only that hub_city for pie chart generation",
          "Filter out cancelled deliveries (delivery_status != 'DELIVERED')",
          "Filter for biker deliveries (driver_modal = 'BIKER')"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Exclude cancelled deliveries from distance calculations",
            "Only include biker deliveries for the main metric",
            "Filter to completed/delivered orders only"
          ],
          [
            "Identify hub_city with maximum average(delivery_distance_meters) where driver_modal == 'BIKER'",
            "Filter all deliveries to only that hub_city for pie chart generation"
          ],
          [
            "Filter out cancelled deliveries (delivery_status != 'DELIVERED')",
            "Filter for biker deliveries (driver_modal = 'BIKER')"
          ]
        ]
      },
      "statistical_tests": {
        "value": [
          "Check for outliers in delivery_distance_meters",
          "Verify distribution of driver modes across cities",
          "Test correlation between delivery distance and city size",
          "Calculate average delivery_distance_meters grouped by hub_city for BIKER drivers",
          "Count deliveries grouped by driver_modal for the identified hub city"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Check for outliers in delivery_distance_meters",
            "Verify distribution of driver modes across cities",
            "Test correlation between delivery distance and city size"
          ],
          [
            "Calculate average delivery_distance_meters grouped by hub_city for BIKER drivers",
            "Count deliveries grouped by driver_modal for the identified hub city"
          ],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Pie chart saved as result.jpg",
          "Chart must follow plot.yaml guidelines",
          "City name must be clearly labeled in output",
          "Generate pie chart following specifications in plot.yaml",
          "Save chart as result.jpg",
          "Pie chart should show distribution of order deliveries by driver_modal",
          "Chart must be for the hub city with largest biker average delivery distance",
          "Pie chart showing the distribution of order deliveries by driver modes in the city with the largest biker average delivery distance.",
          "Save the chart in result.jpg"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [
            "Pie chart saved as result.jpg",
            "Chart must follow plot.yaml guidelines",
            "City name must be clearly labeled in output"
          ],
          [
            "Generate pie chart following specifications in plot.yaml",
            "Save chart as result.jpg",
            "Pie chart should show distribution of order deliveries by driver_modal",
            "Chart must be for the hub city with largest biker average delivery distance"
          ],
          [
            "Pie chart showing the distribution of order deliveries by driver modes in the city with the largest biker average delivery distance.",
            "Save the chart in result.jpg"
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.6018181818181819
  },
  "plot-scatter-002": {
    "m_q": {
      "target_metric": {
        "value": "Average days per order stage for the top 10 cities by sales",
        "confidence": 0.3333333333333333,
        "votes": [
          "",
          "",
          "Average days per order stage for the top 10 cities by sales"
        ]
      },
      "filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "grouping_variables": {
        "value": [
          "city",
          "order stage"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "city",
            "order stage"
          ]
        ]
      },
      "output_cardinality": {
        "value": "plot",
        "confidence": 0.3333333333333333,
        "votes": [
          "",
          "",
          "plot"
        ]
      },
      "sub_questions": {
        "value": [
          "Identify relevant columns for order stages and their timestamps.",
          "Calculate the time difference between order stages for each order.",
          "Calculate total sales for each city.",
          "Identify the top 10 cities by sales.",
          "Calculate the average time difference for each order stage for the top 10 cities."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "Identify relevant columns for order stages and their timestamps.",
            "Calculate the time difference between order stages for each order.",
            "Calculate total sales for each city.",
            "Identify the top 10 cities by sales.",
            "Calculate the average time difference for each order stage for the top 10 cities."
          ]
        ]
      }
    },
    "m_s": {
      "sources": {
        "value": [
          "closing_odds.csv.gz",
          "odds_series.csv.gz",
          "odds_series_b.csv.gz",
          "odds_series_matches.csv.gz",
          "odds_series_b_matches.csv.gz"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "closing_odds.csv.gz",
            "odds_series.csv.gz",
            "odds_series_b.csv.gz",
            "odds_series_matches.csv.gz",
            "odds_series_b_matches.csv.gz"
          ]
        ]
      },
      "schema_conflicts": {
        "value": [
          "Column 'match_date' exists in both 'odds_series.csv.gz' and 'odds_series_matches.csv.gz'.",
          "Column 'match_time' exists in both 'odds_series.csv.gz' and 'odds_series_matches.csv.gz'.",
          "Column 'score_home' and 'score_away' exist in both 'odds_series.csv.gz' and 'odds_series_matches.csv.gz', but 'odds_series_matches.csv.gz' also has a 'score' column. Need to determine which to use or how to combine.",
          "The 'league', 'home_team', 'away_team' columns are named differently in closing_odds.csv.gz"
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "Column 'match_date' exists in both 'odds_series.csv.gz' and 'odds_series_matches.csv.gz'.",
            "Column 'match_time' exists in both 'odds_series.csv.gz' and 'odds_series_matches.csv.gz'.",
            "Column 'score_home' and 'score_away' exist in both 'odds_series.csv.gz' and 'odds_series_matches.csv.gz', but 'odds_series_matches.csv.gz' also has a 'score' column. Need to determine which to use or how to combine.",
            "The 'league', 'home_team', 'away_team' columns are named differently in closing_odds.csv.gz"
          ]
        ]
      }
    },
    "m_u": {
      "unit_annotations": {
        "value": {
          "match_date": "date",
          "match_time": "time",
          "avg_odds_home_win": "odds",
          "avg_odds_draw": "odds",
          "avg_odds_away_win": "odds",
          "home_score": "goals",
          "away_score": "goals"
        },
        "confidence": 0.3333333333333333,
        "votes": [
          {},
          {},
          {
            "match_date": "date",
            "match_time": "time",
            "avg_odds_home_win": "odds",
            "avg_odds_draw": "odds",
            "avg_odds_away_win": "odds",
            "home_score": "goals",
            "away_score": "goals"
          }
        ]
      },
      "scale_issues": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "cross_source_conflicts": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      }
    },
    "m_f": {
      "has_header": {
        "value": true,
        "confidence": 1.0,
        "votes": [
          true,
          true,
          true
        ]
      },
      "delimiter": {
        "value": ",",
        "confidence": 1.0,
        "votes": [
          ",",
          ",",
          ","
        ]
      },
      "encoding": {
        "value": "utf-8",
        "confidence": 1.0,
        "votes": [
          "utf-8",
          "utf-8",
          "utf-8"
        ]
      },
      "sentinel_values": {
        "value": [
          "NA",
          "N/A",
          ""
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "NA",
            "N/A",
            ""
          ]
        ]
      },
      "expected_columns": {
        "value": 6917.0,
        "confidence": 0.33,
        "votes": [
          0.0,
          0.0,
          6917.0
        ]
      },
      "file_format": {
        "value": "csv",
        "confidence": 1.0,
        "votes": [
          "csv",
          "csv",
          "csv"
        ]
      }
    },
    "m_c": {
      "constraints": {
        "value": [
          "Need to define what constitutes an 'order stage' based on the available data.  This likely involves parsing the odds columns in odds_series.csv.gz and odds_series_b.csv.gz to determine when odds changed significantly.",
          "Need to determine how to handle missing or invalid odds data.",
          "Need to determine how to derive city information. It is not directly available in the provided datasets. It might be possible to derive it from the team names, but this would require an external data source mapping team names to cities."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "Need to define what constitutes an 'order stage' based on the available data.  This likely involves parsing the odds columns in odds_series.csv.gz and odds_series_b.csv.gz to determine when odds changed significantly.",
            "Need to determine how to handle missing or invalid odds data.",
            "Need to determine how to derive city information. It is not directly available in the provided datasets. It might be possible to derive it from the team names, but this would require an external data source mapping team names to cities."
          ]
        ]
      },
      "derived_filters": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "statistical_tests": {
        "value": [],
        "confidence": 0.0,
        "votes": [
          [],
          [],
          []
        ]
      },
      "output_format_requirements": {
        "value": [
          "Save the chart as 'result.png'.",
          "Use settings from 'plot.yaml'."
        ],
        "confidence": 0.3333333333333333,
        "votes": [
          [],
          [],
          [
            "Save the chart as 'result.png'.",
            "Use settings from 'plot.yaml'."
          ]
        ]
      }
    },
    "models_used": [
      "deepseek",
      "claude-sonnet",
      "gemini-flash"
    ],
    "overall_confidence": 0.3831666666666666
  }
}